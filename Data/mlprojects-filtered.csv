slug,username,active,description,end_on,location,modified,published,start_on,title,visibility,website,is_active,ML_GROUP,is_affiliate
refugee-learning-accelerator,barrons,False,"<h2>We support computer scientists and engineers from the Middle East to create technologies for refugee learners.&nbsp;</h2><p>The Refugee Learning Accelerator<span style=""font-size: 18px; font-weight: 400;"">&nbsp;provides training, mentorship, and funding to teams of computer scientists, engineers and designers.&nbsp;</span></p><p> </p><h2>Who can apply?&nbsp;</h2><p> </p><ul><li>University students and recent graduates with backgrounds in computer science, design, and electrical engineering,<br></li><li>coming from some of the places most impacted by the current refugee crisis: Jordan, Lebanon, Syria, Iraq, and Palestine.<br></li></ul><p> </p><h2>More information available <a href=""http://refugeelearning.media.mit.edu/"">here</a>.&nbsp;</h2>",2018-12-31,,2017-08-09 18:32:49.146,True,2017-05-01,Refugee Learning Accelerator,PUBLIC,,False,Initiatives,False
refugee-learning-accelerator,ps1,False,"<h2>We support computer scientists and engineers from the Middle East to create technologies for refugee learners.&nbsp;</h2><p>The Refugee Learning Accelerator<span style=""font-size: 18px; font-weight: 400;"">&nbsp;provides training, mentorship, and funding to teams of computer scientists, engineers and designers.&nbsp;</span></p><p> </p><h2>Who can apply?&nbsp;</h2><p> </p><ul><li>University students and recent graduates with backgrounds in computer science, design, and electrical engineering,<br></li><li>coming from some of the places most impacted by the current refugee crisis: Jordan, Lebanon, Syria, Iraq, and Palestine.<br></li></ul><p> </p><h2>More information available <a href=""http://refugeelearning.media.mit.edu/"">here</a>.&nbsp;</h2>",2018-12-31,,2017-08-09 18:32:49.146,True,2017-05-01,Refugee Learning Accelerator,PUBLIC,,False,Initiatives,False
askii,ps1,False,"<p>Askii is an SMS-based system that allows adult learners to study for a certification exam while on their commute. When learners have a spare five minutes, they can simply text Askii to begin their customized lessons. Askii will respond with a curated set of questions and links to content that learners can study on the go. We have begun building this prototype for learners to study for the US Naturalization Exam and plan to expand to other certification courses. Askii is a prototype within the larger Making Learning Work project.</p>",2015-09-01,--Choose Location,2016-12-05 00:16:13.398,True,2015-01-01,Askii,PUBLIC,,False,Initiatives,False
media-lab-virtual-visit,ps1,False,"<p>Media Lab Virtual Visit is intended to open up the doors of the Media Lab to people from all around the world. The visit is hosted on the <a href=""https://www.media.mit.edu/projects/unhangout/overview/"">Unhangout</a> platform, a new way of running large-scale unconferences on the web that was developed at the Media Lab. It is an opportunity for students or potential collaborators to talk with current researchers at the Lab, learn about their work, and share ideas. </p>",2016-08-01,--Choose Location,2018-06-20 19:58:01.122,True,2014-01-01,Media Lab Virtual Visit,PUBLIC,,False,Initiatives,False
making-learning-work,ps1,False,"<p>Improving adult learning, especially for adults who are unemployed or unable to financially support their families, is a challenge that affects the future wellbeing of millions of individuals in the US. We are working with the Joyce Foundation, employers, learning researchers, and the Media Lab community to prototype three to five new models for adult learning that involve technology innovation and behavioral insights. </p>",2016-06-01,--Choose Location,2016-12-05 00:16:23.585,True,2014-01-01,Making Learning Work,PUBLIC,,False,Initiatives,False
read-out-loud,ps1,False,"<p>Read Out Loud is an application that empowers adults learning English to turn almost any reading material into an experience to help them learn. Learners can take a picture of a page of text; the app then scans in the page and presents the learner with a host of additional tools to facilitate reading. They can read the text aloud, which helps learners who are more comfortable with spoken English understand what is written. They can also select words to translate them into their native language. With this prototype, we want to give adult learners more agency to learn from material that focuses on subjects they care about, as well as increase access to English language learning material. Any book from the public library could become learning material with support in their native language. Read Out Loud is a prototype within the larger Making Learning Work project.</p>",2016-06-01,--Choose Location,2016-12-05 00:17:21.549,True,2014-09-01,Read Out Loud,PUBLIC,,False,Initiatives,False
peer-2-peer-university,ps1,False,"<p><a href=""https://www.p2pu.org/en/"">Peer 2 Peer University (P2PU)</a> has developed ""learning circles,"" a model for facilitating in-person study groups at community libraries. Aimed at adult learners, learning circles take advantage of libraries as public community spaces for learning. We curate open, online courses and pair learners up with their peers to foster deeper, more meaningful adult basic educational experiences.</p>",,--Choose Location,2017-02-10 17:12:23.457,True,2015-01-01,Peer 2 Peer University,PUBLIC,https://www.p2pu.org,True,Initiatives,False
open-leadership-camp,ps1,False,"<p>The Open Leadership Camp (OLC) is a new type of professional development program for senior leaders of nonprofit and public sector organizations. It aims to &nbsp;<span style=""font-size: 18px; font-weight: 400;"">apply the principles of open source, open innovation, and the decentralized nature of the web to the way some of our most crucial social sector organizations work.&nbsp;</span></p><p>This project is a collaboration between <a href=""https://www.mozilla.org"">Mozilla</a> and the <a href=""https://www.media.mit.edu/groups/ml-learning/overview/"">ML Learning Initiative</a>, and is&nbsp;<span style=""font-size: 18px; font-weight: 400;"">hosted by MIT Media Lab Director <a href=""https://www.media.mit.edu/people/joi/overview/"">Joi Ito</a> and <a href=""https://www.mozilla.org/en-US/about/leadership/"">Mitchell Baker</a>, co-founder and Executive Chairwoman of the Mozilla Corporation. In March 2017, we brought together our first cohort of 14 participants, including the CEO of Consumer Reports, the CIO of the City of Detroit, and the CEO of WGBH.&nbsp;</span></p>",,,2017-06-19 14:42:03.293,True,2017-01-01,Open Leadership Camp,PUBLIC,,True,Initiatives,False
media-lab-digital-certificates,ps1,False,"<p class="""">Blockchain Certificates is a set of tools, software, and strategies to store and manage digital credentials. Certificates are registered on the bitcoin blockchain, cryptographically signed, and tamper-proof. They can represent or recognize many different types of achievements. After a number of prototypes (we issued digital credentials to Media Lab Director's Fellows and Media Lab alumni) we published our code under an open-source license to enable others to deploy the tools we developed. More information at&nbsp;<a href=""http://blockcerts.org"" class="""">http://blockcerts.org</a>.&nbsp;</p>",,--Choose Location,2019-01-31 17:23:57.299,True,2015-01-01,Digital Academic Credentials,PUBLIC,,True,Initiatives,False
public-library-innovation-exchange,ps1,False,"<p>Public libraries are one of most trusted public institutions in the US and increasingly provide a broad range of education services, ranging from early learning programs, to maker spaces, to adult training. Libraries are not storage places for books, but communities for social change and innovation. The Public Library Innovation Exchange connects Media Lab researchers with librarians to develop new creative learning programs together. We will host how-to materials that help public libraries deploy projects developed at the Media Lab, such as &nbsp;<a href=""http://www.chicagotribune.com/bluesky/series/gadgets/ct-innovative-gifts-bsi-photos-20151210-001-photo.html"">Circuit Stickers</a> or <a href=""https://www.youtube.com/watch?v=rfQqh7iCcOU"">Makey Makey</a>, and we will offer scholarships to support exchanges and residencies to foster collaborative research going forward. The project is funded by Knight Foundation.&nbsp;</p><p>Visit our project website to learn more: <a href=""http://plix.media.mit.edu"">plix.media.mit.edu</a></p>",,,2019-05-30 18:56:51.688,True,2017-03-15,Public Library Innovation Exchange,PUBLIC,https://plix.media.mit.edu,True,Initiatives,False
lego-wayfinder,ps1,False,"<p>The LEGO Wayfinder project combines LEGO, robotics, and seawater into a playground of project-based learning and citizen science for budding engineers and explorers. As part of this outreach program, our team has developed a first prototype of a buildable LEGO marine exploration vehicle kit—addressing some of the design challenges of building for the underwater context.</p><p>Our aim is to build an awareness of the state of the aquatic environment and instill a greater responsibility in shaping our interactions with the environment. To do so, young people will view underwater wonders of the world with their robots and get outside to explore their local waterway. Our approach embraces Seymour Papert’s model of ""low floors"" (where getting started is easy), and ""high ceilings,"" where students can pour their time and collaborative work efforts into creative engineering solutions to carry out a marine science experiment of their own design in the field.</p>",,,2019-04-22 17:15:27.304,True,2018-04-01,LEGO Wayfinder,PUBLIC,,True,Initiatives,False
crowdsourcing-a-manhunt,dsouza,False,"<p>People often say that we live in a small world. In a brilliant experiment, legendary social psychologist Stanley Milgram proved the six degrees of separation hypothesis: that everyone is six or fewer steps away, by way of introduction, from any other person in the world. But how far are we, in terms of time, from anyone on Earth? Our team won the Tag Challenge, a social gaming competition, showing it is possible to find a person, using only his or her mug shot, within 12 hours.</p>",2016-09-01,--Choose Location,2016-12-05 00:16:05.988,True,2015-01-01,Crowdsourcing a Manhunt,PUBLIC,,False,Scalable Cooperation,False
ethics-of-autonomous-vehicles,dsouza,False,"<p>Adoption of self-driving, Autonomous Vehicles (AVs) promises to dramatically reduce the number of traffic accidents, but some inevitable accidents will require AVs to choose the lesser of two evils, such as running over a pedestrian on the road or the sidewalk. Defining the algorithms to guide AVs confronted with such moral dilemmas is a challenge, and manufacturers and regulators will need psychologists to apply methods of experimental ethics to these situations.</p>",,--Choose Location,2018-05-01 19:43:34.342,True,2015-01-01,Ethics of Autonomous Vehicles,PUBLIC,,True,Scalable Cooperation,False
a-voting-based-system-for-ethical-decision-making,dsouza,False,"<p>The problem of ethical decision making presents&nbsp; a grand challenge for modern AI research. Arguably the main obstacle to automating ethical decisions is the lack of a formal specification of ground-truth ethical principles, which have been the subject of debate for centuries among philosophers (e.g., trolley problem).&nbsp;We present an algorithm to automate ethical decisions; using machine learning and computational social choice (new theory of&nbsp;swap-dominance efficient voting rules), we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1.3 million voters through the Moral Machine website.&nbsp;Our proof of concept shows that the decision the system takes is likely to be the same as if we could go to each of the 1.3 million voters, ask for their opinions, and then aggregate their opinions into a choice that satisfies mathematical notions of social justice.&nbsp;</p>",,,2019-04-19 17:41:51.688,True,2017-01-20,A voting-based system for ethical decision making,PUBLIC,,True,Scalable Cooperation,False
moral-machine,dsouza,False,"<p>The Moral Machine is a platform for gathering a human perspective on moral decisions made by machine intelligence, such as self-driving cars. We generate moral dilemmas, where a driverless car must choose the lesser of two evils, such as killing two passengers or five pedestrians. As an outside observer, people judge which outcome they think is more acceptable. They can then see how their responses compare with other people. If they are feeling creative, people can also design their own scenarios, for others to view, share, and discuss.</p><p>Visit the <a href=""http://moralmachine.mit.edu"">Moral Machine</a>.</p>",,--Choose Location,2018-09-26 19:22:25.166,True,2016-01-01,Moral Machine,PUBLIC,http://moralmachine.mit.edu,True,Scalable Cooperation,False
crowdsourcing-a-manhunt,irahwan,False,"<p>People often say that we live in a small world. In a brilliant experiment, legendary social psychologist Stanley Milgram proved the six degrees of separation hypothesis: that everyone is six or fewer steps away, by way of introduction, from any other person in the world. But how far are we, in terms of time, from anyone on Earth? Our team won the Tag Challenge, a social gaming competition, showing it is possible to find a person, using only his or her mug shot, within 12 hours.</p>",2016-09-01,--Choose Location,2016-12-05 00:16:05.988,True,2015-01-01,Crowdsourcing a Manhunt,PUBLIC,,False,Scalable Cooperation,False
deep-empathy,irahwan,False,"<h2><i>What would&nbsp;<b>your city</b>&nbsp;look like after a disaster?</i>&nbsp;</h2><p>Deep Empathy&nbsp;is&nbsp;a collaboration between the&nbsp;<a href=""http://www.media.mit.edu/groups/scalable-cooperation"">Scalable Cooperation</a>&nbsp;group and the&nbsp;<a href=""https://www.unicef.org/innovation"">UNICEF Innovation Office</a>&nbsp;to pursue a scalable way to increase empathy.&nbsp;</p><p>The brutal, six-year-old Syrian war has affected more than 13.5 million people in Syria , including 80% of the country's children—8.4 million young lives shattered by violence and fear. Hundreds of thousands of people have been displaced and their homes destroyed.&nbsp;</p><p>But people generate a response that statistics can't. And technologists—through tools like AI—have opportunities to help people see things differently. We wondered: ""Can AI increase empathy for victims of far-away disasters?"" This question led us to create a provocation for the research community to examine how AI can create narratives to tell the stories of some of the world's most intractable problems.</p>",2018-05-01,,2017-12-04 19:54:02.578,True,2017-12-01,Deep Empathy,PUBLIC,http://deepempathy.mit.edu/,False,Scalable Cooperation,False
becoming-someone,irahwan,False,"<p>Becoming Someone is a multi-modal concept that lives across Medium, Instagram, and human minds. It is comprised of The Ever Contracting Void, &nbsp;an Instagram-based container by Micah Epstein commissioned by Manuel Cebrian and Iyad Rahwan as the visual metaphor for the &nbsp;essay Becoming Someone. The Ever Contracting Void is a media container that exists within &nbsp;<a href=""https://www.instagram.com/directory.of.worlds/"">The Directory of Worlds</a>, an installment in Micah’s ongoing Instagram installation. The Directory is a series of accounts, called ""worlds,"" which serve as spaces for exploration and reprieve from Instagram’s visual overload and hyper-targeted marketing. Where his previous worlds were an opportunity, the Void is a threat. The Void contains nothing but the source code of popular social media websites, which use machine learning to drive the internet ever inwards upon the individual. Trapped in its center is an individual, whose very identity hinges on the viewer double-tapping the screen to like the image. What remains, when the Void closes upon the individual, leaving nothing but pristine white grids and information superhighways?</p><p>Read the articulation <a href=""https://medium.com/mit-media-lab/becoming-someone-54ed1798a1b7"">here</a> and explore the visual container on Instagram <a href=""http://instagram.com/ever.contracting.void"">here</a>.&nbsp;</p>",2018-04-26,,2018-04-23 17:54:11.668,True,2018-04-05,Becoming Someone,PUBLIC,https://medium.com/mit-media-lab/becoming-someone-54ed1798a1b7,False,Scalable Cooperation,False
nostalgia-box,irahwan,False,"<h1>Nostalgia Box</h1><h2>A deep learning visualization of your own memories</h2><h2>By <a href=""http://cs.wellesley.edu/~asimonso/nostalgiabox/"">Aubrey Simonson</a>&nbsp;<br>Commissioned by Manuel Cebrian and Iyad Rahwan</h2><p>Nostalgia Box is a continually shifting soup of memories. Images, curated for their nostalgic emotional impact, are dreamed over one another using a neural style machine learning algorithm, based on the paper ""A Neural Algorithm of Artistic Style"" by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. The resulting images are then overlaid into a video which never fully solidifies into any one image, but instead is always several at once. To someone who is familiar with the content of the images, they are still recognizable. However, a stranger should see only a haze of vague shapes which occasionally contains the suggestion of a face. This is machine learning<i>, </i>stripped of the elements of spam. It is an amalgamation of its creator's digital history, but which is being used as a tool for reflection, rather than as a means to more effectively market products.&nbsp;&nbsp;</p>",2018-12-31,,2018-05-07 22:25:44.225,True,2018-02-01,Nostalgia Box,PUBLIC,http://cs.wellesley.edu/~asimonso/nostalgiabox,False,Scalable Cooperation,False
black-rock-atlas,irahwan,False,"<p>Burning Man is a magical place that gets the best of human creativity and collaboration to flourish. To further understand what makes this magic happen, we are creating the first ever Black Rock Atlas–a map of the social patterns and networks that exist on the playa. </p><p>To do this, we are tracking the decentralized journey of a multitude of vessels through the gift economy of Burning Man with GPS technology and generative photography. The Atlas will explore new ways of community interaction, storytelling, and data visualization.</p>",2018-12-11,,2018-10-15 20:44:54.352,True,2018-08-15,Black Rock Atlas,PUBLIC,https://blackrockatlas.mit.edu,False,Scalable Cooperation,False
nightmare-machine,irahwan,False,"<p>For centuries, across geographies, religions, and cultures, people try to innovate ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity. This challenge is especially important in a time when we wonder what the limits of Artificial Intelligence are: Can machines learn to scare us? Towards this goal, we present you Haunted Faces and Haunted Places: computer generated scary imagery powered by deep learning algorithms!
                    
                </p>",,,2017-01-31 04:41:35.401,True,2016-10-20,Nightmare Machine,PUBLIC,http://nightmare.mit.edu/,True,Scalable Cooperation,False
global-cooperation,irahwan,False,<h1>Measuring Cooperation at Scale</h1>,,,2018-01-10 16:18:38.348,True,2015-09-01,Global Cooperation,PUBLIC,,True,Scalable Cooperation,False
honest-crowds,irahwan,False,"<p>The Honest Crowds project addresses shortcomings of traditional survey techniques in the modern information and big data age. Web survey platforms, such as Amazon's Mechanical Turk and CrowdFlower, bring together millions of surveys and millions of survey participants, which means paying a flat rate for each completed survey may lead to survey responses that lack desirable care and forethought. Rather than allowing survey takers to maximize their reward by completing as many surveys as possible, we demonstrate how strategic incentives can be used to actually reward information and honesty rather than just participation. The incentive structures that we propose provide scalable solutions for the new paradigm of survey and active data collection.</p>",,--Choose Location,2018-01-10 16:32:52.208,True,2015-09-01,Honest Crowds,PUBLIC,,True,Scalable Cooperation,False
human-machine-cooperation,irahwan,False,"<p>Since Alan Turing envisioned Artificial Intelligence (AI), a major driving force behind technical progress has been competition with human cognition (e.g. beating humans in Chess or Jeopardy!). Less attention has been given to developing autonomous machines that learn to cooperate with humans. Cooperation does not require sheer computational power, but relies on intuition, and pre-evolved dispositions toward cooperation, common-sense mechanisms that are difficult to encode in machines. We develop state-of-the-art machine-learning algorithms that cooperate with people and other machines at levels that rival human cooperation in two-player repeated games.</p><p>Scientific writings:&nbsp;<br><span style=""font-size: 18px; font-weight: normal;"">Jacob Crandall, Mayada Oudah, Tennom, Fatimah Ishowo-Oloko, Sherief Abdallah, Jean-François Bonnefon, Manuel Cebrian, Azim Shariff, Michael A. Goodrich, Iyad Rahwan. </span><a style=""font-size: 18px; font-weight: normal;"" href=""https://arxiv.org/abs/1703.06207"">Cooperating with Machines</a><span style=""font-size: 18px; font-weight: normal;"">.&nbsp;	arXiv:1703.06207</span></p>",,--Choose Location,2018-01-16 19:29:48.704,True,2015-01-01,Human-Machine Cooperation,PUBLIC,,True,Scalable Cooperation,False
ethics-of-autonomous-vehicles,irahwan,False,"<p>Adoption of self-driving, Autonomous Vehicles (AVs) promises to dramatically reduce the number of traffic accidents, but some inevitable accidents will require AVs to choose the lesser of two evils, such as running over a pedestrian on the road or the sidewalk. Defining the algorithms to guide AVs confronted with such moral dilemmas is a challenge, and manufacturers and regulators will need psychologists to apply methods of experimental ethics to these situations.</p>",,--Choose Location,2018-05-01 19:43:34.342,True,2015-01-01,Ethics of Autonomous Vehicles,PUBLIC,,True,Scalable Cooperation,False
towards-understanding-the-impact-of-ai-on-labor,irahwan,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-03-22 16:27:19.712,False,2018-03-01,Towards Understanding the Impact of AI on Labor,PUBLIC,http://www.media.mit.edu/~mrfrank,True,Scalable Cooperation,False
governance-artificial-intelligence-social-media,irahwan,False,"<p>Recent rapid advances in Artificial Intelligence (AI) and Machine Learning have raised many questions about the regulatory and governance mechanisms for autonomous machines. This is not about individual gadgets, but about complex, networked systems of humans and algorithms making decisions in business, government, and the media. We need conceptual frameworks for designing new governance architectures for these human-machine social systems. In doing so, it is helpful to learn lessons about human cooperation and governance from political philosophy and cultural anthropology. Read more <a href=""https://arxiv.org/abs/1707.07232"">here</a>.</p>",,,2017-07-30 19:19:31.110,True,2017-05-01,Society-in-the-Loop,PUBLIC,,True,Scalable Cooperation,False
shelley,irahwan,False,"<h2>Project website:&nbsp;<a href=""http://shelley.ai"">shelley.ai&nbsp;<br></a>Human-AI collaborated stories:&nbsp;<a href=""http://stories.shelley.ai"">stories.shelley.ai&nbsp;<br></a>Follow&nbsp;<a href=""http://twitter.com/shelley_ai"">@shelley_ai</a> to collaborate with Shelley!&nbsp;</h2><br><p>For centuries, across geographies, religions, and cultures, people have innovated ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity.&nbsp;This challenge is especially important at a time when we are exploring the limits of artificial intelligence: Can machines learn to scare us?&nbsp;</p><p>In Halloween 2016 we presented the&nbsp;<a href=""http://nightmare.mit.edu/"">Nightmare Machine</a>—computer-generated scary imagery powered by deep learning algorithms.&nbsp;</p><p>This Halloween, we present <b>Shelley:&nbsp;Human-AI Collaborated Horror Stories</b>!&nbsp;</p><p>Shelley is a deep-learning powered AI who was raised reading eerie stories coming from&nbsp;<a href=""http://reddit.com/r/nosleep"">r/nosleep</a>. Now, as an adult—and not unlike Mary Shelley, her Victorian idol—she takes a bit of inspiration in the form of a random seed, or a short snippet of text, and starts creating stories emanating from her creepy creative mind. But what Shelley truly enjoys is working collaboratively with humans, learning from their nightmarish ideas, creating the best scary tales ever. If you want to work with her, respond to the stories she'll start every hour on her Twitter <a href=""http://twitter.com/shelley_ai"">account</a>, and she will write with you the first AI-human horror anthology ever put together!</p>",,,2017-12-12 21:51:49.213,True,2017-10-15,Shelley: Human-AI Collaborated Horror Stories,PUBLIC,http://shelley.ai,True,Scalable Cooperation,False
turingbox,irahwan,False,"<p>TuringBox is a platform&nbsp;that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms.&nbsp;It is a two-sided marketplace. On one side, <i>AI contributors</i> upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">On the other side, </span><i style=""font-size: 18px; font-weight: 400;"">AI examiners</i><span style=""font-size: 18px; font-weight: 400;"">&nbsp;develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.</span></p>",,,2018-04-05 19:00:33.916,True,2018-03-21,TuringBox: Democratizing the study of AI,PUBLIC,http://turingbox.mit.edu,True,Scalable Cooperation,False
a-voting-based-system-for-ethical-decision-making,irahwan,False,"<p>The problem of ethical decision making presents&nbsp; a grand challenge for modern AI research. Arguably the main obstacle to automating ethical decisions is the lack of a formal specification of ground-truth ethical principles, which have been the subject of debate for centuries among philosophers (e.g., trolley problem).&nbsp;We present an algorithm to automate ethical decisions; using machine learning and computational social choice (new theory of&nbsp;swap-dominance efficient voting rules), we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1.3 million voters through the Moral Machine website.&nbsp;Our proof of concept shows that the decision the system takes is likely to be the same as if we could go to each of the 1.3 million voters, ask for their opinions, and then aggregate their opinions into a choice that satisfies mathematical notions of social justice.&nbsp;</p>",,,2019-04-19 17:41:51.688,True,2017-01-20,A voting-based system for ethical decision making,PUBLIC,,True,Scalable Cooperation,False
crowdsourcing-under-attack,irahwan,False,"<p>The Internet has unleashed the capacity for planetary-scale collective problem solving (also known as crowdsourcing). However, the very openness of crowdsourcing makes it vulnerable to sabotage by rogue or competitive actors. To explore the effect of errors and sabotage on the performance of crowdsourcing, we analyze data from the DARPA Shredder Challenge, a prize competition for exploring methods to reconstruct documents shredded by a variety of paper shredding techniques.</p>",,--Choose Location,2019-04-19 17:43:17.995,True,2014-01-01,DARPA Shredder Challenge: Crowdsourcing under attack,PUBLIC,,True,Scalable Cooperation,False
promoting-cooperation-through-peer-pressure,irahwan,False,"<p>Cooperation in a large society of self-interested individuals is notoriously difficult to achieve when the externality of one individual's action is spread thin and wide on the whole society (e.g., in the case of pollution). We introduce a new approach to achieving global cooperation by localizing externalities to one's peers in a social network, thus leveraging the power of peer-pressure to regulate behavior. Global cooperation becomes more like local cooperation.</p>",,--Choose Location,2019-04-19 17:44:53.911,True,2015-01-01,Promoting cooperation through peer pressure,PUBLIC,,True,Scalable Cooperation,False
ai-ethics-and-governance,irahwan,False,"<p>This project will support social scientists, philosophers, and policy and legal scholars who undertake research that aims to impact how artificial intelligence technologies are designed, implemented, understood, and held accountable. It will also provide a platform to create, convene, and support a diverse and powerful network of people and institutions who are working to steer AI in ethically conscious directions, both in fields of specialized AI as well as general AI. &nbsp;The project will investigate the social implications of the maturation and proliferation of AI. It will help catalyze and support research that advances AI in the public interest and fund engineers who want to help define public interest in AI through the code they write and machines they build. </p><p>	The initiative also organized a high-level symposium at the Media Lab on the topic that took place in April 2016 between the academic community and industry leaders working on AI.</p>",,,2018-02-21 21:42:35.499,True,2016-04-01,AI Ethics and Governance,PUBLIC,,True,Scalable Cooperation,False
deepmoji,irahwan,False,"<p><i>Emotional content is an important part of language. There are many use cases now showing that natural language processing is becoming an increasingly important part of consumer products.&nbsp;We are attempting to learn more about human emotions.</i></p><p>In his 2006 book <i>The Emotion Machine</i>, legendary computer scientist Marvin Minsky (co-founder of the field of Artificial Intelligence and one of the founding faculty members of the MIT Media Lab) wrote about the central role of emotions in reasoning—reminding us that AI will only be capable of true commonsense reasoning once it has understood emotions. To Minsky, emotions are not the opposite of rational reason, something to be weeded out before we can think clearly; rather, emotions are just a different way of thinking.</p><p><b><a href=""http://deepmoji.mit.edu/"">TRY DEEPMOJI</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href=""https://deepmoji.mit.edu/contribute/"">HELP TEACH OUR AI ABOUT EMOTIONS</a></b><br></p><p>But this is hardly helpful to a computer scientist trying to construct an emotional machine by programming a concrete set of rules. If you ask two people to explain what makes a particular sentence happy, sad, serious, or sarcastic, you will likely get at least two different opinions. Much of what determines emotional content is context-specific, culturally constructed, and difficult to describe in an explicit set of rules.<br></p>",,,2019-05-29 13:10:23.998,True,2017-08-02,DeepMoji,PUBLIC,http://deepmoji.mit.edu,True,Scalable Cooperation,False
cognitive-limits-of-social-networks,irahwan,False,"<p>There is a wide cultural belief in the power of the Internet and social media as enablers of collective intelligence. They help us spread information rapidly, and learn useful information from each other. But there are fundamental limits to the capabilities of those networks. Understanding these limits is essential to improving social media and allowing society to make the most of it.</p>",,--Choose Location,2017-03-22 21:10:42.720,True,2015-01-01,Cognitive Limits of Social Networks,PUBLIC,,True,Scalable Cooperation,False
the-science-of-ai-research,irahwan,False,"<p>We must proactively tackle the economic, social, and societal implications that accompany the widespread deployment of AI technology. In service to this goal, examining the evolution of AI research itself could provide a valuable input into models of AI's impact (e.g., models of the future of work).&nbsp;</p>",,,2019-03-13 17:23:51.009,True,2018-05-01,The Science of AI Research,PUBLIC,,True,Scalable Cooperation,False
evolution-of-the-social-contract,irahwan,False,"<p>Political constitutions describe the fundamental principles by which nation-states are governed, the political and legal state institutions, the powers, procedures, and duties of those institutions, and the rights and responsibilities of individuals. How do these constitutions develop over long periods of time? What is the interplay between colonial history and global, time varying trends in determining the characteristics of a country's constitution? We explore these questions using new techniques of computational social science.</p>",,,2017-12-11 21:03:28.998,True,2016-07-01,Evolution of the Social Contract,PUBLIC,http://www.alexrutherford.org/constitutionology/,True,Scalable Cooperation,False
moral-machine,irahwan,False,"<p>The Moral Machine is a platform for gathering a human perspective on moral decisions made by machine intelligence, such as self-driving cars. We generate moral dilemmas, where a driverless car must choose the lesser of two evils, such as killing two passengers or five pedestrians. As an outside observer, people judge which outcome they think is more acceptable. They can then see how their responses compare with other people. If they are feeling creative, people can also design their own scenarios, for others to view, share, and discuss.</p><p>Visit the <a href=""http://moralmachine.mit.edu"">Moral Machine</a>.</p>",,--Choose Location,2018-09-26 19:22:25.166,True,2016-01-01,Moral Machine,PUBLIC,http://moralmachine.mit.edu,True,Scalable Cooperation,False
norman,irahwan,False,"<p>We present <a href=""http://norman-ai.mit.edu"">Norman</a>, world's first psychopath AI. Norman was inspired by the fact that the data used to teach a machine learning algorithm can significantly influence its behavior. So when people say that AI algorithms can be biased and unfair, the culprit is often not the algorithm itself, but the biased data that was fed to it. The same method can see very different things in an image, even ""sick"" things, if trained on the wrong (or, the right!) data set. Norman suffered from extended exposure to the darkest corners of Reddit, and represents a case study on the dangers of artificial intelligence gone wrong when biased data is used in machine learning algorithms.&nbsp;</p><p>Norman is an AI that is trained to perform image captioning; a popular deep learning method of generating a&nbsp;textual description of an image. We trained Norman on image captions from an infamous subreddit (its name is redacted due to its graphic content) that is dedicated to documenting and observing the disturbing reality of death. Then, we compared Norman's responses with a standard image-captioning neural network (trained on&nbsp;<a href=""http://mscoco.org/"">MSCOCO</a>&nbsp;dataset) on <a href=""https://en.wikipedia.org/wiki/Rorschach_test"">Rorschach inkblots</a>–a test that is used to detect underlying thought disorders.</p><p>Visit <a href=""http://norman-ai.mit.edu"">norman-ai.mit.edu</a> to explore what Norman sees!<br></p>",,,2019-02-14 19:39:41.066,True,2018-04-01,Norman,PUBLIC,,True,Scalable Cooperation,False
deep-angel-ai,irahwan,False,"<p><b>Deep Angel&nbsp;</b>is an artificial intelligence that erases objects from photographs. The algorithm is hosted on <a href=""http://deepangel.media.mit.edu"">http://deepangel.media.mit.edu</a>, which enables anyone&nbsp;to interact with the AI and explore what it can disappear.</p><p>Part philosophy, part technology, and part art, Deep Angel is designed to spark a series of conversations on technology in our daily lives and AI and media manipulation.&nbsp;&nbsp;</p><p>Deep Angel draws from&nbsp; Walter Benjamin's description of Paul Klee's Angelus Novus, the angel of history who has clairvoyance into the dark side of what appears to be progress. The angel sees the unravelling of all that matters in the world and would like to alert the world about his vision, but he's caught in the storm of progress and can't communicate any messages. The images that Deep Angel generates are intended to deliver the message that Angelus Novus would have sent if he could.&nbsp;</p><p>The algorithm applies computer vision techniques to automatically (1) detect and outline objects in images, (2) remove the outlined object from the image, and (3) imagine what the image would look like if that outlined object were removed from the image. Any image uploaded and transformed by Deep Angel can be published on the Deep Angel website by clicking the ""Publish to Deep Angel"" button.&nbsp;</p><p>The AI's performance varies across photographs. Sometimes, it's impossible to tell what has been disappeared. Other times, the images appear similar to the images from Adrian Piper's <i>Everything</i> series. The more people interact with the algorithm, the more attuned people will be to the potential and limitations of modern AI to manipulate the media. It's now possible to automate the vanishing commissar in Soviet photography, but the AI is not yet perfect. Below are two examples of the Deep Angel AI effect: (1) a gif generated by Deep Angel showing a father and daughter disappearing in the wilderness and (2) two images showing the before and after of Deep Angel peering into a photo of a professional surfer.&nbsp;</p>",,,2019-02-14 19:46:25.924,True,2018-08-06,Deep Angel: The AI behind the aesthetics of absence,PUBLIC,http://deepangel.media.mit.edu/,True,Scalable Cooperation,False
mygoodness,irahwan,False,"<p>There are over one million registered charities in the United States alone, and many more worldwide. How do you choose among them?&nbsp;<br></p><p>MyGoodness is a simple game that helps you understand how you give. In the game, you will make 10 giving decisions. Each decision is between two choices, and you tell us which you prefer.</p><p>At the end of the game, we give you a summary of your ‘goodness’ and how it&nbsp;compares to others. You can share that feedback with whomever you would&nbsp;like.</p>",,,2017-12-15 23:10:00.218,True,2017-12-11,MyGoodness,PUBLIC,,True,Scalable Cooperation,False
identifying-the-human-impacts-of-climate-change,irahwan,False,<p>Climate change is going to alter the environments that we depend on in myriad ways. We're using data to identify and quantify these potential human impacts.&nbsp;</p>,,,2019-04-19 17:44:02.613,True,2016-07-01,Identifying the human impacts of climate change,PUBLIC,,True,Scalable Cooperation,False
future-of-work-ai-automation-labor,irahwan,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-04-03 14:42:46.705,True,2017-06-06,"AI, Automation, Labor, and Cities: How to map the future of work",PUBLIC,,True,Scalable Cooperation,False
machine-behavior,irahwan,False,"<p>Machines powered by artificial intelligence (AI) increasingly                           mediate our social, cultural, economic, and                           political interactions. Understanding the                           behavior of AI systems is essential to our                           ability to control their actions, reap their                           benefits, and minimize their harms. We argue                           this necessitates a broad scientific research                           agenda to study machine behavior that                           incorporates but expands beyond the discipline                           of computer science and requires insights from                           across the sciences. Here we first outline a                           set of questions fundamental to this emerging                           field. We then explore the technical, legal,                           and institutional constraints facing the study                           of machine behavior.</p>",,,2019-04-30 19:56:54.922,True,2019-04-24,Machine Behavior,PUBLIC,,True,Scalable Cooperation,False
tools-for-super-human-time-perception,slavin,False,"<p>Time perception is a fundamental component in our ability to build mental models of our world. Without accurate and precise time perception, we might have trouble understanding speech, fumble social interactions, have poor motor control, hallucinate, or remember events incorrectly. Slight distortions in time perception are commonplace and may lead to slight dyslexia, memory shifts, poor eye-hand coordination, and other relatively benign symptoms, but could a diminishing sense of time signal the onset of a serious brain disorder? Could time perception training help prevent or reverse brain disorders? This project is a series of experimental tools built to assist and increase human time perception. By approaching time-perception training from various perspectives, we hope to find a tool or collection of tools to increase time perception, and in turn discover what an increase in time perception might afford us.</p>",2015-07-01,--Choose Location,2016-12-05 00:16:06.546,True,2014-01-01,Tools for Super-Human Time Perception,PUBLIC,,False,Other,False
darkball,slavin,False,"<p>Cristiano Ronaldo can famously volley a corner kick in total darkness. The magic behind this remarkable feat is hidden in Ronaldo's brain, which enables him to use advance cues to plan upcoming actions. Darkball challenges your brain to do the same, distilling that scenario into its simplest form�intercept a ball in the dark. All you see is all you need.</p>",2015-07-01,--Choose Location,2016-12-05 00:16:20.623,True,2014-09-01,Darkball,PUBLIC,,False,Other,False
deepview-computational-tools-for-chess-spectatorship,slavin,False,"<p>Competitive chess is an exciting spectator sport. It is fast-paced, dynamic, and deeply psychological. Unfortunately, most of the game's drama is only visible to spectators who are themselves expert chess players. DeepView seeks to use computational tools to make the drama of high-level chess accessible to novice viewers. There is a long tradition of software trying to beat human players at chess; DeepView takes advantage of algorithmic tools created in the development of advanced chess engines such as Deep Blue, but instead uses them to understand and explain the styles of individual players and the dynamics of a given match. It puts into the hands of chess commentators powerful data science tools that can calculate player position preferences and likely game outcomes, helping commentators to better explain the exciting human story inside every match.</p>",2015-07-01,--Choose Location,2016-12-05 00:16:27.884,True,2014-01-01,DeepView: Computational Tools for Chess Spectatorship,PUBLIC,,False,Other,False
beneath-the-chip,slavin,False,"<p>Sculptural artifacts that model and reveal the embedded history of human thought and scientific principles hidden inside banal digital technologies. These artifacts provide alternative ways to engage and understand the deepest interior of our everyday devices, below the circuit, below the chip. They build a sense of the machines within the machine, the material, the grit of computation.</p>",2015-09-01,--Choose Location,2016-12-05 00:17:01.746,True,2014-01-01,beneath the chip,PUBLIC,,False,Other,False
amino-a-tamagotchi-for-synthetic-biology,slavin,False,"<p>Amino is a design-driven mini-lab that allows users to carry out a bacterial transformation and enables the subsequent care and feeding of the cells that are grown. Inspired by Tamagotchis, the genetic transformation of an organism's DNA is performed by the user through guided interactions, resulting in a synthetic organism that can be cared for like a pet. Amino is developed using low-cost ways of carrying out lab-like procedures in the home, and is packaged in a suitcase-sized continuous bioreactor for cells.</p>",2015-07-01,--Choose Location,2016-12-05 00:17:06.454,True,2014-09-01,Amino: A Tamagotchi for Synthetic Biology,PUBLIC,,False,Other,False
micropsi-an-architecture-for-motivated-cognition,slavin,False,"<p>The MicroPsi project explores broad models of cognition, built on a motivational system that gives rise to autonomous social and cognitive behaviors. MicroPsi agents are grounded AI agents, with neuro-symbolic representations, affect, top-down/bottom-up perception, and autonomous decision making. We are interested in finding out how motivation informs social interaction (cooperation and competition, communication and deception), learning, and playing; shapes personality; and influences perception and creative problem-solving.</p>",,--Choose Location,2016-12-05 00:16:37.978,True,2014-01-01,MicroPsi: An Architecture for Motivated Cognition,PUBLIC,,True,Other,False
for-once-in-your-life,slavin,False,"<p>""For Once In Your Life..."" is a site-specific interactive radio play that uses the various sensors in a smartphone to determine specific details, such as where the user walks within a space, to dynamically affect the story. It's a blend of experiential theatre, modern choice-based interactive fiction, and audio walks such as the work of Janet Cardiff.</p>",,--Choose Location,2016-12-05 00:17:12.536,True,2015-09-01,For Once In Your Life...,PUBLIC,,True,Other,False
radio_o,slavin,False,"<p>radiO_o is a battery-powered speaker worn by hundreds of party guests, turning each person into a local mobile sound system. The radiO_o broadcast system allows the DJ to transmit sounds over several pirate radio channels to mix sounds between hundreds of speakers roaming around the space and the venue's existing sound system.</p>",,--Choose Location,2016-12-05 00:17:21.437,True,2014-01-01,radiO_o,PUBLIC,,True,Other,False
homeostasis,slavin,False,"<p>A large-scale art installation that investigates the biological systems that represent and embody human life, and their relationship to the built environment. This synthetic organism, built from interconnected microbiological systems, will be sustained in part through its own feedback and feedforward loops, but also through interactions with the architectural systems (like HVAC). As the different systems react and exchange material inputs and outputs, they move towards homeostasis. In the process, Homeostasis creates a new landscape of the human body, in which we can experience the wonder and vulnerability of its interconnected systems.</p>",,--Choose Location,2016-12-05 00:17:13.990,True,2014-09-01,Homeostasis,PUBLIC,,True,Other,False
automatiles,slavin,False,"<p>A tabletop set of cellular automata ready to exhibit complex systems through simple behaviors, AutomaTiles explores emergent behavior through tangible objects. Individually they live as simple organisms, imbued with a simple personality; together they exhibit something ""other"" than the sum of their parts. Through communication with their neighbors, complex interactions arise. What will you discover with AutomaTiles?</p>",,--Choose Location,2017-01-05 22:03:10.469,True,2015-01-01,AutomaTiles,PUBLIC,,True,Other,False
soft-exchange-interaction-design-with-biological-interfaces,slavin,False,"<p>The boundaries and fabric of human experience are continuously redefined by microorganisms, interacting at an imperceptible scale. Though hidden, these systems condition our bodies, environment, and even sensibilities and desires. The proposed works introduce a model of interaction in which the microbiome is an extension of the human sensory system, accessed through a series of biological interfaces that enable exchange. Biological Interfaces transfer discrete behaviors of microbes into information across scales, where it may be manipulated, even if unseen. In the same way the field of HCI has articulated our exchanges with electronic signals, Soft Exchange opens up the question of how to design for this other invisible, though present, and vital material.</p>",,--Choose Location,2016-12-05 00:16:11.253,True,2014-09-01,Soft Exchange: Interaction Design with Biological Interfaces,PUBLIC,,True,Other,False
dice,slavin,False,"<p>Today, algorithms drive our cars, our economy, what we read, and how we play. Modern-day computer games utilize weighted probabilities to make games more competitive, fun, and addicting. In casinos, slot machines--once a product of simple probability--employ similar algorithms to keep players playing. Dice++ takes the seemingly straight probability of rolling a die and determines an outcome with algorithms of its own.</p>",,--Choose Location,2016-12-15 02:58:30.762,True,2014-09-01,Dice++,PUBLIC,,True,Other,False
circuit-storybook,slavin,False,"<p>An interactive picture book that explores storytelling techniques through paper-based circuitry. Sensors, lights, and microcontrollers embedded into the covers, spine, and pages of the book add electronic interactivity to the traditional physical picture book, allowing us to tell new stories in new ways. The current book, ""Ellie,"" tells the adventures of an LED light named Ellie who dreams of becoming a star, and of her journey up to the sky.</p>",,--Choose Location,2016-12-05 00:17:08.516,True,2015-01-01,Circuit Storybook,PUBLIC,,True,Other,False
cognitive-integration-the-nature-of-the-mind,slavin,False,"<p>While we have learned much about human behavior and neurobiology, there is arguably no field that studies the mind itself. We want to overcome the fragmentation of the cognitive sciences. We aim to create models and concepts that bridge between methodologies, and can support theory-driven research. Among the most interesting questions: How do our minds construct the dynamic simulation environment that we subjectively inhabit, and how can this be realized in a neural substrate? How can neuronal representations be compositional? What determines the experiential qualities of cognitive processes? What makes us human?</p>",,--Choose Location,2016-12-05 00:17:08.864,True,2016-01-01,Cognitive Integration: The Nature of the Mind,PUBLIC,,True,Other,False
gamr,slavin,False,"<p>Does how you play reflect who you really are? The Media Lab and Tilburg University are bringing science into the game to figure out the connections between our play style and our cognitive traits. To do that, we are gathering data from League of Legends, World of Warcraft, and Battlefield 4, and Battlefield: Hardline players to gain insights across all the major online game genres (MOBA, MMORPG, and FPS). In return, every participant will get an in-depth GAMR profile that shows their personality, brain type, and gamer type.</p>",,--Choose Location,2016-12-05 00:16:25.916,True,2015-01-01,GAMR,PUBLIC,,True,Other,False
hello-operator,slavin,False,"<p>Hello, Operator! is a vintage telephone switchboard from 1927, refurbished and wired up to a modern computer. It currently runs a time-management game; other games being prototyped are exploring the narrative potential of the system. Overall, the project exists to explore what we gain when we are able to physically engage with the antiquated technology that made the past tick.</p>",,--Choose Location,2016-12-05 00:16:37.619,True,2016-09-01,"Hello, Operator!",PUBLIC,,True,Other,False
holobiont-urbanism-revealing-the-microbiological-world-of-cities,slavin,False,"<p>This project investigates urban metagenomics to reveal the invisible microbiological worlds within our cities. Using honeybees to gather samples and hives modified to capture ""bee debris,"" the project employs genetic sequencing to discern and visualize urban microbiological neighborhoods and render microbiological landscapes of the city. The Holobiont project was first displayed at the Palazzo Mora in the 2016 Venice Architecture Biennale, with an installation that includes a ""metagenomic beehive."" Creative, scientific, development and production collaboration with: Ben Berman, Dr. Elizabeth Henaff, Regina Flores Mir, Dr. Chris Mason, Devora Najjar, Tri-Lox, and Chris Woebken, with contributions from Timo Arnall and Jack Schulze and local beekeepers in Brooklyn, Sydney, and Venice.</p>",,--Choose Location,2016-12-05 00:17:13.934,True,2016-01-01,Holobiont Urbanism: Revealing the Microbiological World of Cities,PUBLIC,,True,Other,False
sneak-a-hybrid-digital-physical-tabletop-game,slavin,False,"<p>Sneak is a hybrid digital tabletop game for two-to-four players about deception, stealth, and social intuition. Each player secretly controls one agent in a procedurally generated supervillain lair. Their mission is to find the secret plans and escape without getting discovered, shot, or poisoned by another player. To accomplish this, players must interact and blend in with a series of computer-controlled henchmen while keeping a close eye on their human opponents for any social cues that might reveal their identity. Sneak introduces a number of systems that are common in video games, but were impractical in tabletop games that did not deeply integrate a smartphone app. These include procedural map generation, NPC pathfinding, dynamic game balancing, and the use of sound.</p>",,--Choose Location,2016-12-05 00:16:11.193,True,2015-01-01,Sneak: A Hybrid Digital-Physical Tabletop Game,PUBLIC,,True,Other,False
learning-empathy,slavin,False,"<p>As part of its broader work around learning, this project is exploring both individualized and community-based models for promoting empathy by designing training methods and developing complementary &nbsp;technologies. The initiative launched 20 Day Stranger app with Playful Systems . The initiative is also working with the Opera of the Future group on a Vocal Vibrations/ Finding Your Voice interactive exhibition targeted towards awareness, empathy, and empowerment. The exhibition debuted in Paris and Cambridge with plans for Mexico City in 2017.
                    
                </p>",,,2018-12-11 18:23:20.052,True,2017-05-01,"Strangers, Voices, and Society",PUBLIC,,True,Other,False
tools-for-super-human-time-perception,cwwang,False,"<p>Time perception is a fundamental component in our ability to build mental models of our world. Without accurate and precise time perception, we might have trouble understanding speech, fumble social interactions, have poor motor control, hallucinate, or remember events incorrectly. Slight distortions in time perception are commonplace and may lead to slight dyslexia, memory shifts, poor eye-hand coordination, and other relatively benign symptoms, but could a diminishing sense of time signal the onset of a serious brain disorder? Could time perception training help prevent or reverse brain disorders? This project is a series of experimental tools built to assist and increase human time perception. By approaching time-perception training from various perspectives, we hope to find a tool or collection of tools to increase time perception, and in turn discover what an increase in time perception might afford us.</p>",2015-07-01,--Choose Location,2016-12-05 00:16:06.546,True,2014-01-01,Tools for Super-Human Time Perception,PUBLIC,,False,Playful Systems,False
darkball,cwwang,False,"<p>Cristiano Ronaldo can famously volley a corner kick in total darkness. The magic behind this remarkable feat is hidden in Ronaldo's brain, which enables him to use advance cues to plan upcoming actions. Darkball challenges your brain to do the same, distilling that scenario into its simplest form�intercept a ball in the dark. All you see is all you need.</p>",2015-07-01,--Choose Location,2016-12-05 00:16:20.623,True,2014-09-01,Darkball,PUBLIC,,False,Playful Systems,False
radio_o,cwwang,False,"<p>radiO_o is a battery-powered speaker worn by hundreds of party guests, turning each person into a local mobile sound system. The radiO_o broadcast system allows the DJ to transmit sounds over several pirate radio channels to mix sounds between hundreds of speakers roaming around the space and the venue's existing sound system.</p>",,--Choose Location,2016-12-05 00:17:21.437,True,2014-01-01,radiO_o,PUBLIC,,True,Playful Systems,False
wheels-of-poseidon,rssmith,False,"<p>Throughout the ages bioluminescence has inspired myths. Long ago, sailors in the Indian Ocean encountered massive bioluminescent blooms as they sailed through the water, lighting the wakes of their ships like the spokes of a wheel carrying them to their destination in a chariot of wind and water.&nbsp; They called this phenomenon “The Wheels of Poseidon."" Our goal is to harness the beauty of bioluminescence to create a new medium for artistic expression. We will generate a living, programmable bioluminescent display, with pixels and voxels built of bioluminescent plankton (<i>Pyrosystis fuciformis</i>) floating freely in the water column and stimulated to glow by a programmable pattern of pressure waves (acoustic waves) in the water. <br></p>",2018-12-31,,2019-04-22 18:07:56.239,True,2018-04-02,Wheels of Poseidon,PUBLIC,,False,Mediated Matter,False
vespers-iii,rssmith,False,"<p>Vespers is a collection of masks exploring what it means to design (with) life. From the relic of the death mask to a contemporary living device, the collection embarks on a journey that begins with an ancient typology and culminates with a novel technology for the design and digital fabrication of adaptive and responsive interfaces. We begin with a conceptual piece and end with a tangible set of tools, techniques and technologies combining programmable matter and programmable life.</p><p>The project points towards an imminent future where wearable interfaces and building skins are customized not only to fit a particular shape, but also a specific material, chemical and even genetic make-up, tailoring the wearable to both the body and the environment which it inhabits.</p><p>Imagine, for example, a wearable interface designed to guide ad-hoc antibiotic formation customized to fit the genetic makeup of its user; or, consider smart packaging or surface coatings devices that can detect contamination; finally, consider environmentally responsive architectural skins that can respond to, and adapt—in real time—to environmental cues. Research at the core of this project offers a new design space for biological augmentation across a wide breadth of application domains, leveraging resolution and scale.</p><p>The collection includes three series. The first series features the death mask as a cultural artefact. The final series features a living mask as an enabling technology. The second series mediates between the two, marking the process of ‘metamorphosis’ between the ancient relic and its contemporaneous interpretation.The living masks in the final series embody habitats that guide, inform and ‘template’ gene expression of living microorganisms. Such microorganisms have been synthetically engineered to produce pigments and/or otherwise useful chemical substances for human augmentation such as vitamins, antibodies or antimicrobial drugs.Combined, the three series of the Vespers collection represent the transition from death to life, or from life to death, depending on one’s reading of the collection.</p>",,,2018-10-23 15:19:03.760,True,2018-04-01,Vespers III,PUBLIC,,True,Mediated Matter,False
vespers,rssmith,False,"<p>Novel technologies for additive manufacturing are enabling design and production at nature’s scale. We can seamlessly vary the physical properties of materials at the resolution of a sperm cell, a muscle cell, or a nerve cell. Stiffness, color, hygroscopy, transparency, conductivity, even scent, can be individually tuned for each three-dimensional pixel within a physical object. The generation of products is therefore no longer limited to assemblages of discrete parts with homogeneous properties. Rather like organs, objects can be computationally ""grown"" and 3D printed to form materially heterogeneous and multi-functional products.</p>",,,2018-05-07 19:48:07.209,True,2016-12-12,Vespers II,PUBLIC,,True,Mediated Matter,False
totems,rssmith,False,"<p>Biodiversity on planet Earth is under momentous threat, with extinction rates estimated between 100 and 1,000 times their pre-human level. The Mediated Matter group has been in search of materials and chemical substances that can sustain and enhance biodiversity across living systems, and that have so far endured the perils of climate change. Melanin is one such substance illustrating biodiversity at the genetic, species, and ecosystem levels.</p>",,,2019-05-07 13:44:31.523,True,2019-02-27,Totems,PUBLIC,,True,Mediated Matter,False
maiden-flight,rssmith,False,"<p>Maiden Flight is an autonomous biological laboratory environment designed for studying the impact of space flight on the sole reproductive node of a bee colony: <b>the queen bee and her retinue.&nbsp;&nbsp;</b></p><p>It represents the first space module of its kind built specifically to cater to queen bees. The hybrid-ecology of the capsule was created to take into account the distributed and uniquely non-human nature of bee biology, in order to consider how to extend the bee reproductive system for environmental extremes. This aim is reflected in the structure of the capsule interior, which was assembled by humans and augmented by the bees’ natural fabrication.&nbsp;</p><p>In May 2019, the Mediated Matter group traveled to Texas to launch two laboratory capsules on Blue Origin’s sub-orbital rocket system, New Shepard. Each custom-designed&nbsp;<b>metabolic support capsule&nbsp;</b>comprised an experimental environment for one queen bee and an attending retinue of 10-20 nurse bees for a parabolic flight to a 100-kilometer micro-gravitational space apogee, and back.</p>",,,2019-05-17 13:11:46.844,True,2019-04-19,Maiden Flight,PUBLIC,,True,Mediated Matter,False
wheels-of-poseidon,danoran,False,"<p>Throughout the ages bioluminescence has inspired myths. Long ago, sailors in the Indian Ocean encountered massive bioluminescent blooms as they sailed through the water, lighting the wakes of their ships like the spokes of a wheel carrying them to their destination in a chariot of wind and water.&nbsp; They called this phenomenon “The Wheels of Poseidon."" Our goal is to harness the beauty of bioluminescence to create a new medium for artistic expression. We will generate a living, programmable bioluminescent display, with pixels and voxels built of bioluminescent plankton (<i>Pyrosystis fuciformis</i>) floating freely in the water column and stimulated to glow by a programmable pattern of pressure waves (acoustic waves) in the water. <br></p>",2018-12-31,,2019-04-22 18:07:56.239,True,2018-04-02,Wheels of Poseidon,PUBLIC,,False,Synthetic Neurobiology,False
sensorknits,danoran,False,"<p>Digital machine knitting is a highly programmable manufacturing process that has been utilized to produce apparel, accessories, and footwear.&nbsp;Our research presents three classes of textile sensors exploiting the resistive, piezoresistive, and capacitive&nbsp;properties of various textile structures enabled by machine knitting with conductive yarn.&nbsp;</p>",,,2019-04-09 13:53:02.936,True,2017-09-01,SensorKnits: Architecting textile sensors with machine knitting,PUBLIC,,True,Synthetic Neurobiology,False
implosion-fabrication-1,danoran,False,"<h2>Shrinking problems in 3D printing</h2><p>Although a range of materials can now be fabricated using additive manufacturing techniques, these usually involve assembly of a series of stacked layers, which restricts three-dimensional (3D) geometry. Oran&nbsp;et al.&nbsp;developed a method to print a range of materials, including metals and semiconductors, inside a gel scaffold (see the Perspective by Long and Williams). When the hydrogels were dehydrated, they shrunk 10-fold, which pushed the feature sizes down to the nanoscale.</p><p>Lithographic nanofabrication is often limited to successive fabrication of two-dimensional (2D) layers. We present a strategy for the direct assembly of 3D nanomaterials consisting of metals, semiconductors, and biomolecules arranged in virtually any 3D geometry. We used hydrogels as scaffolds for volumetric deposition of materials at defined points in space. We then optically patterned these scaffolds in three dimensions, attached one or more functional materials, and then shrank and dehydrated them in a controlled way to achieve nanoscale feature sizes in a solid substrate. We demonstrate that our process, Implosion Fabrication (ImpFab), can directly write highly conductive, 3D silver nanostructures within an acrylic scaffold via volumetric silver deposition. Using ImpFab, we achieve resolutions in the tens of nanometers and complex, non–self-supporting 3D geometries of interest for optical metamaterials.</p>",,,2018-12-13 20:17:08.530,True,2018-01-02,Implosion Fabrication,PUBLIC,,True,Synthetic Neurobiology,False
wheels-of-poseidon,novysan,False,"<p>Throughout the ages bioluminescence has inspired myths. Long ago, sailors in the Indian Ocean encountered massive bioluminescent blooms as they sailed through the water, lighting the wakes of their ships like the spokes of a wheel carrying them to their destination in a chariot of wind and water.&nbsp; They called this phenomenon “The Wheels of Poseidon."" Our goal is to harness the beauty of bioluminescence to create a new medium for artistic expression. We will generate a living, programmable bioluminescent display, with pixels and voxels built of bioluminescent plankton (<i>Pyrosystis fuciformis</i>) floating freely in the water column and stimulated to glow by a programmable pattern of pressure waves (acoustic waves) in the water. <br></p>",2018-12-31,,2019-04-22 18:07:56.239,True,2018-04-02,Wheels of Poseidon,PUBLIC,,False,Object Based Media,False
laser-face,novysan,False,"<p>Our eyes are our our mind's window to the external world. Vision is the primary way we sense our environment, and is a reliable indicator of our attention. As sensors, eyes evolved over millions of years to be fast, precise, and accurate, especially for tracking visual elements of interest (targets), like predators, prey, or baseballs.&nbsp;However, from an HCI perspective, these amazing abilities are also desirable actuators as well as sensors. What if the eyes can not only be a passive window of focus, but an active spatial cursor to indicate our target of focus? Also, what if we add <i>lasers</i>?</p>",2015-12-16,,2017-12-07 04:43:36.579,True,2015-10-01,LaserVision,PUBLIC,,False,Object Based Media,False
radio_o,novysan,False,"<p>radiO_o is a battery-powered speaker worn by hundreds of party guests, turning each person into a local mobile sound system. The radiO_o broadcast system allows the DJ to transmit sounds over several pirate radio channels to mix sounds between hundreds of speakers roaming around the space and the venue's existing sound system.</p>",,--Choose Location,2016-12-05 00:17:21.437,True,2014-01-01,radiO_o,PUBLIC,,True,Object Based Media,False
4k8k-comics,novysan,False,"<p>4K/8K Comics applies the affordances of ultra-high-resolution screens to traditional print media such as comic books, graphic novels, and other sequential art forms. The comic panel becomes the entry point to the corresponding moment in the film adaptation, while scenes from the film indicate the source frames of the graphic novel. The relationships among comics, films, social media, parodies, and other support materials can be navigated using native touch screens, gestures, or novel wireless control devices. Big data techniques are used to sift, store, and explore vast catalogs of long-running titles, enabling sharing and remixing among friends, fans, and collectors.</p>",,--Choose Location,2018-06-20 17:07:10.273,True,2014-01-01,4K/8K Comics,PUBLIC,,True,Object Based Media,False
neaq-2069,novysan,False,"<p>The New England Aquarium was&nbsp;one of the world’s first modern aquariums when it opened its doors in Boston in 1969.&nbsp; Throughout its history, the aquarium has been a leader in innovative ways to share the ocean with the public, including the creation of the&nbsp;&nbsp;Giant Ocean Tank, the largest circular saltwater tank in the world when it opened in 1970.&nbsp;</p><p>Approaching its 50th anniversary, the New England Aquarium is working with the Open Ocean initiative and&nbsp;MIT Design Lab to develop future scenarios depicting what the experience of the aquarium will be in the next 50 years.</p>",,,2018-04-30 16:04:10.248,True,2018-01-22,NEAQ 2069: Envisioning the Future Aquarium Experience,PUBLIC,,True,Object Based Media,False
synthbacteria,novysan,False,"<p>SYNTHetic Biology — A Tribute to Greg Bear’s <i>Blood Music</i></p><p>A musical ecosystem &nbsp;driven by and reacting to the movement of living biological organisms. The bio-SYNTH can take input from a realtime USB microscope or utilize captured video (as in this case).</p><p>Bacteria are tracked and elements such as size, age, velocity, and positions relative to the screen and each other are fed as seed values into a granular synthesizer. Waves are altered and samples are triggered according to an editable rule set.</p><p>Samples from the Mercury and Apollo space program, <i>Sputnik</i>, and science fiction films were inspired by Greg Bear’s 1985 Nebula and Hugo award winning novel <i>Blood Music</i>, and suggest the movements of the quickly evolving ""noocytes"" as they begin to explore their god-like host being Vergil Ulam.</p><p>The installation premiered at the Peabody Essex Museum in Salem, MA on September 18, 2014 and part of the After Hours PEM PM Series.</p>",,,2017-08-17 18:47:12.594,True,2014-08-15,SYNTHBacteria,PUBLIC,,True,Object Based Media,False
programmable-synthetic-hallucinations,novysan,False,<p>We are creating consumer-grade appliances and authoring methodologies that will allow hallucinatory phenomena to be programmed and utilized for information display and narrative storytelling.</p>,,--Choose Location,2019-04-16 16:26:42.218,True,2015-01-01,Programmable Synthetic Hallucinations,PUBLIC,,True,Object Based Media,False
psychicvr,novysan,False,"<p>We present PsychicVR, a proof-of-concept system that integrates a brain-computer interface device and virtual reality headset to improve mindfulness while enjoying a playful immersive experience.&nbsp;The fantasy that any of us could have superhero powers has always inspired us, and by using virtual reality and real-time brain activity sensing we are moving one step closer to making this dream real. We non-invasively monitor and record electrical activity of the brain and incorporate this data into the VR experience using an Oculus Rift and the MUSE headband. By sensing brain waves using a series of EEG sensors, the level of activity is fed back to the user via 3D content in the virtual environment. When users are focused, they are able to make changes in the 3D environment and control their powers. Our system increases mindfulness and helps achieve higher levels of concentration while entertaining the user.</p>",,--Choose Location,2019-04-18 17:06:09.570,True,2015-09-01,PsychicVR,PUBLIC,,True,Object Based Media,False
zero-g-ames,novysan,False,"<p>Games are a uniquely human endeavour, reducing stress and supporting mental well-being. Astronauts aboard the ISS have created their own games using materials at hand and pure creativity.  What if we could create games for them that took particular advantage of aspects of space, such as micro-gravity, and would help keep astronauts mentally engaged, socially connected, and physically relaxed?&nbsp; </p><p>Zero-G-ames is an ongoing series of workshops&nbsp;to explore, discuss, and design the history and future of games in constrained spaces and microgravity environments.</p><p>Let’s play!</p>",,,2018-06-25 17:52:03.983,True,2018-03-10,Zero-G-ames,PUBLIC,,True,Object Based Media,False
aerial-light-field-display,novysan,False,"<p>Suitable for anywhere a ""Pepper's Ghost"" display could be deployed, this display adds 3D with motion parallax, as well as optically relaying the image into free space such that gestural and haptic interfaces can be used to interact with it. The current version is able to display a person at approximately full-size.  </p>",,--Choose Location,2019-02-18 02:25:17.276,True,2015-01-01,Aerial Light-Field Display,PUBLIC,,True,Object Based Media,False
ar-neighbor-hood,mboya,False,"<p>What really divides Cambridge?</p><p>Who can occupy certain spaces in Cambridge? What are&nbsp;the less obvious social and cultural factors at play? When&nbsp;moving to a new neighborhood, one of the first things you&nbsp;might think about is whether you can afford to live there.&nbsp;But for many people, housing is not only a matter of&nbsp;affordability but also a matter of access to community.&nbsp;For others, private housing may not even be an option,&nbsp;so the accessibility of homeless shelters and public&nbsp;indoor spaces become important factors.</p><p>This project is in the prototype phase. It uses census data&nbsp;and augmented reality (AR) to examine the livability of&nbsp;three areas of Cambridge: Harvard Square, Central&nbsp;Square, and Kendall Square. In doing so, we hope to&nbsp;create an intersectional map that helps us understand&nbsp;housing accessibility.</p>",2018-12-12,,2019-04-22 18:08:48.885,True,2018-10-01,AR Neighbor/hood,PUBLIC,,False,Civic Media,False
allo-i-s,mboya,False,"<p>Definition:</p><p><i>combining form</i></p><p>Prefix: <b>allo-</b></p><ol><li>other; different.<br>""allopatric""</li></ol><p><b>im·ag·i·na·tion</b></p><p>noun: imagination; plural noun: imaginations</p><ol><li>the faculty or action of forming new ideas, or images or concepts of external objects not present to the senses.</li></ol><p><b>allo-i</b><b>(s)&nbsp;</b>is a project exploring alternative imaginations through the use of interactive, immersive experiences.</p><p>How can we leverage new technologies in low-cost environments to create location based experiences similar to theme-parks? Arwa is developing a mixed reality tool kit that can be used by creators to create immersive location based experiences at a low cost. She plays with projection mapping, AR/VR/XR, sensor technologies, theatre design, and narrative/story structures.</p><p>We further investigate the socio-economic and mental health impact of immersive spaces in low income communities across Africa.&nbsp;</p>",,,2019-04-22 18:15:04.065,True,2019-01-01,allo-i(s),PUBLIC,,True,Civic Media,False
datavisionari,mboya,False,"<p>dataVisionaRi is an exploration of big data visualizations&nbsp;techniques in VR. The research generates new taxonomy and&nbsp;structures around data visualization in the relatively&nbsp;unexplored space of Virtual Reality. The first demonstration,&nbsp;NodeitAll, is a Media Cloud link map that experiments with&nbsp;various ways to present edges and nodes, playing with&nbsp;interaction and manipulability. Developed in Unity, the&nbsp;scripting is open source so that anyone may input their own&nbsp;CSV files and have a randomly generated network graph.&nbsp;</p>",,,2019-04-22 18:11:31.980,True,2018-09-01,dataVisionaRi,PUBLIC,,True,Civic Media,False
shout,hidalgo,False,<p>Can I borrow your network? Shout! is a marketplace for retweets that allows people to exchange micro-contracts for future retweets. Shout! facilitates the coordination of social media diffusion efforts by groups.</p>,2016-06-01,,2018-05-07 18:22:08.055,True,2015-09-01,Shout!,PUBLIC,,False,Collective Learning,False
collective-learning,hidalgo,False,"<p>Industrial development is the process by which economies learn how to produce new products and services. But how do economies learn? And who do they learn from? &nbsp;<span style=""font-size: 18px; font-weight: normal;"">The literature on economic geography and economic development has emphasized two learning channels: inter-industry learning, which involves learning from related industries; and inter-regional learning, which involves learning from neighboring regions.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">Here we use 25 years of data describing the evolution of China's economy between 1990 and 2015--a period when China multiplied its GDP per capita by a factor of ten--to explore how Chinese provinces diversified their economies.</span><span style=""font-size: 18px; font-weight: normal;"">&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">First, we show that the probability that a province will develop a new industry increases with the number of related industries that are already present in that province, a fact that is suggestive of inter-industry learning. Also, we show that the probability that a province will develop an industry increases with the number of neighboring provinces that are developed in that industry, a fact suggestive of inter-regional learning. Moreover, we find that the combination of these two channels exhibit diminishing returns, meaning that the contribution of either of these learning channels is redundant when the other one is present.&nbsp;</span><br></p><p><span style=""font-size: 18px; font-weight: normal;"">Further, we address endogeneity concerns by using the introduction of high-speed rail as an instrument to isolate the effects of inter-regional learning. Our differences-in-differences (DID) analysis reveals that the introduction of </span>high speed-rail<span style=""font-size: 18px; font-weight: normal;""> increased the industrial similarity of pairs of provinces connected by high-speed rail. Also, industries in provinces that were connected by rail increased their productivity when they were connected by rail to other provinces where that industry was already present. These findings suggest that inter-regional and inter-industry learning played a role in China's great economic expansion.</span><br></p>",2017-09-01,,2017-03-31 00:31:45.552,True,2016-09-01,Collective Learning  in China's Regional Economic Development,PUBLIC,,False,Collective Learning,False
quantify,hidalgo,False,"<p>QUANTIFY is a generalized framework and JavaScript library to allow rapid multi-dimensional ""measurement"" of subjective qualities of media. The goal is to make qualitative metrics quantized. For everything from measuring emotional responses of content to the cultural importance of world landmarks, QUANTIFY helps to elicit the raw human subjectivity that fills much of our lives, and makes it programmatically actionable. </p>",2015-09-01,--Choose Location,2016-12-05 00:16:46.569,True,2014-01-01,QUANTIFY,PUBLIC,,False,Collective Learning,False
mapping-higher-education-network,hidalgo,False,"<p>The mismatch between the supply of graduates’ skills and the needs of the labor market has become increasingly obvious and problematic over the past years. This has prompt policymakers, educators, employers and applicants to reevaluate the role of higher education system. But how do each of these actors perceive higher education? How similar are, according to them, the different degree programs and institutions?</p><p>In this project, we use a data-driven approach to unveil the structure of similarities between degree programs as perceived from the candidates. To that end, we use applicants’ preferences to higher education in Chile and Portugal between the years of 2007 and 2014 as a proxy to measure the similarity between each pair of degree programs. We find that:</p><ol><li>The two structures share the same topological features, despite coming from two different political and social-economical contexts;</li><li>We quantify the mismatch between the current state of the art classification used by educators and policymakers and the structure identify</li><li>We find the existence of strong spatial patterns in the assortment of gender, application scores, demand and unemployment levels; and</li><li>We find that structure of similarities encapsulates non-trivial information about the nature of each degree program, allowing us to predict with high accuracy the level of unemployment by just taking into account the relative position of a degree program in the higher education option set.&nbsp;</li></ol><p>Currently, we are preparing a manuscript to present our findings.&nbsp;</p>",2017-12-31,,2017-10-17 18:44:40.535,True,2016-06-01,Mapping Higher Education Option Space,PUBLIC,,False,Collective Learning,False
industry-space-and-housing-price,hidalgo,False,"<p><span style=""font-size: 18px; font-weight: normal;"">The boom in Chinese housing prices in recent years has given rise to intensive concern about&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">&nbsp;the economic fundamentals of housing prices. This project mainly focuses on giving deep insight into&nbsp;</span>housing prices from the perspective of&nbsp;<span style=""font-size: 18px; font-weight: normal;"">industry composition, especially by checking the characteristics (agglomeration, innovation, diversity, and so on) of a city's position in the industry space.</span><span style=""font-weight: normal; font-size: 16px;"">&nbsp;</span></p>",2017-08-31,,2017-04-03 00:02:27.435,True,2017-02-10,Industry space and housing prices,PUBLIC,,False,Collective Learning,False
opus-exploring-scientific-data-through-visualizations,hidalgo,False,"<p>Scientific managers and administrators need to understand the impact of the research they support. Yet, most of the online tools available to explore scholarly publication data (e.g. Google Scholar and Citeseerx) present atomized views focused on single scholars and papers, failing to put scholars in a social, institutional, and national context; and failing to provide aggregate views for universities and countries. Here, we introduce<span style=""font-size: 18px; font-weight: 400;"">&nbsp;Opus,</span><span style=""font-size: 18px; font-weight: 400;"">&nbsp;an interactive online platform that integrates, aggregates, and visualizes scholarly data from Google Scholar and Microsoft Academic graph. Opus present users with scientific data at five different scales: scholars, countries, organizations, journals, and papers; and at each scale, it provides properly benchmarked visualizations that facilitate the comparison among scholars (e.g. from the same age), organizations, and nations. We build Opus using React as a front-end framework and Replot a new React visualization library developed in the course of this project as its central visualization library. Finally, we will have users compare Opus with similar tools such as Google Scholar, Microsoft Academic, and Citeseerx.</span></p>",2018-07-04,,2017-10-16 23:04:37.291,True,2017-01-10,Opus: Exploring Scientific Data through Visualizations,LAB-INSIDERS,,False,Collective Learning,False
a-new-project,hidalgo,False,"<p>It is well established that countries, regions and institutions tend to develop towards related activities. This implies that, for instance, countries are more likely to enter a new activity that is closer/related with the activities it has already developed. An empirical fact that results from the overlapping of the necessary knowledge of each activity. In this context, the product space—a network relating countries economic activities—has been instrumental in capturing the role of relatedness in the economic development of countries. But, although relatedness seems to be a major driver for the diversification of countries exports and research activities, there are many instances when countries deviate from this norm, but to what extent do they benefit from such actions? Is it possible to pinpoint a particular stage of development of a country in which these exceptions are more likely to occur or are they purely at random?</p><p>Using 50 years of trade date we have analyzed how countries diversify their products portfolios in the context of the Economic Complexity and Product Space10. We have shown that 1) there is an intermediate and non-trivial stage of economic development at which countries are more likely to develop towards unrelated activities; that 2) countries that do so achieve a faster economic growth; and 3) that low and high developed economies are the ones that are more likely to diversify towards related varieties.</p><p>These results have significant implications in the literature of regional development. For instance, recently the European Union presented a regional plan of development, coined as Smart Specialization, which advocates for a one rule that fits all: regions should develop the most related and highest reward activities. Our results suggest more caution. Indeed, our findings point out that the development stage of a country, or a region, plays a determinant role in devising a development strategy. For instance, while low and highly developed regions should look forward to developing related activities, regions at an intermediate level of development should be incentivized to pursue the development of unrelated activities and diversify. These results build up to the conclusions of the previous project (2.1), in the sense that economies should adopt dynamical diversification strategies in which the big challenge is to identify the narrow window for unrelated diversification.</p>",2017-12-31,,2017-10-11 00:23:45.415,True,2016-06-01,Do countries benefit from jumping into unrelated varieties?,PUBLIC,,False,Collective Learning,False
streetscore,hidalgo,False,"<p>StreetScore is a machine learning algorithm that predicts the perceived safety of a streetscape. StreetScore was trained using 2,920 images of streetscapes from New York and Boston and their rankings for perceived safety obtained from a crowdsourced survey. To predict an image's score, StreetScore decomposes this image into features and assigns the image a score based on the associations between features and scores learned from the training dataset. We use StreetScore to create a collection of map visualizations of perceived safety of street views from cities in the United States. StreetScore allows us to scale up the evaluation of streetscapes by several orders of magnitude when compared to a crowdsourced survey. StreetScore can empower research groups working on connecting urban perception with social and economic outcomes by providing high-resolution data on urban perception.</p>",,--Choose Location,2016-12-05 00:17:03.647,True,2014-01-01,StreetScore,PUBLIC,,True,Collective Learning,False
medium-shapes-message,hidalgo,False,"<p>Communication technologies, from printing to social media, affect our historical records by changing the way ideas are spread and recorded. Yet, finding statistical evidence of this fact has been challenging. Here we combine a common causal inference technique (instrumental variable estimation) with a dataset on nearly forty thousand biographies from Wikipedia (Pantheon 2.0) to study the effect of the introduction of printing in European cities on Wikipedia’s digital biographical records. By using a city’s distance to Mainz as an instrument for the adoption of the movable type press, we show that European cities that adopted printing earlier were more likely to become the birthplace of a famous scientist or artist during the years following the invention of printing. We bring these findings to recent communication technologies by showing that the number of radios and televisions in a country correlates with the number of globally famous performing artists and sports players born in that country, even after controlling for GDP, population, and including country and year fixed effects. These findings support the hypothesis that the introduction of communication technologies can bias historical records in the direction of the content that is best suited for each technology.&nbsp;</p>",,,2019-03-08 15:11:50.010,True,2015-12-01,How the medium shapes the message: Printing and the rise of the arts and sciences,PUBLIC,,True,Collective Learning,False
temporal-scales-in-human-collective-forgetting,hidalgo,False,"<p>Collective memory and attention are sustained by two channels: oral communication (communicative memory) and the physical recording of information (cultural memory). Here, we use data on the citation of academic articles and patents, and on the online attention received by songs, movies, and biographies, to describe the temporal decay of the attention received by cultural products. We show that, once we isolate the temporal dimension of the decay, the attention received by cultural products decays following a universal biexponential function. We explain this universality by proposing a mathematical model based on communicative and cultural memory, which fits the data better than previously proposed log-normal and exponential models. Our results reveal that biographies remain in our communicative memory the longest (20–30 years) and music the shortest (about 5.6 years). These findings show that the average attention received by cultural products decays following a universal biexponential function.</p>",,,2019-04-17 19:27:00.707,True,2017-02-01,The universal decay of collective memory and attention,PUBLIC,,True,Collective Learning,False
strategic-diffusion,hidalgo,False,"<p>One of the eternal challenges of economic development is how to identify the economic activities that a country, city, or region should target. During recent years, a large body of research has shown that countries, regions, and cities, are more likely to enter economic activities that are related to the ones they already have. For instance, a region specialized in the exports of frozen fish and crustaceans can more easily start exporting fresh fish than heavy machinery. This research has illuminated a new chapter in the economic development literature, but has left an important question unanswered: what is the right strategy for countries wanting to diversify their economies?&nbsp;</p>",,,2019-04-17 19:27:50.930,True,2018-04-02,What is the optimal way to diversify an economy?,PUBLIC,,True,Collective Learning,False
fold,hidalgo,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Some readers require greater context to understand complex stories.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">FOLD (</span><a href=""http://fold.cm"" style=""font-size: 18px; font-weight: normal;"">fold.cm</a><span style=""font-size: 18px; font-weight: normal;"">) is an open publishing platform with a unique structure that lets writers link media cards to the text of their stories.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">Media cards can contain videos, maps, tweets, music, interactive visualizations, and more.&nbsp;</span></p><p>FOLD is used by journalists, educators, and storytellers around the world.&nbsp;<br></p>",,--Choose Location,2016-12-15 02:27:31.751,True,2014-01-01,FOLD,PUBLIC,,True,Collective Learning,False
vizml,hidalgo,False,"<p>Visualization recommender systems aim to lower the barrier to exploring basic visualizations by automatically generating results for analysts to search and select, rather than manually specify. Here, we demonstrate a novel machine learning-based approach to visualization recommendation that learns visualization design choices from a large corpus of datasets and associated visualizations. First, we identify five key design choices made by analysts while creating visualizations, such as selecting a visualization type and choosing to encode a column along the X- or Y-axis. We train models to predict these design choices using one million dataset-visualization pairs collected from a popular online visualization platform. Neural networks predict these design choices with high accuracy compared to baseline models. We report and interpret feature importances from one of these baseline models. To&nbsp;evaluate the generalizability and uncertainty of our approach, we benchmark with a crowdsourced test set, and show that the performance of our model is comparable to human performance when predicting consensus visualization type, and exceeds that of other visualization recommender systems.&nbsp;</p>",,,2019-05-06 20:23:32.720,True,2017-12-05,VizML: A Machine Learning Approach to Visualization Recommendation,PUBLIC,,True,Collective Learning,False
gifgif,hidalgo,False,"<p>An animated GIF is a magical thing. It has the power to compactly convey emotion, empathy, and context in a subtle way that text or emoticons often miss. GIFGIF is a project to combine that magic with quantitative methods. Our goal is to create a tool that lets people explore the world of GIFs by the emotions they evoke, rather than by manually entered tags. A web site with 200,000 users maps the GIFs to an emotion space and lets you peruse them interactively.</p>",,--Choose Location,2016-12-14 14:00:48.619,True,2014-01-01,GIFGIF,PUBLIC,,True,Collective Learning,False
streetchange,hidalgo,False,"<p>Computer vision uncovers predictors of physical urban change
                    
                </p>",,,2017-07-07 18:51:23.160,True,2015-06-01,Streetchange,PUBLIC,,True,Collective Learning,False
biodigital-space,hidalgo,False,"<p>Biodigital is a fictional virtual reality (VR) experience that combines VR film, immersive 3D environments, and VR data visualization. Biodigital tells the story of humanity as seen from the year 2117, after humans merged with machines in the ""biodigital"" space. The story of Biodigital is about a species that transforms each time it develops new communication technologies. It begins with the histories of printing and television, but then explores the rise of neuroimplantable devices, and the new moral dilemmas they bring to the world.&nbsp;</p><p><span style=""font-size: 18px; font-weight: normal;"">Biodigital is an experiment on mixed media created in a collaboration between Takahito Ito, from NHK, and </span>César&nbsp;<span style=""font-size: 18px; font-weight: normal;"">&nbsp;A. Hidalgo, head of the Collective Learning group. The project explores new forms of storytelling in VR.</span></p>",,,2017-04-05 18:38:52.863,True,2017-01-09,Biodigital,LAB-INSIDERS,,True,Collective Learning,False
pubpub,hidalgo,False,"<p>PubPub reinvents publication to align with the way the web was designed: collaborative, evolving, and open. PubPub uses a graphical format that is deliberately simple and allows illustrations and text that are programs as well as static PDFs. The intention is to create an author-driven,  distributed alternative to academic journals that is tuned to the dynamic nature of many of our modern experiments and discoveries. It is optimized for public discussion and academic journals, and is being used for both.  It is equally useful for a newsroom to develop a story that is intended for both print and online distribution.</p>",,--Choose Location,2018-05-31 18:16:31.248,True,2015-01-01,PubPub,PUBLIC,,True,Collective Learning,False
meet-me-in-the-middle-the-reunification-of-the-german-research-and-innovation-system,hidalgo,False,"<p>In 1990 Germany began the reunification of two separate research systems. Yet, the institutional unification of these system does not necessarily imply their actual unification. Here we study the evolution of the network of co-authorships between East and West German scholars between 1974 and 2014 to identify the fields that integrated more successfully, and also, the factors predicting re-unification success. We find that the unification of the German research network was fast during the 1990s, but then stagnated at an intermediate level of integration. Next, we study the integration of the twenty largest academic fields (by number of publications prior to reunification) and find an inverted U-shaped between a field's East or West ``dominance'' (a measure of the concentration of the scholarly output of a field in East or West Germany prior to 1990) and the field's subsequent level of integration. We check for the robustness of these results by running Monte Carlo simulations, and a differences-in-difference analysis. Both methods confirm that fields that were dominated by either West or East Germany prior to the reunification integrated less than those whose output was balanced among East and West. Finally, we explore the origins of this inverted U-shape relationship by comparing the mixing patterns, and show that this inverted U-shaped relationship can be explained as a consequence of a tendency of scholars from the most productive regions to collaborate preferentially with scholars from other top regions. These results shed light on the mechanisms governing the reintegration of networks in the content of scholarly communities that were separated by institutions.</p>",,,2017-10-11 15:39:59.915,True,2016-04-01,Meet me in the middle: The reunification of the German research and innovation system,PUBLIC,,True,Collective Learning,False
relatedness-knowledge-diffusion-and-the-evolution-of-bilateral-trade,hidalgo,False,"<p>During the last few decades two important intellectual contributions have reshaped our understanding of international trade. On the one hand, work emphasizing trade frictions and extended gravity models has shown that countries trade more with those with whom they share a language, colonial past, or ethnic social relationships. This is interpreted as evidence of trade not being only about differences in factor endowments and transportation costs, but the result of complex social processes where information frictions and social networks play a key role. On the other hand, work emphasizing knowledge diffusion has shown that the probability that a country starts exporting a product increases with the number of related products it already exports. Yet, despite the importance of these two recent findings, little is known about their intersection: does knowledge on how to export to a destination also diffuses among related products? Here, we use bilateral trade data from 2000 to 2015, disaggregated into 1,242 product categories, to create an extended gravity model of bilateral trade that reproduces previous findings (effects of language, distance, colonial past, etc.) and shows that, in addition to these, countries are more likely to increase their exports of a product to a destination when: (i) they export related products to it, (ii) already export that product to some of its neighbors, and (iii) have neighbors who also export the same product to that destination. We interpret these findings as evidence of knowledge diffusion among related products and among geographic neighbors, both in the context of exporters and importers. Then, we explore the magnitude of these effects for new, nascent, and experienced exporters, and also, for groups of products classified according to Lall's technological classification of exports. We find that the effects of product and geographic relatedness are stronger for new exporters, and also, that the effect of product relatedness increases with the technological sophistication of products. These findings support the idea that international trade is shaped by knowledge and information frictions that are partially reduced in the presence of product relatedness.</p>",,,2017-10-11 16:57:36.018,True,2017-03-01,"Relatedness, Knowledge Diffusion, and the Evolution of Bilateral Trade",PUBLIC,,True,Collective Learning,False
immersion-teams,hidalgo,False,<p>MITeams is an email network visualization tool for teams that allows people to see collaboration among team members and the structure of teams.<br></p>,,,2017-10-25 01:00:12.114,True,2016-10-01,MITeams,LAB-INSIDERS,,True,Collective Learning,False
replot,hidalgo,False,"<p>Replot is a new and soon-to-be-open sourced visualization library for the web, built natively on the ReactJS, the extremely popular open-source web framework that powers most modern websites today.</p><p>Replot is written from the ground up in React, rendering native SVG visualizations and fully leveraging the idea of user interfaces being pure functions of an underlying data ""state."" This enables developers to construct rich and engaging visualizations with very few lines of code that bind automatically to their datasets, and animate automatically when their data change.&nbsp;</p><p>Replot also introduces a paradigm shift in customizability of your visualizations, enabling you to trigger transformations in your visualizations by simply manipulating state variables.</p>",,,2017-10-11 19:58:05.810,True,2017-06-01,Replot,LAB-INSIDERS,,True,Collective Learning,False
economic-complexity-and-income-inequality,hidalgo,False,"<p>Decades ago development scholars argued that the productive structure of a country (i. e. the mix of industries operating in the country) constrains its ability to generate and distribute income. They were correct! It was recently shown that the mix of products that a country exports is predictive of its future pattern of diversification and economic growth. But what is the link between a country's productive structure and its ability to distribute income?&nbsp;Here, we combine methods from econometrics, network science, and economic complexity, together with data on income inequality and world trade, to show that countries exporting complex products have lower levels of income inequality than countries exporting simpler products. Using multivariate regression analysis, we show that economic complexity is a significant and negative predictor of income inequality and that this relationship is robust to controlling for aggregate measures of income, institutions, export concentration, and human capital. Moreover, we introduce a measure that associates a product to a level of income inequality equal to the average GINI of the countries exporting that product (weighted by the share the product represents in that country’s export basket). The Product-GINI index, or PGI,&nbsp;can provide important insights on the constraints to inequality imposed by a country's productive structure. Finally, we integrate our results to the Observatory of Economic Complexity, an online resource that allows its users to visualize the structural transformation of over 150 countries&nbsp;and their associated changes in income inequality during 1963–2008.</p>",,--Choose Location,2017-10-10 16:03:06.641,True,2014-09-01,Inequality and the impact of industrial structures,PUBLIC,,True,Collective Learning,False
dive,hidalgo,False,"<p><a href=""https://dive.media.mit.edu""><b>DIVE</b></a> is a web-based data exploration system that lets non-technical users create stories from their data without writing code.&nbsp;DIVE combines semantic data ingestion, recommendation-based visualization and analysis, and dynamic story sharing into a unified workflow.&nbsp;</p><p><b>Links</b><br></p><ul><li>Public version:&nbsp;<a href=""https://dive.media.mit.edu/"">dive.media.mit.edu</a></li><li>Demo video:&nbsp;<a href=""https://dive.media.mit.edu/video"">dive.media.mit.edu/video</a></li><li>Front-end repository:&nbsp;<a href=""https://github.com/MacroConnections/dive-frontend"">github.com/MacroConnections/dive-frontend</a></li><li>Back-end repository:&nbsp;<a href=""http://github.com/MacroConnections/dive-backend"">github.com/MacroConnections/dive-backend</a></li></ul>",,,2018-06-28 14:01:39.643,True,2015-01-01,DIVE,PUBLIC,,True,Collective Learning,False
which-industries-follow-relatedness,hidalgo,False,"<p>Industries are more likely to enter and less likely to exit regions that are densely populated by related industries. Unfortunately, the measures used to estimate the relatedness of industries often combine information about multiple forms of relatedness. Here, we use data on the entire formal sector economy of a large country to construct five different measures of relatedness and compare their ability to predict diversification events. We interpret differences in the ability of these metrics to predict entry events as evidence of the relative importance of each relatedness channel for specific industries. These findings advance our understanding of the forms of relatedness that are more likely to predict regional diversification events for specific industries.</p>",,,2018-05-03 20:55:25.540,True,2017-02-01,Untangling Relatedness: What forms of relatedness predict diversification?,LAB-INSIDERS,,True,Collective Learning,False
trains-of-thought-railroad-access-and-knowledge-diffusion-in-sweden,hidalgo,False,"<p>Industrial diversification is a path-dependent process that leverages knowledge, skills, and new technologies. Because such resources are difficult to move, geography plays a crucial role in determining the future economic activities of countries, regions, and cities. Yet most of the evidence on the geographic diffusion of economic activities is restricted to the last 60 years and relies on correlations.&nbsp;</p><p>This paper analyzes the geographic diffusion of economic activities for Swedish towns between 1850 and 1950, using the evolution of the railroad network as a way to address endogeneity. We use the straight line between Sweden's 10 largest towns as an instrument for train adoption. Our instrumental variable estimates show that regions are more likely to diversify into sectors that are present in their train neighbors, suggesting that the impact of connectivity goes beyond access to markets: connectivity also promotes diffusion of economic activities, even at early stages of development.&nbsp;<br></p>",,,2018-05-07 00:25:12.390,True,2017-11-01,Railroad Access and Diffusion of Industries: Evidence from Sweden during the Second Industrial Revolution,PUBLIC,,True,Collective Learning,False
when-bullying-meets-video-game-theory,hidalgo,False,"<p>Social learning has shown that people are more likely to learn from those who are seen as prestigious, talented, or who share demographic attributes with learners. In order to demonstrate that, many experiments and data-based studies have been conducted in many different systems; however, classroom environments have been understudied, because of different complications in both designing experiments and collecting data.</p><p>Combining both new technologies that are able to capture children's attention, e.g. video games, as well as experimental game theory, which provides us a formal framework to capture children's revealed preferences—a school classroom can provide an ideal environment for controlled social dilemma experiments, whose results can be contrasted against real-life indicators of school-life.</p><p>The connection between cooperation inside a classroom and social relationships is central in our framework. Here, we navigate the social network structure by running a non-anonymous dyadic cooperative (video) game (Fig. 1), in 50 different public primary school classrooms, between grades 3-5, allowing us to map cooperation networks for each classroom.</p><p>From the video game decisions, we build a weighted cooperation network for each classroom. The resulting network structure is able to capture different properties of the classroom, such as academic performance and social co-existence (Fig. 2). First,&nbsp;we find that positions in the social network have a significant power to identify, in an early stage, children who are susceptible to becoming&nbsp; the victims of bullying, and children who have a high probability to&nbsp;be bullies&nbsp;(Fig. 2A). Second,&nbsp;we find a positive and statistically significant relationship between network centrality—measured as the sum of the outcome on the video game—and student’s academic performance (measured as GPA, even controlling for others socio-behavioral characteristics that are correlated with GPA (Fig. 2 B)).</p><p>These results don't just help us to understand the elementary school environment, but also open new avenues for the role of networks in the education system, with a huge potential impact in education public policy. These results are useful inputs for decision makers and physiologists to prevent bullying and improve learning.</p>",,,2018-10-23 15:23:30.759,True,2017-10-17,When bullying meets (video) game theory: A novel framework to understand elementary school environments,PUBLIC,,True,Collective Learning,False
the-laws-of-forgetting-ii,hidalgo,False,"<p>In order to understand how exogenous shocks, like death, impact memorability by remembering, we use a data-set of biographies from Wikipedia for all individuals who have more than 15 different language editions. Here, we focus on different external shocks that are able to trigger remembering, such as Death, Nobel Prize, Academy Awards (Oscars), Ballon d'Or, Golden Globes, and Grammy's. All of these events show an exogenous-critical non-trivial herd behavior, as described by <a href=""http://www.pnas.org/content/105/41/15649"">Crane and Sornette 2008</a>.</p>",,,2018-10-20 16:47:24.281,True,2017-10-16,The laws of forgetting II: How death and exogenous events shape our collective memory,PUBLIC,,True,Collective Learning,False
industry-knowledge,hidalgo,False,"<p>How do regions acquire the knowledge they need to diversify their economic activities? How does the migration of workers among firms and industries contribute to the diffusion of that knowledge? Here we measure the industry-, occupation-, and location-specific knowledge carried by workers from one establishment to the next, using a dataset summarizing the individual work history for an entire country. We study pioneer firms—firms operating in an industry that was not present in a region—because the success of pioneers is the basic unit of regional economic diversification. We find that the growth and survival of pioneers increase significantly when their first hires are workers with experience in a related industry and with work experience in the same location, but not with past experience in a related occupation. We compare these results with new firms that are not pioneers and find that industry-specific knowledge is significantly more important for pioneer than for non-pioneer firms. To address endogeneity we use Bartik instruments, which leverage national fluctuations in the demand for an activity as shocks for local labor supply. The instrumental variable estimates support the finding that industry-specific knowledge is a predictor of the survival and growth of pioneer firms. These findings expand our understanding of the micromechanisms underlying regional economic diversification.</p>",,,2018-12-14 20:31:53.678,True,2017-03-01,"The role of industry, occupation, and location-specific knowledge in the survival of new firms",PUBLIC,,True,Collective Learning,False
kinetic-blocks,ishii,False,"<p>The ability of shape displays to move and manipulate objects enables the assembly, disassembly, and reassembly of different forms and structures using a set of fundamental building blocks (cubes). We present different assembly techniques such as stacking, scaffolding, and catapulting, which allow us to create 3D structures on a shape display.</p>",2015-09-01,--Choose Location,2016-12-05 00:16:09.457,True,2015-01-01,Kinetic Blocks,PUBLIC,,False,Tangible Media,False
unimorph-thin-film-shape-changing-interfaces,ishii,False,"<p>uniMorph is an enabling technology for thin-sheet shape-changing interfaces. It affords actuation powered by environmental temperature changes, as well as computational control of actuation. Additionally, the composite allows a seamless integration of electronic components. We developed a digital fabrication method to enable easy creation of composites with actuation patterns of high accuracy.</p>",2015-09-01,--Choose Location,2016-12-05 00:16:12.153,True,2014-09-01,uniMorph: Thin-Film Shape-Changing Interfaces,PUBLIC,,False,Tangible Media,False
inforce,ishii,False,,2017-05-01,,2017-03-31 00:09:40.562,True,2015-09-25,inFORCE 1.0,LAB-INSIDERS,,False,Tangible Media,False
repose-smart-pillow,ishii,False,"<p>Sleep is perhaps the most critical human activity for well-being, but seldom receives the priority it deserves. We get less rest than we need, and the respite we do achieve is often not in&nbsp;proper or comfortable postures. We present a <b style=""font-size: 18px;"">smart pillow “RePose”</b><span style=""font-size: 18px; font-weight: 400;"">&nbsp;to explore a new genre of soft, shape-changing auto-adaptive furniture.&nbsp;This new class of interactive ""Smart Furniture"" device themed around pillows, couches, and beds are designed to transform for, conform to, and inform the user as they rest. Using inflatable pouches and pressure-sensors, RePose detects uncomfortable or unergonomic usage and changes its shape and stiffness accordingly to promote good posture. The result is a pillow that is comfortable for everyone in every situation, and is capable of a variety of additional functionalities for sensing and actuation.</span><br></p>",2016-09-01,--Choose Location,2017-12-07 03:48:02.073,True,2016-01-01,RePose,PUBLIC,,False,Tangible Media,False
influx,ishii,False,"<p>We present a stiffness-changing interface based on a magneto-rheological (MR) fluid. The device consists of a material&nbsp;surface with electromagnetically&nbsp;induced visco-elasticity,&nbsp;which acts as a proxy for stiffness during tangible interaction with the material. We present several advantages of&nbsp;this enabling technology and outline potential applications&nbsp;and routes for future development.</p>",2015-12-15,,2017-12-07 04:43:01.290,True,2015-11-01,inFlux,PUBLIC,,False,Tangible Media,False
sociabowl,ishii,False,"<p>We introduce SociaBowl, a dynamic table centerpiece to promote positive social dynamics in 2-way cooperative conversations. A centerpiece such as a bowl of food, a decorative flower arrangement, or a container of writing tools, is commonly placed on a table around which people have conversations. We explore the design space for an augmented table and centerpiece to influence how people may interact with one another. We present an initial functional prototype to explore different choices in materiality of feedback, interaction styles, and animation and motion patterns. These aspects are discussed with respect to how it may impact people’s awareness of their turn taking dynamics as well as provide an additional channel for expression. Potential enhancements for future iterations in its design are then outlined based on these findings.</p>",2019-01-07,,2019-05-04 23:42:45.225,True,2018-11-12,SociaBowl: A Dynamic Table Centerpiece to Mediate Group Conversations,PUBLIC,,False,Tangible Media,False
physical-telepresence,ishii,False,"<p>We propose a new approach to physical telepresence, based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In the research paper, we describe the concept of shape transmission, and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of users' body parts can be altered to amplify their capabilities for teleoperation. A preliminary evaluation found that users were able to manipulate simple objects remotely, and found many different techniques for manipulation that highlight the expressive nature of our system.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:44.043,True,2014-09-01,Physical Telepresence,PUBLIC,,False,Tangible Media,False
soundform,ishii,False,"<p>SoundFORMS creates a&nbsp;<span style=""font-size: 18px; font-weight: 400;"">new method for composers of electronic music to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">interact with their compositions. Through the use of a&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">pin-based shape-shifting display, synthesized&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">waveforms are projected in three dimensions in real&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">time affording the ability to hear, visualize, and interact&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">with the timbre of the notes. Two types of music&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">composition are explored: generation of oscillator&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">tones, and triggering of pre-recorded audio samples.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The synthesized oscillating tones have three timbres:&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">sine, sawtooth and square wave. The pre-recorded</span></p><p>audio samples are drum tracks. Through the use of a&nbsp;<span style=""font-size: 18px; font-weight: 400;"">gestural vocabulary, the user can directly touch and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">modify synthesized waveforms.</span></p>",2016-06-01,,2017-05-26 15:59:57.300,True,2016-02-01,SoundFORMS: Manipulating Sound Through Touch,PUBLIC,,False,Tangible Media,False
kinephone,ishii,False,"<p>This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.</p>",2016-08-01,,2017-05-18 01:07:33.691,True,2016-05-01,Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display,PUBLIC,,False,Tangible Media,False
droplet-io,ishii,False,"<p>DropletIO uses aqueous droplets as a form of programmable material for human-material interaction. We demonstrate how fluids in our environment can be &nbsp;programmed droplet-wise to move, merge, and split. Through these operations we can then digitally regulate fluid properties—smell, color, chemical, and biological characteristics. When seamlessly integrated into a range of everyday objects and spaces, droplets become ubiquitous displays and other interactive elements aiding creative activity such as painting, storytelling, and art. Beyond this, programmable droplets have applications in digital biology and chemistry.</p>",2017-04-06,,2017-10-05 21:52:41.593,True,2016-10-01,Droplet IO: Programmable Droplets for Human-material Interaction,LAB-INSIDERS,http://udayan-u.com,False,Tangible Media,False
social-textiles,ishii,False,"<p>The way we represent ourselves in social media is intangible. What we choose to wear is public to the world and we are aware of it. In contrast, what we post online about ourselves reaches thousands of people and generates social consequences, but it doesn’t feel that way. Is the current form of social media really making our relationships better? Current technologies are good at connecting people at a distance, but less so at connecting them within the same environment.</p><p>Social textiles embodies who you are and dynamically reflects your shared interests with people nearby. It enables you to gain access to communities of people in the physical world and enhances social affordances and icebreaking interactions through wearable social messaging.</p><p>Social Textiles embody who you are and dynamically reflect your shared interests with people nearby. They enable you to gain access to communities of people in the physical world and enhance social affordances and icebreaking interactions through wearable social messaging. Social Textiles can connect community members with niche interests, philosophical beliefs, personalities, emotional statuses, and ethical views. They have the potential to enable members to bypass superficial or generic interests through ""filtering"" individuals, in order to tune social experiences toward people who are more compatible.</p>",2015-01-01,--Choose Location,2018-10-12 16:50:34.930,True,2014-09-01,Social Textiles,PUBLIC,,False,Tangible Media,False
cord-uis-controlling-devices-with-augmented-cables,ishii,False,"<p>Cord UIs are sensorial augmented cords that allow for simple, metaphor-rich interactions to interface with their connected devices. Cords offer a large, under-explored space for interactions, as well as unique properties and a diverse set of metaphors that make them potentially interesting tangible interfaces. We use cords as input devices and explore different interactions like tying knots, stretching, pinching, and kinking to control the flow of data and/or power. We also look at ways to use objects in combination with augmented cords to manipulate data or properties of a device. For instance, placing a clamp on a cable can obstruct the audio signal to the headphones. Using special materials such as piezo copolymer cables and stretchable cords, we built five working prototypes to showcase these interactions.</p>",2014-09-01,--Choose Location,2016-12-05 00:17:09.974,True,2014-01-01,Cord UIs: Controlling Devices with Augmented Cables,PUBLIC,,False,Tangible Media,False
lineform,ishii,False,"<p>We propose a novel shape-changing interface that consists of a single line. Lines have several interesting characteristics from the perspective of interaction design: abstractness of data representation; a variety of inherent interactions/affordances; and constraints such as boundaries or borderlines. By using such aspects of lines together with added transformation capability, we present various applications in different scenarios: shape-changing cords, mobiles, body constraints, and data manipulation to investigate the design space of line-based shape-changing interfaces.</p>",,--Choose Location,2018-05-04 15:31:24.514,True,2015-01-01,LineFORM,PUBLIC,,True,Tangible Media,False
hydromorph,ishii,False,"<p>HydroMorph is an interactive display based on shapes formed by a stream of water. Inspired by the membrane formed when a water stream hits a smooth surface (e.g., a spoon), we developed a system that dynamically controls the shape of a water membrane. This project explores a design space of interactions around water shapes, and proposes a set of user scenarios in applications across scales, from the faucet to the fountain. Through this work, we look to enrich our interaction with water, an everyday material, with the added dimension of transformation.</p>",,--Choose Location,2016-12-05 00:17:14.252,True,2016-01-01,HydroMorph,PUBLIC,,True,Tangible Media,False
s-c-a-l-e,ishii,False,"<p>S.C.A.L.E. is a system for detecting localization of an external object or agent, utilizing weight and pressure as a controlling constant for the detection of place and pressure. &nbsp;The system is designed for simple prototyping of interactive products. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2016-12-05 00:16:27.706,True,2016-08-20,S.C.A.L.E.,LAB-INSIDERS,,True,Tangible Media,False
animastage,ishii,False,"<p>We present AnimaStage: a hands-on animated craft platform based on an actuated stage. Utilizing a pin-based shape display, users can animate their crafts made from various materials. Through this system, we intend to lower the barrier for artists and designers to create actuated objects and to contribute to interaction design using shape-changing interfaces for inter-material interactions.</p><p>We introduce a three-phase design process for AnimaStage with examples of animated crafts. We implemented the system with several control modalities that allow users to manipulate the motion of the crafts so that they could easily explore their desired motion through an iterative process. Dynamic landscapes can also be rendered to complement the animated crafts. We conducted a user study to observe the subject and process by which people make crafts using AnimaStage. We invited participants with different backgrounds to design and create crafts using multiple materials and craft techniques. A variety of outcomes and application spaces were found in this study.</p><p><a href=""http://tangible.media.mit.edu/project/animastage/"">Project Page</a></p>",,,2017-06-21 18:35:35.551,True,2017-06-12,AnimaStage,PUBLIC,http://tangible.media.mit.edu/project/animastage,True,Tangible Media,False
conjugate,ishii,False,"<p>A recent focus of our lab has been making use of Tangible Displays and Body Object Space to develop new assistive technologies. As a test case, we prototyped the Mario side-scrolling game for visually impaired users, using body movement analogies to control Mario in the game. Mario and 2D side scrollers present a particularly interesting case, as they keep the main character location in the center of the display and move the world around the character. The shape display itself provides spatial audio of enemy positions. We make use of the AUFLIP sensor platform to pick up body movements—walking and jumping, causing Mario to do the same in-game. This enables users to keep their hands engaged to understand the game landscape, while using their body to control Mario at the same time.&nbsp;</p>",,,2019-02-08 17:48:57.541,True,2017-03-01,CONJURE,PUBLIC,,True,Tangible Media,False
inflated-appetite,ishii,False,"<p>As part of human evolution and revolution, food is among the earliest forms of human interaction, but it has remained essentially unchanged from ancient to modern times. What if we introduced engineered and programmable food materials? With that change, food can change its role from passive to active. Food can ""communicate"" using its inherent behaviors combined with engineering accuracy. Food becomes media and interface. During an MIT winter course we initiated and taught, we encouraged students to design pneumatic food. Students successfully implemented inflatable sugar and cheese products. To inflate food, we use both an engineering approach and a biological approach; to solidify the inflated food, we introduce both heat via the oven, and coldness with liquid nitrogen.</p>",,--Choose Location,2016-12-24 22:05:38.254,True,2016-01-01,Inflated Appetite,PUBLIC,,True,Tangible Media,False
inflatables,ishii,False,"<p>Printflatables is a design and fabrication system for human-scale, functional and dynamic inflatable objects. The user begins with specifying an intended 3D model which is decomposed to two dimensional fabrication geometry. This forms the input for a numerically controlled contact iron that seals layers of thermoplastic fabric. </p><p>In this project, we showcase the system design in detail, the pneumatic primitives that this technique enables and merits of being able to make large, functional and dynamic pneumatic artifacts. We demonstrate the design output through multiple objects which could motivate fabrication of inflatable media and pressure-based interfaces.</p><p><a href=""http://tangible.media.mit.edu/project/printflatables/"">Project Website</a></p>",,,2019-04-17 19:31:11.620,True,2017-05-10,"Printflatables: Printing human-scale, functional, and dynamic inflatable objects",PUBLIC,http://tangible.media.mit.edu/project/printflatables/,True,Tangible Media,False
transform-adaptive-and-dynamic-furniture,ishii,False,"<p>Introducing TRANSFORM, a shape-changing desk. TRANSFORM is an exploration of how shape display technology can be integrated into our everyday lives as interactive, transforming furniture. These interfaces not only serve as traditional computing devices, but also support a variety of physical activities. By creating shapes on demand or by moving objects around, TRANSFORM changes the ergonomics and aesthetic dimensions of furniture, supporting a variety of use cases at home and work: it holds and moves objects like fruit, game tokens, office supplies, and tablets, creates dividers on demand, and generates interactive sculptures to convey messages and audio.</p>",,--Choose Location,2019-04-17 19:36:00.162,True,2014-09-01,TRANSFORM: Adaptive and dynamic furniture,PUBLIC,,True,Tangible Media,False
trans-dock,ishii,False,"<p>We introduce TRANS-DOCK, a passive docking system for pin-based shape displays that enhances the interaction capability for both the output and input. By simply switching the ""transducer"" module to be docked on a single shape display, users can selectively switch between different display sizes and resolutions, movement modalities, as well as pin alignments enabled by the transducers. We introduce a design space consisting of mechanical elements and enabled interaction capabilities. We utilized several mechanical elements to develop the docking hardware for customizable interactions. With this idea, we present potential application spaces, which include digital 3D model explorations, active tangible interface prototyping, gaming, and dynamic house models. TRANS-DOCK intends to expand what a single shape display can do for dynamic physical interactions, by converting arrays of linear motion to several types of dynamic motion in an adaptable and flexible manner.</p>",,,2018-10-05 18:57:23.901,True,2018-08-01,TRANS-DOCK,LAB-INSIDERS,,True,Tangible Media,False
biologic,ishii,False,"<p>Cells’ biomechanical responses to external stimuli have been intensively studied but rarely implemented into devices&nbsp;<span style=""font-size: 18px; font-weight: 400;"">that interact with the human body. We demonstrate that the hygroscopic and biofluorescent behaviors of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">living cells can be engineered to design biohybrid wearables, which give multifunctional responsiveness to hu</span><span style=""font-size: 18px; font-weight: 400;"">man sweat. By depositing genetically tractable microbes on a humidity-inert material to form a heterogeneous&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">multilayered structure, we obtained biohybrid films that can reversibly change shape and biofluorescence intensity&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">within a few seconds in response to environmental humidity gradients. Experimental characterization and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">mechanical modeling of the film were performed to guide the design of a wearable running suit and a fluorescent&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">shoe prototype with bio-flaps that dynamically modulates ventilation in synergy with the body’s need for cooling.</span></p>",,--Choose Location,2018-05-04 15:32:39.880,True,2014-01-01,bioLogic—Science Advances,PUBLIC,,True,Tangible Media,False
droplets,ishii,False,"<p>DropletIO proposes aqueous droplets as a programmable material for biology, art, and design. The DropletIO system can actuate and sense macro-scale droplets&nbsp;(nano-liter to micro-liter)&nbsp;on planar surfaces. The system can precisely move, merge, split, oscillate, and change the shape of droplets. We built custom printed circuit boards that integrate actuation and sensing, which act as building blocks for droplet control on devices of various form factors. We show how DropletIO boards can be integrated into a range of tools for biology, everyday objects as ubiquitous information displays and as an interaction medium for art and entertainment.&nbsp;&nbsp;</p><h2>Droplets in Biology&nbsp;&nbsp;</h2><p>Droplet-based microfluidics is extensively used in biology and chemistry.&nbsp;With DropletIO as the core technology, we are building a desktop machine to automate small volume liquid handling. The programmable system is capable of manipulating tiny droplets of biological samples/reagents with precise volume control. Our desktop machine will reduce lab equipment cost, eliminate human errors, and allow for the scaling of complex biological experiments from lab to production with ease. With the machine we want to bring down the cost of running assays from $10,000 to $10 to bring healthcare to billions of people. </p><p>Our solution replaces current liquid handling built on leaky tubes and unreliable pumping mechanisms with a solid-state device. Our digital device is entirely electronic and compact, a system that inexpensively scales to address complex experiments with small volume liquids. Due to its digital nature, a biologist using our system could define biological protocols by programming, executing, and sharing them. Thus, operations would scale digitally from lab to production. The system furthers cost savings by producing significantly less disposable waste such as pipette tips.</p>",,,2018-01-16 21:51:37.104,True,2017-01-01,DropletIO,LAB-INSIDERS,http://udayan-u.com,True,Tangible Media,False
pneuduino,ishii,False,"<p>Pneuduino is a hardware platform for kids, students, artists, designers, and researchers who are interested in controlling air flow and pressure for their projects. The Pneuduino toolkit is currently used in workshops with high school or college students. While each workshop has a different focus, they all introduce concepts of air as actuator and sensor as well as different fabrication methods to create transforming artifacts. Air is one the most abundant resources on earth. By adding computation ability to air, we can create new types of materials that enable us to design robots that are soft, furniture that is adaptive, clothing that is intelligent, and art pieces that are breathing.</p>",,--Choose Location,2016-12-05 00:17:03.712,True,2015-01-01,Pneuduino,PUBLIC,,True,Tangible Media,False
chainform,ishii,False,"<p>ChainFORM is a modular hardware system for designing linear shape-changing interfaces. Each module is developed based on a servo motor with added flexible circuit board, and is capable of touch detection, visual output, angular sensing, and motor actuation. Moreover, because each module can communicate with other modules linearly, it allows users and designers to adjust and customize the length of the interface. Using the functionality of the hardware system, we propose a wide range of applications, including line-based shape changing display, reconfigurable stylus, rapid prototyping tool for actuated crafts, and customizable haptic glove. We conducted a technical evaluation and a user study to explore capabilities and potential requirements for future improvement.</p>",,--Choose Location,2018-05-04 15:33:24.390,True,2015-09-01,ChainFORM,PUBLIC,,True,Tangible Media,False
materiable-rendering-dynamic-material-properties-with-shape-changing-interfaces,ishii,False,"<p>Shape-changing interfaces give physical shape to digital data so that users can feel and manipulate data with their hands and body. Combining techniques from haptics with the field of shape-changing interfaces, we propose a technique to build a perceptive model of material properties by taking advantage of the shape display's ability to dynamically render flexibility, elasticity, and viscosity in response to the direct manipulation of any computationally rendered physical shape. Using a computer-generated relationship between the manipulated pins and nearby pins in the shape display, we can create human proprioception of various material properties. Our results show that users can identify varying material properties in our simulations through direct manipulation, and that this perception is gathered mainly from their physical relationship (touch) with the shape display and its dynamic movements.</p>",,--Choose Location,2016-12-16 20:08:16.037,True,2015-09-01,Materiable,PUBLIC,,True,Tangible Media,False
sensorknits,ishii,False,"<p>Digital machine knitting is a highly programmable manufacturing process that has been utilized to produce apparel, accessories, and footwear.&nbsp;Our research presents three classes of textile sensors exploiting the resistive, piezoresistive, and capacitive&nbsp;properties of various textile structures enabled by machine knitting with conductive yarn.&nbsp;</p>",,,2019-04-09 13:53:02.936,True,2017-09-01,SensorKnits: Architecting textile sensors with machine knitting,PUBLIC,,True,Tangible Media,False
programmable-droplets,ishii,False,"​<p>State-of-the art liquid handling systems are generally pump-driven systems connected with valves and tubes. These systems are manually assembled, expensive, and unreliable. With the growth of the genomic and drug industries, we are moving toward increasingly complex biological processes requiring very small volume liquid manipulation capability. </p><p>Manually assembled mechanical systems do not scale to parallel manipulation of large amounts of small volume liquids. However, the electronics industry has demonstrated how to build robust integrated systems for information manipulation. With this as our motivation, we look toward electronics and integrated circuits to bring miniaturization, complexity, and integration to enable the next generation of biology.</p>",,,2018-10-19 15:44:35.016,True,2016-01-01,Programmable Droplets,PUBLIC,,True,Tangible Media,False
scale2018,ishii,False,"<p>The SCALE platform adds <b>interactivity and trackability to everyday objects.&nbsp;</b>This load-sensitive surface is composed of only three loadcels beneath the surface, so that the system achieves <b>unobtrusive sensing</b>.</p><p>To capture and analyze human activities without any body-attached devices, a bunch of enabling technologies has been proposed. However, there have still been limitations in conventional approaches: RFID requires costly modification of the objects, and computer vision causes privacy issues. To address these problems, we propose ubiquitous load-sensing systems, networked platforms for giving interactivity and trackability to everyday objects by applying object-localization technology to every surface in our living environment and connecting all platforms with each other. Each platform is capable of localizing the position of external objects, utilizing pressure patterns acquired over time from multiple sensors as variables for the detection of location and weight. In addition, we treat the mass of an object as a unique identifier so that the system can trace the flow or detect consumption across platforms. As applications, we introduce a prototype that makes everyday objects tangible interfaces, privacy-preserving observation systems for family through medicine pill bottles and tooth brushes, and augmentation of existing interfaces.&nbsp;</p>",,,2018-10-22 20:46:39.996,True,2018-01-01,SCALE (2018),LAB-INSIDERS,,True,Tangible Media,False
aeromorph,ishii,False,"<p>The project investigates how to make origami structure with inflatables with various materials. We introduce a universal bending mechanism that creates programmable shape-changing behaviors with paper, plastics, and fabrics. We developed a software tool that generates this bending mechanism for a given geometry, simulates its transformation, and exports the compound geometry as digital fabrication files. A custom heat-sealing head that can be mounted on usual three-axis CNC machines to precisely fabricate the designed transforming material is presented. We envision this technology could be used for designing interactive wearables and toys, and for the packaging industry.
                    
                </p><p>Visit&nbsp;<a href=""http://tangible.media.mit.edu/project/aeromorph/"">http://tangible.media.mit.edu/project/aeromorph/</a>.<br></p><p>Honorable Mention Paper Award, UIST 2016</p>",,,2017-04-24 18:56:38.724,True,2016-11-01,aeroMorph,PUBLIC,http://tangible.media.mit.edu/project/aeromorph/,True,Tangible Media,False
in-force,ishii,False,"<p>We propose a novel tangible interaction with pin-based shape display that can reproduce haptic perception of shape, material stiffness, and heterogeneous internal structures of volumetric shape. This is enabled by newly developed pin display, inFORCE, that can detect the force that is applied to each pin, and exert arbitrary force to contact body and objects at the same time. Our proposed interaction methods enabled people to ""press through"" computationally rendered shapes to understand the internal structure of 3D volumetric information. Our design space explores a range of interaction capability enabled by the Force Shape Display system including capturing physical material properties.</p>",,,2019-03-25 13:24:48.264,True,2017-10-01,inFORCE,LAB-INSIDERS,http://tangible.media.mit.edu/project/inforce/,True,Tangible Media,False
transformative-appetite,ishii,False,"<p>We developed a concept of transformative appetite, where edible 2D films made of common food materials (protein, cellulose or starch) can transform into 3D food during cooking. This transformation process is triggered by water adsorption, and it is strongly compatible with the ‘flat packaging’ concept for substantially reducing shipping costs and storage space. To develop these transformable foods, we performed material-based design, established a hybrid fabrication strategy, and conducted performance simulation. Users can customize food shape transformations through a pre-defined simulation platform, and then fabricate these designed patterns using additive manufacturing. Three application techniques are provided: &nbsp;2D-to-3D folding, hydration-induced wrapping, and temperature-induced self-fragmentation, to present the shape, texture, and interaction with food materials. Based on this concept, several dishes were created in the kitchen, to demonstrate the futuristic dining experience through materials-based interaction design.</p>",,,2019-02-14 19:43:51.271,True,2017-05-07,Transformative Appetite,PUBLIC,,True,Tangible Media,False
cilllia-3d-printed-micro-pillar-structures-for-surface-texture-actuation-and-sensing,ishii,False,"<p>In nature, hair has numerous functions such as providing warmth, adhesion, locomotion, sensing, and a sense of touch, as well as its well-known aesthetic qualities. This work presents a computational method of 3D printing hair structures. It allows us to design and generate hair geometry at 50 micrometer resolution and assign various functionalities to the hair. The ability to fabricate customized hair structures enables us to create superfine surface texture, mechanical adhesion properties, new passive actuators, and touch sensors on a 3D-printed artifact. We also present several applications to show how the 3D-printed hair can be used for designing everyday interactive objects.</p>",,--Choose Location,2019-05-23 19:18:17.084,True,2015-01-01,"Cilllia: 3D-printed micro pillar structures for surface texture, actuation, and sensing",PUBLIC,,True,Tangible Media,False
respire,ishii,False,"<p><b>""What happens if a static object starts to move and react to your gestures like a living creature?""</b></p><p><span style=""font-size: 18px; font-weight: 400;"">The pillow we know and use every night is not passive anymore: a</span>s an interaction between active and active materials, we have developed a soft pillow that can breathe with you.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">reSpire is a soft actuated pillow that breathes in synchronization with the user's respiration.&nbsp; With it, users can touch and feel their breath.</span></p><p>By projecting the breath pattern of the user onto the inflation motion of the neck pillow, reSpire not only enables the user to realize how they have been breathing, like an augmented prosthetic lung, but can also let a loved one feel your presence even when you are apart.</p>",,,2018-05-07 16:45:39.369,True,2018-02-01,reSpire: A soft actuated pillow synchronized with breath patterns for tactile telecommunication,LAB-INSIDERS,http://www.mallcong.com,True,Tangible Media,False
particles,ishii,False,"<p>PARTICLES is a modular hardware system that can dynamically control attraction forces in-between, aiming to provide malleable tangible affordances (e.g. squishing, splitting, and scooping). Each module contains Spherical Gears + Switchable Magnet to variably control the attraction force on the surface. Having a number of these modules, we aim to dynamically tune perceived material property through the attraction force control. We imagine scalable applications such as a novel tangible display for data exploration in medical and geo-science. Another application can be an educational tool that people can learn nano-scale molecule structures and attraction forces through tangible interaction with particles.&nbsp;</p>",,,2018-05-23 19:46:16.346,True,2017-10-01,PARTICLES,LAB-INSIDERS,,True,Tangible Media,False
leakyphones,ishii,False,"<p><strong>LeakyPhones</strong>&nbsp;is a public/private headset that was designed to encourage face-to-face interactions, curiosity, and&nbsp;healthier&nbsp;social skills by letting users ""peek"" into each other's music just by looking at one another.&nbsp;</p><p>Gaze is an important social signal in human interaction.&nbsp;Though its interpretation may vary across cultures, it is generally agreed that eye contact indicates interest&nbsp;and the point of attention in a conversation. Despite this, many common personal computing technologies, such as our smartphones and headphones, require significant visual and auditory attention thereby inhibiting our ability&nbsp;to interact with others. LeakyPhones offers a new approach for addressing this challenge.&nbsp;</p>",,,2019-02-26 20:28:41.011,True,2017-03-16,LeakyPhones,PUBLIC,,True,Tangible Media,False
kinetix,ishii,False,"<p>kinetiX is a transformable material featuring a design that resembles a cellular structure. It consists of rigid plates or rods and elastic hinges. These modular elements can be combined in a wide variety of ways and assembled into multifarious forms.</p><p>This project describes a group of auxetic-inspired material structures that can transform into various shapes upon compression. While the majority of the studies of auxetic materials focus on their mechanical properties and topological variations, our work proposes a parametric design approach that gives auxetic structures the ability to deform beyond shrinking or expanding. To do so, we see the auxetic structure as a parametric four-bar linkage. We developed four cellular-based material structure units composed of rigid plates and elastic/rotary hinges. Different compositions of these units lead to a variety of tunable shape-changing possibilities, such as uniform scaling, shearing, bending and rotating. By tessellating those transformations together, we can create various higher level transformations for design. The simulation is validated by the 3D printed structures.&nbsp;</p><p>&nbsp;We hope this work will inspire research in metamaterials design, shape-changing materials, and transformable architecture.</p>",,,2019-01-24 14:26:13.216,True,2017-03-01,kinetiX,PUBLIC,,True,Tangible Media,False
unimorph-thin-film-shape-changing-interfaces,heibeck,False,"<p>uniMorph is an enabling technology for thin-sheet shape-changing interfaces. It affords actuation powered by environmental temperature changes, as well as computational control of actuation. Additionally, the composite allows a seamless integration of electronic components. We developed a digital fabrication method to enable easy creation of composites with actuation patterns of high accuracy.</p>",2015-09-01,--Choose Location,2016-12-05 00:16:12.153,True,2014-09-01,uniMorph: Thin-Film Shape-Changing Interfaces,PUBLIC,,False,Tangible Media,False
pneuduino,heibeck,False,"<p>Pneuduino is a hardware platform for kids, students, artists, designers, and researchers who are interested in controlling air flow and pressure for their projects. The Pneuduino toolkit is currently used in workshops with high school or college students. While each workshop has a different focus, they all introduce concepts of air as actuator and sensor as well as different fabrication methods to create transforming artifacts. Air is one the most abundant resources on earth. By adding computation ability to air, we can create new types of materials that enable us to design robots that are soft, furniture that is adaptive, clothing that is intelligent, and art pieces that are breathing.</p>",,--Choose Location,2016-12-05 00:17:03.712,True,2015-01-01,Pneuduino,PUBLIC,,True,Tangible Media,False
aeromorph,heibeck,False,"<p>The project investigates how to make origami structure with inflatables with various materials. We introduce a universal bending mechanism that creates programmable shape-changing behaviors with paper, plastics, and fabrics. We developed a software tool that generates this bending mechanism for a given geometry, simulates its transformation, and exports the compound geometry as digital fabrication files. A custom heat-sealing head that can be mounted on usual three-axis CNC machines to precisely fabricate the designed transforming material is presented. We envision this technology could be used for designing interactive wearables and toys, and for the packaging industry.
                    
                </p><p>Visit&nbsp;<a href=""http://tangible.media.mit.edu/project/aeromorph/"">http://tangible.media.mit.edu/project/aeromorph/</a>.<br></p><p>Honorable Mention Paper Award, UIST 2016</p>",,,2017-04-24 18:56:38.724,True,2016-11-01,aeroMorph,PUBLIC,http://tangible.media.mit.edu/project/aeromorph/,True,Tangible Media,False
unimorph-thin-film-shape-changing-interfaces,basheer,False,"<p>uniMorph is an enabling technology for thin-sheet shape-changing interfaces. It affords actuation powered by environmental temperature changes, as well as computational control of actuation. Additionally, the composite allows a seamless integration of electronic components. We developed a digital fabrication method to enable easy creation of composites with actuation patterns of high accuracy.</p>",2015-09-01,--Choose Location,2016-12-05 00:16:12.153,True,2014-09-01,uniMorph: Thin-Film Shape-Changing Interfaces,PUBLIC,,False,Tangible Media,False
mmodm,basheer,False,"<p>MMODM is an online drum machine based on the Twitter streaming API, using tweets from around the world to create and perform musical sequences together in real time. Users anywhere can express 16-beat note sequences across 26 different instruments, using plain-text tweets from any device. Meanwhile, users on the site itself can use the graphical interface to locally DJ the rhythm, filters, and sequence blending. By harnessing this duo of website and Twitter network, MMODM enables a whole new scale of synchronous musical collaboration between users locally, remotely, across a wide variety of computing devices, and across a variety of cultures.</p>",2017-06-01,--Choose Location,2017-05-17 22:49:11.048,True,2015-09-01,MMODM: Massively Multiplayer Online Drum Machine,PUBLIC,http://mmodm.co/,False,Tangible Media,False
unimorph-thin-film-shape-changing-interfaces,clarkds,False,"<p>uniMorph is an enabling technology for thin-sheet shape-changing interfaces. It affords actuation powered by environmental temperature changes, as well as computational control of actuation. Additionally, the composite allows a seamless integration of electronic components. We developed a digital fabrication method to enable easy creation of composites with actuation patterns of high accuracy.</p>",2015-09-01,--Choose Location,2016-12-05 00:16:12.153,True,2014-09-01,uniMorph: Thin-Film Shape-Changing Interfaces,PUBLIC,,False,Tangible Media,False
in-force,clarkds,False,"<p>We propose a novel tangible interaction with pin-based shape display that can reproduce haptic perception of shape, material stiffness, and heterogeneous internal structures of volumetric shape. This is enabled by newly developed pin display, inFORCE, that can detect the force that is applied to each pin, and exert arbitrary force to contact body and objects at the same time. Our proposed interaction methods enabled people to ""press through"" computationally rendered shapes to understand the internal structure of 3D volumetric information. Our design space explores a range of interaction capability enabled by the Force Shape Display system including capturing physical material properties.</p>",,,2019-03-25 13:24:48.264,True,2017-10-01,inFORCE,LAB-INSIDERS,http://tangible.media.mit.edu/project/inforce/,True,Tangible Media,False
jajan-remote-language-learning-in-shared-virtual-space,kevinw,False,"<p>JaJan! is a telepresence system wherein remote users can learn a second language together while sharing the same virtual environment. JaJan! can support five aspects of language learning: learning in context; personalization of learning materials; learning with cultural information; enacting language-learning scenarios; and supporting creativity and collaboration. Although JaJan! is still in an early stage, we are confident that it will bring profound changes to the ways in which we experience language learning and can make a great contribution to the field of second language education.</p>",2015-09-30,--Choose Location,2017-09-05 13:19:06.022,True,2014-01-01,JaJan!: Remote Language Learning in Shared Virtual Space,PUBLIC,,False,Fluid Interfaces,False
handson-a-gestural-system-for-remote-collaboration-using-augmented-reality,kevinw,False,"<p>2D screens, even stereoscopic ones, limit our ability to interact with and collaborate on 3D data. We believe that an augmented reality solution, where 3D data is seamlessly integrated in the real world, is promising. We are exploring a collaborative augmented reality system for visualizing and manipulating 3D data using a head-mounted, see-through display, that allows for communication and data manipulation using simple hand gestures.</p>",,--Choose Location,2019-04-17 20:10:59.187,True,2014-01-01,HandsOn: A gestural system for remote collaboration using augmented reality,PUBLIC,,True,Fluid Interfaces,False
jajan-remote-language-learning-in-shared-virtual-space,pattie,False,"<p>JaJan! is a telepresence system wherein remote users can learn a second language together while sharing the same virtual environment. JaJan! can support five aspects of language learning: learning in context; personalization of learning materials; learning with cultural information; enacting language-learning scenarios; and supporting creativity and collaboration. Although JaJan! is still in an early stage, we are confident that it will bring profound changes to the ways in which we experience language learning and can make a great contribution to the field of second language education.</p>",2015-09-30,--Choose Location,2017-09-05 13:19:06.022,True,2014-01-01,JaJan!: Remote Language Learning in Shared Virtual Space,PUBLIC,,False,Fluid Interfaces,False
cardio,pattie,False,"<p>We are designing interfaces to enhance driver self-awareness through subliminal visual feedback and shape-changing materials. Advancements in sensing technologies make it possible to measure physiological data in the car environment, opening up the possibility of harnessing such data for just-in-time feedback to drivers. </p>",2015-01-01,--Choose Location,2016-12-05 00:16:17.267,True,2014-09-01,CarDio,PUBLIC,,False,Fluid Interfaces,False
glassprov-improv-comedy-system,pattie,False,"<p>As part of a Google-sponsored Glass developer event, we created a Glass-enabled improv comedy show together with noted comedians from ImprovBoston and Big Bang Improv. The actors, all wearing Glass, received cues in real time in the course of their improvisation. In contrast with the traditional model for improv comedy, punctuated by ""freezing"" and audience members shouting suggestions, using Glass allowed actors to seamlessly integrate audience suggestions. Actors and audience members agreed that this was a fresh take on improv comedy. It was a powerful demonstration that cues on Glass are suitable for performance: actors could become aware of the cues without having their concentration or flow interrupted, and then view them at an appropriate time thereafter.</p>",2015-09-30,--Choose Location,2017-09-05 19:36:08.241,True,2014-01-01,GlassProv Improv Comedy System,PUBLIC,,False,Fluid Interfaces,False
hybrid-objects,pattie,False,<p>A web technology-based update of Smarter Objects and the reality editor project.</p>,2015-09-01,--Choose Location,2016-12-05 00:16:30.529,True,2014-01-01,Hybrid Objects,PUBLIC,,False,Fluid Interfaces,False
wordsense-learning-language-in-the-wild,pattie,False,"<p>As more powerful and spatially aware Augmented Reality devices become available, we can leverage the user’s context to embed reality with audio-visual content that enables learning in the wild. Second-language learners can explore their environment to acquire new vocabulary relevant to their current location. Items are identified, ""labeled"" and spoken out loud, allowing users to make meaningful connections between objects and words. As time goes on, word groups and sentences can be customized to the user's current level of competence. When desired, a remote expert can join in real-time for a more interactive ""tag-along"" learning experience.<br></p>",2018-05-31,,2018-08-20 16:21:23.964,True,2016-09-15,WordSense,PUBLIC,,False,Fluid Interfaces,False
tagme,pattie,False,"<p>TagMe is an end-user toolkit for easy creation of responsive objects and environments. It consists of a wearable device that recognizes the object or surface the user is touching. The user can make everyday objects come to life through the use of RFID tag stickers, which are read by an RFID bracelet whenever the user touches the object. We present a novel approach to create simple and customizable rules based on emotional attachment to objects and social interactions of people. Using this simple technology, the user can extend their application interfaces to include physical objects and surfaces into their personal environment, allowing people to communicate through everyday objects in very low-effort ways.</p>",2018-01-01,--Choose Location,2018-10-11 18:48:35.308,True,2014-09-01,TagMe,PUBLIC,,False,Fluid Interfaces,False
remot-io-a-system-for-reaching-into-the-environment-of-a-remote-collaborator,pattie,False,"<p>Remot-IO is a system for mobile collaboration and remote assistance around Internet-connected devices. It uses two head-mounted displays, cameras, and depth sensors to enable a remote expert to be immersed in a local user's point of view, and to control devices in that user's environment. The remote expert can provide guidance through hand gestures that appear in real time in the local user's field of view as superimposed 3D hands. In addition, the remote expert can operate devices in the novice's environment and bring about physical changes by using the same hand gestures the novice would use. We describe a smart radio where the knobs of the radio can be controlled by local and remote users. Moreover, the user can visualize, interact, and modify properties of sound waves in real time by using intuitive hand gestures.</p>",2016-12-31,--Choose Location,2018-08-20 16:23:49.991,True,2015-01-01,Remot-IO: A System for Reaching into the Environment of a Remote Collaborator,PUBLIC,,False,Fluid Interfaces,False
invisibilia-revealing-invisible-data-as-a-tool-for-experiential-learning,pattie,False,"<p>Invisibilia seeks to explore the use of Augmented Reality (AR), head-mounted displays (HMD), and depth cameras to create a system that makes invisible data from our environment visible, combining widely accessible hardware to visualize layers of information on top of the physical world. Using our implemented prototype, the user can visualize, interact with, and modify properties of sound waves in real time by using intuitive hand gestures. Thus, the system supports experiential learning about certain physics phenomena through observation and hands-on experimentation.</p>",2016-08-31,--Choose Location,2018-08-20 16:39:45.640,True,2015-01-01,Invisibilia: Revealing Invisible Data as a Tool for Experiential Learning,PUBLIC,,False,Fluid Interfaces,False
screenspire,pattie,False,"<p>Screen interactions have been shown to contribute to increases in stress, anxiety, and deficiencies in breathing patterns. Since better respiration patterns can have a positive impact on wellbeing, ScreenSpire improves respiration patterns during information work using subliminal biofeedback. By using subtle graphical variations that are tuned to attempt to influence the user subconsciously, user distraction and cognitive load are minimized. To enable a truly seamless interaction, we have adapted an RF-based sensor (ResMed S+ sleep sensor) to serve as a screen-mounted contact-free and respiration sensor. Traditionally, respiration sensing is achieved with either invasive or on-skin sensors (such as a chest belt); having a contact-free sensor contributes to increased ease, comfort, and user compliance, since no special actions are required from the user.</p>",2016-12-31,--Choose Location,2017-08-25 11:59:51.368,True,2015-01-01,ScreenSpire,PUBLIC,,False,Fluid Interfaces,False
masque,pattie,False,"<p>&nbsp;When the body senses itself internally and localizes its actions, it provides the basis for a material sense of self-existence. At the same time, the mind registers the sense of an agency with free will, the sense of being, the cause of voluntary action. Among all interoceptive experiences, respiration is the only one that we can regulate directly. There are many&nbsp;psychophysical breathing exercises&nbsp;to help self-regulation and reflection, that, combined with meditation and yoga, are designed to restore natural, smooth breathing appropriate to the physical needs of the body.&nbsp;</p>",2018-05-31,,2018-10-20 21:47:28.195,True,2017-01-01,Masque,PUBLIC,,False,Fluid Interfaces,False
reality-editor-20,pattie,False,"<p>The Reality Editor is a web browser for the physical world: Point your phone or tablet at a physical object and an interface pops up with information about that object as well as services related to that object. The Reality Editor platform is open and entirely based on web standards making it easy for anyone to create Reality Editor enabled objects as well as Reality Editor applications that integrate the physical and digital world in one experience.<br></p><p>Reality Editor version 2.0<br></p><p>&nbsp;<b><a href=""http://realityeditor.org"">Reality Editor &nbsp;version 2.0</a>&nbsp;</b>is now available for download and adds the following features:</p><ul><li><b>World Wide Web</b> conform content creation.<br></li><li><span style=""font-size: 18px;""><b>Spatial Search</b> -&nbsp;</span>Instantly browse through relevant information in the physical world around you. you to browse reality.<br></li><li><span style=""font-size: 18px;""><b>Bi-Directional AR</b> - A real-time interactions system.</span></li><li><b style=""font-size: 18px;"">Private and Decentralized</b><span style=""font-size: 18px;""> infrastructure for connecting the IoT objects.</span><br></li><li><span style=""font-size: 18px;""><b>Logic Crafting</b> - A visual programming language designed for Augmented Reality.<br></span></li></ul><p>The &nbsp;Reality Editor works on iOS and you can get it <a href=""https://itunes.apple.com/us/app/reality-editor/id997820179""><b>here</b></a>.&nbsp;<span style=""font-size: 18px;"">Try</span><span style=""font-size: 18px;"">&nbsp;it out with our <a href=""https://www.dropbox.com/s/3vd1d2v9bmm7e7s/Reality%20Editor.dmg?dl=1""><b>Starter App</b></a> and some Philips Hue Lights or the Lego WeDo 2.0. Learn more about Logic Crafting in our <a href=""http://realityeditor.org/getting-started/""><b>User Interface 101</b></a>.</span></p><p></p>",2017-12-31,,2018-10-11 18:46:45.692,True,2017-05-22,Reality Editor 2.0,PUBLIC,http://www.valentinheun.com,False,Fluid Interfaces,False
holobits,pattie,False,"<p>The onset of Mixed Reality as a platform offers the opportunity to create new, playful paradigms for building and fostering creativity. The Holobits application leverages the tried and tested features of physical block building platforms like LEGO and introduces the benefits of building in mixed environments to support making and storytelling. The proposed system combines the hand tracking capabilities of the Leap Motion with the spatial mapping of Hololens to enable hands-on building experiences with virtual blocks, denoted as “bits.” These blocks have different attributes and characteristics that determine how they look and behave within the mixed reality building space. The platform also allows users to share their creative building process in a frame-by-frame fashion that enables remixing and reflection on every play session. Holobits allows users to record their interactions with their creations to make animated environments with ease and support storytelling. Another way to enable collaboration is to let kids share models (or download someone else's creation in your space), allowing multiple users to build in a shared physical space or over distances, where a player can “hop” into the physical space of the Hololens user using virtual reality. Last but not least, we intend to integrate the Scratch visual programming toolkit into the Holobits platform to allow users to orchestrate their virtual creations and create the ultimate interactive stories.</p>",2018-05-31,,2018-11-30 17:21:40.954,True,2017-02-13,Holobits: Creativity and fun in Mixed Reality,PUBLIC,,False,Fluid Interfaces,False
kicksoul-a-wearable-system-for-foot-interactions-with-digital-devices,pattie,False,<p>KickSoul is a wearable device that maps natural foot movements into inputs for digital devices. It consists of an insole with embedded sensors that track movements and trigger actions in devices that surround us. We present a novel approach to use our feet as input devices in mobile situations when our hands are busy. We analyze the foot's natural movements and their meaning before activating an action.</p>,2017-08-01,--Choose Location,2017-08-07 14:36:03.410,True,2015-01-01,KickSoul: A Wearable System for Foot Interactions with Digital Devices,PUBLIC,,False,Fluid Interfaces,False
d-Abyss,pattie,False,"<p><b>Can tattoos embrace technology in order to make the skin interactive?</b></p><p>The DermalAbyss project is the result of a collaboration between MIT researchers Katia Vega, Xin Liu, Viirj Kan and Nick Barry and Harvard Medical School researchers Ali Yetisen and Nan Jiang.&nbsp;<br></p><p>DermalAbyss is a proof-of-concept that presents a novel approach to bio-interfaces in which the body surface is rendered an interactive display. Traditional tattoo inks are replaced with biosensors whose colors change in response to variations in the interstitial fluid. It blends advances in biotechnology with traditional methods in tattoo artistry.&nbsp;</p><p>This is a research project, and there are currently no plans to develop Dermal Abyss as a product or to pursue clinical trials.<br></p>",2017-05-31,,2018-04-27 17:45:10.726,True,2016-06-01,DermalAbyss: Possibilities of Biosensors as a Tattooed Interface,PUBLIC,,False,Fluid Interfaces,False
showme-immersive-remote-collaboration-system-with-3d-hand-gestures,pattie,False,"<p>ShowMe is an immersive mobile collaboration system that allows remote users to communicate with peers using video, audio, and gestures. With this research, we explore the use of head-mounted displays and depth sensor cameras to create a system that (1) enables remote users to be immersed in another person's view, and (2) offers a new way of sending and receiving the guidance of an expert through 3D hand gestures. With our system, both users are surrounded in the same physical environment and can perceive real-time inputs from each other. </p>",2017-08-31,--Choose Location,2018-10-12 16:45:04.639,True,2014-01-01,ShowMe: Immersive Remote Collaboration System with 3D Hand Gestures,PUBLIC,,False,Fluid Interfaces,False
social-textiles,pattie,False,"<p>The way we represent ourselves in social media is intangible. What we choose to wear is public to the world and we are aware of it. In contrast, what we post online about ourselves reaches thousands of people and generates social consequences, but it doesn’t feel that way. Is the current form of social media really making our relationships better? Current technologies are good at connecting people at a distance, but less so at connecting them within the same environment.</p><p>Social textiles embodies who you are and dynamically reflects your shared interests with people nearby. It enables you to gain access to communities of people in the physical world and enhances social affordances and icebreaking interactions through wearable social messaging.</p><p>Social Textiles embody who you are and dynamically reflect your shared interests with people nearby. They enable you to gain access to communities of people in the physical world and enhance social affordances and icebreaking interactions through wearable social messaging. Social Textiles can connect community members with niche interests, philosophical beliefs, personalities, emotional statuses, and ethical views. They have the potential to enable members to bypass superficial or generic interests through ""filtering"" individuals, in order to tune social experiences toward people who are more compatible.</p>",2015-01-01,--Choose Location,2018-10-12 16:50:34.930,True,2014-09-01,Social Textiles,PUBLIC,,False,Fluid Interfaces,False
fluxa,pattie,False,"<p>Fluxa is a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a transient wearable display to foster richer self-expression and communication in daily life . It can be used to enhance existing social gestures such as handwaving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a decoration device that generates images around dancing bodies.</p>",2017-11-30,--Choose Location,2018-10-12 16:57:48.806,True,2016-01-01,Fluxa,PUBLIC,,False,Fluid Interfaces,False
oasis,pattie,False,"<p>Oasis won <a href=""http://www.edisonawards.com/winners2017.php"">Silver at the Edison Awards 2017</a>.</p><p>Oasis received the <a href=""https://www.vrst2016.lrz.de/keynotes-award/"">Best Paper Award</a> at VRST 2016.<br></p><p>Oasis is a novel system for automatically generating immersive and interactive virtual reality environments using the real world as a template. The system captures indoor scenes in 3D, detects obstacles like furniture and walls, and maps walkable areas to enable real-walking in the generated virtual environment. Depth data is additionally used for recognizing and tracking objects during the VR experience. The detected objects are paired with virtual counterparts to leverage the physicality of the real world for a tactile experience. Our system allows a casual user to easily create and experience VR in any indoor space of arbitrary size and shape without requiring specialized equipment or training.</p><p>Oasis can be used, for example, to create storyspaces where friends and family can remotely participate in a session of storytelling around the campfire. The freedom to move around and interact with the virtual world allows for a new form of storytelling when combined with &nbsp;traditional &nbsp;narration techniques like vocalization, movement, and gestures. We call this <b>human-in-the-loop storytelling</b>,&nbsp;distinguishing it from current VR storytelling experiences where the software system is the storyteller.</p>",2017-08-31,,2018-08-20 16:24:31.868,True,2016-02-01,Oasis,PUBLIC,,False,Fluid Interfaces,False
skrin,pattie,False,"<p>Skrin is an exploration project on digitalized body skin surface using embedded electronics and prosthetics. Human skin is a means for protection, a mediator of our senses, and a presentation of our selves. Through several projects, we expand the expression capacity of the body's surface and emphasize the dynamic aesthetics of body texture by technological means.&nbsp;<span style=""font-size: 18px;"">Working with conventional special effect makeup artists, we “hide” electronics into silicone which is applied onto skin and covered by cosmetics. The digitalized skin surface is connected with the affective experience, while the illuminated body is a representation of internal state.</span></p><p>Working with bionic pop artist Viktoria Modesta, we deployed the project in Music Tech Festival Berlin 2016 and transformed her body as a canvas along with the performance.</p>",2017-06-30,--Choose Location,2018-12-05 14:35:37.656,True,2016-01-01,Skrin,PUBLIC,,False,Fluid Interfaces,False
open-hybrid,pattie,False,"<p>Open Hybrid is an open source augmented reality platform for physical computing and Internet of Things. It is based on the web and Arduino.</p><p>This platform allows you to:</p><ul><li>Create augmented reality content with HTML tools</li><li>Create augmented reality without any knowledge of 3D programming</li><li>Connect the functionality of objects with a simple drag and drop paradigm</li><li>Program your physical hybrid objects and connect them to the AR-UI using Arduino</li></ul><p>Learn more about this project at: <a href=""https://web.archive.org/web/20180314110710/http://www.openhybrid.org/"">http://www.openhybrid.org</a></p>",2017-06-30,--Choose Location,2018-10-20 23:06:14.400,True,2015-01-01,Open Hybrid,PUBLIC,,False,Fluid Interfaces,False
tree,pattie,False,"<p><i>Tree</i> is a virtual experience that transforms you into a rainforest tree. With your arms as branches and body as the trunk, you experience the tree’s growth from a seedling into its fullest form and witness its fate firsthand.&nbsp;Collaborating with director Milica Zec and Winslow Porter, we designed and constructed the entire tactile experience throughout the film. With precisely controlled physical elements including vibration, heat, fan and body haptics, the team created a fully immersive virtual reality storytelling to, where the audience no longer watches but is transformed into a new identity, a giant tree in the peruvian rainforest.</p><p><i>Tree</i> debuted at Sundance Film Festival 2017 New Frontier and also had its presentation in Tribeca Film Festival 2017. &nbsp;</p><p>The project is part of our research about body ownership illusion in virtual reality (early project: <a href=""https://www.media.mit.edu/projects/treesense/overview/"">TreeSense</a>).&nbsp;The tactile experience is crucial for establishing a body ownership illusion instead of restricting the experience to the visual world. We aim to have the audience not just see, but feel and believe ""being"" a tree.&nbsp;</p>",2017-06-30,,2018-10-20 22:01:11.764,True,2017-01-18,Tree,PUBLIC,,False,Fluid Interfaces,False
auris-creating-affective-virtual-spaces-from-music,pattie,False,"<p>Light, color, texture, geometry and other architectural design elements have been shown to produce predictable and measurable effects on our minds, brains, and bodies. This suggests spaces that can mirror or transform feelings or serve specific purposes like improving learning or enhancing wellbeing can be designed. With Auris, we take a first step towards the design of such spaces in virtual reality by attempting to automatically generate affective virtual environments that can affect our emotions. The input to Auris is a song (audio and lyrics) and the output is a VR world that encapsulates the mood and content of the song.</p>",2018-07-31,,2018-08-20 16:22:53.032,True,2017-02-01,Auris: Creating Affective Virtual Spaces from Music,PUBLIC,,False,Fluid Interfaces,False
cue,pattie,False,"<h2>Exploring contextual multimodal cues as memory aids&nbsp;</h2><p><b>Objective&nbsp;<br></b></p><p>We are exploring the potential of proximity-triggered contextual audio and visual cues to help early-stage Alzheimer’s patients&nbsp;recall familiar people and places. In particular, we are using proximity beacons to determine when the user is physically close to another person, such as a loved one. The beacons will then trigger cues in the form of:</p><ol><li>audio conveying contextual information such as name, relationship, time/place/details of last interaction;</li><li>images and video (using AR) showing previous interactions along with text displaying contextual information; and</li><li>music in the form of specific songs associated with specific individuals.</li></ol><p><b>Research Questions</b></p><p>We’re interested in tackling the following questions:</p><ul><li>Which cue modalities are the most effective in improving recognition in early-stage Alzheimer’s patients?</li><li>What advantages and challenges are afforded by each of the different modalities?</li></ul>",2017-12-20,,2018-12-14 17:56:32.014,True,2017-09-22,Cue,LAB-INSIDERS,,False,Fluid Interfaces,False
reality-editor,pattie,False,"<p>The Reality Editor is a new kind of tool for empowering you to connect and manipulate the functionality of physical objects. Just point the camera of your smartphone at an object and its invisible capabilities will become visible for you to edit. Drag a virtual line from one object to another and create a new relationship between these objects. With this simplicity, you are able to master the entire scope of connected objects.</p>",2017-06-30,--Choose Location,2018-04-30 14:19:37.322,True,2014-01-01,Reality Editor,PUBLIC,http://www.realityeditor.org,False,Fluid Interfaces,False
words-in-motion,pattie,False,"<p>Embodied theories of language propose that the way we communicate verbally is grounded in our bodies. Nevertheless, the way a second language is conventionally taught does not capitalize on kinesthetic modalities. The tracking capabilities of room-scale virtual reality systems afford a way to incorporate kinesthetic learning in language education.&nbsp; Words in Motion is a virtual reality language learning system that reinforces associations between word-action pairs by recognizing a student’s movements and presenting the corresponding name of the performed action in the target language. Experiments with Words in Motion suggest that the kinesthetic approach in virtual reality has less immediate learning gain in comparison to a text-only condition. However, virtual kinesthetic learners showed significantly higher retention rates after a week of exposure. Positive correlation between the times a word-action pair was executed and the times a word was remembered by the subjects, supports the premise that virtual reality can impact language learning by leveraging kinesthetic elements.</p>",2018-05-31,,2018-08-20 16:26:03.600,True,2017-02-13,Words in Motion,PUBLIC,,False,Fluid Interfaces,False
inner-child,pattie,False,"<p>Children can be better at learning than adults. Socio-psychological factors often inhibit adults in learning environments; they are afraid to make mistakes or look silly around their peers and as a result don’t engage with the material as a child would. In language learning, this becomes even more prominent, as children are less afraid of making weird sounds or mispronouncing new words, whereas older students hesitate to speak out loud.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Not only are children less constrained by socio-psychological pressures, but they tend to be naturally more curious and open to new ideas when learning. They approach the task of learning, in many ways, differently than an adult would.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: 400;"">Inner Child intends to tap into the potential benefits of changing the way we perceive ourselves by altering our body and environment in virtual reality.&nbsp;Inner Child is a virtual reality experience developed on the HTC Vive that embodies users in the virtual avatar of a child. The platform leverages the tracking capabilities of room-scale virtual reality to create the illusion of feeling younger, so that learners can approach learning with a more childlike mindset.&nbsp;</span></p><p>Results from our user study suggest that simply having a virtual body impacts the acquisition of vocabulary in virtual reality in a positive way. Furthermore, the illusion of being younger in VR improved participants’ performance in subsequent recall tests both immediately and one week after exposure to the system, which was dependent on the type of body a participant inhabited in the virtual scenario. These results can radically change the way we learn in the classrooms of the future, suggesting that we can alter our notion of self to enhance the way we approach learning on a subconscious level.&nbsp;</p>",2018-05-31,,2018-08-20 16:27:41.575,True,2018-01-01,Inner Child,LAB-INSIDERS,,False,Fluid Interfaces,False
breathvr,pattie,False,"<p>Breathing actions are used to augment controller-based input by giving superpowers to players in two VR games. Blowing out long and strong turns you into a fire-breathing dragon, while holding your breath sends you into stealth mode.&nbsp; By using&nbsp; breathing as a directly controlled physiological signal, BreathVR can facilitate unique and engaging play experiences through natural interaction in single and multiplayer virtual reality games. Paper available here:&nbsp;<a href=""http://web.media.mit.edu/~sra/breathvr.html"">http://web.media.mit.edu/~sra/breathvr.html</a></p>",2018-07-31,,2018-08-20 16:27:18.375,True,2017-08-16,BreathVR,PUBLIC,,False,Fluid Interfaces,False
galvr-a-novel-collaboration-interface-using-gvs,pattie,False,"<p>GVS or galvanic vestibular stimulation is a technology that directly affects a user's vestibular system by altering their sense of balance and direction. It works through electrical stimulation via electrodes placed on the mastoid bones behind each ear. In standing users, GVS evokes a prolonged ""galvanic body sway."" In walking users, it affects balance and causes users to stagger in the anodal direction. However, in walking users, with their head pitched forward, it causes them to turn smoothly from their planned trajectory in the anodal direction. Dark Room is a cooperative asymmetrical ""escape the room"" style game played by a VR and a PC user, inspired by the single-player mobile game Dark Echo. The PC user controls the walking direction of the VR user to guide them around virtual or physical obstacles. The VR player uses echolocation to detect obstacles. Video and paper available here:&nbsp;<a href=""http://web.media.mit.edu/~sra/gvs.html"">http://web.media.mit.edu/~sra/gvs.html</a></p>",2018-07-31,,2019-04-19 17:08:16.864,True,2017-08-15,A VR Collaboration Interface Using Galvanic Vestibular Stimulation,PUBLIC,,False,Fluid Interfaces,False
bin-ary-detecting-the-state-of-organic-trash-to-prevent-insalubrity,pattie,False,"<p>Bin-ary is a self-contained gas detector that analyzes organic trash odor compounds and releases a subtle burst of scent when bad odor is detected. The prototype is meant to be used as a plugin to make trash bins and dumpsters smarter and prevent insalubrity in cities and villages. In order to detect the state of organic trash we primarily focused on the chemical compounds that are produced when food starts to rot or ripen: Hydrogen Sulfide (H2S), Ethylene (C2H4) and Carbon Dioxide (CO2).</p>",2015-09-01,--Choose Location,2016-12-05 00:17:07.567,True,2015-09-01,Bin-ary: Detecting the State of Organic Trash to Prevent Insalubrity,PUBLIC,,False,Fluid Interfaces,False
cord-uis-controlling-devices-with-augmented-cables,pattie,False,"<p>Cord UIs are sensorial augmented cords that allow for simple, metaphor-rich interactions to interface with their connected devices. Cords offer a large, under-explored space for interactions, as well as unique properties and a diverse set of metaphors that make them potentially interesting tangible interfaces. We use cords as input devices and explore different interactions like tying knots, stretching, pinching, and kinking to control the flow of data and/or power. We also look at ways to use objects in combination with augmented cords to manipulate data or properties of a device. For instance, placing a clamp on a cable can obstruct the audio signal to the headphones. Using special materials such as piezo copolymer cables and stretchable cords, we built five working prototypes to showcase these interactions.</p>",2014-09-01,--Choose Location,2016-12-05 00:17:09.974,True,2014-01-01,Cord UIs: Controlling Devices with Augmented Cables,PUBLIC,,False,Fluid Interfaces,False
autoemotive,pattie,False,,,,2016-10-05 23:50:05.077,False,2015-12-01,AutoEmotive,PUBLIC,,True,Fluid Interfaces,False
food-attack,pattie,False,"<p>The rise in wearable devices and the desire to quantify various aspects of everyday activities has provided the opportunity to offer just-in-time triggers to aid in achieving pre-determined goals. While a lot is known about the effectiveness of messaging in marketing efforts, less is known about the effectiveness of these marketing techniques on in-the-moment decision-making. We designed an experiment to determine if a simple solution of using just-in-time persuasive messaging could influence participants' eating habits and what types of messaging could be most effective in this effort. Our solution utilizes a head-mounted display to present health-based messages to users as they make real-time snack choices. We are able show that this method is effective and more feasible than current efforts to influence eating habits.</p>",,--Choose Location,2016-12-05 00:16:44.086,True,2015-01-01,Food Attack,PUBLIC,,True,Fluid Interfaces,False
smilecatcher,pattie,False,"<p>Our hectic and increasingly digital lives can have a negative effect on our health and wellbeing. Some authors have argued that we socialize less frequently with other people in person and that people feel increasingly lonely. Loneliness has been shown to significantly affect health and wellbeing in a negative way. To combat this, we designed a game, SmileCatcher, which encourages players to engage in in-person, social interactions and get others to smile. Participants wear a device that takes regular pictures of what is in front of them and the system analyzes the pictures captured to detect the number of smiles.</p>",,--Choose Location,2016-12-05 00:16:44.188,True,2014-01-01,SmileCatcher,PUBLIC,,True,Fluid Interfaces,False
wearable-lab-on-body,pattie,False,"<p>Wearables are being widely researched for monitoring individual's health and wellbeing. Current generation wearable devices sense an individual's physiological data such as heart rate, respiration, electrodermal activity, and EEG,  but lack in sensing their biological counterparts, which drive the majority of an individual's physiological signals. On the other hand, biosensors for detecting biochemical markers are currently limited to one-time use, are non-continuous, and don't provide flexibility in choosing which biomarker they sense. We present ""wearable lab on body,"" a platform for active continuous monitoring of human biomarkers from the biological fluid.&nbsp;<br></p><p><i>To appear in IEEE Engineering for Biology and Medicine Society (EMBC) - Pataranutaporn et. al., 2019</i></p>",,,2019-05-10 04:27:17.379,True,2018-08-30,Wearable Lab on Body,PUBLIC,,True,Fluid Interfaces,False
scanner-grabber,pattie,False,"<p>Scanner Grabber is a digital police scanner that enables reporters to record, playback, and export audio, as well as archive public safety radio (scanner) conversations. Like a TiVo for scanners, it's an update on technology that has been stuck in the last century. It's a great tool for newsrooms. For instance, a problem for reporters is missing the beginning of an important police incident because they have stepped away from their desk at the wrong time. Scanner Grabber solves this because conversations can be played back. Also, snippets of exciting audio, for instance a police chase, can be exported and embedded online. Reporters can listen to files while writing stories, or listen to older conversations to get a more nuanced grasp of police practices or long-term trouble spots. Editors and reporters can use the tool for collaborating, or crowdsourcing/public collaboration.</p>",,--Choose Location,2016-12-05 00:16:48.984,True,2014-09-01,Scanner Grabber,PUBLIC,,True,Fluid Interfaces,False
attentivu,pattie,False,"<p>It is increasingly hard for adults and children alike to be attentive given the increasing amounts of information and distractions surrounding us. </p><p>We have developed AttentivU: a device, in a socially acceptable form factor of a pair of glasses, that a person can put on in moments when he/she wants/needs to be attentive. </p><p>The AttentivU glasses use brain activity (electroencephalography - EEG) as well as eye movements (electrooculography - EOG) sensors to measure engagement of a person in real-time and provide either audio or haptic feedback to the user when their engagement is low, thereby nudging them to become engaged again. </p><p>We have tested the first generation prototype of the device in workplace and class- room settings with over 100 subjects. We have performed experiments with people studying or working by themselves, viewing online lectures as well as listening to classroom lectures. The obtained results show that our device makes a person more attentive and produces improved learning and work performance outcomes.&nbsp;</p><p>We have now finished the first tests of the glasses (second prototype) with more than 30 subjects who were performing driving task in the simulator or using the glasses during everyday activities like reading, watching videos or writing.&nbsp;</p><p>The novelty of our system is that it is meant to be used in the moment, that is, in the context where sustained attention is necessary. In order to make real-world use possible, we have developed a socially acceptable, inconspicuous form factor: a pair of glasses that contain EEG and EOG electrodes as well as a an amplifier, Bluetooth LTE module, and a speaker for bone-conduction auditory feedback. The user can optionally receive the feedback or nudges through a wireless vibration brooch that can be attached where desired and remains invisible.&nbsp;</p><p>We envision a future in which people can decide when they want to be more attentive and can in those moments put on their AttentivU glasses to help them be focused.&nbsp;</p><p>Project Lead: Nataliya Kosmyna, Ph.D</p><p>Project Team: Caitlin Morris,&nbsp;Thanh Nguyen and Pattie Maes.</p>",,,2019-05-24 18:43:31.915,True,2018-01-01,AttentivU,PUBLIC,,True,Fluid Interfaces,False
bioessence,pattie,False,<h2><b>A wearable olfactory display that monitors cardio-respiratory information to support mental wellbeing.</b></h2><p>BioEssence is a novel wearable olfactory&nbsp;display that provides just-in-time release of scents based on&nbsp;the physiological state of the wearer. The device can release up&nbsp;to three scents and passively captures subtle chest vibrations&nbsp;associated with the beating of the heart and respiration through&nbsp;clothes.&nbsp;<br></p>,,,2019-05-11 00:16:22.469,True,2017-12-01,BioEssence,PUBLIC,http://www.judithamores.com/essence,True,Fluid Interfaces,False
stem-accessibility-tool,pattie,False,"<p>We are developing a very intuitive and interactive platform to make complex information--especially science, technology, engineering, and mathematics (STEM) material--truly accessible to blind and visually impaired students by using a tactile device with no loss of information compared with printed materials. A key goal of this project is to develop tactile information-mapping protocols through which the tactile interface can best convey educational and other graphical materials.</p>",,--Choose Location,2016-12-05 00:16:52.685,True,2014-01-01,STEM Accessibility Tool,PUBLIC,,True,Fluid Interfaces,False
watch,pattie,False,"<p>WATCH is a system that attempts to measure the possible influence that a new time-management interface will have on improving the habits of a user. Users set goals for each of the activities detected by the app. Detected activities include physical activity and time spent in pre-defined locations. An Andriod app (WATCH) on their personal phones is able to track their activities (running, walking, and sitting) as well as their GPS location. Their progress in comparison to their goals is displayed on their home screens as a pie chart.</p>",,--Choose Location,2016-12-05 00:16:58.002,True,2015-01-01,WATCH,PUBLIC,,True,Fluid Interfaces,False
enlight,pattie,False,"<p>In physics education, virtual simulations have given us the ability to show and explain phenomena that are otherwise invisible to the naked eye. However, experiments with analog devices still play an important role. They allow us to verify theories and discover ideas through experiments that are not constrained by software. What if we could combine the best of both worlds? We achieve that by building our applications on a projected augmented reality system. By projecting onto physical objects, we can paint the phenomena that are invisible. With our system, we have built ""physical playgrounds"": simulations that are projected onto the physical world and that respond to detected objects in the space. Thus, we can draw virtual field lines on real magnets, track and provide history on the location of a pendulum, or even build circuits with both physical and virtual components.</p>",,--Choose Location,2016-12-05 00:16:23.642,True,2014-01-01,Enlight,PUBLIC,,True,Fluid Interfaces,False
cocoverse,pattie,False,<h2>Real-time collaborative self-expression in virtual reality&nbsp;</h2>,,,2019-04-17 20:08:49.312,True,2016-10-03,CocoVerse: A playground for co-creation and communication in virtual reality,PUBLIC,,True,Fluid Interfaces,False
augmented-airbrush,pattie,False,"<p>We present an augmented handheld airbrush that allows unskilled painters to experience the art of spray painting. Inspired by similar smart tools for fabrication, our handheld device uses 6DOF tracking, mechanical augmentation of the airbrush trigger, and a specialized algorithm to let the painter apply color only where indicated by a reference image. It acts both as a physical spraying device and as an intelligent digital guiding tool that provides manual and computerized control. Using an inverse rendering approach allows for a new augmented painting experience with unique results. We present our novel hardware design, control software, and a discussion of the implications of human-computer collaborative painting.</p>",,--Choose Location,2016-12-05 00:17:07.027,True,2014-09-01,Augmented Airbrush,PUBLIC,,True,Fluid Interfaces,False
inflatables,pattie,False,"<p>Printflatables is a design and fabrication system for human-scale, functional and dynamic inflatable objects. The user begins with specifying an intended 3D model which is decomposed to two dimensional fabrication geometry. This forms the input for a numerically controlled contact iron that seals layers of thermoplastic fabric. </p><p>In this project, we showcase the system design in detail, the pneumatic primitives that this technique enables and merits of being able to make large, functional and dynamic pneumatic artifacts. We demonstrate the design output through multiple objects which could motivate fabrication of inflatable media and pressure-based interfaces.</p><p><a href=""http://tangible.media.mit.edu/project/printflatables/"">Project Website</a></p>",,,2019-04-17 19:31:11.620,True,2017-05-10,"Printflatables: Printing human-scale, functional, and dynamic inflatable objects",PUBLIC,http://tangible.media.mit.edu/project/printflatables/,True,Fluid Interfaces,False
the-challenge,pattie,False,"<p>Mental wellbeing is intimately tied to both social support and physical activity. The Challenge is a tool aimed at promoting social connections and decreasing sedentary activity in a workplace environment. Our system asks participants to sign up for short physical challenges and pairs them with a partner to perform the activity. Social obligation and social consensus are leveraged to promote participation. Two experiments were conducted in which participants' overall activity levels were monitored with a fitness tracker. In the first study, we show that the system can improve users' physical activity, decrease sedentary time, and promote social connection. As part of the second study, we provide a detailed social network analysis of the participants, demonstrating that users' physical activity and participation depends strongly on their social community.</p>",,--Choose Location,2016-12-05 00:16:31.390,True,2015-01-01,The Challenge,PUBLIC,,True,Fluid Interfaces,False
thinking-cap,pattie,False,"<p><span style=""font-size: 18px; font-weight: 400;"">Peoples' mindsets, meaning their beliefs about their own intellectual abilities, affect their effort and thereby their performance on tasks. The goal of this project is to investigate if we can change peoples' mindsets using a technological intervention.&nbsp;</span></p><p>The Thinking Cap is a wearable system that communicates praise for effort and ability in order to improve the resilience&nbsp;and self-esteem of the student wearing it and thus&nbsp; positively influence their motivation and academic achievements.&nbsp;</p><p><span style=""font-size: 18px; font-weight: 400;"">The Thinking Cap is built into a ""Sorting Hat"" from the Harry Potter franchise, which we equipped with an embedded electroencephalography (EEG) headset and a Bluetooth speaker. We chose this “magical” object from the well-known film/book franchise because popular press articles&nbsp;have&nbsp;suggested that people are likely to believe they possess the traits the Sorting Hat tells them they have, and consequently behave in related ways. One goal of this study is to investigate these findings in more depth. In our study we measure the self-esteem of children before and after the “intervention of the hat” to determine whether we observe any changes in their self-perception. The Sorting Hat could be replaced by any other object that a child may believe has ""magical"" powers. The hat uses established state-of-the-art Brain-Computer Interface (BCI) algorithms to recognize several mental processes like motor, auditory, or visual imagery as well as cognitive load and engagement level of the child (see also a related&nbsp; project from our group called </span><a href=""https://www.media.mit.edu/projects/attentivu/overview/"" style=""font-size: 18px; font-weight: 400;"">AttentivU</a><span style=""font-size: 18px; font-weight: 400;"">). In an initial phase, the hat is used to recognize and report on the brain patterns of the child. We use supervised and unsupervised ML algorithms to train the system by asking the user explicitly to imagine/visualize either a simple movement or an object in their head (binary classification in most of the cases). The hat ""tells"" the child, via the Bluetooth speaker embedded in the hat, which of the two things he/she is thinking about.&nbsp;We hypothesize that, by demonstrating this basic capability of the hat to recognize their brain activity, the child will develop trust in the hat’s abilities to know him or her. Thus, when the hat in a later phase praises the child for their ability or effort on a task (e.g., a math test), the child is likely to be affected by its suggestions in their future performance (""You are doing well on this test now, let's do one more!""). We hypothesize that using the hat can thus lead to improved academic performance.&nbsp;</span><br></p><p>If you are interested in participating in this study (your kid should be at least eight years old), please contact us at nkosmyna@media.mit.edu.&nbsp; &nbsp; &nbsp;</p>",,,2019-04-16 17:38:17.218,True,2017-12-01,Thinking Cap,PUBLIC,,True,Fluid Interfaces,False
elowan-a-plant-robot-hybrid,pattie,False,"<p>Elowan is a cybernetic lifeform, a plant in direct dialogue with a machine. Using its own internal electrical signals, the plant is interfaced with a robotic extension that drives it toward light.</p>",,,2018-12-06 19:07:07.895,True,2017-04-01,Elowan: A plant-robot hybrid,PUBLIC,http://harpreetsareen.com,True,Fluid Interfaces,False
chill-out,pattie,False,"<p>Temperature influences our perception and cognition both consciously and subconsciously. These effects are rooted in our bodily experiences and interactions with the environment, and are even embedded as metaphors in our language. By learning how temperature affects us in different contexts, we can make use of that knowledge to create interventions that help us with personal growth.</p><p>This project seeks to apply thermal interfaces to assist with emotion and attention regulation. Stress and attention levels can be inferred using implicit user inputs such as electrodermal activity, heart rate variability, and relative facial temperature. This information can then be used to determine appropriate thermal feedback to implicitly modify the user’s perception and aid with emotional and attention regulation in a minimally disruptive fashion.</p>",,,2018-04-25 20:05:40.570,True,2018-02-01,Chill.out,PUBLIC,,True,Fluid Interfaces,False
move-u,pattie,False,"<p>MoveU is a wearable vestibular stimulation device for providing proprioceptive haptic feedback in virtual reality (VR).&nbsp; The device induces sensations of motion corresponding to virtual motion, thereby increasing immersion in VR and reducing cybersickness.&nbsp;</p><p>MoveU non-invasively stimulates the vestibular system using a technique called galvanic vestibular stimulation (GVS).&nbsp;GVS is a specific way to elicit vestibular reflexes&nbsp;<span style=""font-size: 18px; font-weight: 400;"">using electrical current&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">that has been used for over a century to study the function of the vestibular system. In addition to GVS, the device supports physiological sensing by connecting heart rate, electrodermal activity, and other sensors&nbsp; using a plug and play mechanism.&nbsp;MoveU supports multiple categories of virtual reality applications with different types of virtual motions such as driving, navigating by flying, teleporting, or riding.&nbsp;</span></p>",,,2019-05-06 20:24:10.520,True,2018-07-15,MoveU,PUBLIC,,True,Fluid Interfaces,False
nevermind,pattie,False,"<p>NeverMind is an interface and application designed to support human memory. We combine the memory palace memorization method with augmented reality technology to create a tool to help anyone memorize more effectively. Early experiments conducted with a prototype of NeverMind suggest that the long-term memory recall accuracy of sequences of items is nearly tripled compared to paper-based memorization tasks. With this project, we hope to make the memory palace method accessible to novices and demonstrate one way augmented reality can support learning.</p>",,,2019-04-18 17:07:28.479,True,2016-09-01,NeverMind: Using AR for memorization,PUBLIC,,True,Fluid Interfaces,False
handson-a-gestural-system-for-remote-collaboration-using-augmented-reality,pattie,False,"<p>2D screens, even stereoscopic ones, limit our ability to interact with and collaborate on 3D data. We believe that an augmented reality solution, where 3D data is seamlessly integrated in the real world, is promising. We are exploring a collaborative augmented reality system for visualizing and manipulating 3D data using a head-mounted, see-through display, that allows for communication and data manipulation using simple hand gestures.</p>",,--Choose Location,2019-04-17 20:10:59.187,True,2014-01-01,HandsOn: A gestural system for remote collaboration using augmented reality,PUBLIC,,True,Fluid Interfaces,False
theme-engineering-dreams,pattie,False,,,,2019-02-05 14:34:26.214,True,2018-09-01,Theme | Engineering Dreams,PUBLIC,,True,Fluid Interfaces,False
theme-memory-enhancement,pattie,False,<p><br></p>,,,2018-12-10 23:33:12.385,True,2018-09-01,Theme | Memory Enhancement,LAB-INSIDERS,,True,Fluid Interfaces,False
supporting-decision-making,pattie,False,<p><br></p>,,,2018-12-12 19:06:27.833,True,2018-09-01,Theme | Supporting Decision Making,LAB-INSIDERS,,True,Fluid Interfaces,False
theme-attention-mindfulness-and-health,pattie,False,,,,2018-12-10 23:57:20.911,True,2018-09-01,"Theme | Attention, Mindfulness, and Health",LAB-INSIDERS,,True,Fluid Interfaces,False
theme-platforms-for-sensing-and-interventions,pattie,False,,,,2019-03-06 16:39:24.020,True,2018-09-01,Theme | Platforms for Sensing and Interventions,LAB-INSIDERS,,True,Fluid Interfaces,False
theme-immersive-and-wearable-learning,pattie,False,,,,2018-12-17 15:29:22.813,True,2018-09-01,Theme |  Immersive and Wearable Learning,LAB-INSIDERS,,True,Fluid Interfaces,False
essence,pattie,False,"<p>The sense of smell is perhaps the most pervasive of all senses, but it is also one of the least understood and least exploited in HCI. We present Essence, the first olfactory computational necklace that can be remotely controlled through a smartphone and can vary the intensity and frequency of the released scent based on biometric or contextual data.</p>",,--Choose Location,2019-05-14 13:43:24.753,True,2016-09-01,Essence,PUBLIC,http://www.judithamores.com/essence,True,Fluid Interfaces,False
lotuscent,pattie,False,"​<blockquote><b>The lotus flower is an ancient symbol that has been associated with spiritual awakening or enlightenment.</b></blockquote><p>In yoga and meditation, the lotus flower has been used as a symbolic support for the mind. The heart can be visualized as a lotus flower unfolding at the center of the chest.&nbsp;In Egyptian mythology, Nefertem was the lotus god of healing and perfume. Inspired by these mythologies, symbols, and practices, we created <i>Lotuscent.</i></p>",,,2019-05-14 18:35:01.765,True,2018-11-01,Lotuscent: Targeted Memory Reactivation for Wellbeing using Scent and VR biofeedback,PUBLIC,http://www.judithamores.com,True,Fluid Interfaces,False
wearable-wisdom,pattie,False,"<p>Having good mentors and role models is important for personal growth. Knowledge, advice, and inspiration from people a person admires can help motivate people when making life choices. Empirical research has shown that having a role model and insights from a mentor can positively affect the performance and progression of a person's career. However, such advice is not always available from the right people at the right time. Some of our personal heroes have passed away leaving only their writings and other artifacts. We present Wearable Wisdom, a context-aware, audio-based system in a glasses form factor for mediating wisdom from personal mentors to users. Our novel system offers just-in-time knowledge, advice, and inspiration from these mentors, based on the user's inquiry and current context. It does so by performing automated semantic analysis of the mentors' written text and selecting the most relevant quote to the user's inquiry.&nbsp;</p>",,,2019-04-28 03:26:25.734,True,2018-10-18,Wearable Wisdom,PUBLIC,,True,Fluid Interfaces,False
wearable-biotech,pattie,False,"<p>Wearable Biocomputer explores the intersection of wearable computation and biological computation. We designed on-body interfaces for culturing genetically engineered bacteria to sense, process, and actuate.&nbsp;</p>",,,2019-04-29 15:14:59.273,True,2019-04-27,Wearable Biocomputer,LAB,,True,Fluid Interfaces,False
Biological-Enhancement,pattie,False,"<h2><b>Lab on Body, Synthetic Biology, and Bio-Digital Systems for Health and Human Enhancement</b></h2>",,,2019-05-10 15:15:39.601,True,2019-02-03,Theme | Wearable Biotech Enhancement,PUBLIC,,True,Fluid Interfaces,False
human-microbe-interaction,pattie,False,"<p>One of the current foci within the HCI community is to understand and augment human capabilities using physiological and biological data. Microbes living on, inside, and around the human play significant roles in life, from improving health to causing infectious diseases. As the knowledge of human-microbe interaction continues to unfold, we propose a framework for microbial HCI based on a growing body of work aiming to observe, integrate, and modify microorganisms in interactive systems. Our motivation for the framework is to advancing the next generation of biological HCI and exploring novel human-microbe interfaces across contexts, scales, and species.</p>",,,2019-04-29 16:39:12.651,True,2018-11-01,Microbial Augmentation Interfaces,LAB-INSIDERS,,True,Fluid Interfaces,False
psychicvr,pattie,False,"<p>We present PsychicVR, a proof-of-concept system that integrates a brain-computer interface device and virtual reality headset to improve mindfulness while enjoying a playful immersive experience.&nbsp;The fantasy that any of us could have superhero powers has always inspired us, and by using virtual reality and real-time brain activity sensing we are moving one step closer to making this dream real. We non-invasively monitor and record electrical activity of the brain and incorporate this data into the VR experience using an Oculus Rift and the MUSE headband. By sensing brain waves using a series of EEG sensors, the level of activity is fed back to the user via 3D content in the virtual environment. When users are focused, they are able to make changes in the 3D environment and control their powers. Our system increases mindfulness and helps achieve higher levels of concentration while entertaining the user.</p>",,--Choose Location,2019-04-18 17:06:09.570,True,2015-09-01,PsychicVR,PUBLIC,,True,Fluid Interfaces,False
express,pattie,False,"<p>We are developing a new and exciting tool for expression in paint, combining technology and art to bring together the physical and the virtual through the use of robotics, artificial intelligence, signal processing, and wearable technology. Our technology promotes expression in paint not only by making it a lot more accessible, but also by making it flexible, adaptive, and fun, for everyone across the entire spectrum of abilities. With the development of the technology, new forms of art also emerge, such as hyper, hybrid, and collaborative painting. All of these can be extended to remote operation (or co-operation) thanks to the modular system design. For example, a parent and a child can be painting together even when far apart; a disabled person can experience an embodied painting experience; and medical professionals can reach larger populations with physical therapy, occupational therapy, and art therapy, including motor/neuromuscular impaired persons.</p>",,--Choose Location,2018-05-07 19:44:41.673,True,2015-01-01,Express,PUBLIC,,True,Fluid Interfaces,False
alterego,pattie,False,"<p>AlterEgo&nbsp;is a&nbsp; non-invasive, wearable, peripheral neural interface that allows humans to converse in natural language with machines, artificial intelligence assistants, services, and other people without any voice—without opening their mouth, and without externally observable movements—simply by articulating words internally.&nbsp; The feedback to the user is given through audio, via bone conduction,&nbsp; without disrupting the user's usual auditory perception, and making the interface closed-loop. This enables an human-computer interaction that is subjectively experienced as completely internal to the human user—like speaking to one's self.&nbsp;&nbsp;AlterEgo seeks to combine humans and computers—such that computing, the Internet, and AI would weave into human personality as an internal “second self” and augment human cognition and abilities.</p><p>The wearable system captures peripheral neural signals when internal speech articulators are volitionally and neurologically activated, during a user's internal articulation of words. This enables a user to transmit and receive streams of information to and from a computing device or any other person without any observable action, in discretion, without unplugging the user from her environment, without invading the user's&nbsp; privacy.&nbsp;</p>",,,2019-05-17 00:56:53.288,True,2017-09-01,AlterEgo,PUBLIC,,True,Fluid Interfaces,False
serosa,pattie,False,"<p>Exploring the connection between our two minds: the one in our head and the one in our body.&nbsp;</p><p>The mind-gut connection has flourished as a research area in the past few decades, elucidating the&nbsp; key role of the enteric nervous system (ENS or ""gut-brain"") in stress, affect, and memory. However, this connection has not been explored for wearable technology—applications and research for cognitive phenomena remain biased towards the cerebrum. In this project, we are non-invasively acquiring gastric myoelectric activity from the abdomen to evaluate the potential for a new area of wearable technology that can inform users on affective, stress, or memory states based on signals produced by the ENS.</p>",,,2018-05-09 13:05:47.805,True,2017-10-01,Serosa,LAB-INSIDERS,,True,Fluid Interfaces,False
physiohmd,pattie,False,"<p>Virtual and augmented reality headsets are unique as they have access to our facial area, an area that presents an excellent opportunity for always-available input and insight into the user's state. Their position on the face makes it possible to capture bio-signals as well as facial expressions.&nbsp; The&nbsp; PhysioHMD platform&nbsp;introduces a software and hardware modular interface built for collecting affect and physiological data from users wearing a head-mounted display. The platform enables researchers and developers to aggregate and interpret signals in real-time and use them to develop novel, personalized interactions, as well as evaluate virtual experiences. Our design offers seamless integration with standard HMDs, requiring minimal setup effort for developers and those with less experience using game engines. The PhysioHMD platform is a flexible architecture that offers an interface that is not only easy to extend but also complemented by a suite of tools for testing and analysis. We hope that PhysioHMD can become a universal, publicly available testbed for VR and AR researchers.</p><p>To create a seamless experience, we have integrated several bio-signal sensors into the faceplate of an HTC VIVE VR headset and utilized the Shimmer3 sensor for emotion-sensing. For the collection of Galvanic Skin Response, dry electrodes were positioned on the forehead area due to the fact that it is one of the areas most dense with sweat glands. GSR data reflects emotional arousal, but in order to identify how arousal, valence, motivation, and cognition interact in response to physical or psychological stimuli, it becomes necessary to complement GSR with other biosensors. For the heart rate, a PPG (photoplethysmogram) sensor, which senses the rate of blood flow by utilizing light to monitor the heart’s pumping action, was placed in the temple region of the user. This is done to get insights into the respondent's physical state, anxiety and stress levels (arousal), and to determine how changes in their physiological state relate to their actions and decisions.<br></p>",,,2018-11-14 19:05:00.306,True,2017-02-01,PhysioHMD,PUBLIC,,True,Fluid Interfaces,False
neuroknit,pattie,False,"<p>NeuroKnit is the interplay between the physical and virtual that is explored in response to the current lack of culture, expression, and emotions in VR  experiences; we propose a two-fold solution. First, the integration of bio-signal sensors into the HMD and techniques to detect aspects of the emotional state of the user. Second, the use of this data to generate expressive avatars.</p>",,,2019-06-07 12:59:19.489,True,2017-07-01,Emotional Beasts Parte Dos: NeuroKnit,PUBLIC,,True,Fluid Interfaces,False
SensorySynchrony,pattie,False,"<p>Space and space flight are extreme environments for the human body due to exposure to microgravity and high radiation levels. While the brain is neuroplastic and adapts to different habitats by learning over time, sudden changes in the environment and unpreparedness for it can totally hinder the functioning of the individual.&nbsp;The physiological changes caused by microgravity include vestibular problems causing space motion sickness, bone demineralization, skeletal muscle atrophy, cardiovascular problems, and more.&nbsp;The primary goal of this research project is to investigate vestibular system stimulation techniques to combat motion sickness and create more intuitive experiences when being in non-natural gravity environments.</p><p>Motion sickness is theorized to be either a cause of sensory mismatch between visual and vestibular afferent nerves(inter-sensory) or between semicircular and otolith nerve in the vestibular system (intra-sensory). The magnitude of alteration and the latency between the sensory inputs also contributes to the severeness of the motion sickness.&nbsp;To combat the non-congruent changes in sensory signals while transitioning into space, we propose to investigate vestibular neuromodulation techniques for facilitating adaptation in a more natural way, appeasing the effects of motion sickness and use the altered gravity to create novel experiences in virtual/augmented reality devices.</p><p>We built a prototype for multipole vestibular stimulation for simulating acceleration in roll and pitch axis. The prototype will be tested on the upcoming zero gravity flight for minimizing the effects of alterations between micro and hyper gravity phases.</p>",,,2019-05-31 13:37:03.535,True,2018-09-24,Sensory Synchrony,LAB,,True,Fluid Interfaces,False
cyborg-botany,pattie,False,"<p>Plants can sense the environment, other living entities and regenerate, actuate or grow in response.&nbsp;Our interaction and communication channels with plant organisms in nature are subtle - whether it be looking at their color, orientation, moisture, position of flowers, leaves and such. This subtlety stands in contrast to our interactions with artificial electronic devices&nbsp;that are centered in&nbsp;and around the screens, requiring full attention and induce cognitive load.&nbsp; We envision bringing such interaction out from the screens back into natural world around us.&nbsp;</p><p>Beyond external indicators, plants also have electrochemical signals and response mechanisms inside them that make them very similar to our electronic devices.&nbsp;To tap into such capacities already built in nature, we propose a new convergent view of interaction design. Our goal is to&nbsp;<i>merge and power our electronic functionalities with existing biological functions of living plant</i>s.&nbsp;Through Cyborg Botany, we re-appropriate some of these natural capabilities of plants for our interactive functions.&nbsp;</p>",,,2019-05-09 14:53:38.490,True,2017-01-03,"Cyborg Botany: Augmented plants as sensors, displays, and actuators",PUBLIC,http://harpreetsareen.com,True,Fluid Interfaces,False
engineering-dreams,pattie,False,"<p>Our dream is a sci-fi future, where dreams are controllable.&nbsp;</p><p>We are working to build technology that interfaces with the sleeping mind.&nbsp;As the dreamer descends into sleep, we track different sleep-stages using brain activity, muscle tension, heart rate, and movement data.&nbsp;External stimuli in the form of scent, audio, and muscle stimulation affect the content of the dreams. We are working on integrating multiple projects&nbsp; developed at the Fluid Interfaces group towards a vision where sleep is controllable.</p><p><a href=""http://dreams.media.mit.edu"">dreams.media.mit.edu</a><br></p>",,,2019-04-30 15:12:45.024,True,2018-03-21,Engineering Dreams,PUBLIC,,True,Fluid Interfaces,False
byte_it,pattie,False,"<h2>Discreet Teeth Gestures for Mobile Device Interaction</h2><p>Byte.it is a miniaturized, discreet interface that uses teeth gestures for hands-free input for wearable computing.&nbsp;<br></p><p>As humans, we are constantly seeking to communicate and consume information, and mobile devices give us&nbsp; access the world wide web, our digital selves, and all our digital assets with the touch of our fingers . Context can temporarily reduce our abilities to perform certain activities, preventing us from having a fluid interaction with mobile computing. Hands are not always available, and sustained visual attention is often required for successful task performance and social norms. Current screen-based interfaces are not designed to be used by a person engaged in another attention demanding activity such as walking , talking, or driving, leading to ineffective interactions and even dangerous situations.</p><p>Audio interfaces are a potential solution as they can provide a high-bandwidth communication channel without requiring visual attention. Speech has been the predominant interaction modality for audio interfaces, but it can be ineffective in situations with loud environmental noise, or inappropriate in certain social or dynamic on-the-go contexts. Recent work has explored teeth gestures as a solution for interaction in these contexts, but these attempts are limited by the number of gestural primitives recognized (bandwidth) and the discreteness of the interfaces used to detect these gestures.</p><p>Byte.it expands on this work by exploring the use of a smaller and more unobtrusively positioned sensor (accelerometer and gyroscope) for detecting tooth clicks of different groups of teeth and bite slides for everyday human-computer interaction.&nbsp;Initial results show that an unobtrusive position on the lower mastoid close to the mandibular condyle can be used to classify teeth tapping of four different teeth groups&nbsp;(front, back, left, and right teeth click)&nbsp;with an accuracy of 89 percent, or an accuracy of 84 percent for seven different teeth clicking and bite sliding gestures (front,&nbsp;left, and right click, and front, back, left, and right slide).</p><p>The applications currently being explored are centered around dynamic, on-the-go, hands-and-eyes-free contexts. For example, (1) controlling the different commands of a media player, such as play/pause, volume, and current time of a song, podcast, or audiobook. Productivity-wise, being able to subtly (2) start and stop audio recordings of conversations or meetings, and tag relevant events that might be worth reviewing later. Teeth gestures could also allow for a discrete and rapid way to (3) accept or reject incoming &nbsp;alerts, notifications, and reminders, while minimizing task-switch time.&nbsp;A minimal set of teeth gestures could also enable the seamless&nbsp;(4)&nbsp;access of information streams such as &nbsp;messages, emails, news, or relevant notes about the person, place, and/or time of interest that could enhance the current interaction.</p><p>This research aims to investigate the following:</p><p>1) Understand how people could use teeth gestures to perform specific interaction commands in order to establish a standardized teeth gesture language.</p><p>2) Identify the optimal position of the sensor to achieve the highest gesture classification accuracy possible while ensuring a discreet form factor.</p><p>3) Measure the performance of our classification algorithm in the wild, while sitting, standing, walking, running, and cycling.</p><p>4) Assess the usability of the interface + applications in the wild.</p>",,,2019-05-09 16:26:43.481,True,2017-09-07,Byte.it,PUBLIC,http://www.tomasvega.com,True,Fluid Interfaces,False
askii,jnazare,False,"<p>Askii is an SMS-based system that allows adult learners to study for a certification exam while on their commute. When learners have a spare five minutes, they can simply text Askii to begin their customized lessons. Askii will respond with a curated set of questions and links to content that learners can study on the go. We have begun building this prototype for learners to study for the US Naturalization Exam and plan to expand to other certification courses. Askii is a prototype within the larger Making Learning Work project.</p>",2015-09-01,--Choose Location,2016-12-05 00:16:13.398,True,2015-01-01,Askii,PUBLIC,,False,Social Machines,False
making-learning-work,jnazare,False,"<p>Improving adult learning, especially for adults who are unemployed or unable to financially support their families, is a challenge that affects the future wellbeing of millions of individuals in the US. We are working with the Joyce Foundation, employers, learning researchers, and the Media Lab community to prototype three to five new models for adult learning that involve technology innovation and behavioral insights. </p>",2016-06-01,--Choose Location,2016-12-05 00:16:23.585,True,2014-01-01,Making Learning Work,PUBLIC,,False,Social Machines,False
read-out-loud,jnazare,False,"<p>Read Out Loud is an application that empowers adults learning English to turn almost any reading material into an experience to help them learn. Learners can take a picture of a page of text; the app then scans in the page and presents the learner with a host of additional tools to facilitate reading. They can read the text aloud, which helps learners who are more comfortable with spoken English understand what is written. They can also select words to translate them into their native language. With this prototype, we want to give adult learners more agency to learn from material that focuses on subjects they care about, as well as increase access to English language learning material. Any book from the public library could become learning material with support in their native language. Read Out Loud is a prototype within the larger Making Learning Work project.</p>",2016-06-01,--Choose Location,2016-12-05 00:17:21.549,True,2014-09-01,Read Out Loud,PUBLIC,,False,Social Machines,False
family-learning-coach,jnazare,False,"<p>Developed as part of the Playful Words research project at the MIT Media Lab's Laboratory for Social Machines, Learning Loops aims to make literacy learning—both on and off the screen—a family experience. We create small-scale coach-family networks centered around children’s play on custom-built, open-ended literacy learning apps. Building Coach-family networks helps to empower children as authors and facilitate their narrative development. </p><p>StoryBlocks, a Learning Loops app, aims to promote literacy and social-emotional development through storytelling for children ages 6-10. StoryBlocks allows children to create and customize their own comic-style stories. These stories are analyzed using tools developed by the Learning Loops team to document children’s narrative development, and support Coaches as they provide personalized scaffolding for children’s narratives.&nbsp;</p><p><b><i>How does the Learning Loop work?</i></b></p><p>Data captured from a child’s use of StoryBlocks is streamed via the internet to cloud servers, and can immediately be accessed from a remote location by the child’s Coach. We have developed a Coach’s dashboard, called the Coach Console, powered by play analytics which enables a Coach to rapidly inspect play traces collected from a child’s activity and pull out their salient achievements, or meaningful moments. The Coaches then translate these moments into short personalized messages for the caregiver to inform them on their child’s narrative progress and provide suggestions for how to encourage new activities using StoryBlocks, together with background knowledge about their child’s path to literacy. Caregivers communicate with Coaches via text messages. Coaches can also help the children expand their sphere of learning and exploration by providing feedback on children's stories and suggesting new story starters&nbsp; directly to the child’s device that are based on trends in the child’s play data.</p><p><a href=""http://learningloops.org"">More info is available on the Learning Loops website.</a></p>",,,2019-04-11 16:00:08.728,True,2017-11-01,Learning Loops,PUBLIC,,True,Social Machines,False
playful-words,jnazare,False,"<p>While there are a number of literacy technology solutions developed for individuals, the role of social—or networked—literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.</p><p>By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.</p><p>To learn more about this project, please check out:&nbsp;<a href=""http://playfulwords.org/"">http://playfulwords.org/</a></p>",,--Choose Location,2018-04-30 20:28:15.298,True,2014-09-01,Playful Words,PUBLIC,,True,Social Machines,False
play-analytics,jnazare,False,"<p>Analyzing detailed data from SpeechBlocks&nbsp;to understand how kids engage with constructionist literacy learning technologies, with the goal of empowering caregivers (e.g. parents, older siblings, tutors) with these insights.</p>",,,2018-04-30 20:56:57.445,True,2016-02-01,Play Analytics,PUBLIC,,True,Social Machines,False
media-lab-digital-certificates,jnazare,False,"<p class="""">Blockchain Certificates is a set of tools, software, and strategies to store and manage digital credentials. Certificates are registered on the bitcoin blockchain, cryptographically signed, and tamper-proof. They can represent or recognize many different types of achievements. After a number of prototypes (we issued digital credentials to Media Lab Director's Fellows and Media Lab alumni) we published our code under an open-source license to enable others to deploy the tools we developed. More information at&nbsp;<a href=""http://blockcerts.org"" class="""">http://blockcerts.org</a>.&nbsp;</p>",,--Choose Location,2019-01-31 17:23:57.299,True,2015-01-01,Digital Academic Credentials,PUBLIC,,True,Social Machines,False
storyblocks,jnazare,False,"<p><span style=""font-size: 18px; font-weight: 400;"">StoryBlocks aims to promote creative expression, literacy development, and social-emotional development through storytelling for children ages six to ten. In this app, children create personally generated, comic-style stories by inserting characters, setting emotions, typing dialogue, using words to insert images that customize scenes, and recording their voices to narrate their unique stories. With StoryBlocks, we can collect a corpus of children’s stories in order to build analysis tools that&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">can document children’s narrative development over time, and support coaches in providing personalized scaffolding for children’s narratives.&nbsp;</span></p>",,,2019-04-11 15:39:53.269,True,2016-07-01,StoryBlocks,PUBLIC,,True,Social Machines,False
askii,mres,False,"<p>Askii is an SMS-based system that allows adult learners to study for a certification exam while on their commute. When learners have a spare five minutes, they can simply text Askii to begin their customized lessons. Askii will respond with a curated set of questions and links to content that learners can study on the go. We have begun building this prototype for learners to study for the US Naturalization Exam and plan to expand to other certification courses. Askii is a prototype within the larger Making Learning Work project.</p>",2015-09-01,--Choose Location,2016-12-05 00:16:13.398,True,2015-01-01,Askii,PUBLIC,,False,Lifelong Kindergarten,False
spin,mres,False,"<p>Spin is a photography turntable system that lets you capture how your DIY projects come together over time. With Spin, you can create GIFs and videos of your projects that you can download and share on Twitter, Facebook, or any other social network.</p>",2016-08-31,--Choose Location,2016-12-05 00:16:18.135,True,2015-01-01,Spin,PUBLIC,,False,Lifelong Kindergarten,False
diy-devices,mres,False,"<p>Using digital fabrication and embedded computation to allow individuals to make their own devices. This effort started by creating open-source DIY versions of common devices (speakers, radios, mice, and cellphones) each combining a custom electronic circuit board and digitally-fabricated enclosure. The current focus is on creating devices with unique functionality, aesthetics, or production processes. One early prototype is of a special-purpose internet-connected device, whose behavior can be customized by the person creating it. Another experiment explores the possibilities of automated circuit board assembly services and their implications for open-source hardware. Most importantly, we're beginning to develop resources to enable others to design and build custom devices through meaningful and educational creative processes. These efforts are still in an early stage, but we're interested in finding ways to transition from reproducing existing devices to helping people create a diverse set of new ones.</p>",2014-01-01,--Choose Location,2016-12-05 00:16:21.735,True,2014-01-01,DIY Devices,PUBLIC,,False,Lifelong Kindergarten,False
para,mres,False,"<p>Procedural representations, enabled through programming, are a powerful tool for digital illustration, but writing code conflicts with the intuitiveness and immediacy of direct manipulation. Para is a digital illustration tool that uses direct manipulation to define and edit procedural artwork. Through creating and altering vector paths, artists can define iterative distributions and parametric constraints. Para makes it easier for people to create generative artwork, and creates an intuitive workflow between manual and procedural drawing methods.</p>",2016-09-01,--Choose Location,2016-12-11 15:31:18.256,True,2014-09-01,Para,PUBLIC,,False,Lifelong Kindergarten,False
making-with-stories,mres,False,"<p>We are developing a set of participatory ""maker"" activities to engage youth in creating tangible projects that depict stories about themselves and their worlds. These activities introduce electronics and computational tools as a medium to create, connect, express, and derive meaning from personal narratives. For example, we are offering workshops where participants design sewable circuits and bring them together to create a collaborative Story Quilt. Through the Making with Stories project we are exploring how story-based pedagogy can inspire youth participation in arts and engineering within formal and informal learning environments.</p>",2015-09-01,--Choose Location,2016-12-05 00:16:35.326,True,2014-01-01,Making with Stories,PUBLIC,,False,Lifelong Kindergarten,False
novice-design-of-interactive-products,mres,False,"<p>Despite recent widespread interest in hobbyist electronics and the maker movement, the design of printed circuit boards (PCBs) remains an obscure and often intimidating activity. This project attempts to introduce PCB design and production to new audiences by creating examples, activities, and other resources that provide context and motivation for those practices. We've developed a series of interactive lights that demonstrate the creation of useable products with simple circuits. These examples introduce novices to the space of possibilities and provide them with a starting point for creating their own designs. In workshops, novices design, produce, assemble, and program their own electronic circuits. These workshops provide an entry point to understanding the way that electronic products are made and an opportunity for discussion and reflection about how more people might get involved in their production.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:49.276,True,2014-01-01,Novice Design of Interactive Products,PUBLIC,,False,Lifelong Kindergarten,False
desafio-aprendizagem-criativa-brasil-2018,mres,False,"<p><b>ATENÇÃO:</b>&nbsp;Saiu o <b>resultado do Desafio Aprendizagem Criativa Brasil 2018</b>! Clique <a href=""https://www.media.mit.edu/posts/resultado-do-desafio-aprendizagem-criativa-brasil-2018/"">aqui</a> para conhecer os fellows e os projetos selecionados!</p><p>----</p><p>O Desafio Aprendizagem Criativa Brasil é uma iniciativa da Fundação Lemann e do MIT Media Lab que visa fomentar a implementação de soluções inovadoras – novas tecnologias, produtos e serviços – que ajudem a tornar a educação brasileira mais mão na massa, significativa, colaborativa e lúdica.</p><p>O Desafio também tem como objetivo identificar, conectar e apoiar indivíduos brasileiros – artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decisão – que possam ter um papel-chave no avanço de práticas de Aprendizagem Criativa, especialmente no que se refere a projetos mão na massa envolvendo programação e construção no mundo físico, em escolas públicas (de Educação Infantil ao Ensino Médio) e ambientes de aprendizagem não formais de todo o Brasil.</p><p>Os representantes dos projetos selecionados ganharão uma&nbsp;<i>Creative Learning Fellowship&nbsp;</i>para ajudar a implementar seu trabalho.&nbsp;</p><p><b>As inscrições vão até o&nbsp;dia&nbsp; 9 de fevereiro e devem ser feitas única e exclusivamente através do formulário abaixo.</b></p><p>Clique <a href=""https://docs.google.com/document/d/1N6AgIZc7W6544cJKw5dsVmNSTgTzt1LAyMOiDDSKaqU/edit?usp=sharing"">aqui</a> para a&nbsp;<b>chamada de projetos</b>&nbsp;completa.<br></p><p>Clique <a href=""https://www.questionpro.com/t/AN4HdZbPeN"">aqui</a>&nbsp;para o&nbsp;<b>formulário de inscrição</b>.</p><p>Clique&nbsp;<a href=""https://docs.google.com/document/d/1LpYYcImuoZCeIm7XRcE1bBP62wdDgLQIfPmVN0_7AxM/edit?usp=sharing"">aqui</a>&nbsp;para respostas às&nbsp;<b>perguntas mais frequentes</b>.</p><p><b>Atenção</b>: &nbsp;esta página será atualizada periodicamente com mais informações sobre o Desafio. Discussões sobre o edital estão ocorrendo no&nbsp;&nbsp;<a href=""https://forum.aprendizagemcriativa.org/t/chamada-de-projetos-desafio-aprendizagem-criativa-brazil-2018/"">fórum da Rede Brasileira de Aprendizagem Criativa</a>.&nbsp;</p>",2018-12-31,,2018-12-03 22:52:00.488,True,2018-01-22,Desafio Aprendizagem Criativa Brasil 2018,PUBLIC,,False,Lifelong Kindergarten,False
slap-snap-tap,mres,False,"<p>Slap Snap Tap combines wearable sensors with physical block programming to enable enhanced expression through movement. By slapping on a set of sensor straps, snapping in code that links movement triggers to sound actions, and tapping the sensors to activate a play experience, users can combine motion and sound in creative ways. A dancer can create music through movement; an athlete can add emphasis to her performance; demonstrators can synchronize and amplify a chant; and anyone can create sound effects for life moments. Slap Snap Tap is a method of the Slay Play endeavor which aims to broaden participation in computational creation by using movement as a pathway into computational thinking.</p>",2016-12-31,--Choose Location,2017-03-31 19:41:01.413,True,2016-01-01,Slap Snap Tap,PUBLIC,,False,Lifelong Kindergarten,False
start-making,mres,False,"<p>The Lifelong Kindergarten group is collaborating with the Museum of Science in Boston to develop materials and workshops that engage young people in ""maker"" activities in Computer Clubhouses around the world, with support from Intel. The activities introduce youth to the basics of circuitry, coding, crafting, and engineering. In addition, graduate students are testing new maker technologies and workshops for Clubhouse staff and youth. The goal of the initiative is to help young people from under-served communities gain experience and confidence in their ability to design, create, and invent with new technologies.</p>",2016-08-31,--Choose Location,2016-12-05 00:17:01.293,True,2014-01-01,Start Making!,PUBLIC,,False,Lifelong Kindergarten,False
desafio-aprendizagem-criativa,mres,False,"<p><b>ATENÇÃO:</b> Saiu o <b>resultado do Desafio Aprendizagem Criativa Brasil 2017</b>! Clique <a href=""https://www.media.mit.edu/posts/resultado-do-desafio-aprendizagem-criativa-brasil-2017/"">aqui</a> para conhecer os fellows e projetos selecionados!</p><p>----</p><p>&nbsp;Desafio Aprendizagem Criativa Brasil é uma iniciativa da Fundação Lemann e do MIT Media Lab que visa fomentar a implementação de soluções inovadoras – novas tecnologias, produtos e serviços – que ajudem a tornar a educação brasileira mais mão na massa, significativa, colaborativa e lúdica.</p><p>O Desafio também tem como objetivo identificar, conectar e apoiar indivíduos brasileiros – artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decisão – que possam ter um papel-chave no avanço de práticas de Aprendizagem Criativa, especialmente no que se refere a projetos mão na massa envolvendo programação e construção no mundo físico, em escolas públicas (de Educação Infantil ao Ensino Médio) e ambientes de aprendizagem não formais de todo o Brasil.</p><p>Os representantes dos projetos selecionados ganharão uma <i>Creative Learning Fellowship </i> para ajudar a implementar seu trabalho.&nbsp;</p><p><b>As inscrições vão até o dia 5 de fevereiro de 2017 e devem ser feitas única e exclusivamente através do formulário abaixo.</b><br></p><p>Clique <a href=""https://www.dropbox.com/s/taxr6e878q1e5h6/Desafio%20Aprendizagem%20Criativa%20Brasil%20-%2020170125a.pdf?dl=1"">aqui</a>&nbsp;para a <b>chamada de projetos</b> completa.<br></p><p>Clique <a href=""https://www.tfaforms.com/4597543"">aqui</a> para o <b>formulário de inscrição</b>.</p><p>Clique <a href=""https://docs.google.com/document/d/1N2GKlkc_t83Kp0V4dgR_PzeCp5FJvhov-93OnBXpY0Y/edit?usp=sharing"">aqui</a> para respostas às <b>perguntas mais frequentes</b>.</p><p><b>Atenção</b>: &nbsp;esta página será atualizada periodicamente com mais informações sobre o Desafio. Discussões sobre o edital estão ocorrendo no <a href=""http://forum.aprendizagemcriativa.org/t/chamada-de-projetos-desafio-aprendizagem-criativa-brazil/238"">fórum da Rede Brasileira de Aprendizagem Criativa</a>.</p>",2017-12-31,,2018-01-22 15:34:04.822,True,2017-01-13,Desafio Aprendizagem Criativa Brasil 2017,PUBLIC,,False,Lifelong Kindergarten,False
scratch-data-blocks,mres,False,"<p>Scratch Community Blocks is an NSF-funded project that extends the Scratch programming language to enable youth to analyze and visualize their own learning and participation in the Scratch online community. With Scratch Community Blocks, youth in the Scratch community can easily access, analyze, and represent data about the ways they program, share, and discuss Scratch projects.</p>",,--Choose Location,2016-12-05 00:17:22.736,True,2014-01-01,Scratch Community Blocks,PUBLIC,,True,Lifelong Kindergarten,False
microworlds,mres,False,"<p>The MIT Scratch Team is exploring ways to make it easier for newcomers to get started creating with coding. We are designing ""microworlds""— customized versions of the Scratch editor that contain a small set of blocks for making projects based on a theme. </p><p>Microworlds offer a more creative entry point to coding. While many introductory coding experiences focus on engaging children in puzzles with one right answer, microworlds provide an open-ended experience, enabling children to explore, experiment, and create, while still providing a more simplified and scaffolded entry point into coding.</p><p>Each microworld includes subset of the Scratch programming blocks that are most relevant and useful for the particular interest area, along with specialized graphical assets related to the interest area. In addition to aligning with a particular interest area, each microworld highlights how coding can enable young people to create projects and express ideas with code. For example, by tinkering with the music microworld, young people can see how they can use code to make musical melodies and beats; by tinkering with the soccer microworld, young people can see how they can use coding to make objects move and start building their own game. </p><p>The project is part of the <a href=""http://scratch.mit.edu/info/codingforall"">Coding for All project</a>. The Coding for All project brings together an interdisciplinary research team from the MIT Media Lab, the Digital Media and Learning Hub at University of California Irvine, and Harvard University’s Berkman Center for Internet and Society to develop new online tools and activities to engage more young people in developing computational fluency, particularly youth from groups currently underrepresented in computing.&nbsp;</p>",,,2017-04-05 01:53:28.281,True,2015-09-01,Microworlds,PUBLIC,,True,Lifelong Kindergarten,False
scratch-pad,mres,False,"<p>ScratchBit is an effort to enable children to create more seamlessly in both the physical and digital world by creating a dedicated physical interface for the&nbsp;<a href=""https://scratch.mit.edu"">Scratch</a>&nbsp;programming language and environment. Designed to be rugged, low cost, and highly composable, the ScratchBit allows children to take the materials around them—such as cardboard, clothes, skateboards, and trees—and &nbsp;transform them into inputs to their digital creations on Scratch. Unlike the <a href=""http://makeymakey.com/"">Makey Makey</a> which was designed to make these connections electronically, the ScratchBit is designed to create these connections through motion and mechanism.</p>",,,2018-11-03 16:11:24.635,True,2016-09-01,ScratchBit,PUBLIC,,True,Lifelong Kindergarten,False
desafio-aprendizagem-criativa-brasil-2019,mres,False,"<p><b>ATENÇÃO:&nbsp;</b>Saiu o&nbsp;<b>resultado do Desafio Aprendizagem Criativa Brasil 2019!</b>&nbsp;Clique&nbsp;<a href=""https://www.media.mit.edu/posts/resultado-do-desafio-aprendizagem-criativa-brasil-2019/"">aqui</a>&nbsp;para conhecer os fellows e os projetos selecionados!</p><p>----</p><p>O Desafio Aprendizagem Criativa Brasil visa fomentar a implementação de soluções inovadoras que ajudem a tornar a educação brasileira mais criativa, prazerosa, relevante, colaborativa e inclusiva para crianças e jovens de todo o país.</p><p>Organizado pela <a href=""http://aprendizagemcriativa.org/"">Rede Brasileira de Aprendizagem Criativa</a>, e contando com o apoio da <a href=""http://fundacaolemann.org.br/"">Fundação Lemann</a> e do <a href=""http://media.mit.edu/"">MIT Media Lab</a>, o Desafio também tem como objetivo identificar, conectar e apoiar indivíduos brasileiros – artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decisão – que possam ter um papel-chave no avanço de práticas de aprendizagem criativa em escolas públicas (do Ensino Fundamental ao Ensino Médio) e ambientes de aprendizagem não formais de todo o Brasil.</p><p>Os representantes dos projetos selecionados ganharão uma&nbsp;<i>Creative Learning Fellowship&nbsp;</i>para ajudar a implementar seu trabalho.&nbsp;</p><p><b>As inscrições vão até o&nbsp;dia&nbsp; 13 de janeiro de 2019 e devem ser feitas única e exclusivamente através do formulário abaixo.</b></p><p>Clique&nbsp;<a href=""https://docs.google.com/document/d/1xmpCN_IDsOiRwd5KqDD3aeAAa46qXCKR5DpLWzwng7U/edit?usp=sharing"">aqui</a>&nbsp;para a&nbsp;<b>chamada de projetos</b>&nbsp;completa.<br></p><p>Clique&nbsp;<a href=""https://aprendizagemcriativa.fluidreview.com/"">aqui</a>&nbsp;para o&nbsp;<b>formulário de inscrição</b>.</p><p>Clique&nbsp;<a href=""https://docs.google.com/document/d/1LpYYcImuoZCeIm7XRcE1bBP62wdDgLQIfPmVN0_7AxM/edit?usp=sharing"">aqui</a>&nbsp;para respostas às&nbsp;<b>perguntas mais frequentes</b>.</p><p><b>Atenção</b>: &nbsp;esta página será atualizada periodicamente com mais informações sobre o Desafio. Discussões sobre o edital estão ocorrendo no&nbsp;<a href=""https://forum.aprendizagemcriativa.org/t/chamada-de-projetos-desafio-aprendizagem-criativa-brasil-2019/3547/10"">&nbsp;fórum da Rede Brasileira de Aprendizagem Criativa.</a>&nbsp;</p>",,,2019-02-23 00:21:05.368,True,2018-12-03,Desafio Aprendizagem Criativa Brasil 2019,PUBLIC,,True,Lifelong Kindergarten,False
computational-tinkering,mres,False,"<p>As children tinker with materials in the world, they are constantly putting things together and taking them apart. They are learning through play—trying out new ideas, exploring alternate paths, making adjustments, imagining new possibilities, expressing themselves creatively. In the process, they learn about the creative process and develop as creative thinkers.</p><p>As digital technologies enter the lives of children, there is risk that they will crowd out tinkering, with children spending more time watching screens than tinkering with materials. Yet, in our work, we have seen how digital technologies can also be used to open up new opportunities for tinkering.</p><p>Working in collaboration with the Tinkering Studio at the Exploratorium, Fondazione Reggio Children and the LEGO Foundation, we are developing a new generation of tools, activities, and spaces to support playful investigation and experimentation, integrating digital and physical materials.&nbsp;</p><p>The new activities will enable children to engage in new types of inquiry into light, sound, motion, and storytelling. In the initial set of activities, called ""light play,"" children can program colored lights and moving objects to make dynamic patterns of shadows.<br></p>",,,2018-03-27 10:44:38.668,True,2016-09-01,Computational Tinkering,PUBLIC,,True,Lifelong Kindergarten,False
creative-learning-in-brazil,mres,False,"<p>The Lemann Creative Learning Program is a collaboration between the MIT Media Lab and the Lemann Foundation to foster creative learning in Brazilian public education.&nbsp;</p><p>Established in February 2015, the program designs new technologies, support materials, and innovative initiatives to engage Brazilian public schools, afterschool centers, and families in learning practices that are more hands-on, creative, and centered on students' interests and ideas.&nbsp;</p><p>---</p><p>O Programa Lemann de Aprendizagem Criativa é uma colaboração entre o MIT Media Lab e a Fundação Lemann visando incentivar a aprendizagem criativa na educação pública do Brasil.</p><p>Criado em fevereiro de 2015, o programa cria novas tecnologias, materiais de apoio e iniciativas que ajudem escolas públicas, organizações de educação não formal, e famílias a implementar práticas de aprendizagem que sejam mais mão na massa, criativas e centradas nos interesses dos alunos.</p>",,--Choose Location,2018-12-11 21:36:57.377,True,2015-01-01,Creative Learning in Brazil / Aprendizagem Criativa no Brasil,PUBLIC,http://aprendizagemcriativa.org/,True,Lifelong Kindergarten,False
teen-summit,mres,False,"<p>Teen Summit is a biennial week-long Youth Leadership event that brings Clubhouse youth together from each of the 100 Clubhouses internationally. Youth leaders explore issues relevant to them and propose solutions through the creative use of innovative, high-end technologies. The 2018 Teen Summit will take place in late July at Boston University, featuring a college and career fair, collaborative cross-cultural activities, and many other opportunities for educational, career, and personal growth.</p>",,,2018-11-03 16:33:08.852,True,2017-10-02,Clubhouse Teen Summit,PUBLIC,http://www.computerclubhouse.org/teensummit,True,Lifelong Kindergarten,False
collective-learning,gaojian,False,"<p>Industrial development is the process by which economies learn how to produce new products and services. But how do economies learn? And who do they learn from? &nbsp;<span style=""font-size: 18px; font-weight: normal;"">The literature on economic geography and economic development has emphasized two learning channels: inter-industry learning, which involves learning from related industries; and inter-regional learning, which involves learning from neighboring regions.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">Here we use 25 years of data describing the evolution of China's economy between 1990 and 2015--a period when China multiplied its GDP per capita by a factor of ten--to explore how Chinese provinces diversified their economies.</span><span style=""font-size: 18px; font-weight: normal;"">&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">First, we show that the probability that a province will develop a new industry increases with the number of related industries that are already present in that province, a fact that is suggestive of inter-industry learning. Also, we show that the probability that a province will develop an industry increases with the number of neighboring provinces that are developed in that industry, a fact suggestive of inter-regional learning. Moreover, we find that the combination of these two channels exhibit diminishing returns, meaning that the contribution of either of these learning channels is redundant when the other one is present.&nbsp;</span><br></p><p><span style=""font-size: 18px; font-weight: normal;"">Further, we address endogeneity concerns by using the introduction of high-speed rail as an instrument to isolate the effects of inter-regional learning. Our differences-in-differences (DID) analysis reveals that the introduction of </span>high speed-rail<span style=""font-size: 18px; font-weight: normal;""> increased the industrial similarity of pairs of provinces connected by high-speed rail. Also, industries in provinces that were connected by rail increased their productivity when they were connected by rail to other provinces where that industry was already present. These findings suggest that inter-regional and inter-industry learning played a role in China's great economic expansion.</span><br></p>",2017-09-01,,2017-03-31 00:31:45.552,True,2016-09-01,Collective Learning  in China's Regional Economic Development,PUBLIC,,False,Collective Learning,False
relatedness-knowledge-diffusion-and-the-evolution-of-bilateral-trade,gaojian,False,"<p>During the last few decades two important intellectual contributions have reshaped our understanding of international trade. On the one hand, work emphasizing trade frictions and extended gravity models has shown that countries trade more with those with whom they share a language, colonial past, or ethnic social relationships. This is interpreted as evidence of trade not being only about differences in factor endowments and transportation costs, but the result of complex social processes where information frictions and social networks play a key role. On the other hand, work emphasizing knowledge diffusion has shown that the probability that a country starts exporting a product increases with the number of related products it already exports. Yet, despite the importance of these two recent findings, little is known about their intersection: does knowledge on how to export to a destination also diffuses among related products? Here, we use bilateral trade data from 2000 to 2015, disaggregated into 1,242 product categories, to create an extended gravity model of bilateral trade that reproduces previous findings (effects of language, distance, colonial past, etc.) and shows that, in addition to these, countries are more likely to increase their exports of a product to a destination when: (i) they export related products to it, (ii) already export that product to some of its neighbors, and (iii) have neighbors who also export the same product to that destination. We interpret these findings as evidence of knowledge diffusion among related products and among geographic neighbors, both in the context of exporters and importers. Then, we explore the magnitude of these effects for new, nascent, and experienced exporters, and also, for groups of products classified according to Lall's technological classification of exports. We find that the effects of product and geographic relatedness are stronger for new exporters, and also, that the effect of product relatedness increases with the technological sophistication of products. These findings support the idea that international trade is shaped by knowledge and information frictions that are partially reduced in the presence of product relatedness.</p>",,,2017-10-11 16:57:36.018,True,2017-03-01,"Relatedness, Knowledge Diffusion, and the Evolution of Bilateral Trade",PUBLIC,,True,Collective Learning,False
collective-learning,bjun,False,"<p>Industrial development is the process by which economies learn how to produce new products and services. But how do economies learn? And who do they learn from? &nbsp;<span style=""font-size: 18px; font-weight: normal;"">The literature on economic geography and economic development has emphasized two learning channels: inter-industry learning, which involves learning from related industries; and inter-regional learning, which involves learning from neighboring regions.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">Here we use 25 years of data describing the evolution of China's economy between 1990 and 2015--a period when China multiplied its GDP per capita by a factor of ten--to explore how Chinese provinces diversified their economies.</span><span style=""font-size: 18px; font-weight: normal;"">&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">First, we show that the probability that a province will develop a new industry increases with the number of related industries that are already present in that province, a fact that is suggestive of inter-industry learning. Also, we show that the probability that a province will develop an industry increases with the number of neighboring provinces that are developed in that industry, a fact suggestive of inter-regional learning. Moreover, we find that the combination of these two channels exhibit diminishing returns, meaning that the contribution of either of these learning channels is redundant when the other one is present.&nbsp;</span><br></p><p><span style=""font-size: 18px; font-weight: normal;"">Further, we address endogeneity concerns by using the introduction of high-speed rail as an instrument to isolate the effects of inter-regional learning. Our differences-in-differences (DID) analysis reveals that the introduction of </span>high speed-rail<span style=""font-size: 18px; font-weight: normal;""> increased the industrial similarity of pairs of provinces connected by high-speed rail. Also, industries in provinces that were connected by rail increased their productivity when they were connected by rail to other provinces where that industry was already present. These findings suggest that inter-regional and inter-industry learning played a role in China's great economic expansion.</span><br></p>",2017-09-01,,2017-03-31 00:31:45.552,True,2016-09-01,Collective Learning  in China's Regional Economic Development,PUBLIC,,False,Collective Learning,False
meet-me-in-the-middle-the-reunification-of-the-german-research-and-innovation-system,bjun,False,"<p>In 1990 Germany began the reunification of two separate research systems. Yet, the institutional unification of these system does not necessarily imply their actual unification. Here we study the evolution of the network of co-authorships between East and West German scholars between 1974 and 2014 to identify the fields that integrated more successfully, and also, the factors predicting re-unification success. We find that the unification of the German research network was fast during the 1990s, but then stagnated at an intermediate level of integration. Next, we study the integration of the twenty largest academic fields (by number of publications prior to reunification) and find an inverted U-shaped between a field's East or West ``dominance'' (a measure of the concentration of the scholarly output of a field in East or West Germany prior to 1990) and the field's subsequent level of integration. We check for the robustness of these results by running Monte Carlo simulations, and a differences-in-difference analysis. Both methods confirm that fields that were dominated by either West or East Germany prior to the reunification integrated less than those whose output was balanced among East and West. Finally, we explore the origins of this inverted U-shape relationship by comparing the mixing patterns, and show that this inverted U-shaped relationship can be explained as a consequence of a tendency of scholars from the most productive regions to collaborate preferentially with scholars from other top regions. These results shed light on the mechanisms governing the reintegration of networks in the content of scholarly communities that were separated by institutions.</p>",,,2017-10-11 15:39:59.915,True,2016-04-01,Meet me in the middle: The reunification of the German research and innovation system,PUBLIC,,True,Collective Learning,False
relatedness-knowledge-diffusion-and-the-evolution-of-bilateral-trade,bjun,False,"<p>During the last few decades two important intellectual contributions have reshaped our understanding of international trade. On the one hand, work emphasizing trade frictions and extended gravity models has shown that countries trade more with those with whom they share a language, colonial past, or ethnic social relationships. This is interpreted as evidence of trade not being only about differences in factor endowments and transportation costs, but the result of complex social processes where information frictions and social networks play a key role. On the other hand, work emphasizing knowledge diffusion has shown that the probability that a country starts exporting a product increases with the number of related products it already exports. Yet, despite the importance of these two recent findings, little is known about their intersection: does knowledge on how to export to a destination also diffuses among related products? Here, we use bilateral trade data from 2000 to 2015, disaggregated into 1,242 product categories, to create an extended gravity model of bilateral trade that reproduces previous findings (effects of language, distance, colonial past, etc.) and shows that, in addition to these, countries are more likely to increase their exports of a product to a destination when: (i) they export related products to it, (ii) already export that product to some of its neighbors, and (iii) have neighbors who also export the same product to that destination. We interpret these findings as evidence of knowledge diffusion among related products and among geographic neighbors, both in the context of exporters and importers. Then, we explore the magnitude of these effects for new, nascent, and experienced exporters, and also, for groups of products classified according to Lall's technological classification of exports. We find that the effects of product and geographic relatedness are stronger for new exporters, and also, that the effect of product relatedness increases with the technological sophistication of products. These findings support the idea that international trade is shaped by knowledge and information frictions that are partially reduced in the presence of product relatedness.</p>",,,2017-10-11 16:57:36.018,True,2017-03-01,"Relatedness, Knowledge Diffusion, and the Evolution of Bilateral Trade",PUBLIC,,True,Collective Learning,False
industry-knowledge,bjun,False,"<p>How do regions acquire the knowledge they need to diversify their economic activities? How does the migration of workers among firms and industries contribute to the diffusion of that knowledge? Here we measure the industry-, occupation-, and location-specific knowledge carried by workers from one establishment to the next, using a dataset summarizing the individual work history for an entire country. We study pioneer firms—firms operating in an industry that was not present in a region—because the success of pioneers is the basic unit of regional economic diversification. We find that the growth and survival of pioneers increase significantly when their first hires are workers with experience in a related industry and with work experience in the same location, but not with past experience in a related occupation. We compare these results with new firms that are not pioneers and find that industry-specific knowledge is significantly more important for pioneer than for non-pioneer firms. To address endogeneity we use Bartik instruments, which leverage national fluctuations in the demand for an activity as shocks for local labor supply. The instrumental variable estimates support the finding that industry-specific knowledge is a predictor of the survival and growth of pioneer firms. These findings expand our understanding of the micromechanisms underlying regional economic diversification.</p>",,,2018-12-14 20:31:53.678,True,2017-03-01,"The role of industry, occupation, and location-specific knowledge in the survival of new firms",PUBLIC,,True,Collective Learning,False
cardio,changzj,False,"<p>We are designing interfaces to enhance driver self-awareness through subliminal visual feedback and shape-changing materials. Advancements in sensing technologies make it possible to measure physiological data in the car environment, opening up the possibility of harnessing such data for just-in-time feedback to drivers. </p>",2015-01-01,--Choose Location,2016-12-05 00:16:17.267,True,2014-09-01,CarDio,PUBLIC,,False,Fluid Interfaces,False
screenspire,changzj,False,"<p>Screen interactions have been shown to contribute to increases in stress, anxiety, and deficiencies in breathing patterns. Since better respiration patterns can have a positive impact on wellbeing, ScreenSpire improves respiration patterns during information work using subliminal biofeedback. By using subtle graphical variations that are tuned to attempt to influence the user subconsciously, user distraction and cognitive load are minimized. To enable a truly seamless interaction, we have adapted an RF-based sensor (ResMed S+ sleep sensor) to serve as a screen-mounted contact-free and respiration sensor. Traditionally, respiration sensing is achieved with either invasive or on-skin sensors (such as a chest belt); having a contact-free sensor contributes to increased ease, comfort, and user compliance, since no special actions are required from the user.</p>",2016-12-31,--Choose Location,2017-08-25 11:59:51.368,True,2015-01-01,ScreenSpire,PUBLIC,,False,Fluid Interfaces,False
kicksoul-a-wearable-system-for-foot-interactions-with-digital-devices,changzj,False,<p>KickSoul is a wearable device that maps natural foot movements into inputs for digital devices. It consists of an insole with embedded sensors that track movements and trigger actions in devices that surround us. We present a novel approach to use our feet as input devices in mobile situations when our hands are busy. We analyze the foot's natural movements and their meaning before activating an action.</p>,2017-08-01,--Choose Location,2017-08-07 14:36:03.410,True,2015-01-01,KickSoul: A Wearable System for Foot Interactions with Digital Devices,PUBLIC,,False,Fluid Interfaces,False
social-textiles,changzj,False,"<p>The way we represent ourselves in social media is intangible. What we choose to wear is public to the world and we are aware of it. In contrast, what we post online about ourselves reaches thousands of people and generates social consequences, but it doesn’t feel that way. Is the current form of social media really making our relationships better? Current technologies are good at connecting people at a distance, but less so at connecting them within the same environment.</p><p>Social textiles embodies who you are and dynamically reflects your shared interests with people nearby. It enables you to gain access to communities of people in the physical world and enhances social affordances and icebreaking interactions through wearable social messaging.</p><p>Social Textiles embody who you are and dynamically reflect your shared interests with people nearby. They enable you to gain access to communities of people in the physical world and enhance social affordances and icebreaking interactions through wearable social messaging. Social Textiles can connect community members with niche interests, philosophical beliefs, personalities, emotional statuses, and ethical views. They have the potential to enable members to bypass superficial or generic interests through ""filtering"" individuals, in order to tune social experiences toward people who are more compatible.</p>",2015-01-01,--Choose Location,2018-10-12 16:50:34.930,True,2014-09-01,Social Textiles,PUBLIC,,False,Fluid Interfaces,False
cardio,rboldu,False,"<p>We are designing interfaces to enhance driver self-awareness through subliminal visual feedback and shape-changing materials. Advancements in sensing technologies make it possible to measure physiological data in the car environment, opening up the possibility of harnessing such data for just-in-time feedback to drivers. </p>",2015-01-01,--Choose Location,2016-12-05 00:16:17.267,True,2014-09-01,CarDio,PUBLIC,,False,Fluid Interfaces,False
tagme,rboldu,False,"<p>TagMe is an end-user toolkit for easy creation of responsive objects and environments. It consists of a wearable device that recognizes the object or surface the user is touching. The user can make everyday objects come to life through the use of RFID tag stickers, which are read by an RFID bracelet whenever the user touches the object. We present a novel approach to create simple and customizable rules based on emotional attachment to objects and social interactions of people. Using this simple technology, the user can extend their application interfaces to include physical objects and surfaces into their personal environment, allowing people to communicate through everyday objects in very low-effort ways.</p>",2018-01-01,--Choose Location,2018-10-11 18:48:35.308,True,2014-09-01,TagMe,PUBLIC,,False,Fluid Interfaces,False
cardio,amores,False,"<p>We are designing interfaces to enhance driver self-awareness through subliminal visual feedback and shape-changing materials. Advancements in sensing technologies make it possible to measure physiological data in the car environment, opening up the possibility of harnessing such data for just-in-time feedback to drivers. </p>",2015-01-01,--Choose Location,2016-12-05 00:16:17.267,True,2014-09-01,CarDio,PUBLIC,,False,Fluid Interfaces,False
tagme,amores,False,"<p>TagMe is an end-user toolkit for easy creation of responsive objects and environments. It consists of a wearable device that recognizes the object or surface the user is touching. The user can make everyday objects come to life through the use of RFID tag stickers, which are read by an RFID bracelet whenever the user touches the object. We present a novel approach to create simple and customizable rules based on emotional attachment to objects and social interactions of people. Using this simple technology, the user can extend their application interfaces to include physical objects and surfaces into their personal environment, allowing people to communicate through everyday objects in very low-effort ways.</p>",2018-01-01,--Choose Location,2018-10-11 18:48:35.308,True,2014-09-01,TagMe,PUBLIC,,False,Fluid Interfaces,False
remot-io-a-system-for-reaching-into-the-environment-of-a-remote-collaborator,amores,False,"<p>Remot-IO is a system for mobile collaboration and remote assistance around Internet-connected devices. It uses two head-mounted displays, cameras, and depth sensors to enable a remote expert to be immersed in a local user's point of view, and to control devices in that user's environment. The remote expert can provide guidance through hand gestures that appear in real time in the local user's field of view as superimposed 3D hands. In addition, the remote expert can operate devices in the novice's environment and bring about physical changes by using the same hand gestures the novice would use. We describe a smart radio where the knobs of the radio can be controlled by local and remote users. Moreover, the user can visualize, interact, and modify properties of sound waves in real time by using intuitive hand gestures.</p>",2016-12-31,--Choose Location,2018-08-20 16:23:49.991,True,2015-01-01,Remot-IO: A System for Reaching into the Environment of a Remote Collaborator,PUBLIC,,False,Fluid Interfaces,False
invisibilia-revealing-invisible-data-as-a-tool-for-experiential-learning,amores,False,"<p>Invisibilia seeks to explore the use of Augmented Reality (AR), head-mounted displays (HMD), and depth cameras to create a system that makes invisible data from our environment visible, combining widely accessible hardware to visualize layers of information on top of the physical world. Using our implemented prototype, the user can visualize, interact with, and modify properties of sound waves in real time by using intuitive hand gestures. Thus, the system supports experiential learning about certain physics phenomena through observation and hands-on experimentation.</p>",2016-08-31,--Choose Location,2018-08-20 16:39:45.640,True,2015-01-01,Invisibilia: Revealing Invisible Data as a Tool for Experiential Learning,PUBLIC,,False,Fluid Interfaces,False
computational-food,amores,False,"<p>Computational Food is a series of experiments around the shape-changing nature of food and its associated unique sensory experiences. We used food&nbsp;as a <i>medium</i>&nbsp;and a <i>platform</i> to develop dynamic, shape-changing prototypes that are edible or that enhance the culinary experience.</p>",2014-12-31,,2019-04-29 22:10:01.940,True,2014-09-16,Computational Food,PUBLIC,,False,Fluid Interfaces,False
showme-immersive-remote-collaboration-system-with-3d-hand-gestures,amores,False,"<p>ShowMe is an immersive mobile collaboration system that allows remote users to communicate with peers using video, audio, and gestures. With this research, we explore the use of head-mounted displays and depth sensor cameras to create a system that (1) enables remote users to be immersed in another person's view, and (2) offers a new way of sending and receiving the guidance of an expert through 3D hand gestures. With our system, both users are surrounded in the same physical environment and can perceive real-time inputs from each other. </p>",2017-08-31,--Choose Location,2018-10-12 16:45:04.639,True,2014-01-01,ShowMe: Immersive Remote Collaboration System with 3D Hand Gestures,PUBLIC,,False,Fluid Interfaces,False
social-textiles,amores,False,"<p>The way we represent ourselves in social media is intangible. What we choose to wear is public to the world and we are aware of it. In contrast, what we post online about ourselves reaches thousands of people and generates social consequences, but it doesn’t feel that way. Is the current form of social media really making our relationships better? Current technologies are good at connecting people at a distance, but less so at connecting them within the same environment.</p><p>Social textiles embodies who you are and dynamically reflects your shared interests with people nearby. It enables you to gain access to communities of people in the physical world and enhances social affordances and icebreaking interactions through wearable social messaging.</p><p>Social Textiles embody who you are and dynamically reflect your shared interests with people nearby. They enable you to gain access to communities of people in the physical world and enhance social affordances and icebreaking interactions through wearable social messaging. Social Textiles can connect community members with niche interests, philosophical beliefs, personalities, emotional statuses, and ethical views. They have the potential to enable members to bypass superficial or generic interests through ""filtering"" individuals, in order to tune social experiences toward people who are more compatible.</p>",2015-01-01,--Choose Location,2018-10-12 16:50:34.930,True,2014-09-01,Social Textiles,PUBLIC,,False,Fluid Interfaces,False
bin-ary-detecting-the-state-of-organic-trash-to-prevent-insalubrity,amores,False,"<p>Bin-ary is a self-contained gas detector that analyzes organic trash odor compounds and releases a subtle burst of scent when bad odor is detected. The prototype is meant to be used as a plugin to make trash bins and dumpsters smarter and prevent insalubrity in cities and villages. In order to detect the state of organic trash we primarily focused on the chemical compounds that are produced when food starts to rot or ripen: Hydrogen Sulfide (H2S), Ethylene (C2H4) and Carbon Dioxide (CO2).</p>",2015-09-01,--Choose Location,2016-12-05 00:17:07.567,True,2015-09-01,Bin-ary: Detecting the State of Organic Trash to Prevent Insalubrity,PUBLIC,,False,Fluid Interfaces,False
deep-reality,amores,False,"<p>We present an interactive virtual reality (VR) experience&nbsp;that uses biometric information for reflection and relaxation.&nbsp;We monitor in real-time brain activity using a modified version&nbsp;of the Muse EEG and track heart rate (HR) and electro&nbsp;dermal activity (EDA) using an Empatica E4 wristband. We&nbsp;use this data to procedurally generate 3D creatures and&nbsp;change the lighting of the environment to reflect the internal&nbsp;state of the viewer in a set of visuals depicting an underwater&nbsp;audiovisual composition. These 3D creatures are&nbsp;created to unconsciously influence the body signals of the&nbsp;observer via subtle pulses of light, movement and sound.&nbsp;We aim to decrease heart rate and respiration by subtle,&nbsp;almost imperceptible light flickering, sound pulsations and&nbsp;slow movements of these creatures to increase relaxation.</p>",,,2019-05-09 19:18:17.333,True,2017-04-01,"Deep Reality: An underwater VR experience to promote relaxation by unconscious HR, EDA and brain activity biofeedback",PUBLIC,,True,Fluid Interfaces,False
sleep-staging-EEG,amores,False,"<p>Automatic and real-time sleep scoring is necessary to&nbsp;develop user interfaces that trigger stimuli in specific sleep stages.&nbsp;However, most automatic sleep scoring systems have been focused&nbsp;on offline data analysis. We present the first, real-time sleep staging&nbsp;system that uses deep learning without the need for servers&nbsp;in a smartphone application for a wearable EEG. We employ&nbsp;real-time adaptation of a single channel Electroencephalography&nbsp;(EEG) to infer from a Time-Distributed Convolutional Neural&nbsp;Network (CNN). Polysomnography (PSG)—<b>the gold standard&nbsp;for sleep staging—requires a human scorer and is both complex&nbsp;and resource-intensive</b>. Our work demonstrates an end-to-end,&nbsp;smartphone-based pipeline that can infer sleep stages in just&nbsp;single 30-second epochs, with an overall accuracy of 83.5% on&nbsp;20-fold cross validation for 5-stage classification of sleep stages&nbsp;using the open Sleep-EDF dataset.&nbsp;</p>",,,2019-05-09 19:22:28.325,True,2018-09-01,Real-time Smartphone-based Sleep Staging using 1-Channel EEG,PUBLIC,http://www.judithamores.com,True,Fluid Interfaces,False
bioessence,amores,False,<h2><b>A wearable olfactory display that monitors cardio-respiratory information to support mental wellbeing.</b></h2><p>BioEssence is a novel wearable olfactory&nbsp;display that provides just-in-time release of scents based on&nbsp;the physiological state of the wearer. The device can release up&nbsp;to three scents and passively captures subtle chest vibrations&nbsp;associated with the beating of the heart and respiration through&nbsp;clothes.&nbsp;<br></p>,,,2019-05-11 00:16:22.469,True,2017-12-01,BioEssence,PUBLIC,http://www.judithamores.com/essence,True,Fluid Interfaces,False
essence,amores,False,"<p>The sense of smell is perhaps the most pervasive of all senses, but it is also one of the least understood and least exploited in HCI. We present Essence, the first olfactory computational necklace that can be remotely controlled through a smartphone and can vary the intensity and frequency of the released scent based on biometric or contextual data.</p>",,--Choose Location,2019-05-14 13:43:24.753,True,2016-09-01,Essence,PUBLIC,http://www.judithamores.com/essence,True,Fluid Interfaces,False
lotuscent,amores,False,"​<blockquote><b>The lotus flower is an ancient symbol that has been associated with spiritual awakening or enlightenment.</b></blockquote><p>In yoga and meditation, the lotus flower has been used as a symbolic support for the mind. The heart can be visualized as a lotus flower unfolding at the center of the chest.&nbsp;In Egyptian mythology, Nefertem was the lotus god of healing and perfume. Inspired by these mythologies, symbols, and practices, we created <i>Lotuscent.</i></p>",,,2019-05-14 18:35:01.765,True,2018-11-01,Lotuscent: Targeted Memory Reactivation for Wellbeing using Scent and VR biofeedback,PUBLIC,http://www.judithamores.com,True,Fluid Interfaces,False
wearable-biotech,amores,False,"<p>Wearable Biocomputer explores the intersection of wearable computation and biological computation. We designed on-body interfaces for culturing genetically engineered bacteria to sense, process, and actuate.&nbsp;</p>",,,2019-04-29 15:14:59.273,True,2019-04-27,Wearable Biocomputer,LAB,,True,Fluid Interfaces,False
Biological-Enhancement,amores,False,"<h2><b>Lab on Body, Synthetic Biology, and Bio-Digital Systems for Health and Human Enhancement</b></h2>",,,2019-05-10 15:15:39.601,True,2019-02-03,Theme | Wearable Biotech Enhancement,PUBLIC,,True,Fluid Interfaces,False
psychicvr,amores,False,"<p>We present PsychicVR, a proof-of-concept system that integrates a brain-computer interface device and virtual reality headset to improve mindfulness while enjoying a playful immersive experience.&nbsp;The fantasy that any of us could have superhero powers has always inspired us, and by using virtual reality and real-time brain activity sensing we are moving one step closer to making this dream real. We non-invasively monitor and record electrical activity of the brain and incorporate this data into the VR experience using an Oculus Rift and the MUSE headband. By sensing brain waves using a series of EEG sensors, the level of activity is fed back to the user via 3D content in the virtual environment. When users are focused, they are able to make changes in the 3D environment and control their powers. Our system increases mindfulness and helps achieve higher levels of concentration while entertaining the user.</p>",,--Choose Location,2019-04-18 17:06:09.570,True,2015-09-01,PsychicVR,PUBLIC,,True,Fluid Interfaces,False
engineering-dreams,amores,False,"<p>Our dream is a sci-fi future, where dreams are controllable.&nbsp;</p><p>We are working to build technology that interfaces with the sleeping mind.&nbsp;As the dreamer descends into sleep, we track different sleep-stages using brain activity, muscle tension, heart rate, and movement data.&nbsp;External stimuli in the form of scent, audio, and muscle stimulation affect the content of the dreams. We are working on integrating multiple projects&nbsp; developed at the Fluid Interfaces group towards a vision where sleep is controllable.</p><p><a href=""http://dreams.media.mit.edu"">dreams.media.mit.edu</a><br></p>",,,2019-04-30 15:12:45.024,True,2018-03-21,Engineering Dreams,PUBLIC,,True,Fluid Interfaces,False
spin,ttseng,False,"<p>Spin is a photography turntable system that lets you capture how your DIY projects come together over time. With Spin, you can create GIFs and videos of your projects that you can download and share on Twitter, Facebook, or any other social network.</p>",2016-08-31,--Choose Location,2016-12-05 00:16:18.135,True,2015-01-01,Spin,PUBLIC,,False,Lifelong Kindergarten,False
start-making,ttseng,False,"<p>The Lifelong Kindergarten group is collaborating with the Museum of Science in Boston to develop materials and workshops that engage young people in ""maker"" activities in Computer Clubhouses around the world, with support from Intel. The activities introduce youth to the basics of circuitry, coding, crafting, and engineering. In addition, graduate students are testing new maker technologies and workshops for Clubhouse staff and youth. The goal of the initiative is to help young people from under-served communities gain experience and confidence in their ability to design, create, and invent with new technologies.</p>",2016-08-31,--Choose Location,2016-12-05 00:17:01.293,True,2014-01-01,Start Making!,PUBLIC,,False,Lifelong Kindergarten,False
reporting-reviewing-and-responding-to-harassment-on-twitter,jnmatias,False,"<p>In November 2014, Twitter granted Women, Action, and the Media (WAM!) authorized status to report harassment to the company. In three weeks, WAM! reviewers assessed 811 incoming reports of harassment and escalated 161 reports to Twitter, ultimately seeing Twitter carry out 70 account suspensions, 18 warnings, and one deleted account. This document presents quantitative and qualitative findings from this three-week project. Our findings focus on the people reporting and receiving harassment, the kinds of harassment that were reported, Twitter's response to harassment reports, the process of reviewing harassment reports, and challenges for harassment reporting processes.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:10.973,True,2015-01-01,"Reporting, Reviewing, and Responding to Harassment on Twitter",PUBLIC,,False,Civic Media,False
the-peoples-bot,jnmatias,False,"<p>Telepresent robots are often pitched as a technology to extend the influence of those who already have money and power. We want to use robotic telepresence for the public good�broadening access, supporting public interest reporting, and funding access initiatives.</p>",2014-09-01,--Choose Location,2016-12-05 00:16:55.263,True,2014-01-01,The People's Bot,PUBLIC,,False,Civic Media,False
student-legal-services-for-innovation,jnmatias,False,"<p>Should students be prosecuted for innovative projects? In December 2014, four undergraduates associated with the Media Lab were subpoenaed by the New Jersey Attorney General after winning a programming competition with a bitcoin-related proof of concept. We worked with MIT administration and the Electronic Frontier Foundation to support the students and establish legal support for informal innovation. In September 2015, MIT announced the creation of a new clinic for business and cyberlaw.</p>",2016-10-01,--Choose Location,2016-12-05 00:17:22.864,True,2014-01-01,Student Legal Services for Innovation,PUBLIC,,False,Civic Media,False
whose-lives-matter-in-the-news,jnmatias,False,"<p>Since the killing of Michael Brown, the Black Lives Matter movement has organized on social media to draw attention to the deaths of unarmed black people killed by US police. Have news organizations responded to this demand, and have we seen a significant change over time in reporting about those deaths?</p><p>In this analysis of deaths from January 2013 through June 2016, we show that an unarmed black person killed by US police received 10.5x the incidence rate of news articles after Michael Brown’s death than those killed before, but that the predicted number of articles is no longer significantly different from 2013 levels.</p>",,,2016-12-13 16:23:36.317,True,2015-07-01,Whose Lives Matter in the News?,PUBLIC,,True,Civic Media,False
civilservant,jnmatias,False,"<p>The CivilServant project supports online communities to run their own experiments on the effects of moderation practices on antisocial behavior, harassment, discrimination, and community well-being online.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">All results are published to an open repository of collective knowledge on practices that contribute to fair, flourishing social life online.</span></p><p>The first experiment, in a 13.2 million subscriber community, showed that <a href=""http://civilservant.io/moderation_experiment_r_science_rule_posting.html"">posting rules at the top of conversations prevents problems and increases engagement</a>.</p>",,,2019-04-19 18:55:02.391,True,2016-07-01,CivilServant: User-led randomized trials online,PUBLIC,http://civilservant.io,True,Civic Media,False
going-dark-collective-action-in-the-reddit-blackout,jnmatias,False,"<p>How do people who lead communities on online platforms join together in mass collective action to influence platform operators? Going Dark analyzes a protest against the social news platform reddit by moderators of 2,278 communities in July of 2015. These moderators collectively disabled their communities, preventing millions of readers from accessing major parts of reddit and convincing the company to negotiate over their demands. This study reveals social factors—including the work of moderators, relations among moderators, relations with platform operators, factors within communities, and the isolation of a community—that can lead to participation in mass collective action against a platform.</p>",,--Choose Location,2019-04-19 18:58:39.891,True,2015-01-01,Going Dark: Collective action in the Reddit Blackout,PUBLIC,,True,Civic Media,False
peer-appreciation-in-the-workplace,jnmatias,False,"<p>Organizations are deploying gratitude-tracking systems to encourage appreciation, promote pro-sociality, and monitor employee wellbeing. We present the case study of one such system, called Gratia, adopted by a Fortune 500 company for over four years. We analyzed 422,209 messages of thanks and examined temporal patterns of appreciation, reciprocity, and repeated interactions. We also compared the formal organizational chart to the informal network expressed through the system. We found that gratitude is strongly reciprocated, that time between thanks is relatively long, and that it is predominantly given to peers outside one's immediate team.</p>",,--Choose Location,2018-10-20 01:44:04.200,True,2014-01-01,Peer Appreciation in the Workplace,PUBLIC,,True,Civic Media,False
glassprov-improv-comedy-system,swgreen,False,"<p>As part of a Google-sponsored Glass developer event, we created a Glass-enabled improv comedy show together with noted comedians from ImprovBoston and Big Bang Improv. The actors, all wearing Glass, received cues in real time in the course of their improvisation. In contrast with the traditional model for improv comedy, punctuated by ""freezing"" and audience members shouting suggestions, using Glass allowed actors to seamlessly integrate audience suggestions. Actors and audience members agreed that this was a fresh take on improv comedy. It was a powerful demonstration that cues on Glass are suitable for performance: actors could become aware of the cues without having their concentration or flow interrupted, and then view them at an appropriate time thereafter.</p>",2015-09-30,--Choose Location,2017-09-05 19:36:08.241,True,2014-01-01,GlassProv Improv Comedy System,PUBLIC,,False,Fluid Interfaces,False
body-quest,swgreen,False,"<p>Body Quest is a room-scale virtual reality playground for learning about biology and chemistry. Learning about how complex microscopic 3D structures interact is hard on paper, and only slightly easier with videos or passive 3D simulations. Interactive, room-scale VR environments open up new possibilities for building an intuitive and visual understanding of these subjects—and it can even be fun!</p><p>Our video was submitted to the DOE EdSim Challenge. It is a fully functional prototype—the mixed reality video representing the physical and virtual experiences were composited in real-time by calibrating physical and virtual cameras, while filming in front of a green screen. The prototype is built around one particular biochemical interaction, whereby a viral protein cleaves sugar off the end of a mucus chain. We hope to develop future learning interactions around a general simulation backend, which will host both structured and unstructured learning experiences, including games.</p><p><br></p>",2017-08-31,,2018-08-20 16:43:43.257,True,2016-12-15,Body Quest: A Room-Scale VR Playground for Biology and Chemistry,PUBLIC,,False,Fluid Interfaces,False
cocoverse,swgreen,False,<h2>Real-time collaborative self-expression in virtual reality&nbsp;</h2>,,,2019-04-17 20:08:49.312,True,2016-10-03,CocoVerse: A playground for co-creation and communication in virtual reality,PUBLIC,,True,Fluid Interfaces,False
vr-physics-lab,swgreen,False,"<p>Room-scale virtual reality opens up exciting new possibilities for exploratory learning. Phenomena that otherwise cannot be experienced directly (e.g. subjects that are microscopic, remote, or dangerous) can be transformed into environments that are immersive, interactive and social. Electrostatic Playground is a VR physics lab where multiple users can explore and discover principles of electrostatics through experimentation.&nbsp;<span style=""font-size: 18px;"">It also concretizes abstract notions of electrostatics in the form of tangible, interactive objects. Users can learn by directly manipulating physics objects while receiving real-time feedback from the environment. We've incorporated the ability to record these interactions in order to provide a means of authoring content, reviewing one's notes, and teaching others. Electrostatic Playground is a multi-user lab where users can explore and discover principles in electrostatics.</span><br></p>",,,2019-04-17 20:09:56.460,True,2016-06-01,Electrostatic Playground: A multi-user virtual reality physics learning experience,PUBLIC,,True,Fluid Interfaces,False
inforce,lajv,False,,2017-05-01,,2017-03-31 00:09:40.562,True,2015-09-25,inFORCE 1.0,LAB-INSIDERS,,False,Tangible Media,False
transform-adaptive-and-dynamic-furniture,lajv,False,"<p>Introducing TRANSFORM, a shape-changing desk. TRANSFORM is an exploration of how shape display technology can be integrated into our everyday lives as interactive, transforming furniture. These interfaces not only serve as traditional computing devices, but also support a variety of physical activities. By creating shapes on demand or by moving objects around, TRANSFORM changes the ergonomics and aesthetic dimensions of furniture, supporting a variety of use cases at home and work: it holds and moves objects like fruit, game tokens, office supplies, and tablets, creates dividers on demand, and generates interactive sculptures to convey messages and audio.</p>",,--Choose Location,2019-04-17 19:36:00.162,True,2014-09-01,TRANSFORM: Adaptive and dynamic furniture,PUBLIC,,True,Tangible Media,False
materiable-rendering-dynamic-material-properties-with-shape-changing-interfaces,lajv,False,"<p>Shape-changing interfaces give physical shape to digital data so that users can feel and manipulate data with their hands and body. Combining techniques from haptics with the field of shape-changing interfaces, we propose a technique to build a perceptive model of material properties by taking advantage of the shape display's ability to dynamically render flexibility, elasticity, and viscosity in response to the direct manipulation of any computationally rendered physical shape. Using a computer-generated relationship between the manipulated pins and nearby pins in the shape display, we can create human proprioception of various material properties. Our results show that users can identify varying material properties in our simulations through direct manipulation, and that this perception is gathered mainly from their physical relationship (touch) with the shape display and its dynamic movements.</p>",,--Choose Location,2016-12-16 20:08:16.037,True,2015-09-01,Materiable,PUBLIC,,True,Tangible Media,False
in-force,lajv,False,"<p>We propose a novel tangible interaction with pin-based shape display that can reproduce haptic perception of shape, material stiffness, and heterogeneous internal structures of volumetric shape. This is enabled by newly developed pin display, inFORCE, that can detect the force that is applied to each pin, and exert arbitrary force to contact body and objects at the same time. Our proposed interaction methods enabled people to ""press through"" computationally rendered shapes to understand the internal structure of 3D volumetric information. Our design space explores a range of interaction capability enabled by the Force Shape Display system including capturing physical material properties.</p>",,,2019-03-25 13:24:48.264,True,2017-10-01,inFORCE,LAB-INSIDERS,http://tangible.media.mit.edu/project/inforce/,True,Tangible Media,False
inforce,djfitz,False,,2017-05-01,,2017-03-31 00:09:40.562,True,2015-09-25,inFORCE 1.0,LAB-INSIDERS,,False,Tangible Media,False
ReVeal,djfitz,False,"<p>We are surrounded by digital information that we can't see or touch. &nbsp;The most common computer interfaces, 2D screens, are devoid of any inherent spatial context. Virtual reality lets us immerse ourselves visually in digital worlds, but requires cumbersome head-mounted displays. Meanwhile, regaining a sense of touch in VR with current haptic feedback systems is even more obtrusive and disappointing. Worse, we can only experience these virtual worlds to the exclusion of the real world, cutting ourselves off from real objects and people around us. &nbsp;</p><p>ReVeal is a mobile tangible interface that serves as an unobtrusive intermediary between coincident virtual and real physical worlds. Operating within a room-scale virtual reality system, the shape display takes the form of virtual objects ""under"" it, so the same objects can exist in the same place at the same time in both the real and virtual worlds. A handheld projector, tracked by a virtual camera, lets users reveal the virtual objects, like shining a flashlight to illuminate the invisible.&nbsp;</p>",2017-04-08,,2017-05-02 05:52:32.865,True,2016-09-01,ReVeal,LAB,,False,Tangible Media,False
repose-smart-pillow,djfitz,False,"<p>Sleep is perhaps the most critical human activity for well-being, but seldom receives the priority it deserves. We get less rest than we need, and the respite we do achieve is often not in&nbsp;proper or comfortable postures. We present a <b style=""font-size: 18px;"">smart pillow “RePose”</b><span style=""font-size: 18px; font-weight: 400;"">&nbsp;to explore a new genre of soft, shape-changing auto-adaptive furniture.&nbsp;This new class of interactive ""Smart Furniture"" device themed around pillows, couches, and beds are designed to transform for, conform to, and inform the user as they rest. Using inflatable pouches and pressure-sensors, RePose detects uncomfortable or unergonomic usage and changes its shape and stiffness accordingly to promote good posture. The result is a pillow that is comfortable for everyone in every situation, and is capable of a variety of additional functionalities for sensing and actuation.</span><br></p>",2016-09-01,--Choose Location,2017-12-07 03:48:02.073,True,2016-01-01,RePose,PUBLIC,,False,Tangible Media,False
laser-face,djfitz,False,"<p>Our eyes are our our mind's window to the external world. Vision is the primary way we sense our environment, and is a reliable indicator of our attention. As sensors, eyes evolved over millions of years to be fast, precise, and accurate, especially for tracking visual elements of interest (targets), like predators, prey, or baseballs.&nbsp;However, from an HCI perspective, these amazing abilities are also desirable actuators as well as sensors. What if the eyes can not only be a passive window of focus, but an active spatial cursor to indicate our target of focus? Also, what if we add <i>lasers</i>?</p>",2015-12-16,,2017-12-07 04:43:36.579,True,2015-10-01,LaserVision,PUBLIC,,False,Tangible Media,False
influx,djfitz,False,"<p>We present a stiffness-changing interface based on a magneto-rheological (MR) fluid. The device consists of a material&nbsp;surface with electromagnetically&nbsp;induced visco-elasticity,&nbsp;which acts as a proxy for stiffness during tangible interaction with the material. We present several advantages of&nbsp;this enabling technology and outline potential applications&nbsp;and routes for future development.</p>",2015-12-15,,2017-12-07 04:43:01.290,True,2015-11-01,inFlux,PUBLIC,,False,Tangible Media,False
cultural-lens,djfitz,False,"<p>Cultural Lens is a speculative design project that reverses traditional gender responsibilities around clothing and personal appearance, interrogating the status of prevalent historical arguments around modesty and etiquette. Every society, culture, and religion has implicit or explicit expectations for women's clothing and public appearance, which has recently led to debates around issues for human rights, freedom, and self-expression.</p><p>Cultural Lens asks society-at-large to share the burden of enforcing public appearance for women. Instead of forcing women to wear clothing styles that are deemed acceptable by the public, Cultural Lens allows members of the public that might be offended–by, for example, perceived immodesty or improper etiquette–to have the freedom to selectively filter the appearance of the women they see in public to conform  literally to their view of how women should look.&nbsp;</p><p>The system uses a Microsoft HoloLens to implement an Augmented Reality visual field for the user.  The system can identify people and faces in the view, classify gender, and apply visual filters to their appearance according to the user's preference. For example, Cultural Lens can add digital veil to the faces of all women the user observes.&nbsp;<br></p>",,,2017-03-29 20:15:11.938,True,2017-01-03,Cultural Lens,PUBLIC,,True,Tangible Media,False
in-force,djfitz,False,"<p>We propose a novel tangible interaction with pin-based shape display that can reproduce haptic perception of shape, material stiffness, and heterogeneous internal structures of volumetric shape. This is enabled by newly developed pin display, inFORCE, that can detect the force that is applied to each pin, and exert arbitrary force to contact body and objects at the same time. Our proposed interaction methods enabled people to ""press through"" computationally rendered shapes to understand the internal structure of 3D volumetric information. Our design space explores a range of interaction capability enabled by the Force Shape Display system including capturing physical material properties.</p>",,,2019-03-25 13:24:48.264,True,2017-10-01,inFORCE,LAB-INSIDERS,http://tangible.media.mit.edu/project/inforce/,True,Tangible Media,False
friendship-reciprocity-behavioral-change,amaatouq,False,"<p>When we analyzed self-reported relationship surveys from several experiments around the world (from human subjects, not hobbits!), we found that while most people assume friendships to be two-way, only about half of friendships are indeed reciprocal. In itself this may seem like an interesting but minor finding, but this large proportion of asymmetric friendships translates to a major effect on the ability of individuals to persuade others to cooperate or change their behavior.</p><p>For example, when we examined the properties of friendship networks and how the directionality of ties can impact the level of influence that individuals exert on one another (based on analysis of a fitness and physical activity intervention where information about physical activity was collected passively by smartphones), we found that the program was more effective when a unilateral friendship tie existed from the buddy (the person applying peer pressure) to the subject (the person receiving the pressure) than when the friendship tie was from the subject to the buddy. In this example, reciprocal friendships are best, but having a buddy who thinks of the subject as a friend is the next best relationship. We attribute the difference to our peer-to-peer incentive mechanism—as buddies were rewarded based on the progress of the subject, there are likely to be differences in communication when the buddy believes the subject to be their friend versus when they do not.</p><p>The findings of this work have significant consequences for designing interventions that seek to harness social influence:</p><p>Intervention designers, whether with fitness programs, smoking cessation programs, or any other attempt to change a subject’s behavior, can't rely on how the subject perceives the relationship with the buddy to create effectiveness.</p><p>Also, we shouldn’t assume people with a high number of social ties are “influencers.” Such people are no better and often are worse than average at exerting social influence. Our results suggest that this is because many of those ties either are not reciprocal or go in the wrong direction, and therefore won’t lead to effective persuasion.</p><p>We demonstrate that an assumption common in previous studies of social influence, namely that friendships are created equal or reciprocal by default, is erroneous, which may have significantly biased the research results.</p><p>We hope that by understanding the factors and network properties that impact the level of social influence individuals exert on one another, we can be more effective at promoting behavioral change, disseminating new ideas, and even promoting products.</p>",2016-08-18,,2016-12-05 00:17:27.755,True,2015-05-01,Friendship Reciprocity and Behavioral Change,PUBLIC,,False,Human Dynamics,False
social-physics-of-unemployment,amaatouq,False,"<p>Earlier studies proved that behavior is highly shaped and constrained by one's social networks, and demonstrated ways in which individuals can manipulate these networks to achieve specific goals. A great example is the much-studied ""strength of weak ties"" hypothesis, which states that the strength of a tie between A and B increases with the overlap of their friendship circles, resulting in an important role for weak ties in connecting communities. Mark Granovetter first proposed this idea in a study that emphasized the nature of the tie between job changers in a Boston suburb and the contacts who provided the necessary information for them to obtain new employment.  Basically, although people with whom the job seekers had strong ties were more motivated to provide information, the structural position of weak ties played a more important role. The implication is that those to whom one is weakly tied are more likely to move in different circles, and will thus have access to different information than the people to whom you are tied more strongly. </p><p>Much of our knowledge about how mobility, social networks, communication, and education affect the economic status of individuals and cities has been obtained through complex and costly surveys, with an update rate ranging from fortnights to decades. However, recent studies have shown the value of mobile phone data as an enabling methodology for demographic modeling and measurement.</p><p>Many of our daily routines are driven by activities either afforded by our economic status or related to maintaining or improving it, from our movements around the city, to our daily schedules, to our communication with others. As such, we expect to be able to measure passive patterns and behavioral indicators, using mobile phone data, that could describe local unemployment rates.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">To investigate this question, we examined anonymized mobile phone metadata combined with beneficiaries' records from an unemployment benefit program. We found that aggregated activity, social, and mobility patterns strongly correlate with unemployment. Furthermore, we constructed a simple model to produce accurate reconstructions of district-level unemployment from mobile communication patterns alone.</span></p><p>Our results suggest that reliable and cost-effective indicators of economic activity could be built based on passively collected and anonymized mobile phone data. With similar data being collected every day by telecommunication services across the world, survey-based methods of measuring community socioeconomic status could potentially be augmented or replaced by such passive sensing methods.</p>",,,2019-06-05 19:17:28.176,True,2014-07-23,Social Physics of Unemployment,PUBLIC,,True,Human Dynamics,False
friendship-reciprocity-behavioral-change,sandy,False,"<p>When we analyzed self-reported relationship surveys from several experiments around the world (from human subjects, not hobbits!), we found that while most people assume friendships to be two-way, only about half of friendships are indeed reciprocal. In itself this may seem like an interesting but minor finding, but this large proportion of asymmetric friendships translates to a major effect on the ability of individuals to persuade others to cooperate or change their behavior.</p><p>For example, when we examined the properties of friendship networks and how the directionality of ties can impact the level of influence that individuals exert on one another (based on analysis of a fitness and physical activity intervention where information about physical activity was collected passively by smartphones), we found that the program was more effective when a unilateral friendship tie existed from the buddy (the person applying peer pressure) to the subject (the person receiving the pressure) than when the friendship tie was from the subject to the buddy. In this example, reciprocal friendships are best, but having a buddy who thinks of the subject as a friend is the next best relationship. We attribute the difference to our peer-to-peer incentive mechanism—as buddies were rewarded based on the progress of the subject, there are likely to be differences in communication when the buddy believes the subject to be their friend versus when they do not.</p><p>The findings of this work have significant consequences for designing interventions that seek to harness social influence:</p><p>Intervention designers, whether with fitness programs, smoking cessation programs, or any other attempt to change a subject’s behavior, can't rely on how the subject perceives the relationship with the buddy to create effectiveness.</p><p>Also, we shouldn’t assume people with a high number of social ties are “influencers.” Such people are no better and often are worse than average at exerting social influence. Our results suggest that this is because many of those ties either are not reciprocal or go in the wrong direction, and therefore won’t lead to effective persuasion.</p><p>We demonstrate that an assumption common in previous studies of social influence, namely that friendships are created equal or reciprocal by default, is erroneous, which may have significantly biased the research results.</p><p>We hope that by understanding the factors and network properties that impact the level of social influence individuals exert on one another, we can be more effective at promoting behavioral change, disseminating new ideas, and even promoting products.</p>",2016-08-18,,2016-12-05 00:17:27.755,True,2015-05-01,Friendship Reciprocity and Behavioral Change,PUBLIC,,False,Human Dynamics,False
visualizing-patterns-of-segregation,sandy,False,"<p>Segregation, or income inequality, is one of the major problems of our society. Residential segregation has long been of research interest in fields such as sociology, economics, and psychology. But our behaviors are also segregated. In this project we want to visualize how human behaviors—like conversations in Twitter, mobility around the cities, or purchasing—show patterns of segregation. Our objective is to allow people to understand the segregation of behaviors in their cities to increase the awareness of that problem, but also to show how we can address potential solutions by using different layers of big datasets.</p>",2017-06-30,,2017-04-05 18:53:49.527,True,2017-03-27,Visualizing patterns of segregation,PUBLIC,,False,Human Dynamics,False
data-pop-alliance,sandy,False,"<p>Data-Pop Alliance is a joint initiative on big data and development with a goal of helping to craft and leverage the new ecosystem of big data--new personal data, new tools, new actors--to improve decisions and empower people in a way that avoids the pitfalls of a new digital divide, de-humanization, and de-democratization. Data-Pop Alliance aims to serve as a designer, broker, and implementer of ideas and activities, bringing together institutions and individuals around common principles and objectives through collaborative research, training and capacity building, technical assistance, convening, knowledge curation, and advocacy. Our thematic areas of focus include official statistics, socio-economic and demographic methods, conflict and crime, climate change and environment, literacy, and ethics.</p>",,--Choose Location,2016-12-05 00:16:20.681,True,2014-01-01,Data-Pop Alliance,PUBLIC,,True,Human Dynamics,False
enigma,sandy,False,"<p>A peer-to-peer network, enabling different parties to jointly store and run computations on data while keeping the data completely private. Enigma's computational model is based on a highly optimized version of secure multi-party computation, guaranteed by a verifiable secret-sharing scheme. For storage, we use a modified distributed hashtable for holding secret-shared data. An external blockchain is utilized as the controller of the network, manages access control and identities, and serves as a tamper-proof log of events. Security deposits and fees incentivize operation, correctness, and fairness of the system. Similar to Bitcoin, Enigma removes the need for a trusted third party, enabling autonomous control of personal data. For the first time, users are able to share their data with cryptographic guarantees regarding their privacy.</p>",,--Choose Location,2018-02-12 20:37:17.158,True,2015-01-01,Enigma,PUBLIC,,True,Human Dynamics,False
leveraging-leadership-expertise-more-effectively-in-organizations,sandy,False,"<p>We believe that the narrative of only listening to experts or trusting the wisdom of the crowd blindly is flawed. Instead we have developed a system that weighs experts and lay-people differently and dynamically and show that a good balance is required. We show that our methodology leads to a 15 percent improvement in mean performance, 15 percent decrease in variance, and almost 30 percent increase in Sharpe-type ratio in a real online market.</p>",,--Choose Location,2019-04-19 14:45:04.070,True,2015-01-01,Leveraging leadership expertise more effectively in organizations,PUBLIC,,True,Human Dynamics,False
blockchain-a-new-framework-for-swarm-robotic-systems,sandy,False,"<p>Swarms of robots will revolutionize many applications, from targeted material delivery to farming. However, the characteristics that make them ideal for certain future applications, such as robot autonomy or decentralized control, can also be an obstacle when transferring this technology from academia to real-world problems. Blockchain, an emerging technology, demonstrates that by combining peer-to-peer networks with cryptographic algorithms, a group of agents can reach agreements without the need for a controlling authority. The combination of blockchain with other distributed systems, such as robotic swarm systems, can provide the necessary capabilities to make robotic swarm operations more secure, autonomous, flexible, and even profitable.</p>",,,2019-04-19 14:39:26.259,True,2017-09-01,Blockchain: A new framework for robotic swarm systems,PUBLIC,http://www.eduardocastello.com,True,Human Dynamics,False
bandicoot-a-python-toolbox-for-mobile-phone-metadata,sandy,False,"<p>bandicoot provides a complete, easy-to-use environment for researchers using mobile phone metadata. It allows them to easily load their data, perform analysis, and export their results with a few lines of code. It computes 100+ standardized metrics in three categories: individual (number of calls, text response rate), spatial (radius of gyration, entropy of places), and social network (clustering coefficient, assortativity). The toolbox is easy to extend and contains extensive documentation with guides and examples.</p>",,--Choose Location,2019-04-19 14:40:08.264,True,2015-09-01,bandicoot: A Python toolbox for mobile phone metadata,PUBLIC,,True,Human Dynamics,False
collective-sensemaking-in-cryptocurrency-community,sandy,False,"<p>Participants in cryptocurrency markets are in constant communication with each other about the latest coins&nbsp;and news releases. Do these conversations build hype through the contagiousness of excitement, help the&nbsp;community process information, or play some other role? Using a novel dataset from a major cryptocurrency&nbsp;forum, we conduct an exploratory study of the characteristics of online discussion around cryptocurrencies. We find that coins with more information available and higher levels of&nbsp;technical innovation are associated with higher quality discussion. People who talk about serious coins tend&nbsp;to participate in discussion displaying signatures of collective intelligence and information processing, while&nbsp;people who talk about less serious coins tend to display signatures of hype and naïvety. Interviews with&nbsp;experienced forum members also confirm these quantitative findings. These results highlight the varied roles&nbsp;of discussion in the cryptocurrency ecosystem and suggest that discussion of serious coins may be oriented&nbsp;towards earnest, perhaps more accurate, attempts at discovering which coins are likely to succeed.&nbsp;</p>",,,2019-04-19 14:41:02.807,True,2016-06-01,Collective sensemaking in cryptocurrency community,PUBLIC,,True,Human Dynamics,False
data-for-refugees,sandy,False,"<p>Data for refugees is a big data challenge whereby Turk Telekom opens a large dataset of anonymized mobile phone records to research groups for the purpose of providing better living conditions to Syrian refugees in Turkey.&nbsp;&nbsp;</p><p>We introduce different measures extracted from mobile phone metadata to study the integration of refugees along three dimensions: (1) social integration, (2) spatial integration, and (3) economic integration through signatures of employment activity. We use these measures to compare integration across different regions in Turkey and find striking differences both in the distributions of these dimensions and the relations between them.&nbsp;<br></p><p>The paper is currently under review but will be shared soon.&nbsp;</p>",,,2019-04-19 14:42:49.278,True,2018-06-01,Data for Refugees,PUBLIC,,True,Human Dynamics,False
deepshop-understanding-purchase-patterns-via-deep-learning,sandy,False,"<p>The recent availability of quantitative behavioral data provides an opportunity to study human behavior at unprecedented scale. Using large-scale financial transaction data, we propose a novel deep learning framework for understanding human purchase patterns and testing the link between them and the existence of individual financial troubles. Our work opens new possibilities in studying human behavioral traits using state-of-the-art machine learning techniques, without the need for hand-engineered features.</p>",,--Choose Location,2019-04-19 14:43:35.517,True,2016-01-01,DeepShop: Understanding purchase patterns via deep learning,PUBLIC,,True,Human Dynamics,False
on-the-reidentifiability-of-credit-card-metadata,sandy,False,"<p>Even when real names and other personal information are stripped from metadata datasets, it is often possible to use just a few pieces of information to identify a specific person. Here, we study three months of credit card records for 1.1 million people and show that four spatiotemporal points are enough to uniquely reidentify 90 percent of individuals. We show that knowing the price of a transaction increases the risk of reidentification by 22 percent, on average. Finally, we show that even data sets that provide coarse information at any or all of the dimensions provide little anonymity, and that women are more reidentifiable than men in credit card metadata.</p>",,--Choose Location,2019-04-19 14:48:17.075,True,2015-01-01,On the re-identifiability of credit card metadata,PUBLIC,,True,Human Dynamics,False
recurrent-neural-network-in-context-free-next-location-prediction,sandy,False,"<p>Location prediction is a critical building block in many location-based services and transportation management. This project explores the issue of next-location prediction based on the longitudinal movements of the locations individuals have visited, as observed from call detail decords (CDR). In a nutshell, we apply recurrent neural network (RNN) to next-location prediction on CDR. RNN can take in sequential input with no restriction on the dimensions of the input. The method can infer the hidden similarities among locations and interpret the semantic meanings of the locations. We compare the proposed method with Markov and a Naive Model proving that RNN has better accuracy in location prediction. </p>",,--Choose Location,2019-04-19 14:52:06.657,True,2016-01-01,Recurrent neural network in context-free next-location prediction,PUBLIC,,True,Human Dynamics,False
robochain-a-secure-data-sharing-framework-for-human-robot-interaction,sandy,False,"<p> A learning framework for secure, decentralized, computationally efficient data and model sharing among multiple robot units installed at multiple sites.<br></p><p>Robots have potential to revolutionize the way we interact with the world around us. One of their greatest potentials is in the domain of mobile health, where they can be used to facilitate clinical interventions. However, to accomplish this, robots need to have access to our private data in order to learn from these data and improve their interaction capabilities. To enhance this learning process, knowledge sharing among multiple robot units is the natural step forward. However, to date, there is no well-established framework which allows for such data sharing while preserving the privacy of the users, such as hospital patients. To this end, we introduce RoboChain: the first learning framework for secure, decentralized, computationally efficient data and model sharing among multiple robot units installed at multiple sites such as hospitals. RoboChain builds upon and combines the latest advances in open data access, blockchain technologies, and machine learning. We illustrate this framework using the example of a clinical intervention conducted in a private network of hospitals. Specifically, we lay down the system architecture that allows multiple robot units, conducting the interventions at different hospitals, to perform efficient learning without compromising the data privacy.&nbsp;&nbsp;</p>",,,2019-04-19 14:53:57.705,True,2018-03-01,RoboChain: A secure data-sharing framework for human-robot interaction,PUBLIC,http://www.eduardocastello.com,True,Human Dynamics,False
the-ripple-effect-your-are-more-influential-than-you-think,sandy,False,"<p>The well-known ""small-world"" phenomenon indicates that an individual can be connected with any other in the world through a limited number of personal acquaintances. Furthermore, Nicholas and Fowler show that not only are we connected to each other, but we could also shape the behavior of our friends' friends. In this project, we are interested in understanding how social influence propagates and triggers behavioral change in social networks. Specifically, we analyze a large-scale, one-month international event held in the European country of Andorra using country-wide mobile phone data, and investigate the change in the likelihood of attending the event for people that have been&nbsp;<span style=""font-size: 18px; font-weight: normal;"">influenced by and are of different social distances from the attendees.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">Our results suggest that social influence exhibits the ripple effect, decaying across social distances from the source but persisting up to six degrees of separation. We further show that influence decays as communication delay increases and intensity decreases. Such ripple effect in social communication can lead to important policy implications in applications where it is critical to trigger behavior change in the population.</span></p>",,,2019-04-19 14:55:02.488,True,2016-08-01,The Ripple Effect: You are more influential than you think,PUBLIC,,True,Human Dynamics,False
social-bridges-in-community-purchase-behavior,sandy,False,"<p>The understanding and modeling of social influence on human economic behavior in city environments can have important implications. In this project, we study human purchase behavior at a community level and argue that people who live in different communities but work at similar locations could act as ""social bridges"" that link their respective communities and make the community purchase behavior similar through the possibility of social learning through face-to-face interactions.</p>",,--Choose Location,2016-12-05 00:17:04.354,True,2015-09-01,Social Bridges in Community Purchase Behavior,PUBLIC,,True,Human Dynamics,False
yvas-untitled-project-2,sandy,False,"<p>OPAL is a project to allow for private data to be used in privacy-conscientious ways for good. Collaborating companies can use OPAL's open platform and algorithms behind their own firewalls to extract key development indicators. OPAL grew out of the recognition that accessing big data sources for research and policy purposes has been a conundrum. To date, data held by private companies, such as large-scale mobile phone data, have been accessed and analyzed externally, either through data challenges, or through bilateral agreements. While these types of engagements offered evidence of big data's promise and demand, these modalities limit the full realization of its potential. By ""sending the code to the data"" rather than the other way around, OPAL seeks to address these challenges and develop data services on the basis of greater trust between all parties involved.</p>",,--Choose Location,2016-10-24 19:44:56.655,False,2016-01-01,OPAL: Privacy-Conscientious Use of Mobile Phone Data,PUBLIC,,True,Human Dynamics,False
secure-sharing-of-wildlife-data,sandy,False,"<h2>Leveraging the power of platforms, big data, and advanced analytics for species protection and the public good in a privacy-preserving, scalable, and sustainable manner</h2><p>Modern tracking technology enables new ways of mining data in the wild. It allows wildlife monitoring centers to permanently collect geospatial data in a non-intrusive manner and in real time. Unfortunately, such sensible data is exposed to fraud and misuse and there is already a first reported case of ""cyber-poaching."" Based on stolen geospatial data, poachers can easily track and kill animals. Meanwhile, cautious monitoring centers limited data access for research and public use. We propose a novel privacy-preserving system to allow these monitoring centers to securely answer questions from the research community and the public while the raw data is protected against unauthorized third parties. Based on the core system, several new applications are conceivable, such as a mobile app for preventing conflicts between human and wildlife or for engaging people in wildlife donation. Besides providing a solution and working on specific use cases, the intention of this project is to start a discussion about the need for data protection in the animal world. </p><p>
                    
                </p>",,,2017-05-24 15:32:00.205,True,2017-02-01,Secure Sharing of Wildlife Data,PUBLIC,http://www.WildlifeData.org,True,Human Dynamics,False
evolution-strategies-applied-to-collective-intelligence,sandy,False,"<p>We build recommender bots that use machine learning and network analytics to create personalized recommendations for users on various social and financial platforms. We show that bots that work not just on the raw user data, but instead build on human intuition, do far better. We are in the process of live testing these bots on various platforms.&nbsp;</p>",,,2017-04-05 18:49:11.444,True,2017-03-10,Social Learning Recommender Bots,PUBLIC,,True,Human Dynamics,False
prediction-markets-leveraging-internal-knowledge-to-beat-industry-prediction-experts,sandy,False,"<p>Markets are notorious for bubbles and bursts. Other research has found that crowds of lay-people can replace even leading experts to predict everything from product sales to the next big diplomatic event. In this project, we leverage both threads of research to see how prediction markets can be used to predict business and technological innovations, and use them as a model to fix financial bubbles. For example, a prediction market was rolled out inside of Intel and the experiment was very successful, and led to better predictions than the official Intel forecast 75 percent of the time. Prediction markets also led to as much as a 25 percent reduction in mean squared error over the prediction of official experts at Google, Ford, and Koch industries.</p>",,--Choose Location,2019-04-19 14:51:23.617,True,2015-01-01,Prediction Markets: Leveraging internal knowledge to beat industry prediction experts,PUBLIC,,True,Human Dynamics,False
social-ai-and-extended-intelligence,sandy,False,"<p>There is a deep fear that human jobs will be replaced by AI. Rather than racing against the machines, our aim is to show that a human-AI combination will perform better than humans and AI working alone. Although no man is better than a machine for some tasks, ""no machine is better than a man with a machine"" (Paul Tudor Jones) . Thus, by building ""bots"" that are compatible with human behavior, and specifically leverage the manner in which humans use social information, we have been able to build bots that extend human intelligence capabilities. In a large-scale financial trading experiment, we have shown that groups of humans and ""socially compatible"" AI bots can successfully incorporate human intuition into their decisions and consequently not only do better than humans alone, but also do better than similar AI bots that use only objective information.
                    
                </p>",,,2019-02-12 15:14:05.048,True,2016-07-01,Social AI and Extended Intelligence,PUBLIC,,True,Human Dynamics,False
open-badges,sandy,False,"<p>We present Open Badges, an open-source framework and toolkit for measuring and shaping face-to-face social interactions using either custom hardware devices or smart phones, and real-time web-based visualizations. Open Badges is a modular system that allows researchers to monitor and collect interaction data from people engaged in real-life social settings.</p>",,--Choose Location,2019-01-10 19:19:48.517,True,2014-09-01,Open Badges,PUBLIC,,True,Human Dynamics,False
urban-swarms,sandy,False,"<p>Modern cities have to respond to the growing demands of more efficient and sustainable urban development, as well as an increased quality of life. In this context, the cities of the future will need the ability to gain insight about current urban conditions and react dynamically to them. According to this view, ""smart cities"" can be seen as cybernetic urban environments in which different agents (e.g., citizens) and actuators (e.g., robots) exploit the city-wide infrastructure as a medium to operate synergistically.<b><i> Urban Swarms</i></b> explores the feasibility of swarm robotics systems in urban environments. By using bio-inspired methods, a swarm of robots is able to handle important urban systems and infrastructures, improving their efficiency and autonomy. A diverse set of simulation experiments were designed and conducted using real-world GIS data. Results show that the proposed combination is able to outperform current approaches. <i><b>Urban Swarms</b></i> not only aims to show the efficiency of our proposed solution, but also to give insights about how to design and customize these systems.&nbsp;<a href=""https://www.media.mit.edu/projects/cityscope-volpe/overview/"" style=""font-size: 18px; font-weight: 400;"">CityScope</a><span style=""font-size: 18px; font-weight: 400;"">&nbsp;Volpe ABM model has been customized to integrate Swarm behavior using the </span><a href=""https://gama-platform.github.io/"" style=""font-size: 18px; font-weight: 400;"">Gama Platform</a><span style=""font-size: 18px; font-weight: 400;""> as an </span><a href=""https://github.com/mitmedialab/UrbanSwarms"" style=""font-size: 18px; font-weight: 400;"">open source project</a><span style=""font-size: 18px; font-weight: 400;"">.&nbsp;</span></p>",,,2019-03-12 15:37:36.573,True,2018-10-01,Urban Swarms,PUBLIC,http://www.eduardocastello.com,True,Human Dynamics,False
improving-official-statistics-in-emerging-markets-using-machine-learning-and-mobile-phone-data,sandy,False,"<p><span style=""font-size: 18px; font-weight: 400;"">Mobile phones are one of the fastest growing technologies in the developing world with global penetration rates reaching 90%. Mobile phone data, also called CDR, are generated every time phones are used and recorded by carriers at scale. CDR have generated groundbreaking insights in public health, official statistics, and logistics. However, the fact that most phones in developing countries are prepaid means that the data lacks key information about the user, including gender and other demographic variables. This precludes numerous uses of this data in social science and development economic research. It furthermore severely prevents the development of humanitarian applications such as the use of mobile phone data to target aid towards the most vulnerable groups during crisis.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: 400;"">We developed a framework to extract more than 1,400 features from standard mobile phone data and used them to predict useful individual characteristics and group estimates. We here present a systematic cross-country study of the applicability of machine learning for dataset augmentation at low cost. We validate our framework by showing how it can be used to reliably predict gender and other information for more than half a million people in two countries. We show how standard machine learning algorithms trained on only 10,000 users are sufficient to predict individual’s gender with an accuracy ranging from 74.3 to 88.4% in a developed country and from 74.5 to 79.7% in a developing country using only metadata. This is significantly higher than previous approaches and, once calibrated, gives highly accurate estimates of gender balance in groups. Performance suffers only marginally if we reduce the training size to 5,000, but significantly decreases in a smaller training set. We finally show that our indicators capture a large range of behavioral traits using factor analysis and that the framework can be used to predict other indicators of vulnerability such as age or socio-economic status. Mobile phone data has a great potential for good and our framework allows this data to be augmented with vulnerability and other information at a fraction of the cost.</span></p>",,,2018-10-19 20:56:21.936,True,2016-06-01,Improving official statistics in emerging markets using machine learning and mobile phone data,PUBLIC,,True,Human Dynamics,False
location-recommendations-based-on-large-scale-call-detail-records,sandy,False,"<p>Growth in leisure travel has become increasingly significant economically, socially, and environmentally. However, flexible but uncoordinated travel behaviors exacerbate traffic congestion. Mobile phone records not only reveal human mobility patterns, but also enable us to manage travel demand for system efficiency. We propose a location recommendation system that infers personal preferences while accounting for constraints imposed by road capacity in order to manage travel demand. We first infer unobserved preferences using a machine learning technique from phone records. We then formulate an optimization method to improve system efficiency. Coupling mobile phone data with traffic counts and road network infrastructures collected in Andorra, this study shows that uncoordinated travel behaviors lead to longer average travel delay, implying opportunities in managing travel demand by collective decisions. The interplay between congestion relief and overall satisfied location preferences observed in extensive simulations indicate that moderate sacrifices of individual utility lead to significant travel time savings. Specifically, the results show that under full compliance rate, travel delay fell by 52 percent at a cost of 31 percent less satisfaction. Under 60 percent compliance rate, 41 percent travel delay is saved with a 17 percent reduction in satisfaction.This research highlights the effectiveness of the synergy among collective behaviors in increasing system efficiency. </p>",,--Choose Location,2018-10-19 21:02:37.566,True,2015-12-01,Managing Travel Demand: Location recommendation for system efficiency,PUBLIC,,True,Human Dynamics,False
social-capital-accounting,sandy,False,"<p>To better understand and improve the quality of our lives, there has been a need for measuring non-economic capital such as social capital and natural capital in addition to economic capital. Quantifying non-economic capital, however, is not easy and has not been widespread. In this project, we propose a system where individuals can start measuring their social capital, turning them into a real-world asset that enables the improvement their economic wellbeing, while preserving individual privacy and security.</p>",,,2018-11-15 19:26:04.996,True,2018-06-01,Social Capital Accounting,PUBLIC,,True,Human Dynamics,False
opal-health,sandy,False,<h1><b>Open Algorithms (OPAL)</b></h1>,,,2018-10-19 21:07:55.443,True,2017-11-01,OPAL 4 Health,PUBLIC,https://www.shadaalsalamah.com/,True,Human Dynamics,False
the-atlas-of-inequality,sandy,False,"<h2><b>Segregation is hurting our societies and especially our cities. But economic inequality isn't just limited to neighborhoods. The restaurants, stores, and other places we visit in cities are all unequal in their own way.&nbsp;</b></h2><p>The Atlas of Inequality &nbsp;shows the income inequality of people who visit different places in the Boston metro area. It uses aggregated anonymous location data from digital devices to estimate people's incomes and where they spend their time.&nbsp;Using that data, we've made our own <b>place inequality </b><b>metric</b> to capture how unequal the incomes of visitors to each place are. Economic inequality isn't just limited to neighborhoods; it's part of the places you visit every day.</p><p>Try it yourself here:</p><h2><a href=""http://inequality.media.mit.edu""><b>The Atlas of Inequality</b></a></h2><p>The Atlas of Inequality is a project from the Human Dynamics group at the <a href=""https://www.media.mit.edu/"">MIT Media Lab</a> and the Department of Mathematics at <a href=""http://www.uc3m.es/"">Universidad Carlos III de Madrid</a>.</p><p>It is part of a broader initiative to understand human behavior in our cities and how large-scale problems like transportation, housing, segregation, or inequality depend in part on the emergent patterns of people’s individual opportunities and choices.</p>",,,2019-03-20 17:07:49.005,True,2019-03-01,The Atlas of Inequality,PUBLIC,https://inequality.media.mit.edu,True,Human Dynamics,False
active-fairness,sandy,False,"<h2>Algorithmic Fairness</h2><p>Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Substantial work in algorithmic fairness has surged, focusing on either post-processing trained models, constraining learning processes, or pre-processing training data.&nbsp;Recent work has proposed optimal post-processing methods that randomize classification decisions on a fraction of individuals in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concerns due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail.&nbsp;</p><h2>Active Fairness</h2><p>The present work proposes an alternative <b>active framework for fair classification</b>, where, in deployment, a decision maker adaptively acquires information according to the needs of different groups or individuals towards balancing disparities in classification performance. We propose two such methods where information collection is adapted to group- and individual-level needs, respectively. We show on real-world datasets that these can achieve: 1) <b>calibration and single error parity</b> (e.g., equal opportunity) and 2) <b>parity in both false positive and false negative rates</b> (e.g., equal odds). Moreover, we show that, by leveraging their additional degree of freedom, active approaches can <b>outperform randomization-based classifiers previously considered optimal</b>, while also avoiding limitations such as intra-group unfairness.</p>",,,2019-01-02 20:52:50.453,True,2018-01-31,Active Fairness in Algorithmic Decision Making,PUBLIC,,True,Human Dynamics,False
the-trade-off-between-the-utility-and-privacy-risks-of-location-data-and-implications-for-data-as-a-public-good,sandy,False,"<p><i>Paper presented at the ""Connected Life 2019: Data &amp; Disorder"" conference at the Oxford Internet Institute.</i></p><p>High-resolution individual geolocation data passively collected from mobile phones is increasingly sold in private markets and shared with researchers.</p><p>This data poses significant security, privacy, and ethical risks: it’s been shown that users can be re-identified in such datasets, and its collection rarely involves their full consent or knowledge. This data is valuable to private firms (e.g. targeted marketing) but also presents clear value as a public good. Recent public interest research has demonstrated that high-resolution location data can more accurately measure segregation in cities and provide inexpensive transit modeling. But as data is aggregated to mitigate its re-identifiability risk, its value as a good diminishes. How do we rectify the clear security and safety risks of this data, its high market value, and its potential as a resource for public good? We extend the recently proposed concept of a tradeoff curve that illustrates the relationship between dataset utility and privacy. We then hypothesize how this tradeoff differs between private market use and its potential use for public good. We further provide real-world examples of how high resolution location data, aggregated to varying degrees of privacy protection, can be used in the public sphere and how it is currently used by private firms.</p>",,,2019-05-24 15:26:51.045,False,2019-03-01,The Tradeoff Between the Utility and Risk of Location Data and Implications for Public Good,LAB,,True,Human Dynamics,False
social-physics-of-unemployment,sandy,False,"<p>Earlier studies proved that behavior is highly shaped and constrained by one's social networks, and demonstrated ways in which individuals can manipulate these networks to achieve specific goals. A great example is the much-studied ""strength of weak ties"" hypothesis, which states that the strength of a tie between A and B increases with the overlap of their friendship circles, resulting in an important role for weak ties in connecting communities. Mark Granovetter first proposed this idea in a study that emphasized the nature of the tie between job changers in a Boston suburb and the contacts who provided the necessary information for them to obtain new employment.  Basically, although people with whom the job seekers had strong ties were more motivated to provide information, the structural position of weak ties played a more important role. The implication is that those to whom one is weakly tied are more likely to move in different circles, and will thus have access to different information than the people to whom you are tied more strongly. </p><p>Much of our knowledge about how mobility, social networks, communication, and education affect the economic status of individuals and cities has been obtained through complex and costly surveys, with an update rate ranging from fortnights to decades. However, recent studies have shown the value of mobile phone data as an enabling methodology for demographic modeling and measurement.</p><p>Many of our daily routines are driven by activities either afforded by our economic status or related to maintaining or improving it, from our movements around the city, to our daily schedules, to our communication with others. As such, we expect to be able to measure passive patterns and behavioral indicators, using mobile phone data, that could describe local unemployment rates.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">To investigate this question, we examined anonymized mobile phone metadata combined with beneficiaries' records from an unemployment benefit program. We found that aggregated activity, social, and mobility patterns strongly correlate with unemployment. Furthermore, we constructed a simple model to produce accurate reconstructions of district-level unemployment from mobile communication patterns alone.</span></p><p>Our results suggest that reliable and cost-effective indicators of economic activity could be built based on passively collected and anonymized mobile phone data. With similar data being collected every day by telecommunication services across the world, survey-based methods of measuring community socioeconomic status could potentially be augmented or replaced by such passive sensing methods.</p>",,,2019-06-05 19:17:28.176,True,2014-07-23,Social Physics of Unemployment,PUBLIC,,True,Human Dynamics,False
open-trialchain,sandy,False,"<p>In this project, we are motivated to address clinical trials issues using Blockchain technology. We propose, OPEN TrialChain, a privacy-preserving Blockchain-based data sharing infrastructure that uses open algorithms between stakeholder of the federation in the clinical trials ecosystem. OPEN TrialChain balances between the sharing of clinical data and the need for subject’s privacy protection by allowing queries on decentralized raw datasets from which it returns aggregated safe answers that are blinded (i.e. anonymized). Using Type II Diabetes as a case study, we study the adoption of OPEN TrialChain and how it can address clinical trials issues. Results show that OPEN TrialChain, first, encourages pharmaceutical companies to report their trial results with higher fidelity, as well as, federated ones to provide more detailed results in return for their peers’ detailed results. Furthermore, it allows for multiple studies to be queried for results from underrepresented demographics, producing greater insight from previously ignored minorities. Finally, analysis done using OPEN TrialChain should illustrate meaningful results without violating the privacy of individuals. Eventually, OPEN TrialChain is expected to optimize the clinical trial ecosystem by improving patient safety, saving lives, cutting drug development costs, encouraging transparent results, preserving patients privacy, and maintaining pharmaceuticals integrity.</p>",,,2019-04-01 17:38:57.336,False,2018-01-01,OPEN TrialChain,PUBLIC,,True,Human Dynamics,False
smart-2-opal,sandy,False,"<p>Privacy-preserving mHealth application using Open Algorithm (OPAL) architecture to address urgent care challenges in Riyadh, Saudi Arabia.</p>",,,2019-04-01 17:39:27.492,False,2017-11-01,SMART^2 OPAL,PUBLIC,,True,Human Dynamics,False
healthy-blockchain,sandy,False,<p>Achieving a safe privacy-preserving information sharing environment for individualized care using blockchain-based technology in multiple use cases in the healthcare space.</p>,,,2019-04-01 17:40:08.489,False,2017-09-01,Healthy Blockchain,PUBLIC,,True,Human Dynamics,False
basic,sandy,False,"<p>Autonomous vehicles (AVs), drones, and robots will revolutionize our way of traveling and understanding urban space. In order to operate, all of these devices are expected to collect and analyze a lot of sensitive data about our daily activities. However, current operational models for these devices have extensively relied on centralized models of managing these data. The security of these models unveiled significant issues.</p><p>This project&nbsp; proposes BASIC, the Blockchained Agent-based Simulator for Cities. This tool aims to verify the feasibility of the use of blockchain in simulated urban scenarios by considering the communication between agents through&nbsp;<i>smart contracts</i>. In order to test the proposed tool, we implemented a car-sharing model within the city of Cambridge (Massachusetts, USA). In this research, the relevant literature was explored, new methods were developed, and different solutions were designed and tested. Finally, conclusions about the feasibility of the combination between blockchain technology and agent-based simulations were drawn.</p><p>Developed using&nbsp;<a href=""https://gama-platform.github.io/"">Gama Platform</a>.&nbsp;&nbsp;</p><p>Click <a href=""https://github.com/mitmedialab/Basic"">here</a> for the Open Source Repository.</p>",,,2019-04-23 20:08:21.338,True,2018-09-03,BASIC: Blockchained Agent-based Simulator for Cities,PUBLIC,,True,Human Dynamics,False
machine-behavior,sandy,False,"<p>Machines powered by artificial intelligence (AI) increasingly                           mediate our social, cultural, economic, and                           political interactions. Understanding the                           behavior of AI systems is essential to our                           ability to control their actions, reap their                           benefits, and minimize their harms. We argue                           this necessitates a broad scientific research                           agenda to study machine behavior that                           incorporates but expands beyond the discipline                           of computer science and requires insights from                           across the sciences. Here we first outline a                           set of questions fundamental to this emerging                           field. We then explore the technical, legal,                           and institutional constraints facing the study                           of machine behavior.</p>",,,2019-04-30 19:56:54.922,True,2019-04-24,Machine Behavior,PUBLIC,,True,Human Dynamics,False
diy-devices,mellis,False,"<p>Using digital fabrication and embedded computation to allow individuals to make their own devices. This effort started by creating open-source DIY versions of common devices (speakers, radios, mice, and cellphones) each combining a custom electronic circuit board and digitally-fabricated enclosure. The current focus is on creating devices with unique functionality, aesthetics, or production processes. One early prototype is of a special-purpose internet-connected device, whose behavior can be customized by the person creating it. Another experiment explores the possibilities of automated circuit board assembly services and their implications for open-source hardware. Most importantly, we're beginning to develop resources to enable others to design and build custom devices through meaningful and educational creative processes. These efforts are still in an early stage, but we're interested in finding ways to transition from reproducing existing devices to helping people create a diverse set of new ones.</p>",2014-01-01,--Choose Location,2016-12-05 00:16:21.735,True,2014-01-01,DIY Devices,PUBLIC,,False,Lifelong Kindergarten,False
novice-design-of-interactive-products,mellis,False,"<p>Despite recent widespread interest in hobbyist electronics and the maker movement, the design of printed circuit boards (PCBs) remains an obscure and often intimidating activity. This project attempts to introduce PCB design and production to new audiences by creating examples, activities, and other resources that provide context and motivation for those practices. We've developed a series of interactive lights that demonstrate the creation of useable products with simple circuits. These examples introduce novices to the space of possibilities and provide them with a starting point for creating their own designs. In workshops, novices design, produce, assemble, and program their own electronic circuits. These workshops provide an entry point to understanding the way that electronic products are made and an opportunity for discussion and reflection about how more people might get involved in their production.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:49.276,True,2014-01-01,Novice Design of Interactive Products,PUBLIC,,False,Lifelong Kindergarten,False
start-making,mellis,False,"<p>The Lifelong Kindergarten group is collaborating with the Museum of Science in Boston to develop materials and workshops that engage young people in ""maker"" activities in Computer Clubhouses around the world, with support from Intel. The activities introduce youth to the basics of circuitry, coding, crafting, and engineering. In addition, graduate students are testing new maker technologies and workshops for Clubhouse staff and youth. The goal of the initiative is to help young people from under-served communities gain experience and confidence in their ability to design, create, and invent with new technologies.</p>",2016-08-31,--Choose Location,2016-12-05 00:17:01.293,True,2014-01-01,Start Making!,PUBLIC,,False,Lifelong Kindergarten,False
the-foodome-building-a-comprehensive-knowledge-graph-of-food,dkroy,False,"<p>The Foodome addresses how to create deeper understanding and predictive intelligence about the relationships between how we talk and learn about food, and what we actually eat. Our aim is to build a food learning machine that comprehensively maps, for any given food, its form, function, production, distribution, marketing, science, policy, history, and culture (as well as the connections among all of these aspects). We are gathering and organizing a wide variety of data, including news/social content, recipes and menus, and sourcing and purchase information. We then use human-machine learning to uncover patterns within and among the heterogeneous food-related data. Long term, the Foodome is meant to help improve our understanding of, access to, and trust in food that is good for us; find new connections between food and health; and even predict impacts of local and global events on food.</p>",2017-05-31,--Choose Location,2017-10-16 15:28:49.377,True,2015-09-01,The Foodome: Building a Comprehensive Knowledge Graph of Food,PUBLIC,,False,Social Machines,False
aina-aerial-imaging-and-network-analysis,dkroy,False,"<p>This project is aimed at building a machine learning pipeline that will discover and predict links between the visible structure of villages and cities (using satellite and aerial imaging) and their inhabiting social networks. The goal is to estimate digitally invisible villages in India and Sub-Saharan Africa. By estimating the social structure of these communities, our goal is to enable targeted intervention and optimized distribution of information, education technologies, goods, and medical aid. Currently, this pipeline is implemented using a GPU-powered Deep Learning system. It is able to detect buildings and roads and provide detailed information about the organization of the villages. The output will be used to construct probabilistic models of the underlying social network of the village. Moreover, it will provide information on the population, distribution of wealth, rate and direction of development (when longitudinal imaging data is available), and disaster profile of the village.</p>",2016-12-31,--Choose Location,2017-02-16 16:35:50.415,True,2015-01-01,AINA: Aerial Imaging and Network Analysis,PUBLIC,,False,Social Machines,False
visible-communities,dkroy,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Vast regions of the world are unmapped by com­mercial services, and communities living there are digitally invisible. Visible Communities is a system that combines what local people using smartphones see on the ground with what computers can detect from satellite images, to create an interactive map at a fine resolution that continuously improves. The map captures both spatial and social data: houses and the paths connecting them, and the households living there and their relationships.</span><br></p><p>Enabling communities to put themselves on the map is a powerful way to increase their own visibility, and in turn serves institutional needs to improve infrastructure planning and humanitarian aid delivery. Existing approaches to do community-driven mapping either require outside experts to facilitate, or the results are lower-tech and not easy to keep up to date. In collaboration with Partners in Health (PIH), and supported by the MIT Tata Center, we are piloting this social machine in a sparsely populated, hilly region with a Community Health Worker (CHW) network in Burera, Rwanda.</p><p>The smartphone app enables CHWs to self-map their communities. We are intentionally designing an intuitive pre-literacy touch interface, enabling a wide range of users to participate without training. By removing barriers for people at the base of the socio-economic pyramid and designing with social dynamics in mind, we hope to unlock existing, self-motivated human potential.</p>",2017-05-31,--Choose Location,2017-10-16 15:35:47.765,True,2016-01-01,Visible Communities,PUBLIC,,False,Social Machines,False
spoken-opinion-summarization,dkroy,False,"<p>Talk radio exerts significant influence on the political and social dynamics of the United States, but labor-intensive data collection and curation processes have prevented previous works from studying its content at scale. Over the past year, the Laboratory for Social Machines and Cortico have created a talk radio ingest system to record and automatically transcribe audio from more than 160 stations around the country. Using these transcripts, we propose novel compression-based methods for unsupervised summarization of spoken opinion in conversational dialogue. By relying on an unsupervised framework that obviates the need for labeled data, the summarization task becomes largely agnostic to human input beyond necessary decisions regarding model architecture, input data, and output length. As a result, trained models are able to produce a more accurate depiction of opinion. Using the outputs of my proposed methods, we conduct a case study to examine the variability of public opinion across America. In the interests of reproducibility and further research, we open-source all code and data used.&nbsp;</p>",2019-06-07,,2019-04-17 00:15:52.766,True,2018-08-01,Spoken Opinion Summarization,PUBLIC,https://github.com/shayneobrien,False,Social Machines,False
responsive-communities-pilot-project-in-jun-spain,dkroy,False,"<p>To gain insights into how digital technologies can make local governments more responsive and deepen citizen engagement, we are studying the Spanish town of Jun (population 3,500). For the last four years, Jun has been using Twitter as its principal medium for citizen-government communication. We are mapping the resulting social networks and analyzing the dynamics of the Twitter interactions, in order to better understand the initiative's impact on the town. Our long-term goal is to determine whether the system can be replicated at scale in larger communities, perhaps even major cities.</p>",2016-12-31,--Choose Location,2017-02-16 16:31:17.794,True,2014-09-01,"Responsive Communities: Pilot Project in Jun, Spain",PUBLIC,,False,Social Machines,False
the-spread-of-false-and-true-info-online,dkroy,False,"<p>We investigated the spread of all of the <i>verified</i> news stories–verified as either true or false–distributed on Twitter from 2006 to 2017.</p><p>With a data set of roughly 126K stories tweeted by around 3M people over 4.5M times, we classified news as true or false using information from six independent fact-checking organizations that exhibited 95-98% agreement on the classifications. False information &nbsp;spread significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political information than for false information about terrorism, natural disasters, science, urban legends, or financial information. We found that false information was more novel than true information, which suggests that people were more likely to share novel information. While false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust.<br></p><p>Contrary to conventional wisdom, robots accelerated the spread of both true and false news at the same rate. This implies that false news spreads more than the truth because humans–not robots–are more likely to spread it.<br></p>",2018-05-31,,2018-06-08 00:37:43.310,True,2016-01-01,The Spread of True and False Information Online,PUBLIC,,False,Social Machines,False
journalism-mapping-and-analytics-project-jmap,dkroy,False,"<p>Over the last two decades, digital technologies have flattened old hierarchies in the news business and opened the conversation to a multitude of new voices. To help comprehend this promising but chaotic new public sphere, we're building a ""social news machine"" that will provide a structured view of the place where journalism meets social media. The basis of our project is a two-headed data ingest. On one side, all the news published online 24/7 by a sample group of influential US media outlets. On the other, all Twitter comments of the journalists who produced the stories. The two streams will be joined through network analysis and algorithmic inference. In future work we plan to expand the analysis to include all the journalism produced by major news outlets and the overall public response on Twitter, shedding new light on such issues as bias, originality, credibility, and impact.</p>",2016-05-31,--Choose Location,2016-12-05 00:17:15.736,True,2014-09-01,Journalism Mapping and Analytics Project (JMAP),PUBLIC,,False,Social Machines,False
human-atlas,dkroy,False,"<p>This project aims to map and analyze the publicly knowable social connections of various communities, allowing us to gain unprecedented insights about the social dynamics in such communities. Most analyses of this sort map online social networks, such as Twitter, Facebook, or LinkedIn. While these networks encode important aspects of our lives (e.g., our professional connections) they fail to capture many real-world relationships. Most of these relationships are, in fact, public and known to the community members. By mapping this publicly knowable graph, we get a unique view of the community that allows us to gain deeper understanding of its social dynamics. To this end, we built a web-based tool that is simple, easy to use, and allows the community to map itself. Our goal is to deploy this tool in communities of different sizes, including the Media Lab community and the Spanish town of Jun.</p>",2016-12-31,--Choose Location,2017-04-11 21:40:23.904,True,2015-01-01,Human Atlas,LAB-INSIDERS,,False,Social Machines,False
shapeblocks,dkroy,False,"<p>ShapeBlocks is a play analytics observatory that tracks, remembers, and aids players in building traditional LEGO-style structures. As players build a structure using these blocks, an underlying geometry engine analyzes the players' moves and suggests next steps (if a target structure is provided). The players can see real-time updates of what they are building in 3D. Instead of only suggesting, the AI learns from the players' moves and corrects itself through reinforcement learning. This essentially gives an opportunity for children and machines to learn shapes and geometry together.</p><p>Other use cases include urban design, and interactive strategy games and/or storytelling experiences that fuse the physical and virtual world together.</p><p>This is a work in progress. The hardware is complete, and the AI tool and games are currently being built.</p>",,,2016-12-05 00:17:27.676,True,2016-08-01,ShapeBlocks,PUBLIC,,True,Social Machines,False
flipfeed,dkroy,False,"<p class=""""><a href=""https://chrome.google.com/webstore/detail/flipfeed/glfjakcglibkihmaaekjcaefcbebgcfg?hl=en-US&amp;gl=US"">FlipFeed is a Google Chrome Extension</a> that enables Twitter users to replace their own feed with that of another real Twitter user. Powered by deep learning and social network analysis, feeds are selected based on inferred political ideology (""left"" or ""right"") and served to users of the extension. For example, a right-leaning user who uses FlipFeed may load and navigate a left-leaning user's feed to observe the news stories, commentary, and other content they consume. The user can then decide to flip back to their own feed or repeat the process with another feed. We hope tools like FlipFeed will enable us to explore how social media platforms can be used to mitigate, rather than exacerbate, ideological polarization by helping people explore and empathize with different perspectives.<br></p>",,,2017-03-08 16:10:50.709,True,2016-11-11,FlipFeed,PUBLIC,http://flipfeed.media.mit.edu,True,Social Machines,False
playful-words,dkroy,False,"<p>While there are a number of literacy technology solutions developed for individuals, the role of social—or networked—literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.</p><p>By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.</p><p>To learn more about this project, please check out:&nbsp;<a href=""http://playfulwords.org/"">http://playfulwords.org/</a></p>",,--Choose Location,2018-04-30 20:28:15.298,True,2014-09-01,Playful Words,PUBLIC,,True,Social Machines,False
conversation-trees-1,dkroy,False,"<p>An alternative viewing experience for large-scale conversations on Twitter. Each conversation tree visualizes the structure of replies to a single tweet. The purpose of this project is to explore a hidden dimension of virality. Not all tweets that elicit a large volume of comments behave the same under the hood.&nbsp; These structural patterns, when combined with sentiment analysis and/or toxicity, can reveal a deeper story.</p>",,,2019-04-18 14:50:10.698,True,2019-04-17,Conversation Trees,PUBLIC,,True,Social Machines,False
the-electome-measuring-responsiveness-in-the-2016-election,dkroy,False,"<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “<a href=""http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/"">Horse Race of Ideas</a>”).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf"">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/""> share of conversation</a> or coverage that <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/"">any given issue </a>or candidate commands on Twitter and in the news media, respectively—and how the two platforms are aligned<br></li><li>which issues are <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/"">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of “<a href=""http://fusion.net/story/304384/election-incivility-on-twitter/"">incivility</a>” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=""http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/""> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM’s deployment of Electome analytics has been <a href=""http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will"">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets—including the <a href=""https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/"">Washington Post</a>, <a href=""http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps"">Bloomberg News</a>, <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf"">CNN Politics</a> and Fusion—as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=""http://www.debates.org/"">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates’ moderators and credentialed journalists<br></li><li>also collaborated with the <a href=""http://ropercenter.cornell.edu/"">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center’s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>",,--Choose Location,2019-04-19 18:40:58.879,True,2015-09-01,The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,True,Social Machines,False
conversation-health-twitter-toxicity-network-structure,dkroy,False,"<p>We investigate the nature of toxic conversations on Twitter, focusing on the important subset of public discourse related to mainstream news. We analyze 71,000 conversations prompted by tweets from five major news outlets, comprising more than 12.3 million tweets and replies posted by 1.13 million users. We study the relationship between tweet toxicity, the reply-tree structure of the tweets, and the network of social connections between the tweet authors at three main levels: the individual level of the tweet authors, the dyadic level (between a tweet author and a reply author), and the overall reply-tree and follow graph structure of the conversation.</p>",,,2019-04-10 20:19:57.770,True,2018-09-01,Conversation health: Twitter toxicity and network structure,LAB-INSIDERS,,True,Social Machines,False
conversational-health-loneliness-on-reddit,dkroy,False,"<p>Loneliness is becoming a global epidemic. As many as 33 percent of Americans report being chronically lonely, with similar percentages being reported in countries around the world. Additionally, this percentage has risen in recent years. Many are turning to online forums as a way to connect with others about their feelings of loneliness and to begin to reduce these feelings. However, oftentimes, posts go unresponded to and online conversations do not take place, perhaps because those conversing did not find a connection with each other, potentially leaving the poster feeling even more lonely. This research explores the how health of conversation should be defined in online support conversations and analyzes the characteristics of conversation that contribute to healthier conversation.&nbsp;</p>",,,2019-04-17 13:57:33.414,True,2018-09-01,Conversational Health: Loneliness on Reddit,PUBLIC,,True,Social Machines,False
pathways,dkroy,False,"<p>Our social networks influence our sense of what's possible: we can't aspire to be cancer researchers, activists, or artificial intelligence engineers if we've never been exposed to these as possibilities.  Unfortunately, many children grow up in environments replete with exposure gaps, impeding awareness and ultimately limiting their conceptions of which opportunities are available to them.</p><p>Pathways is a web application that seeks to scaffold career exploration and introspection among young people in order to help them explore a) what kinds of topics they might pursue in the future, b) in which capacities they might pursue these topics, and c) examples of education and career pathways others have traversed to get where they are today.  The tool uses several data science and machine learning techniques to process self-reported education and career data from thousands of individuals in the Greater Boston area.</p><p>Ultimately, we hope tools like Pathways can help enhance exposure and spark new social network ties that help foster greater upward mobility and an improved quality of life.</p>",,,2019-04-10 23:26:21.291,True,2018-05-01,Pathways,PUBLIC,,True,Social Machines,False
local-voices-network,dkroy,False,"<p><b>What is Cortico and the Local Voices Network?</b></p><p><a href=""https://www.cortico.ai/"">Cortico</a>, a non-profit 501(c)(3) in cooperation with <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">MIT’s Laboratory for Social Machines</a>, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we’re developing a <b>public conversation network</b>&nbsp;called the&nbsp;<a href=""https://lvn.org/"">Local Voices Network</a>&nbsp;(LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.</p><p>LVN combines in-person and digital listening to host, analyze and connect community conversations at scale.&nbsp;Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network&nbsp;is designed around three core efforts:</p><ul><li>Facilitating in-person community dialogue that enables participants to listen, learn, and be heard</li><li>Connecting facilitators and conversations digitally across boundaries</li><li>Opening a new listening channel for journalists, leaders, and the community at large<br></li></ul><p><b>Why is this work important?</b></p><p>Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular “tribes” hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.</p>",,,2019-04-16 16:12:25.376,True,2018-09-01,Cortico: The Local Voices Network,PUBLIC,http://www.lvn.org,True,Social Machines,False
story-learning-machine,dkroy,False,"<p>The Storytelling project uses machine-based analytics to identify the qualities of engaging and marketable media. By developing models with the ability to “read” emotional arcs and semantic narrative video content, our researchers aim to map video story structure across many story types and formats.</p><p>To complement this content-based analysis, our researchers are also developing methods to analyze how emotional and semantic narratives affect viewer engagement with these stories. By tracking “referrals” of video URLs on social media networks, our researchers hope to identify how stories of different types and genres diffuse across networks, who influences this spread, and how video story distribution might be optimized. Given this project’s two-pronged strategy, our hope is to develop a robust story learning machine that uniquely maps the relationship between story structure and engagement across networks.</p>",,,2018-06-12 19:49:38.670,True,2017-02-23,The Story Learning Machine,PUBLIC,,True,Social Machines,False
affective-network,dkroy,False,"<p><a href=""https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US"">Try Affective Network!</a></p><p><span style=""font-size: 18px;"">Emotional contagion in online social networks has been of great interest over the past years. Previous studies have mainly focused on finding evidence of affection contagion in homophilic atmospheres. However, these studies have overlooked users' awareness of the sentiments they share and consume online. In this work, we present an experiment with Twitter users that aims to help them better understand which emotions they experience on this social network. We introduce&nbsp;</span><a href=""https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US"" style=""font-size: 18px;""><b>Affective Network</b></a><span style=""font-size: 18px;"">&nbsp;(Aff-Net), a Google Chrome extension that enables Twitter users to filter and make explicit (through colored visual marks) the emotional content in their news feed.</span><br></p><p>The extension is powered by machine learning algorithms that classify tweets into different sentiment categories: positive posts tend to use happy or surprising language; negative posts tend to use sad, angry, or disgusting language; and posts without strong emotional language are classified as neutral.</p><p>Affective Network aims to help social media users better understand which emotions they tend to consume on social media, and how these emotions can spread through their social networks. It was built by researchers at the <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">Laboratory for Social Machines&nbsp;</a>and the&nbsp;<a href=""https://www.media.mit.edu/groups/affective-computing/overview/"">Affective Computing</a> group at the <a href=""https://www.media.mit.edu/"">MIT Media Lab</a>.</p><p>Note that Affective Network does not necessarily reflect the official position of the MIT Media Lab regarding the benefits and drawbacks of filtering out specific emotional content.</p><p><a href=""https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US"">Try Affective Network!</a></p>",,,2019-05-17 19:51:18.500,True,2018-12-01,Affective Network,PUBLIC,https://affectivenetwork.media.mit.edu,True,Social Machines,False
story-comprehension,dkroy,False,"<p>The ability to automatically understand and infer characters' goals and their emotional states is key towards better narrative comprehension. Reasoning about mental representations of various characters in a narrative has been referred to as Theory of Mind (ToM) reasoning. In this work, we propose an unsupervised neural network that  exploits the personal stories on social media and incorporates commonsense knowledge about characters' motivations and reactions to generate interpretable trajectories of characters' mental states. We find that our model is capable of learning coherent mental representations from characters' actions and their affect states. We evaluate our model using a publicly available dataset for mental state tracking of characters in short commonsense stories.&nbsp;</p>",,,2019-04-18 15:04:09.320,True,2019-02-10,Story Comprehension,PUBLIC,,True,Social Machines,False
social-media-mirror,dkroy,False,"<p>Social Mirror is a web application that helps Twitter users interactively explore the politically active parts of their social network.  Worsening political polarization over the past several years has exacerbated ideological echo chambers, which in turn have further fueled polarization by widening knowledge and empathy gaps between disparate groups.  We hope digital tools like Social Mirror can help inspire self-reflection, and ultimately, intellectual humility by providing people with a new view of their social media ecosystems and helping them form new network connections.</p>",,,2018-11-15 19:09:55.909,True,2017-06-01,Social Mirror,PUBLIC,,True,Social Machines,False
the-foodome-building-a-comprehensive-knowledge-graph-of-food,lukeglw,False,"<p>The Foodome addresses how to create deeper understanding and predictive intelligence about the relationships between how we talk and learn about food, and what we actually eat. Our aim is to build a food learning machine that comprehensively maps, for any given food, its form, function, production, distribution, marketing, science, policy, history, and culture (as well as the connections among all of these aspects). We are gathering and organizing a wide variety of data, including news/social content, recipes and menus, and sourcing and purchase information. We then use human-machine learning to uncover patterns within and among the heterogeneous food-related data. Long term, the Foodome is meant to help improve our understanding of, access to, and trust in food that is good for us; find new connections between food and health; and even predict impacts of local and global events on food.</p>",2017-05-31,--Choose Location,2017-10-16 15:28:49.377,True,2015-09-01,The Foodome: Building a Comprehensive Knowledge Graph of Food,PUBLIC,,False,Social Machines,False
olive-ai-nutrition-management,lukeglw,False,"<p>Nutritional assessment is an extremely important problem for every American. Studies suggest that as many as 90% of Americans fall short of Vitamins D&amp;E as a result of their regular dietary habits, and up to 50% of Americans can’t get enough Vitamin A &amp; Calcium.</p><p>This problem is even more prominent in less wealthy communities, where not only food budget is more limited, but education in basic nutritional facts also more lacking. Even if full records of daily food intake are available, knowledge about nutrients in foods is needed to reflect on recent food consumption and subsequently act to nutritionally complement recent habits.</p><p>Therefore, there are two major obstacles stopping many ordinary Americans from healthily managing their diets. The first is recording dietary intake, and the second is interpreting nutritional profiles from the foods you’re eating. It’s after these two steps that insight into nutritional intake can be inferred and insights into dietary balance can be made. We set out to drastically lower the efforts involved in both steps by utilizing machine learning technologies. Image recognition technologies will be utilized to allow easy recording of dietary intake via photos, and nutrition data will be subsequently inferred based on USDA’s nutritional database, which also serves as the basis for nutritional evaluations from dietary records by nutritionists.</p><p>The result is a machine agent that can assist users&nbsp;to&nbsp;keep track of their dietary habits, and feedback nutritional deficiencies and suggestions to improve current diet to the user when needed.</p><p><br></p><p><br></p>",2017-06-09,,2019-02-14 16:55:54.219,True,2016-11-01,Olive - AI Nutrition Management,PUBLIC,,False,Social Machines,False
the-foodome-building-a-comprehensive-knowledge-graph-of-food,mmv,False,"<p>The Foodome addresses how to create deeper understanding and predictive intelligence about the relationships between how we talk and learn about food, and what we actually eat. Our aim is to build a food learning machine that comprehensively maps, for any given food, its form, function, production, distribution, marketing, science, policy, history, and culture (as well as the connections among all of these aspects). We are gathering and organizing a wide variety of data, including news/social content, recipes and menus, and sourcing and purchase information. We then use human-machine learning to uncover patterns within and among the heterogeneous food-related data. Long term, the Foodome is meant to help improve our understanding of, access to, and trust in food that is good for us; find new connections between food and health; and even predict impacts of local and global events on food.</p>",2017-05-31,--Choose Location,2017-10-16 15:28:49.377,True,2015-09-01,The Foodome: Building a Comprehensive Knowledge Graph of Food,PUBLIC,,False,Fluid Interfaces,False
aina-aerial-imaging-and-network-analysis,mmv,False,"<p>This project is aimed at building a machine learning pipeline that will discover and predict links between the visible structure of villages and cities (using satellite and aerial imaging) and their inhabiting social networks. The goal is to estimate digitally invisible villages in India and Sub-Saharan Africa. By estimating the social structure of these communities, our goal is to enable targeted intervention and optimized distribution of information, education technologies, goods, and medical aid. Currently, this pipeline is implemented using a GPU-powered Deep Learning system. It is able to detect buildings and roads and provide detailed information about the organization of the villages. The output will be used to construct probabilistic models of the underlying social network of the village. Moreover, it will provide information on the population, distribution of wealth, rate and direction of development (when longitudinal imaging data is available), and disaster profile of the village.</p>",2016-12-31,--Choose Location,2017-02-16 16:35:50.415,True,2015-01-01,AINA: Aerial Imaging and Network Analysis,PUBLIC,,False,Fluid Interfaces,False
journalism-mapping-and-analytics-project-jmap,mmv,False,"<p>Over the last two decades, digital technologies have flattened old hierarchies in the news business and opened the conversation to a multitude of new voices. To help comprehend this promising but chaotic new public sphere, we're building a ""social news machine"" that will provide a structured view of the place where journalism meets social media. The basis of our project is a two-headed data ingest. On one side, all the news published online 24/7 by a sample group of influential US media outlets. On the other, all Twitter comments of the journalists who produced the stories. The two streams will be joined through network analysis and algorithmic inference. In future work we plan to expand the analysis to include all the journalism produced by major news outlets and the overall public response on Twitter, shedding new light on such issues as bias, originality, credibility, and impact.</p>",2016-05-31,--Choose Location,2016-12-05 00:17:15.736,True,2014-09-01,Journalism Mapping and Analytics Project (JMAP),PUBLIC,,False,Fluid Interfaces,False
the-electome-measuring-responsiveness-in-the-2016-election,mmv,False,"<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “<a href=""http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/"">Horse Race of Ideas</a>”).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf"">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/""> share of conversation</a> or coverage that <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/"">any given issue </a>or candidate commands on Twitter and in the news media, respectively—and how the two platforms are aligned<br></li><li>which issues are <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/"">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of “<a href=""http://fusion.net/story/304384/election-incivility-on-twitter/"">incivility</a>” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=""http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/""> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM’s deployment of Electome analytics has been <a href=""http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will"">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets—including the <a href=""https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/"">Washington Post</a>, <a href=""http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps"">Bloomberg News</a>, <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf"">CNN Politics</a> and Fusion—as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=""http://www.debates.org/"">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates’ moderators and credentialed journalists<br></li><li>also collaborated with the <a href=""http://ropercenter.cornell.edu/"">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center’s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>",,--Choose Location,2019-04-19 18:40:58.879,True,2015-09-01,The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,True,Fluid Interfaces,False
mnemo,mmv,False,"<p>Mnemo is an integrated system to support human biographical memory.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Mnemo is directed to serve people with impaired memory (e.g. Alzheimer's patients)&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">by providing intuitive ways to benefit from large amounts of personal data.</span></p>",,,2018-10-20 17:54:46.318,True,2018-01-01,Mnemo,PUBLIC,,True,Fluid Interfaces,False
vr-maze-in-zero-gravity,mmv,False,"<p>The brain uses space to index,&nbsp;organize, and retrieve memories. However, our sense of space depends on our perception of gravity. We plan to test and understand the effect&nbsp;of altering gravity on human memory. Our experiment consists of a virtual reality experience that exposes the user to a sequence of small random mazes. We will compare the results of the experiment under different gravitational conditions.</p>",,,2017-11-21 20:53:28.558,True,2017-08-01,VR Maze in Zero Gravity,PUBLIC,,True,Fluid Interfaces,False
insight-deep-neurofeedback,mmv,False,"<p>inSight is a brain decoding system. It uses&nbsp; generative models (BigGAN and MusicVAE) to stimulate the brain with synthetic, but natural-looking videos and melodies. The brain activity of the user is then recorded using an EEG headset. The recorded data is then processed to train an encoder to interpret the brain activity in terms of the latent space of the generative model, effectively allowing the system to generate video and music from the user's brain activity. inSight can be used for applications such as neurofeedback therapies, creativity, human-human communication and search.&nbsp;</p>",,,2019-04-08 18:18:55.780,True,2018-12-01,inSight: Deep Neurofeedback,PUBLIC,,True,Fluid Interfaces,False
q-ai-that-makes-the-graph-of-your-memories,mmv,False,"<p>With the exponential increase of personal data in the forms of images, videos, emails, and social media posts, the time is ripe for building personal AIs that utilize these data to enhance the productivity and creativity of the users. Training AI algorithms require labeled and processed data.</p><p><span style=""font-size: 18px; font-weight: 400;"">However,&nbsp;annotating data is time-consuming and often regarded as the bottleneck of supervised learning. Most tools used for data labeling are tailored for the needs of data-scientists and researchers and are far from being useful for general users.&nbsp; The users of these systems need to know the ontology of possible labels beforehand and use complex interfaces and workflows to maintain the consistency and quality of the resulting dataset. ""</span><span style=""font-size: 18px;"">Q""</span><span style=""font-size: 18px; font-weight: 400;""> aims to reformulate data annotation as an engaging conversation by asking appropriate questions and automatically highlighting possible regions of interest. To come up with relevant questions, Q learns from the Wikidata public knowledge graph by computing the probable properties and relationships of entities. It also utilizes the previously annotated pieces of data to speed up the process.</span></p>",,,2019-04-08 18:21:50.443,True,2019-01-01,Q: An intelligent conversational interface for personal data labeling,PUBLIC,,True,Fluid Interfaces,False
the-foodome-building-a-comprehensive-knowledge-graph-of-food,soroush,False,"<p>The Foodome addresses how to create deeper understanding and predictive intelligence about the relationships between how we talk and learn about food, and what we actually eat. Our aim is to build a food learning machine that comprehensively maps, for any given food, its form, function, production, distribution, marketing, science, policy, history, and culture (as well as the connections among all of these aspects). We are gathering and organizing a wide variety of data, including news/social content, recipes and menus, and sourcing and purchase information. We then use human-machine learning to uncover patterns within and among the heterogeneous food-related data. Long term, the Foodome is meant to help improve our understanding of, access to, and trust in food that is good for us; find new connections between food and health; and even predict impacts of local and global events on food.</p>",2017-05-31,--Choose Location,2017-10-16 15:28:49.377,True,2015-09-01,The Foodome: Building a Comprehensive Knowledge Graph of Food,PUBLIC,,False,Social Machines,False
the-spread-of-false-and-true-info-online,soroush,False,"<p>We investigated the spread of all of the <i>verified</i> news stories–verified as either true or false–distributed on Twitter from 2006 to 2017.</p><p>With a data set of roughly 126K stories tweeted by around 3M people over 4.5M times, we classified news as true or false using information from six independent fact-checking organizations that exhibited 95-98% agreement on the classifications. False information &nbsp;spread significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political information than for false information about terrorism, natural disasters, science, urban legends, or financial information. We found that false information was more novel than true information, which suggests that people were more likely to share novel information. While false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust.<br></p><p>Contrary to conventional wisdom, robots accelerated the spread of both true and false news at the same rate. This implies that false news spreads more than the truth because humans–not robots–are more likely to spread it.<br></p>",2018-05-31,,2018-06-08 00:37:43.310,True,2016-01-01,The Spread of True and False Information Online,PUBLIC,,False,Social Machines,False
human-atlas,soroush,False,"<p>This project aims to map and analyze the publicly knowable social connections of various communities, allowing us to gain unprecedented insights about the social dynamics in such communities. Most analyses of this sort map online social networks, such as Twitter, Facebook, or LinkedIn. While these networks encode important aspects of our lives (e.g., our professional connections) they fail to capture many real-world relationships. Most of these relationships are, in fact, public and known to the community members. By mapping this publicly knowable graph, we get a unique view of the community that allows us to gain deeper understanding of its social dynamics. To this end, we built a web-based tool that is simple, easy to use, and allows the community to map itself. Our goal is to deploy this tool in communities of different sizes, including the Media Lab community and the Spanish town of Jun.</p>",2016-12-31,--Choose Location,2017-04-11 21:40:23.904,True,2015-01-01,Human Atlas,LAB-INSIDERS,,False,Social Machines,False
the-electome-measuring-responsiveness-in-the-2016-election,soroush,False,"<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “<a href=""http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/"">Horse Race of Ideas</a>”).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf"">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/""> share of conversation</a> or coverage that <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/"">any given issue </a>or candidate commands on Twitter and in the news media, respectively—and how the two platforms are aligned<br></li><li>which issues are <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/"">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of “<a href=""http://fusion.net/story/304384/election-incivility-on-twitter/"">incivility</a>” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=""http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/""> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM’s deployment of Electome analytics has been <a href=""http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will"">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets—including the <a href=""https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/"">Washington Post</a>, <a href=""http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps"">Bloomberg News</a>, <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf"">CNN Politics</a> and Fusion—as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=""http://www.debates.org/"">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates’ moderators and credentialed journalists<br></li><li>also collaborated with the <a href=""http://ropercenter.cornell.edu/"">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center’s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>",,--Choose Location,2019-04-19 18:40:58.879,True,2015-09-01,The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,True,Social Machines,False
story-learning-machine,soroush,False,"<p>The Storytelling project uses machine-based analytics to identify the qualities of engaging and marketable media. By developing models with the ability to “read” emotional arcs and semantic narrative video content, our researchers aim to map video story structure across many story types and formats.</p><p>To complement this content-based analysis, our researchers are also developing methods to analyze how emotional and semantic narratives affect viewer engagement with these stories. By tracking “referrals” of video URLs on social media networks, our researchers hope to identify how stories of different types and genres diffuse across networks, who influences this spread, and how video story distribution might be optimized. Given this project’s two-pronged strategy, our hope is to develop a robust story learning machine that uniquely maps the relationship between story structure and engagement across networks.</p>",,,2018-06-12 19:49:38.670,True,2017-02-23,The Story Learning Machine,PUBLIC,,True,Social Machines,False
the-foodome-building-a-comprehensive-knowledge-graph-of-food,russell5,False,"<p>The Foodome addresses how to create deeper understanding and predictive intelligence about the relationships between how we talk and learn about food, and what we actually eat. Our aim is to build a food learning machine that comprehensively maps, for any given food, its form, function, production, distribution, marketing, science, policy, history, and culture (as well as the connections among all of these aspects). We are gathering and organizing a wide variety of data, including news/social content, recipes and menus, and sourcing and purchase information. We then use human-machine learning to uncover patterns within and among the heterogeneous food-related data. Long term, the Foodome is meant to help improve our understanding of, access to, and trust in food that is good for us; find new connections between food and health; and even predict impacts of local and global events on food.</p>",2017-05-31,--Choose Location,2017-10-16 15:28:49.377,True,2015-09-01,The Foodome: Building a Comprehensive Knowledge Graph of Food,PUBLIC,,False,Social Machines,False
the-electome-measuring-responsiveness-in-the-2016-election,russell5,False,"<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “<a href=""http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/"">Horse Race of Ideas</a>”).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf"">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/""> share of conversation</a> or coverage that <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/"">any given issue </a>or candidate commands on Twitter and in the news media, respectively—and how the two platforms are aligned<br></li><li>which issues are <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/"">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of “<a href=""http://fusion.net/story/304384/election-incivility-on-twitter/"">incivility</a>” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=""http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/""> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM’s deployment of Electome analytics has been <a href=""http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will"">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets—including the <a href=""https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/"">Washington Post</a>, <a href=""http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps"">Bloomberg News</a>, <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf"">CNN Politics</a> and Fusion—as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=""http://www.debates.org/"">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates’ moderators and credentialed journalists<br></li><li>also collaborated with the <a href=""http://ropercenter.cornell.edu/"">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center’s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>",,--Choose Location,2019-04-19 18:40:58.879,True,2015-09-01,The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,True,Social Machines,False
local-voices-network,russell5,False,"<p><b>What is Cortico and the Local Voices Network?</b></p><p><a href=""https://www.cortico.ai/"">Cortico</a>, a non-profit 501(c)(3) in cooperation with <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">MIT’s Laboratory for Social Machines</a>, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we’re developing a <b>public conversation network</b>&nbsp;called the&nbsp;<a href=""https://lvn.org/"">Local Voices Network</a>&nbsp;(LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.</p><p>LVN combines in-person and digital listening to host, analyze and connect community conversations at scale.&nbsp;Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network&nbsp;is designed around three core efforts:</p><ul><li>Facilitating in-person community dialogue that enables participants to listen, learn, and be heard</li><li>Connecting facilitators and conversations digitally across boundaries</li><li>Opening a new listening channel for journalists, leaders, and the community at large<br></li></ul><p><b>Why is this work important?</b></p><p>Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular “tribes” hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.</p>",,,2019-04-16 16:12:25.376,True,2018-09-01,Cortico: The Local Voices Network,PUBLIC,http://www.lvn.org,True,Social Machines,False
story-learning-machine,russell5,False,"<p>The Storytelling project uses machine-based analytics to identify the qualities of engaging and marketable media. By developing models with the ability to “read” emotional arcs and semantic narrative video content, our researchers aim to map video story structure across many story types and formats.</p><p>To complement this content-based analysis, our researchers are also developing methods to analyze how emotional and semantic narratives affect viewer engagement with these stories. By tracking “referrals” of video URLs on social media networks, our researchers hope to identify how stories of different types and genres diffuse across networks, who influences this spread, and how video story distribution might be optimized. Given this project’s two-pronged strategy, our hope is to develop a robust story learning machine that uniquely maps the relationship between story structure and engagement across networks.</p>",,,2018-06-12 19:49:38.670,True,2017-02-23,The Story Learning Machine,PUBLIC,,True,Social Machines,False
tv-audience-tribes,russell5,False,"<p>How can we make sense of the expressed—and latent—interests of millions of TV show audience members online? We analyze millions of users on Twitter who follow a set of TV shows, discovering affinities between users and identifying clusters that reflect what they're interested in (e.g., soccer, hip hop, the environment). This ""interest map"" enables us to identify audience clusters that might be likely to engage with certain TV shows or genres, potentially supporting the creation, distribution, and promotion of more impactful stories.</p>",,,2019-04-18 17:30:58.538,True,2018-11-26,TV Audience Tribes,LAB-INSIDERS,,True,Social Machines,False
the-foodome-building-a-comprehensive-knowledge-graph-of-food,pralav,False,"<p>The Foodome addresses how to create deeper understanding and predictive intelligence about the relationships between how we talk and learn about food, and what we actually eat. Our aim is to build a food learning machine that comprehensively maps, for any given food, its form, function, production, distribution, marketing, science, policy, history, and culture (as well as the connections among all of these aspects). We are gathering and organizing a wide variety of data, including news/social content, recipes and menus, and sourcing and purchase information. We then use human-machine learning to uncover patterns within and among the heterogeneous food-related data. Long term, the Foodome is meant to help improve our understanding of, access to, and trust in food that is good for us; find new connections between food and health; and even predict impacts of local and global events on food.</p>",2017-05-31,--Choose Location,2017-10-16 15:28:49.377,True,2015-09-01,The Foodome: Building a Comprehensive Knowledge Graph of Food,PUBLIC,,False,Social Machines,False
auris-creating-affective-virtual-spaces-from-music,pralav,False,"<p>Light, color, texture, geometry and other architectural design elements have been shown to produce predictable and measurable effects on our minds, brains, and bodies. This suggests spaces that can mirror or transform feelings or serve specific purposes like improving learning or enhancing wellbeing can be designed. With Auris, we take a first step towards the design of such spaces in virtual reality by attempting to automatically generate affective virtual environments that can affect our emotions. The input to Auris is a song (audio and lyrics) and the output is a VR world that encapsulates the mood and content of the song.</p>",2018-07-31,,2018-08-20 16:22:53.032,True,2017-02-01,Auris: Creating Affective Virtual Spaces from Music,PUBLIC,,False,Social Machines,False
flipfeed,pralav,False,"<p class=""""><a href=""https://chrome.google.com/webstore/detail/flipfeed/glfjakcglibkihmaaekjcaefcbebgcfg?hl=en-US&amp;gl=US"">FlipFeed is a Google Chrome Extension</a> that enables Twitter users to replace their own feed with that of another real Twitter user. Powered by deep learning and social network analysis, feeds are selected based on inferred political ideology (""left"" or ""right"") and served to users of the extension. For example, a right-leaning user who uses FlipFeed may load and navigate a left-leaning user's feed to observe the news stories, commentary, and other content they consume. The user can then decide to flip back to their own feed or repeat the process with another feed. We hope tools like FlipFeed will enable us to explore how social media platforms can be used to mitigate, rather than exacerbate, ideological polarization by helping people explore and empathize with different perspectives.<br></p>",,,2017-03-08 16:10:50.709,True,2016-11-11,FlipFeed,PUBLIC,http://flipfeed.media.mit.edu,True,Social Machines,False
the-electome-measuring-responsiveness-in-the-2016-election,pralav,False,"<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “<a href=""http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/"">Horse Race of Ideas</a>”).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf"">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/""> share of conversation</a> or coverage that <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/"">any given issue </a>or candidate commands on Twitter and in the news media, respectively—and how the two platforms are aligned<br></li><li>which issues are <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/"">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of “<a href=""http://fusion.net/story/304384/election-incivility-on-twitter/"">incivility</a>” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=""http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/""> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM’s deployment of Electome analytics has been <a href=""http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will"">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets—including the <a href=""https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/"">Washington Post</a>, <a href=""http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps"">Bloomberg News</a>, <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf"">CNN Politics</a> and Fusion—as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=""http://www.debates.org/"">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates’ moderators and credentialed journalists<br></li><li>also collaborated with the <a href=""http://ropercenter.cornell.edu/"">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center’s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>",,--Choose Location,2019-04-19 18:40:58.879,True,2015-09-01,The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,True,Social Machines,False
pathways,pralav,False,"<p>Our social networks influence our sense of what's possible: we can't aspire to be cancer researchers, activists, or artificial intelligence engineers if we've never been exposed to these as possibilities.  Unfortunately, many children grow up in environments replete with exposure gaps, impeding awareness and ultimately limiting their conceptions of which opportunities are available to them.</p><p>Pathways is a web application that seeks to scaffold career exploration and introspection among young people in order to help them explore a) what kinds of topics they might pursue in the future, b) in which capacities they might pursue these topics, and c) examples of education and career pathways others have traversed to get where they are today.  The tool uses several data science and machine learning techniques to process self-reported education and career data from thousands of individuals in the Greater Boston area.</p><p>Ultimately, we hope tools like Pathways can help enhance exposure and spark new social network ties that help foster greater upward mobility and an improved quality of life.</p>",,,2019-04-10 23:26:21.291,True,2018-05-01,Pathways,PUBLIC,,True,Social Machines,False
story-learning-machine,pralav,False,"<p>The Storytelling project uses machine-based analytics to identify the qualities of engaging and marketable media. By developing models with the ability to “read” emotional arcs and semantic narrative video content, our researchers aim to map video story structure across many story types and formats.</p><p>To complement this content-based analysis, our researchers are also developing methods to analyze how emotional and semantic narratives affect viewer engagement with these stories. By tracking “referrals” of video URLs on social media networks, our researchers hope to identify how stories of different types and genres diffuse across networks, who influences this spread, and how video story distribution might be optimized. Given this project’s two-pronged strategy, our hope is to develop a robust story learning machine that uniquely maps the relationship between story structure and engagement across networks.</p>",,,2018-06-12 19:49:38.670,True,2017-02-23,The Story Learning Machine,PUBLIC,,True,Social Machines,False
story-comprehension,pralav,False,"<p>The ability to automatically understand and infer characters' goals and their emotional states is key towards better narrative comprehension. Reasoning about mental representations of various characters in a narrative has been referred to as Theory of Mind (ToM) reasoning. In this work, we propose an unsupervised neural network that  exploits the personal stories on social media and incorporates commonsense knowledge about characters' motivations and reactions to generate interpretable trajectories of characters' mental states. We find that our model is capable of learning coherent mental representations from characters' actions and their affect states. We evaluate our model using a publicly available dataset for mental state tracking of characters in short commonsense stories.&nbsp;</p>",,,2019-04-18 15:04:09.320,True,2019-02-10,Story Comprehension,PUBLIC,,True,Social Machines,False
media-lab-virtual-visit,srishti,False,"<p>Media Lab Virtual Visit is intended to open up the doors of the Media Lab to people from all around the world. The visit is hosted on the <a href=""https://www.media.mit.edu/projects/unhangout/overview/"">Unhangout</a> platform, a new way of running large-scale unconferences on the web that was developed at the Media Lab. It is an opportunity for students or potential collaborators to talk with current researchers at the Lab, learn about their work, and share ideas. </p>",2016-08-01,--Choose Location,2018-06-20 19:58:01.122,True,2014-01-01,Media Lab Virtual Visit,PUBLIC,,False,Lifelong Kindergarten,False
media-lab-digital-certificates,srishti,False,"<p class="""">Blockchain Certificates is a set of tools, software, and strategies to store and manage digital credentials. Certificates are registered on the bitcoin blockchain, cryptographically signed, and tamper-proof. They can represent or recognize many different types of achievements. After a number of prototypes (we issued digital credentials to Media Lab Director's Fellows and Media Lab alumni) we published our code under an open-source license to enable others to deploy the tools we developed. More information at&nbsp;<a href=""http://blockcerts.org"" class="""">http://blockcerts.org</a>.&nbsp;</p>",,--Choose Location,2019-01-31 17:23:57.299,True,2015-01-01,Digital Academic Credentials,PUBLIC,,True,Lifelong Kindergarten,False
media-lab-virtual-visit,kamcco,False,"<p>Media Lab Virtual Visit is intended to open up the doors of the Media Lab to people from all around the world. The visit is hosted on the <a href=""https://www.media.mit.edu/projects/unhangout/overview/"">Unhangout</a> platform, a new way of running large-scale unconferences on the web that was developed at the Media Lab. It is an opportunity for students or potential collaborators to talk with current researchers at the Lab, learn about their work, and share ideas. </p>",2016-08-01,--Choose Location,2018-06-20 19:58:01.122,True,2014-01-01,Media Lab Virtual Visit,PUBLIC,,False,Initiatives,False
making-learning-work,kamcco,False,"<p>Improving adult learning, especially for adults who are unemployed or unable to financially support their families, is a challenge that affects the future wellbeing of millions of individuals in the US. We are working with the Joyce Foundation, employers, learning researchers, and the Media Lab community to prototype three to five new models for adult learning that involve technology innovation and behavioral insights. </p>",2016-06-01,--Choose Location,2016-12-05 00:16:23.585,True,2014-01-01,Making Learning Work,PUBLIC,,False,Initiatives,False
peer-2-peer-university,kamcco,False,"<p><a href=""https://www.p2pu.org/en/"">Peer 2 Peer University (P2PU)</a> has developed ""learning circles,"" a model for facilitating in-person study groups at community libraries. Aimed at adult learners, learning circles take advantage of libraries as public community spaces for learning. We curate open, online courses and pair learners up with their peers to foster deeper, more meaningful adult basic educational experiences.</p>",,--Choose Location,2017-02-10 17:12:23.457,True,2015-01-01,Peer 2 Peer University,PUBLIC,https://www.p2pu.org,True,Initiatives,False
open-leadership-camp,kamcco,False,"<p>The Open Leadership Camp (OLC) is a new type of professional development program for senior leaders of nonprofit and public sector organizations. It aims to &nbsp;<span style=""font-size: 18px; font-weight: 400;"">apply the principles of open source, open innovation, and the decentralized nature of the web to the way some of our most crucial social sector organizations work.&nbsp;</span></p><p>This project is a collaboration between <a href=""https://www.mozilla.org"">Mozilla</a> and the <a href=""https://www.media.mit.edu/groups/ml-learning/overview/"">ML Learning Initiative</a>, and is&nbsp;<span style=""font-size: 18px; font-weight: 400;"">hosted by MIT Media Lab Director <a href=""https://www.media.mit.edu/people/joi/overview/"">Joi Ito</a> and <a href=""https://www.mozilla.org/en-US/about/leadership/"">Mitchell Baker</a>, co-founder and Executive Chairwoman of the Mozilla Corporation. In March 2017, we brought together our first cohort of 14 participants, including the CEO of Consumer Reports, the CIO of the City of Detroit, and the CEO of WGBH.&nbsp;</span></p>",,,2017-06-19 14:42:03.293,True,2017-01-01,Open Leadership Camp,PUBLIC,,True,Initiatives,False
media-lab-digital-certificates,kamcco,False,"<p class="""">Blockchain Certificates is a set of tools, software, and strategies to store and manage digital credentials. Certificates are registered on the bitcoin blockchain, cryptographically signed, and tamper-proof. They can represent or recognize many different types of achievements. After a number of prototypes (we issued digital credentials to Media Lab Director's Fellows and Media Lab alumni) we published our code under an open-source license to enable others to deploy the tools we developed. More information at&nbsp;<a href=""http://blockcerts.org"" class="""">http://blockcerts.org</a>.&nbsp;</p>",,--Choose Location,2019-01-31 17:23:57.299,True,2015-01-01,Digital Academic Credentials,PUBLIC,,True,Initiatives,False
public-library-innovation-exchange,kamcco,False,"<p>Public libraries are one of most trusted public institutions in the US and increasingly provide a broad range of education services, ranging from early learning programs, to maker spaces, to adult training. Libraries are not storage places for books, but communities for social change and innovation. The Public Library Innovation Exchange connects Media Lab researchers with librarians to develop new creative learning programs together. We will host how-to materials that help public libraries deploy projects developed at the Media Lab, such as &nbsp;<a href=""http://www.chicagotribune.com/bluesky/series/gadgets/ct-innovative-gifts-bsi-photos-20151210-001-photo.html"">Circuit Stickers</a> or <a href=""https://www.youtube.com/watch?v=rfQqh7iCcOU"">Makey Makey</a>, and we will offer scholarships to support exchanges and residencies to foster collaborative research going forward. The project is funded by Knight Foundation.&nbsp;</p><p>Visit our project website to learn more: <a href=""http://plix.media.mit.edu"">plix.media.mit.edu</a></p>",,,2019-05-30 18:56:51.688,True,2017-03-15,Public Library Innovation Exchange,PUBLIC,https://plix.media.mit.edu,True,Initiatives,False
lego-wayfinder,kamcco,False,"<p>The LEGO Wayfinder project combines LEGO, robotics, and seawater into a playground of project-based learning and citizen science for budding engineers and explorers. As part of this outreach program, our team has developed a first prototype of a buildable LEGO marine exploration vehicle kit—addressing some of the design challenges of building for the underwater context.</p><p>Our aim is to build an awareness of the state of the aquatic environment and instill a greater responsibility in shaping our interactions with the environment. To do so, young people will view underwater wonders of the world with their robots and get outside to explore their local waterway. Our approach embraces Seymour Papert’s model of ""low floors"" (where getting started is easy), and ""high ceilings,"" where students can pour their time and collaborative work efforts into creative engineering solutions to carry out a marine science experiment of their own design in the field.</p>",,,2019-04-22 17:15:27.304,True,2018-04-01,LEGO Wayfinder,PUBLIC,,True,Initiatives,False
media-perspective,ethanz,False,"<p>Media Perspective brings a data visualization into 3D space. This data sculpture represents mainstream media coverage of Net Neutrality over 15 months, during the debate over the FCC's classification of broadband services. Each transparent pane shows a slice in time, allowing users to physically move and look through the timeline. The topics cutting through the panes show how attention shifted between aspects of the debate over time. </p>",2016-01-01,--Choose Location,2016-12-05 00:16:23.229,True,2015-01-01,Media Perspective,PUBLIC,,False,Civic Media,False
encuestacdmx,ethanz,False,"<p>Everyone in the city is an expert on their own experience of that city. So how might we integrate new forms of citizen input into the planning and transformation of public spaces around Mexico City, using both digital and non-digital strategies? EncuestaCDMX is a civic technology platform developed with the Laboratorio para la Ciudad that combines in-person surveys and responses from a version of the Action Path location-based survey app to inform city planning decisions. The survey responses power a real-time public dashboard of the feedback available to both city planners and residents for accountability.</p>",2015-10-01,--Choose Location,2016-12-05 00:16:34.976,True,2015-09-01,EncuestaCDMX,PUBLIC,http://erhardtgraeff.com,False,Civic Media,False
intrepid,ethanz,False,"<p>Every 98 seconds, a person in the United States is sexually abused. Every 16 hours,&nbsp;<span style=""font-size: 18px; font-weight: 400;"">a woman in the United States is murdered by her romantic partner or ex-partner.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">Sexual abuse, assault, and harassment are regarded as some of the most common&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">human rights violations in the world by the United Nations. Our work examines&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">methods to prevent sexual assault, from pre-historic times to latest technologies,&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">to inform contemporary designs. Intrepid investigates multiple methods to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">detect initial signs of assault and develop methods for communication and prevention&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">of assault. We also explore olfactory stimuli as a potential means to prevent sexual&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">assault in real-time.</span></p>",2017-06-30,,2018-04-20 17:21:38.623,True,2016-01-04,Intrepid,PUBLIC,,False,Civic Media,False
out-for-change-transformative-media-organizing-project,ethanz,False,"<p>The Out for Change Transformative Media Organizing Project (OCTOP) links LGBTQ, Two-Spirit, and allied media makers, online organizers, and tech-activists across the United States. In 2013-2014, we are conducting a strengths/needs assessment of the media and organizing capacity of the movement, as well as offering a series of workshops and skillshares around transmedia organizing. The project is guided by a core group of project partners and advisers who work with LGBTQ and Two-Spirit folks. The project is supported by faculty and staff at the MIT Center for Civic Media, Research Action Design and by the Ford Foundation's Advancing LGBT Rights Initiative.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:44.784,True,2014-01-01,Out for Change: Transformative Media Organizing Project,PUBLIC,,False,Civic Media,False
the-peoples-bot,ethanz,False,"<p>Telepresent robots are often pitched as a technology to extend the influence of those who already have money and power. We want to use robotic telepresence for the public good�broadening access, supporting public interest reporting, and funding access initiatives.</p>",2014-09-01,--Choose Location,2016-12-05 00:16:55.263,True,2014-01-01,The People's Bot,PUBLIC,,False,Civic Media,False
slap-snap-tap,ethanz,False,"<p>Slap Snap Tap combines wearable sensors with physical block programming to enable enhanced expression through movement. By slapping on a set of sensor straps, snapping in code that links movement triggers to sound actions, and tapping the sensors to activate a play experience, users can combine motion and sound in creative ways. A dancer can create music through movement; an athlete can add emphasis to her performance; demonstrators can synchronize and amplify a chant; and anyone can create sound effects for life moments. Slap Snap Tap is a method of the Slay Play endeavor which aims to broaden participation in computational creation by using movement as a pathway into computational thinking.</p>",2016-12-31,--Choose Location,2017-03-31 19:41:01.413,True,2016-01-01,Slap Snap Tap,PUBLIC,,False,Civic Media,False
media-meter-focus,ethanz,False,<p>Media Meter Focus shows focus mapping of global media attention. What was covered in the news this week? Did the issues you care about get the attention you think they deserved? Did the media talk about these topics in the way you want them to? The tool-set also shows news topics mapped against country locations.</p>,2015-01-01,--Choose Location,2016-12-05 00:17:02.737,True,2014-01-01,Media Meter Focus,PUBLIC,,False,Civic Media,False
mixed-mode-systems-in-disaster-response,ethanz,False,"<p>Pure networks and pure hierarchies both have distinct strengths and weaknesses. These become glaringly apparent during disaster response. By combining these modes, their strengths (predictability, accountability, appropriateness, adaptability) can be optimized, and their weaknesses (fragility, inadequate resources) can be compensated for. Bridging these two worlds is not merely a technical challenge, but also a social issue.</p>",2016-01-01,--Choose Location,2016-12-05 00:17:03.349,True,2015-01-01,Mixed-Mode Systems in Disaster Response,PUBLIC,,False,Civic Media,False
zl-vortice,ethanz,False,"<p>This project is currently promoting a survey of data from the East Side (Zona Leste) of the city of Sao Paulo, Brazil. The aim is to detect the landscape dynamics: infrastructure and urban planning, critical landscapes, housing, productive territory, recycling, and public space. The material will be made available on a digital platform, accessible by computers and mobile devices: a tool specially developed to enable local communities to disseminate productive and creative practices that occur in the area, as well as to enable a greater participation in the formulation of public policies. ZL Vortice is an instrument that will serve to strengthen social, productive, and cultural networks of the region.</p>",2016-01-01,--Choose Location,2017-01-08 20:05:40.680,True,2015-01-01,ZL Vortice,PUBLIC,,False,Civic Media,False
openscope,ethanz,False,"<p>OpenScope is an open source project that combines three components for anyone to explore the micro world anytime, anywhere. The 3D-printable open hardware turns your smartphone into a 200x microscope, the image processing application helps you recognize specific objects, and the online community allows you to share and contribute your findings from the microscope. OpenScope is expanding microscopy technologies beyond research laboratories and transforming the way we interact with the micro world.</p>",2017-01-01,--Choose Location,2018-05-04 10:52:12.271,True,2016-01-01,OpenScope,PUBLIC,http://oi7.me,False,Civic Media,False
framework-for-consent-policies,ethanz,False,"<p>This checklist is designed to help projects that include an element of data collection to develop appropriate consent policies and practices. The checklist can be especially useful for projects that use digital or mobile tools to collect, store, or publish data, yet understand the importance of seeking the informed consent of individuals involved (the data subjects). This checklist does not address the additional considerations necessary when obtaining the consent of groups or communities, nor how to approach consent in situations where there is no connection to the data subject. This checklist is intended for use by project coordinators, and can ground conversations with management and project staff in order to identify risks and mitigation strategies during project design or implementation. It should ideally be used with the input of data subjects.</p>",2015-01-01,--Choose Location,2016-12-05 00:17:12.642,True,2014-01-01,Framework for Consent Policies,PUBLIC,,False,Civic Media,False
mapping-the-globe,ethanz,False,<p>Mapping the Globe is an interactive tool and map that helps us understand where the <i>Boston Globe</i> directs its attention. Media attention matters—in quantity and quality. It helps determine what we talk about as a public and how we talk about it. Mapping the Globe tracks where the paper's attention goes and what that attention looks like across different regional geographies in combination with diverse data sets like population and income. Produced in collaboration with the <i>Boston Glob</i>e.</p>,2015-09-01,--Choose Location,2016-12-05 00:17:16.960,True,2014-09-01,Mapping the Globe,PUBLIC,,False,Civic Media,False
student-legal-services-for-innovation,ethanz,False,"<p>Should students be prosecuted for innovative projects? In December 2014, four undergraduates associated with the Media Lab were subpoenaed by the New Jersey Attorney General after winning a programming competition with a bitcoin-related proof of concept. We worked with MIT administration and the Electronic Frontier Foundation to support the students and establish legal support for informal innovation. In September 2015, MIT announced the creation of a new clinic for business and cyberlaw.</p>",2016-10-01,--Choose Location,2016-12-05 00:17:22.864,True,2014-01-01,Student Legal Services for Innovation,PUBLIC,,False,Civic Media,False
mission-wildlife,ethanz,False,"<p>Mission Wildlife is a research collaboration between San&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Diego Zoo Global and the MIT Center for Civic Media to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">explore the potential for interactive technologies in&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">conservation education. In particular, we used&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">augmented reality (AR) to focus visitors’ attention&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">towards survival threats to endangered species. The&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">project was deployed during the 100th anniversary&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">year (2016) of the San Diego Zoo. Visitors competed&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">against each other to trigger 3D animations from&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">animal signage in the zoo and shared results on social&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">media to spread awareness about conservation issues.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">This case study demonstrates how AR can be tied to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">efforts to expand awareness of social issues.</span></p>",2017-06-20,,2018-05-04 10:49:30.350,True,2016-06-20,Mission Wildlife,PUBLIC,http://oi7.me,False,Civic Media,False
newspix,ethanz,False,"<p>NewsPix is a simple news-engagement application that helps users encounter breaking news in the form of high-impact photos. It is currently a Chrome browser extension (mobile app to come) that is customizable for small and large news organizations. Currently, when users open a new, blank page in Chrome, they get a new tab with tiles that show recently visited pages. NewsPix replaces that view with a high-quality picture from a news site. Users interested in more information about the photo can click through to the news site. News organizations can upload photos ranging from breaking news to historic sporting events, with photos changing every time a new tab is clicked.</p>",,--Choose Location,2016-12-05 00:17:03.242,True,2014-01-01,NewsPix,PUBLIC,,True,Civic Media,False
first-upload,ethanz,False,"<p>First Upload is a tool for verifying the authenticity of news imagery. It helps find the first upload of imagery, particularly videos. Finding the person who uploaded a video is a key to determining authenticity, because often it is necessary to contact that person directly. It is being developed with input from YouTube and Bloomberg. Currently we have a working prototype, built for the YouTube site.</p>",,--Choose Location,2016-12-05 00:16:25.181,True,2015-01-01,First Upload,PUBLIC,,True,Civic Media,False
gender-shades,ethanz,False,"<p>The Gender Shades project pilots an intersectional approach to inclusive product testing for AI.</p><h1><b>Algorithmic Bias Persists</b></h1><p>Gender Shades is a preliminary excavation of the inadvertent negligence that will cripple the age of automation and further exacerbate inequality if left to fester. The deeper we dig, the more remnants of bias we will find in our technology. We cannot afford to look away this time, because the stakes are simply too high. &nbsp;We risk losing the gains made with the civil rights movement and women's movement under the false assumption of machine neutrality. Automated systems are not inherently neutral. They reflect the priorities, preferences, and prejudices—the coded gaze—of those who have the power to mold artificial intelligence.</p>",,,2018-04-30 15:22:20.722,True,2017-01-01,Gender Shades,PUBLIC,http://www.gendershades.org,True,Civic Media,False
databasic,ethanz,False,"<p>DataBasic is a suite of web-based tools that give people fun and relevant ways learn how to work with data. Existing tools focus on operating on data quickly to create some output, rather than focusing on helping learners understand how to work with data. This fails the huge population of data literacy learners, who are trying to build their capacity in various ways. Our tools focus on the user as learner. They provide introductory activities, connect to people with fun sample datasets, and connect to other tools and techniques for working with data. We strongly believe in building tools focused on learners, and are putting those ideas into practice on these tools and activities. Visit <a href=""https://databasic.io"">databasic.io</a> today to try it out!</p>",,--Choose Location,2018-04-30 15:30:39.371,True,2015-09-01,DataBasic,PUBLIC,,True,Civic Media,False
scanner-grabber,ethanz,False,"<p>Scanner Grabber is a digital police scanner that enables reporters to record, playback, and export audio, as well as archive public safety radio (scanner) conversations. Like a TiVo for scanners, it's an update on technology that has been stuck in the last century. It's a great tool for newsrooms. For instance, a problem for reporters is missing the beginning of an important police incident because they have stepped away from their desk at the wrong time. Scanner Grabber solves this because conversations can be played back. Also, snippets of exciting audio, for instance a police chase, can be exported and embedded online. Reporters can listen to files while writing stories, or listen to older conversations to get a more nuanced grasp of police practices or long-term trouble spots. Editors and reporters can use the tool for collaborating, or crowdsourcing/public collaboration.</p>",,--Choose Location,2016-12-05 00:16:48.984,True,2014-09-01,Scanner Grabber,PUBLIC,,True,Civic Media,False
the-babbling-brook,ethanz,False,"<p>The Babbling Brook is an unnamed neighborhood creek in Waltham, MA, that winds its way to the Charles River. With the help of networked sensors and real-time processing, the brook constantly tweets about the status of its water quality, including thoughts and bad jokes about its own environmental and ontological condition. Currently, the Babbling Brook senses temperature and depth and cross-references that information with real-time weather data to come up with extremely bad comedy. Thanks to Brian Mayton, the Responsive Environments group, and Tidmarsh Farms Living Observatory for their support.</p>",,--Choose Location,2016-12-05 00:16:54.870,True,2014-09-01,The Babbling Brook,PUBLIC,,True,Civic Media,False
netstories,ethanz,False,"<p>Recent years have witnessed a surge in online digital storytelling tools, enabling users to more easily create engaging multimedia narratives. Increasing Internet access and powerful in-browser functionality have laid the foundation for the proliferation of new online storytelling technologies, ranging from tools for creating interactive online videos to tools for data visualization. While these tools may contribute to diversification of online storytelling capacity, sifting through tools and understanding their respective limitations and affordances poses a challenge to storytellers. The NetStories research initiative explores emergent online storytelling tools and strategies through a combination of analyzing tools, facilitating story-hack days, and creating an online database of storytelling tools.</p>",,--Choose Location,2016-12-05 00:16:40.787,True,2014-01-01,NetStories,PUBLIC,,True,Civic Media,False
bias-by-us,ethanz,False,"<p><b>Bias by us</b>&nbsp;envisions a future of media diversity by understanding the bias of today.</p><p>Our work seeks to understand how the US media ecosystem reports on underrepresented minorities. By using natural language processing algorithms and data-intensive models, we aim to uncover underlying stereotypes, associations, and modes of narration that media produces and reproduces when covering minority related events. We perform a multi- and cross-platform analysis, capturing media dynamics on different social media platforms and traditional media outlets across the political spectrum. Besides understanding how media language portrays underrepresented minorities, we locate effects on and associations to political saliency and bias-motivated crime.<br></p><p>By understanding media bias and its effects on underrepresented minorities, we reflect on the conditions that can ensure a diverse and inclusive US media ecosystem.</p>",2019-06-15,,2019-04-21 19:06:53.374,True,2019-02-18,Bias by us,PUBLIC,,True,Civic Media,False
gobo,ethanz,False,"<h1>Your social media. Your rules.</h1><p><a href=""http://gobo.social"">Gobo</a> is an experiment, not a startup. We’re building it to change the conversation on social media and imagine a better version of it. This is a technology-to-think-with—a tool we want you to play with and push against. Gobo is being built by a small team at <a href=""https://www.media.mit.edu/groups/civic-media/overview/"">MIT Media Lab's Center for Civic Media</a>, where we work on technologies for social change.</p><p>For questions, feedback, and musings, you can reach the Gobo team at <a href=""mailto:gobo@media.mit.edu"">gobo@media.mit.edu</a>.</p><h2>Control your own feed</h2><p>Social media companies use algorithms to control what we see on our feeds, but we don’t know how these algorithms work. &nbsp;As a result, we’re often unaware why certain posts show up in our feed while others don’t. Gobo allows you to control the algorithms, or a set of “rules,” so you can decide what gets shown on your feed and know why.</p><h2>Connect multiple platforms</h2><p>We believe that multiple social media platforms should exist to serve different purposes. However, it’s not easy to keep up with all these platforms, especially when your data can’t be easily shared between them. Gobo allows you to connect up to three platforms, so you can view all of your feeds in one place. </p><h2>See what gets hidden</h2><p>We believe that transparency can help you better understand what you see on social media and keep platforms accountable for algorithmic bias. Gobo tells you why certain posts are hidden based on the rules you set. It also shows you how many posts are hidden, so you can understand the overall impact of the rules you set.</p><h2>Expand your perspective</h2><p>Social media companies make assumptions about what we want to see based on what we read and click on. They tend to show us content we’re already engaging with, reinforcing our echo chambers. Instead of assuming what you want to see, Gobo allows you to add unfamiliar perspectives into your feed, so you can better understand the range of opinions that are shared online.</p>",,,2019-04-10 14:21:45.965,True,2017-09-01,Gobo,PUBLIC,https://gobo.social,True,Civic Media,False
whose-lives-matter-in-the-news,ethanz,False,"<p>Since the killing of Michael Brown, the Black Lives Matter movement has organized on social media to draw attention to the deaths of unarmed black people killed by US police. Have news organizations responded to this demand, and have we seen a significant change over time in reporting about those deaths?</p><p>In this analysis of deaths from January 2013 through June 2016, we show that an unarmed black person killed by US police received 10.5x the incidence rate of news articles after Michael Brown’s death than those killed before, but that the predicted number of articles is no longer significantly different from 2013 levels.</p>",,,2016-12-13 16:23:36.317,True,2015-07-01,Whose Lives Matter in the News?,PUBLIC,,True,Civic Media,False
our-2-sense,ethanz,False,"<p>Innovation and investment in Smart City infrastructure has led to an increasing number of sensors in our urban environment and greater arsenals of data, stored in the cloud and viewed primarily by experts (if at all). &nbsp;We are interested in shifting the current Smart City paradigm to one where sensor data is explored in the physical places where it is being collected, by the communities that inhabit the space. &nbsp;Building on existing sensor infrastructure deployed through Chicago’s Array of Things, we are partnering with the School of the Art Institute of Chicago &nbsp;to explore tools and processes that integrate local residents in gathering and analyzing data &nbsp;in order to spark dialogue, learning, and collaborative revisioning of our cities. </p>",,--Choose Location,2016-12-05 00:17:19.600,True,2016-01-01,Our 2 Sense,LAB,,True,Civic Media,False
fold,ethanz,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Some readers require greater context to understand complex stories.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">FOLD (</span><a href=""http://fold.cm"" style=""font-size: 18px; font-weight: normal;"">fold.cm</a><span style=""font-size: 18px; font-weight: normal;"">) is an open publishing platform with a unique structure that lets writers link media cards to the text of their stories.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">Media cards can contain videos, maps, tweets, music, interactive visualizations, and more.&nbsp;</span></p><p>FOLD is used by journalists, educators, and storytellers around the world.&nbsp;<br></p>",,--Choose Location,2016-12-15 02:27:31.751,True,2014-01-01,FOLD,PUBLIC,,True,Civic Media,False
code4rights,ethanz,False,"<p>Code4Rights promotes human rights through technology education. By facilitating the development of rights-focused mobile applications in workshops and an online course, Code4Rights enables participants to create meaningful technology for their communities in partnership with local organizations. For example, Code4Rights, in collaboration with It Happens Here, a grassroots organization focused on addressing sexual violence, created the First Response Oxford App to address sexual violence at Oxford University. Over 30 young women contributed to the creation of the app, which provides survivors of sexual violence and friends of survivors with information about optional ways to respond, essential knowledge about support resources, critical contact details, and answers to frequently asked questions.  </p>",,--Choose Location,2016-12-05 00:16:54.952,True,2014-09-01,Code4Rights,PUBLIC,http://www.code4rights.org,True,Civic Media,False
deepstream,ethanz,False,"<p>Citizens and journalists are increasingly choosing to live stream civic events. But live streams are currently hard to find and lack in-depth information about the events being documented. DeepStream seeks to increase participation in this emergent form of media by creating tools for live stream curation. Users can add relevant news stories, images, tweets, and other media to almost any live or on-demand video to create more informative and engaging viewing experiences. To help find relevant videos, Deepstream includes a search engine that lets you find live streams across multiple platforms with a single search query.</p><p>By lowering the technical barriers to creating enhanced live and on-demand videos, Deepstream makes it possible for newsrooms or individuals to curate the chaos of live streams from major global events, add media to video in real-time like fact-checking live political debates, or create enhanced version of documentaries with extra footage and related stories that appear at specific times. Our goal is to connect viewers to global events in a way that emphasizes local perspectives and deeper engagement, while maintaining the experience of immediacy and authenticity that is an essential part of live streaming.</p>",,--Choose Location,2016-12-05 00:16:35.962,True,2014-09-01,DeepStream,PUBLIC,http://www.deepstream.tv,True,Civic Media,False
open-water-project,ethanz,False,"<p>The Open Water Project aims to develop and curate a set of low-cost, open source tools enabling communities everywhere to collect, interpret, and share their water quality data. Traditional water monitoring uses expensive, proprietary technology, severely limiting the scope and accessibility of water quality data. Homeowners interested in testing well water, watershed managers concerned about fish migration and health, and other groups could benefit from an open source, inexpensive, accessible approach to water quality monitoring. We're developing low-cost, open source hardware devices that will measure some of the most common water quality parameters, using designs that makes it possible for anyone to build, modify, and deploy water quality sensors in their own neighborhood.</p>",,--Choose Location,2016-12-05 00:16:38.427,True,2014-01-01,Open Water Project,PUBLIC,,True,Civic Media,False
algorithmic-justice-league,ethanz,False,"<b><a href=""http://www.ajlunited.org"">www.ajlunited.org</a></b><br><p>An unseen force is rising—helping to determine who is hired, granted a loan, or even how long someone spends in prison. This force is called the coded gaze.</p><p> </p><p>However, many people are unaware of the growing impact of the coded gaze and the rising need for fairness, accountability, and transparency in coded systems. Without knowing discriminatory practices are at play, citizens are unable to affirm their rights or identify violations.</p><p>The Algorithmic Justice League aims to:</p><ol><li>highlight algorithmic bias through provocative media and interactive exhibitions<br></li><li>provide space for people to voice concerns and experiences with coded discrimination&nbsp;</li><li>develop practices for accountability during the design, development, and deployment phases of coded systems.</li></ol>",,,2018-04-30 15:28:35.674,True,2016-10-14,Algorithmic Justice League,PUBLIC,http://www.poetofcode.com,True,Civic Media,False
civilservant,ethanz,False,"<p>The CivilServant project supports online communities to run their own experiments on the effects of moderation practices on antisocial behavior, harassment, discrimination, and community well-being online.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">All results are published to an open repository of collective knowledge on practices that contribute to fair, flourishing social life online.</span></p><p>The first experiment, in a 13.2 million subscriber community, showed that <a href=""http://civilservant.io/moderation_experiment_r_science_rule_posting.html"">posting rules at the top of conversations prevents problems and increases engagement</a>.</p>",,,2019-04-19 18:55:02.391,True,2016-07-01,CivilServant: User-led randomized trials online,PUBLIC,http://civilservant.io,True,Civic Media,False
linkedout,ethanz,False,"<p>LinkedOut aims to define and build solutions to facilitate societal reentry for formerly incarcerated individuals.</p><p>In collaboration with the Office of Returning Citizens (ORC), an office under the City of Boston that facilitates reentry, LinkedOut is working to design a reentry infrastructure to shut the revolving door of incarceration and reincarceration.</p><p>Our work seeks to break down the structural and societal barriers to successful reentry. At the structural level, we are building technologies to develop richer data biographies of the lives of returning citizens, and in doing so provide policymakers with the robust data required for effective reentry programs. Importantly, our goal is to embed values in technology design to humanize the reentry process. On the societal level, we have begun to expose holes in the moral fabric of society as we draw attention to the dehumanization and stigmatization of returning citizens. These invisible societal norms disempower returning citizens from rebuilding their lives. Thus, our work demands a reimagination of our current justice system—one that rehabilitates rather than retributes, that embraces rather than excludes—to design for a successful reentry journey.</p><h1><b>Ongoing Project</b></h1><h2><b>Pathfinder: How it works</b></h2><p>Pathfinder is a personalized case management platform that helps case managers design and keep track of returning citizens’ transition back into society. It makes real-time recommendations in the categories of healthcare, housing, employment, education, financial health, and community engagement based on the unique needs of each returning citizen. Beyond tracking the reentry journey of returning citizens, Pathfinder helps service providers identify and track community service gaps in meeting the needs of their clients.</p><p>In summary, Pathfinder has three key aims:</p><ol><li>Develop individualized case management plans for returning citizens</li><li>Streamline work process for service providers to improve efficacy of services</li><li>Capture community-level determinants of reentry for more informed reentry policies and programs</li></ol><p><b>Collaborators</b></p><p>Pathfinder is a collaboration between, with, and for returning citizens. We are partnering with <a href=""https://www.codersbeyondbars.org/"">Coders Beyond Bars</a>, a non-profit that teaches returning citizens to code; through this partnership, returning citizens will co-design and co-develop Pathfinder with the MIT Media Lab.</p>",,,2019-04-16 16:34:52.993,True,2018-09-17,LinkedOut: Codesigning societal reentry with returning citizens,PUBLIC,https://rubezchong.com/,True,Civic Media,False
peer-appreciation-in-the-workplace,ethanz,False,"<p>Organizations are deploying gratitude-tracking systems to encourage appreciation, promote pro-sociality, and monitor employee wellbeing. We present the case study of one such system, called Gratia, adopted by a Fortune 500 company for over four years. We analyzed 422,209 messages of thanks and examined temporal patterns of appreciation, reciprocity, and repeated interactions. We also compared the formal organizational chart to the informal network expressed through the system. We found that gratitude is strongly reciprocated, that time between thanks is relatively long, and that it is predominantly given to peers outside one's immediate team.</p>",,--Choose Location,2018-10-20 01:44:04.200,True,2014-01-01,Peer Appreciation in the Workplace,PUBLIC,,True,Civic Media,False
civic-entertainment,ethanz,False,"<p>Civic Entertainment is a project based at the Center for Civic Media that explores the intersection of civic engagement with film, television, radio, theatre, literature, and digital entertainment. The project aims to study the modes in which entertainment can create greater knowledge of public institutions, motivate citizens towards democratic duties, and present effective strategies of social and political change.</p><p>The research focuses on studying the ways in which fiction media can affect change in thought and behavior, develops case studies of past and existing films and television shows that reflect or carry elements of civic engagement, explores the representation of protest and activism in popular culture, and experiments with techniques to balance civic education with humor or drama within entertainment.</p><p>The project has a key focus on Indian entertainment and works with Civic Studios (www.civicstudios.com), a Mumbai-based production firm, on creating civic entertainment content for young people across India.</p>",,,2019-03-15 19:38:40.415,True,2018-03-20,Civic Entertainment,PUBLIC,,True,Civic Media,False
dancing-data,ethanz,False,"<p>Every day, data about our environment, education, routes and other aspects of our lives become available.&nbsp;&nbsp;Yet, most people often struggle to understand, use and relate to these data. Especially because the access to all this data is often in formats that only few can retrieve, read, and understand.</p><p>In this project, we seek to&nbsp;disrupt the way we present and interact with datasets, using art as a vehicle to tell our stories informed by datasets.&nbsp;&nbsp;<b>Data dancing is a data experience.</b> “<i>A data experience takes data off the screen and puts it into the physical world</i>” [<a href=""http://lauraperovich.com/thesisPerovich.pdf"">Perovich L., 2015</a>].&nbsp;<br></p><p>This project explores&nbsp;the ways in which data can be physicalized through human expression.&nbsp;We are working with&nbsp;members of the community to co-compose dance performances informed by datasets and, validate the use of dance and acting as methods to enhance data literacy.&nbsp;</p>",2020-05-01,,2019-06-07 20:04:38.751,True,2019-05-01,Data Dancing,PUBLIC,,True,Civic Media,False
media-perspective,rahulb,False,"<p>Media Perspective brings a data visualization into 3D space. This data sculpture represents mainstream media coverage of Net Neutrality over 15 months, during the debate over the FCC's classification of broadband services. Each transparent pane shows a slice in time, allowing users to physically move and look through the timeline. The topics cutting through the panes show how attention shifted between aspects of the debate over time. </p>",2016-01-01,--Choose Location,2016-12-05 00:16:23.229,True,2015-01-01,Media Perspective,PUBLIC,,False,Civic Media,False
encuestacdmx,rahulb,False,"<p>Everyone in the city is an expert on their own experience of that city. So how might we integrate new forms of citizen input into the planning and transformation of public spaces around Mexico City, using both digital and non-digital strategies? EncuestaCDMX is a civic technology platform developed with the Laboratorio para la Ciudad that combines in-person surveys and responses from a version of the Action Path location-based survey app to inform city planning decisions. The survey responses power a real-time public dashboard of the feedback available to both city planners and residents for accountability.</p>",2015-10-01,--Choose Location,2016-12-05 00:16:34.976,True,2015-09-01,EncuestaCDMX,PUBLIC,http://erhardtgraeff.com,False,Civic Media,False
out-for-change-transformative-media-organizing-project,rahulb,False,"<p>The Out for Change Transformative Media Organizing Project (OCTOP) links LGBTQ, Two-Spirit, and allied media makers, online organizers, and tech-activists across the United States. In 2013-2014, we are conducting a strengths/needs assessment of the media and organizing capacity of the movement, as well as offering a series of workshops and skillshares around transmedia organizing. The project is guided by a core group of project partners and advisers who work with LGBTQ and Two-Spirit folks. The project is supported by faculty and staff at the MIT Center for Civic Media, Research Action Design and by the Ford Foundation's Advancing LGBT Rights Initiative.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:44.784,True,2014-01-01,Out for Change: Transformative Media Organizing Project,PUBLIC,,False,Civic Media,False
mapping-the-globe,rahulb,False,<p>Mapping the Globe is an interactive tool and map that helps us understand where the <i>Boston Globe</i> directs its attention. Media attention matters—in quantity and quality. It helps determine what we talk about as a public and how we talk about it. Mapping the Globe tracks where the paper's attention goes and what that attention looks like across different regional geographies in combination with diverse data sets like population and income. Produced in collaboration with the <i>Boston Glob</i>e.</p>,2015-09-01,--Choose Location,2016-12-05 00:17:16.960,True,2014-09-01,Mapping the Globe,PUBLIC,,False,Civic Media,False
databasic,rahulb,False,"<p>DataBasic is a suite of web-based tools that give people fun and relevant ways learn how to work with data. Existing tools focus on operating on data quickly to create some output, rather than focusing on helping learners understand how to work with data. This fails the huge population of data literacy learners, who are trying to build their capacity in various ways. Our tools focus on the user as learner. They provide introductory activities, connect to people with fun sample datasets, and connect to other tools and techniques for working with data. We strongly believe in building tools focused on learners, and are putting those ideas into practice on these tools and activities. Visit <a href=""https://databasic.io"">databasic.io</a> today to try it out!</p>",,--Choose Location,2018-04-30 15:30:39.371,True,2015-09-01,DataBasic,PUBLIC,,True,Civic Media,False
gobo,rahulb,False,"<h1>Your social media. Your rules.</h1><p><a href=""http://gobo.social"">Gobo</a> is an experiment, not a startup. We’re building it to change the conversation on social media and imagine a better version of it. This is a technology-to-think-with—a tool we want you to play with and push against. Gobo is being built by a small team at <a href=""https://www.media.mit.edu/groups/civic-media/overview/"">MIT Media Lab's Center for Civic Media</a>, where we work on technologies for social change.</p><p>For questions, feedback, and musings, you can reach the Gobo team at <a href=""mailto:gobo@media.mit.edu"">gobo@media.mit.edu</a>.</p><h2>Control your own feed</h2><p>Social media companies use algorithms to control what we see on our feeds, but we don’t know how these algorithms work. &nbsp;As a result, we’re often unaware why certain posts show up in our feed while others don’t. Gobo allows you to control the algorithms, or a set of “rules,” so you can decide what gets shown on your feed and know why.</p><h2>Connect multiple platforms</h2><p>We believe that multiple social media platforms should exist to serve different purposes. However, it’s not easy to keep up with all these platforms, especially when your data can’t be easily shared between them. Gobo allows you to connect up to three platforms, so you can view all of your feeds in one place. </p><h2>See what gets hidden</h2><p>We believe that transparency can help you better understand what you see on social media and keep platforms accountable for algorithmic bias. Gobo tells you why certain posts are hidden based on the rules you set. It also shows you how many posts are hidden, so you can understand the overall impact of the rules you set.</p><h2>Expand your perspective</h2><p>Social media companies make assumptions about what we want to see based on what we read and click on. They tend to show us content we’re already engaging with, reinforcing our echo chambers. Instead of assuming what you want to see, Gobo allows you to add unfamiliar perspectives into your feed, so you can better understand the range of opinions that are shared online.</p>",,,2019-04-10 14:21:45.965,True,2017-09-01,Gobo,PUBLIC,https://gobo.social,True,Civic Media,False
whose-lives-matter-in-the-news,rahulb,False,"<p>Since the killing of Michael Brown, the Black Lives Matter movement has organized on social media to draw attention to the deaths of unarmed black people killed by US police. Have news organizations responded to this demand, and have we seen a significant change over time in reporting about those deaths?</p><p>In this analysis of deaths from January 2013 through June 2016, we show that an unarmed black person killed by US police received 10.5x the incidence rate of news articles after Michael Brown’s death than those killed before, but that the predicted number of articles is no longer significantly different from 2013 levels.</p>",,,2016-12-13 16:23:36.317,True,2015-07-01,Whose Lives Matter in the News?,PUBLIC,,True,Civic Media,False
data-culture-project,rahulb,False,"<p><b>Struggling to build your organization's ability to work with data? Use our hands-on learning program to kickstart your data culture.</b></p><p><a href=""http://datacultureproject.org"">datacultureproject.org</a></p><p>Data is everywhere right now. But many organizations like yours are struggling to figure out how to build capacity to work with data. You don't need a data scientist; you need a <strong>data culture</strong>.</p><p>Use our self-service learning program to facilitate fun, creative introductions for the non-technical folks in your organization. These are not boring spreadsheet trainings. The free tools and activities on the linked website are hands-on and designed to meet people where they are across your organization, and build their capacity to work with data. Try to kickstart your data culture by running one activity per month as a brown bag lunch. Our videos and facilitation guides will lead you through running them yourself.</p>",,,2018-05-06 22:26:19.647,True,2017-09-01,Data Culture Project,PUBLIC,http://datacultureproject.org,True,Civic Media,False
dancing-data,rahulb,False,"<p>Every day, data about our environment, education, routes and other aspects of our lives become available.&nbsp;&nbsp;Yet, most people often struggle to understand, use and relate to these data. Especially because the access to all this data is often in formats that only few can retrieve, read, and understand.</p><p>In this project, we seek to&nbsp;disrupt the way we present and interact with datasets, using art as a vehicle to tell our stories informed by datasets.&nbsp;&nbsp;<b>Data dancing is a data experience.</b> “<i>A data experience takes data off the screen and puts it into the physical world</i>” [<a href=""http://lauraperovich.com/thesisPerovich.pdf"">Perovich L., 2015</a>].&nbsp;<br></p><p>This project explores&nbsp;the ways in which data can be physicalized through human expression.&nbsp;We are working with&nbsp;members of the community to co-compose dance performances informed by datasets and, validate the use of dance and acting as methods to enhance data literacy.&nbsp;</p>",2020-05-01,,2019-06-07 20:04:38.751,True,2019-05-01,Data Dancing,PUBLIC,,True,Civic Media,False
media-perspective,elplatt,False,"<p>Media Perspective brings a data visualization into 3D space. This data sculpture represents mainstream media coverage of Net Neutrality over 15 months, during the debate over the FCC's classification of broadband services. Each transparent pane shows a slice in time, allowing users to physically move and look through the timeline. The topics cutting through the panes show how attention shifted between aspects of the debate over time. </p>",2016-01-01,--Choose Location,2016-12-05 00:16:23.229,True,2015-01-01,Media Perspective,PUBLIC,,False,Civic Media,False
out-for-change-transformative-media-organizing-project,elplatt,False,"<p>The Out for Change Transformative Media Organizing Project (OCTOP) links LGBTQ, Two-Spirit, and allied media makers, online organizers, and tech-activists across the United States. In 2013-2014, we are conducting a strengths/needs assessment of the media and organizing capacity of the movement, as well as offering a series of workshops and skillshares around transmedia organizing. The project is guided by a core group of project partners and advisers who work with LGBTQ and Two-Spirit folks. The project is supported by faculty and staff at the MIT Center for Civic Media, Research Action Design and by the Ford Foundation's Advancing LGBT Rights Initiative.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:44.784,True,2014-01-01,Out for Change: Transformative Media Organizing Project,PUBLIC,,False,Civic Media,False
hybrid-autonomous-shared-bike-fleet-deployment-simulator,mcllin,False,"<p>Cities around the world are striving to improve livability by way of reducing dependency on fossil-fuels cars. How might we leverage the autonomous technology to help fulfill this vision, while ensuring the flow of people and goods across the city? The Persuasive Electric Vehicle (PEV) is a small, on-demand, shared, agile, autonomous, and functionally hybrid tricycle. We believe it will become a critical platform in the constellation of emerging mobility systems. The PEV will be a shared bike platform for people's inner-urban and last-mile travel needs, and for delivering goods on-demand around the clock. To deploy it in the real world, it is necessary to match the fleet supply with its demand. This simulator enables cities around the world to forecast the fleet size based on proxy demands from taxis, shared bikes, shared car services, and Call Detail Records (CDR).</p>",2016-01-01,--Choose Location,2016-12-08 17:15:49.102,True,2015-01-01,Hybrid Autonomous Shared Bike Fleet Deployment Simulator,PUBLIC,,False,City Science,False
mod,mcllin,False,"<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>’s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>",,,2019-04-23 14:03:41.132,True,2017-08-01,Theme   |   Mobility On-Demand,PUBLIC,,True,City Science,False
fleet-simulation,mcllin,False,"<h2><i>Forecasting the supply of fleets to meet emerging travel demands and service needs in cities</i></h2><p>The availability of vehicles is a critical factor behind successful shared-use mobility services. Proper management of supply-demand dynamics is paramount for achieving viability in a new mobility service, as achieving scale often requires a large capital investment. Under-supplying the fleet would result in low service availability and user dissatisfaction; over-supplying results in inefficient use of capital. In addition, as a new shared mobility platform diversifies its service across both passenger and freight delivery, its required scale of operation and investment becomes more difficult to estimate. &nbsp;</p><p>In this fleet deployment and optimization research, the City Science group aims to create an accessible simulation tool to enable cities to forecast the size of deployment of new shared mobility services using the Persuasive Electric Vehicle delivering passengers and packages as an initial test case. The simulation tool also provides a platform for testing fleet rebalancing and service-hub strategies.</p>",,,2019-04-17 19:44:17.413,True,2016-04-01,Service deployment simulation and optimization for mixed-use delivery fleets,PUBLIC,,True,City Science,False
hmi,mcllin,False,"<h2><i>Facilitating coexistence, trust-building, and collaboration among people and machines.</i></h2><p>New modes of 21st century urban transportation are becoming increasingly lightweight, electrified, connected, shared, and autonomous. Cohabitation of humans and machines is an increasingly important question, and one which requires careful attention and design.&nbsp; We strive to enable new forms of human-machine co-existence, trust, and collaboration.</p><h2>This work focuses on enabling:</h2><ol><li>Intuitive and effective two-way communication between vehicles and pedestrians;</li><li>Street safety and traffic-yielding mechanisms; and</li><li>Behavior change related to the adoption of active mobility mode, or electric assist.</li></ol>",,,2019-04-17 19:49:25.786,True,2016-11-01,Human-machine cooperation (HMC) for lightweight autonomous robots,PUBLIC,,True,City Science,False
pev,mcllin,False,"<h1><b>An Alternative Autonomous Revolution&nbsp;</b></h1><h2><i>System design for emerging urban contexts and societal aspirations</i></h2><p>The Persuasive Electric Vehicle (PEV) aims to solve urban mobility challenges with a healthy, convenient, sustainable alternative to cars. The PEV is a low-cost, agile, shared-use autonomous bike that can be either an electrically assisted tricycle for passenger commuting or an autonomous carrier for package delivery.</p><p>The PEV uses standard bicycle components and is lightweight (&lt;50kg) yet robust. Its sensors are easy to reconfigure and it has a 250W mid-drive electric motor and 10Ah battery pack that provides 25 miles of travel per charge and a top speed of 20 miles per hour.</p><p>Our vision for the PEV: a rider summons the PEV through a phone app, and the nearest available PEV arrives autonomously to meet the rider. Upon completing the trip, the PEV simply moves on to its next passenger or package pickup.&nbsp; The PEV can be autonomous, operated by the rider, or provide the rider with an electric assist. PEV's operate in bike lanes, avoiding the congestion and adding incentives to make more bikeable cities.</p>",,--Choose Location,2018-07-19 21:12:27.919,True,2014-09-01,Persuasive Electric Vehicle (PEV),PUBLIC,,True,City Science,False
andorra-living-lab,mcllin,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,City Science,False
smart-infra,mcllin,False,"<h1>Scalable Urban Infrastructure for Human-Machine Cohabitation</h1><h2><i>New infrastructure to help sustain public-sector participation and operation, and maximize public interest and safety.</i></h2><p>Advancements in autonomous technology have led automobile makers and tech companies to focus on reinventing the automobile—increasing computational capability and enhancing sensor systems. But due to strict road-safety regulations, this vehicle-centric, inside-out approach may take years to materialize, and when it does, restricting “autonomy” to selected vehicles will limit autonomy’s impact on street safety and accessibility.</p><p>To address current issues, The City Science group focuses on ways to offload often-heavy computational requirements from the vehicle through affordable interventions to street infrastructure by creating human-machine readable traffic signs and urban markers.</p><p>With the support of a new genre of smart urban infrastructure, we believe this “autonomy-lite” approach will soon allow lightweight autonomous vehicles to be widely deployed and navigate smoothly in most urban environments.&nbsp;</p>",,,2018-03-28 22:18:15.132,True,2017-05-01,Urban Tattoo,PUBLIC,,True,City Science,False
andorra-mobility,mcllin,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile</a></p><p>With no airport or train service, most of the 8 million tourists who visit Andorra each year arrive by car, making traffic management and parking some of the country's most important challenges. We are currently developing different projects spanning from data science to the deployment of autonomous vehicles to help address these issues.<br></p>",,,2017-10-25 05:56:26.309,True,2016-09-01,Andorra | Mobility,PUBLIC,,True,City Science,False
city-science-lab-shanghai,mcllin,False,"<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>",,,2019-05-16 20:43:38.677,True,2017-02-14,City Science Lab Shanghai,PUBLIC,,True,City Science,False
torque,mcllin,False,"<h1>Open-Source Autonomous Platform for Educational and Service Design Applications</h1><h2><i>How can new technologies respond to society’s diverse industrial, socio-economic, and educational needs?</i></h2><p>Despite AI and robotics being widely trumpeted as keys to the new Industrial Revolution, access to their development remains largely restricted to companies and institutions that are rich in capital and/or data, potentially further deepening the socio-economic disparity observed across continents. As a likely result, these new technologies generate limited positive externalities. For instance, are automobiles really the most critical area in need of self-driving technology? Where else might AI and robotics be applied to lead to increased urban livability, socioeconomic equity, and the vibrancy of local businesses?</p><p>Building upon the architecture of MIT’s open-source race car platform, the City Science group introduces&nbsp;a new open-ended and heavy-duty self-driving platform.&nbsp;&nbsp;Torque is intended to be used by educators and makers and is ideal for hackathons and classroom instruction. Torque will soon allow rapid prototyping of usage scenarios and services for various contexts and needs.&nbsp;</p>",,,2018-05-04 15:23:33.802,True,2017-07-01,Torque,PUBLIC,,True,City Science,False
city-science-andorra,mcllin,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
hybrid-autonomous-shared-bike-fleet-deployment-simulator,kll,False,"<p>Cities around the world are striving to improve livability by way of reducing dependency on fossil-fuels cars. How might we leverage the autonomous technology to help fulfill this vision, while ensuring the flow of people and goods across the city? The Persuasive Electric Vehicle (PEV) is a small, on-demand, shared, agile, autonomous, and functionally hybrid tricycle. We believe it will become a critical platform in the constellation of emerging mobility systems. The PEV will be a shared bike platform for people's inner-urban and last-mile travel needs, and for delivering goods on-demand around the clock. To deploy it in the real world, it is necessary to match the fleet supply with its demand. This simulator enables cities around the world to forecast the fleet size based on proxy demands from taxis, shared bikes, shared car services, and Call Detail Records (CDR).</p>",2016-01-01,--Choose Location,2016-12-08 17:15:49.102,True,2015-01-01,Hybrid Autonomous Shared Bike Fleet Deployment Simulator,PUBLIC,,False,City Science,False
cityscope-mark-iva-riyadh,kll,False,"<p>We recently led a workshop in Saudi Arabia, with staff from the Riyadh Development Authority, to test a new version of our CityScope platform. With only an hour to work, four teams of five professionals competed to develop a redevelopment proposal for a neighborhood near the city center. The platform evaluated their designs according to energy, daylighting, and walkability.</p>",2016-09-01,--Choose Location,2018-06-26 14:28:55.162,True,2014-09-01,CityScope Riyadh,PUBLIC,http://irawinder.com/blog/riyadh,False,City Science,False
cityscope-barcelona,kll,False,"<p>The ""Barcelona"" demo is an independent prototype designed to model and simulate human interactions within a Barcelona-like urban environment. Different types of land use (residential, office, and amenities) are configured into urban blocks and analyzed with agent-based techniques.</p>",2016-01-01,--Choose Location,2018-04-27 14:41:51.014,True,2015-09-01,CityScope Barcelona,PUBLIC,,False,City Science,False
cityoffice,kll,False,"<p>Architectural robotics enable a hyper-efficient, dynamically reconfigurable co-working space that accommodates a wide range of activities in a small area.</p>",2016-12-01,--Choose Location,2017-10-18 00:44:53.325,True,2014-09-01,City Office,GROUP,,False,City Science,False
gsk-manufacturing-initiative,kll,False,"<p>This project is the first of two projects in collaboration with GSK. We are developing a computational simulation that allows a human user (or AI) to test drug manufacturing investment scenarios for an entire portfolio over multiple years. We aspire to help decision-makers understand the possible impact of new techniques such as CBM on selected key performance metrics. This game like simulation allows various stakeholders to come together and make collaborative decisions regarding the entire supply chain. The software works dynamically with a Tactile Matrix, which is an interactive decision support system that allows users to instantly and collaboratively explore the models in an approachable, tangible way. </p><p>Screenshots courtesy of Ira Winder. Photos by Nina Lutz.</p>",2018-06-01,,2018-08-20 20:42:11.340,True,2017-01-09,GSK Manufacturing Initiative,PUBLIC,,False,City Science,False
gsk-places-initiative,kll,False,"<p>This project is part of a parallel research endeavor with GSK Manufacturing. By simulating how scientists at the Upper Providence site interact with one another and the space around them, we hope to help assist future renovations in a range of GSK locations. This was motivated by GSK’s drive to improve spatial configuration within their organization, including various open office environments and even smart labs.&nbsp;</p><p>We hope this project will serve as both a packaged decision support system and a framework for GSK scientists and stakeholders to reconfigure with their own spatial inquiries and case studies. Our goals can be enumerated as below.&nbsp;</p><ol>
<li>Design a decision support tool that allows R&amp;D to understand how changes to physical environments and adjacencies may have an impact on key workplace indicators. Encourage data-driven demonstration and discussion of decisions related to spatial changes.&nbsp;</li>
<li>Employ spatial mathematical models to calculate key workplace indicators, including</li>
<ol>

<li>Space utilization</li>
<li>Time accessibility between amenities</li>
<li>Synergy, defined as potential for interaction among researchers</li>

</ol>
<li>Deploy the decision support tool as an evolving platform that exists in two forms:&nbsp;</li>
<ol>

<li>Tangible user interface “Tactile Matrix”</li>
<li>Traditional executable application that can be used on personal machines with mouse and keyboard interface.&nbsp;</li>
<li>Provide transparent source code and open source, off the shelf technologies that allows for adaptation and customization for various case studies and applications.&nbsp;</li>

</ol>
</ol><br><p>Screenshots courtesy of Nina Lutz.&nbsp;<br></p>",2018-06-01,,2018-08-20 20:43:34.774,True,2017-01-09,GSK Places Initiative,PUBLIC,,False,City Science,False
geobits,kll,False,"<p>This is an open source geospatial exploration tool. Using various public APIs including Open Street Map and the United States Census, we can make dynamic, flexible models of how people are moving through the city. These models include accessibility in cities, multimodal transportation networks, and diversity. Overall this allows anyone with or without an urban planning background to build strong models with geospatial and urban data. This system works dynamically with a Tactile Matrix, which is an interactive decision support system that allows users to instantly and collaboratively explore the models in a tangible way.</p><p>Photos by Nina Lutz.&nbsp;</p>",2018-06-01,,2018-09-04 19:47:57.544,True,2016-07-11,GeoBits,PUBLIC,,False,City Science,False
singapore-pedestrian-accessibility,kll,False,<p>This project focused on pedestrian accessibility in collaboration with Singapore Centre for Liveable Cities. Researchers and planners came together to design an interface that would allow both citizens and planners to interact with a model regarding pedestrian accessibility. The tangible interface allows users to come together to have conversations and make interventions to make the case study area more accessible for pedestrians.&nbsp;</p>,2018-06-01,,2018-08-22 03:18:47.497,True,2016-06-06,Singapore Pedestrian Accessibility,PUBLIC,,False,City Science,False
city-game,kll,False,"<p>Planning a city is a complex task requiring collaboration between multiple stakeholders with different, and often conflicting, goals and objectives. Researchers have studied the role of technology in group collaboration for many years. It has been noted that when the task between collaborators increases in complexity, such as in a decision-making process, the use of computer technology could either enhance, or disturb, the collaboration process. City Game evaluates the impact of computer interfaces on a multi-objective negotiation problem. Using a tangible user interface (TUI) is more effective for multi-objective group decision-making than a graphical or multitouch user interface; this project will focus on designing and developing a TUI and a serious game for an urban planning scenario. We will test the game on different computer interfaces to evaluate the decision-making process between different collaborators with conflicting objectives.</p>",2016-12-01,--Choose Location,2017-10-11 13:06:14.048,True,2015-09-01,City Game,PUBLIC,,False,City Science,False
OLD_replace,kll,False,"<p>RePlace is a 3D data visualization platform that takes the normally invisible activity of sensors and data loggers and overlays that information as 3D sprites in the environment. RePlace uses an augmented reality viewfinder as a window into the data environment, and enables interaction with real-time and historic data feeds through the viewfinder. </p>",2016-12-01,--Choose Location,2018-05-04 18:23:12.980,True,2015-09-01,RePlace,PUBLIC,,False,City Science,False
3d-mobility,kll,False,"<p><i style=""font-size: 18px; font-weight: 400;"">Unfolding the way we move.&nbsp;</i><br></p><p>Mobility has shaped the built environment since humans started settling together. From industrial towns to post-industrial innovation and service hubs, the mobility mode of the era was key in shaping not only the physical attributes of cities, but also the efficacy. In order to allocate the massive migration from rural areas, cities are growing and becoming more dense. Although high density can minimize transportation cost and energy, several problems start to appear if they are not planned carefully. Urban ventilation potential is reduced and open seen spaces are limited, compromising our experience and life quality. Residential, office, and retail get closer but remain arranged in conventional ways. A two-dimensional street that organizes the way we live and keeps transportation methods are in permanent conflict. Too fast for those who live in it, and too slow and congested for those that go by.</p><p>Urban mobility is becoming more electric, more autonomous, more shared, and more connected, indicators that call for a mobility revolution, and designers have the chance to reinvent the way city is experienced.</p><p>Today, more than ever, the scale and rate of urban expansion is making mobility solutions a key concern, which will impact large segments of the global population since it is estimated that by 2050, more that two thirds of the global population will be living in cities. We&nbsp; propose a new experience and mobility around cities, unfolding the city networks and using its third dimensions, different mobility, speed modes (static, mass transportation, internal transportation), public areas appearing in rooftops, and mix-use spaces in&nbsp;intersticial&nbsp;parts of buildings. Through simulation as a tool, we can understand the impact of this new disrupting mobility system, avoiding to repeat mistakes like those made in the past.&nbsp;</p>",,,2019-06-12 14:47:59.274,True,2018-07-09,3D Mobility,PUBLIC,https://guadalupebabio.com,True,City Science,False
mod,kll,False,"<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>’s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>",,,2019-04-23 14:03:41.132,True,2017-08-01,Theme   |   Mobility On-Demand,PUBLIC,,True,City Science,False
bike-swarm,kll,False,"<p>As bikes navigate city streets after dark, they are often equipped with lights. The lights make the bikes visible to cars or other bikers, and the hazards of traffic less dangerous.</p><p>Imagine that as solitary bikes come together, their lights begin to pulsate at the same cadence. The bikers may not know each other, or may only be passing each other briefly, but for the moments they are together, their lights synchronize. The effect is a visually united presence, as groups of bikes illuminate themselves with a gently pulsing, collective light source.</p>",,,2019-06-11 16:37:33.942,True,2019-02-01,[bike] swarm,PUBLIC,http://aberke.com,True,City Science,False
cityscope-hamburg,kll,False,"<p><a href=""https://medium.com/mit-media-lab/shifting-priorities-finding-places-9ad3bdbe38b8"">Read more about this project here</a></p><p>MIT City Science is working with HafenCity University to develop CityScope for the neighborhood of Rothenburgsort in Hamburg, Germany. The goal is to create an interactive stakeholder engagement tool that also serves as the platform for joint research of modules for city simulation. Researchers are developing modules for walkability, neighborhood connectivity, energy efficiency, and economic activity, among others.</p>",,--Choose Location,2019-05-10 19:07:36.178,True,2015-01-01,City Science Lab Hamburg,PUBLIC,,True,City Science,False
test-project-265,kll,False,<p>What an am;zing research project</p>,,,2019-03-27 20:19:27.918,True,2019-03-26,Test project 265,GROUP,,True,City Science,False
OLD_cityscope-mark-ii-scout,kll,False,"<p>The CityScope ""Scout"" prototype integrates augmented reality with real-time mathematical modeling of geospatial systems. In practice, the technology transforms any tabletop into a canvas for land-use planning and walkability optimization. Users perform rapid prototyping with LEGO bricks and receive real-time simulation and evaluation feedback.</p>",,--Choose Location,2017-10-16 03:13:05.718,True,2014-01-01,CityScope Mark II: Scout,PUBLIC,,True,City Science,False
OLD_cityscope-mark-iii-dynamic-3d,kll,False,"<p>The Dynamic 3D prototype allows users to edit a digital model by moving physical 3D abstractions of building typologies. Movements are automatically detected, scanned, and digitized so as to generate inputs for computational analysis. 3D information is also projected back onto the model to give the user feedback while edits are made.</p>",,--Choose Location,2017-10-16 03:03:54.492,True,2014-01-01,CityScope Mark III: Dynamic 3D,PUBLIC,,True,City Science,False
OLD_cityscope-mark-ivb-land-usetransportation,kll,False,"<p>CityScope MarkIVb is programmed to demonstrate and model the relationship between land use (live and work), population density, parking supply and demand, and traffic congestion. </p>",,--Choose Location,2017-10-16 03:05:09.935,True,2014-09-01,CityScope Mark IVb: Land Use/Transportation,PUBLIC,,True,City Science,False
traffic-andorra,kll,False,"<h2>Data Fusion&nbsp;for Dynamic Traffic Prediction</h2><p>Traffic congestion has huge negative impacts on the productivity, health and personal lives of city dwellers. To manage this problem&nbsp;effectively, transportation engineers need to predict traffic congestion throughout the road network at all hours of the day.&nbsp;Prediction of traffic typically involves travel surveys that are expensive, time consuming and do not capture temporal variation in travel demand.&nbsp;However,&nbsp;anonymised&nbsp;location data from mobile phones present an alternative source of data which is passively collected, widely available and naturally captures temporal trends.&nbsp;On the other hand, these data contain other biases and so if we use these data for transportation models, we must take care to correct for these biases using more reliable data. As part of the City Science collaboration with Andorra, we used&nbsp;a&nbsp;Bayesian network to build a calibrated transportation model for the country based on&nbsp;geolocated telecoms data and validated using a small sample of traffic counts.</p>",,,2019-04-17 19:43:06.032,True,2017-01-01,Dynamic Traffic Prediction in Andorra: a Bayesian network approach,PUBLIC,,True,City Science,False
a-future-forward-proposal-for-a-system-of-streets-and-autonomous-vehicles,kll,False,"<p>ABSTRACT</p><p>The impending introduction of autonomous vehicles (AVs) has posed regulatory and ethical questions regarding how they should operate. Much of the previous literature on this subject has explored these questions with an underlying model of streets based on the present.</p><p>This paper takes a different approach by putting forward a future vision for streets where privately owned and operated vehicles are no longer dominant and shared transit is more pervasive. In doing so, this paper expands the current discussions around individual AVs to the system of streets they will occupy. &nbsp;It views the topology of streets and the rules that govern them, coupled with the vehicles that move through the streets, as an autonomous system, or machine.  This project proposes updates to this autonomous system in order to build a more equitable system for a future where AVs will be ubiquitous.  The paper presents a design of two parts in order to ensure that AVs operate in the public’s best interests:</p><ol><li>An update to the laws that govern the use of roads, vehicle regulations and safety standards.&nbsp;&nbsp;</li><li>A requirement that AV decision making code be open sourced.</li></ol>",2020-09-01,,2019-01-08 20:15:32.709,False,2018-12-01,A Future-Forward Proposal for a System of Streets and Autonomous Vehicles,PUBLIC,https://github.com/aberke/moral-machine-simulation,True,City Science,False
last-mile-logistics,kll,False,"<p>Developed by Ira Winder with the MIT Centre for Transportation and Logistics, the model seeks to use real population data and create a simulation to optimize delivery cost and coverage. This could be modified and applied to many disciplines, industries, and population types. The platform has the user place stores on a Tactile Matrix, a type of tangible interface, and displays the output of their potential delivery coverage and cost. This optimization game of sorts is a whole new approach to maximizing delivery potential. The interactive interface and layers of finely granulated and detailed data allow the user to make meaningful interventions and see the intertwining of many rich data sets. </p><p>Photos by James Li. Video by Nina Lutz.&nbsp;</p>",,,2017-10-16 15:43:32.494,True,2016-01-04,Last Mile Logistics,PUBLIC,,True,City Science,False
andorra-innovation,kll,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>The MIT Media Lab's City Science research group, the University of Andorra, and national and international companies are collaborating in order to bring an innovative ecosystem into the capital of Andorra. This innovation district aims to engage local citizens, researchers, and R&amp;D from the companies in order to build together an Andorran living lab, an ""innovation district"" where national and international companies can test and deploy their products and ideas and cultivate human capital.</p><p><b>Current Projects</b></p><ul><li>Andorra Innovation Space</li><li>Andorra Cultural Heritage</li><li>Drones patterns and flows, collaboration living lab<br></li><li>Young Future</li></ul>",,,2018-07-09 18:49:41.844,True,2016-09-01,Andorra | Innovation,PUBLIC,,True,City Science,False
places,kll,False,"<p>Changing Places researchers  are developing scalable strategies for creating hyper-efficient, technology-enabled spaces that can help make living more affordable, productive, enjoyable, and creative for urban dwellers.<br></p>",,,2019-05-24 21:06:01.300,True,2017-08-01,Theme | Changing Places,PUBLIC,,True,City Science,False
pev,kll,False,"<h1><b>An Alternative Autonomous Revolution&nbsp;</b></h1><h2><i>System design for emerging urban contexts and societal aspirations</i></h2><p>The Persuasive Electric Vehicle (PEV) aims to solve urban mobility challenges with a healthy, convenient, sustainable alternative to cars. The PEV is a low-cost, agile, shared-use autonomous bike that can be either an electrically assisted tricycle for passenger commuting or an autonomous carrier for package delivery.</p><p>The PEV uses standard bicycle components and is lightweight (&lt;50kg) yet robust. Its sensors are easy to reconfigure and it has a 250W mid-drive electric motor and 10Ah battery pack that provides 25 miles of travel per charge and a top speed of 20 miles per hour.</p><p>Our vision for the PEV: a rider summons the PEV through a phone app, and the nearest available PEV arrives autonomously to meet the rider. Upon completing the trip, the PEV simply moves on to its next passenger or package pickup.&nbsp; The PEV can be autonomous, operated by the rider, or provide the rider with an electric assist. PEV's operate in bike lanes, avoiding the congestion and adding incentives to make more bikeable cities.</p>",,--Choose Location,2018-07-19 21:12:27.919,True,2014-09-01,Persuasive Electric Vehicle (PEV),PUBLIC,,True,City Science,False
andorra-dynamic-urban-planning,kll,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>",,,2019-02-25 15:17:09.063,True,2016-09-01,Andorra | Dynamic Urban Planning,PUBLIC,,True,City Science,False
city-science-lab-aalto,kll,False,"<p>Aalto University, Finland, and the MIT Media Lab’s City Science group are co-developing a version of the MIT CityScope platform for urban analysis, efficient resource utilization, and spatial programming for campus development, using Otaniemi as a testbed. Aalto joins a network of City Science collaborators which includes Tongji University (Shanghai), Taipei Tech (Taiwan), HafenCity University (Hamburg), and ActuaTech (Andorra).</p>",,,2019-05-07 19:59:14.315,True,2017-05-01,City Science Lab Aalto,PUBLIC,,True,City Science,False
citymatrix,kll,False,"<h2><br>An Urban Decision-Support System Augmented by Artificial Intelligence<br></h2><p>The decision-making process in urban design and urban planning is outdated. Currently, urban decision-making is mostly a top-down process, with community participation only in its late stages. Furthermore, many design decisions are subjective, rather than based on quantifiable performance and data. Current tools for urban planning do not allow both expert and non-expert stakeholders to explore a range of complex scenarios rapidly with real-time feedback.&nbsp;</p><p>CityMatrix was an effort towards evidence-based, democratic decision-making. Its contributions lie in the application of Machine Learning as a versatile, quick, accurate, and low-cost approach to enable real-time feedback of complex urban simulations and the implementation of the optimization searching algorithms to provide open-ended decision-making suggestions.&nbsp;The goals of CityMatrix were:&nbsp;</p><br><ol><li><i>Designing an intuitive Tangible User Interface (TUI) to improve the accessibility of the decision-making process for non-experts.&nbsp;</i></li><li><i>Creating real-time feedback on multi-objective urban performances to help users evaluate their decisions, thus to enable rapid, collaborative decision-making.&nbsp;</i></li><li><i>Constructing a suggestion-making system that frees stakeholders from excessive, quantitative considerations and allows them to focus on the qualitative aspects of the city, thus helping them define and achieve their goals more efficiently.</i></li></ol><p>CityMatrix was augmented by Artificial Intelligence (AI) techniques including Machine Learning simulation predictions and optimization search algorithms. The hypothesis explored in this work was that the decision quality could be improved by the organic combination of both strengths of human intelligence and machine intelligence.</p><p>The system was pilot-tested and evaluated by comparing the problem-solving results of volunteers, with or without AI suggestions. Both quantitative and qualitative analytic results showed that CityMatrix is a promising tool that helps both professional and non-professional users understand the city better to make more collaborative and better-informed decisions.&nbsp;</p>",,,2018-10-17 18:10:46.064,True,2016-02-26,CityMatrix,PUBLIC,https://media.mit.edu/people/ryanz,True,City Science,False
andorra-living-lab,kll,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,City Science,False
Reversed-Urbanism,kll,False,"<h2>Predicting Urban Performance through Behavioral Patterns in Temporal Telecom Data</h2><p>This study explores a novel method to analyze diverse behavioral patterns in large urban populations and to associate them with discrete urban features. This work utilizes machine learning and anonymized telecom data to understand which fragments of the city has greater potential to attract dense and diverse populations over longer periods of time. Finally, this work suggests a road map for building spatial prediction tools in an effort to improve city-design and planning processes.&nbsp;&nbsp;</p><p><b><a href=""https://cityscope.github.io/CS_Andorra_RNC/"">Click here for an interactive visualization of this study</a>&nbsp;<br><br></b></p><p><b>Advisors:</b>&nbsp;Kent Larson&nbsp;and&nbsp;Esteban Moro<br><b>Thanks to</b> Andorra Telecom, ActuaTech,&nbsp;Núria Macià. <br>Data was&nbsp;obtained by Andorra Telecom as part of MIT Media Lab City Science and the State of Andorra collaboration.&nbsp;</p>",,,2019-02-24 23:21:12.068,True,2017-07-01,Reversed Urbanism,PUBLIC,http://ArielNoyman.com,True,City Science,False
persuasive-cities,kll,False,"<p>Persuasive Cities research is aimed at advancing urban spaces to facilitate societal changes. According to social science research, any well-designed environment can become a strong influencer of what people think and do. There is an endlessly dynamic interaction between a person, a particular behavior, and a specific environment. Persuasive Cities research leverages this knowledge to engineer persuasive environments and interventions for altering human behavior on a societal level. This research is focused on socially engaging environments for supporting entrepreneurship and innovation, reshaping routines and behavioral patterns in urban spaces, deploying intelligent outdoor sensing for shifting mobility modes, enhancing environmentally friendly behaviors through social norms, introducing interactive public feedback channels to alter attitudes at scale, engaging residents through socially influencing systems, exploring methods for designing persuasive neighborhoods, and fostering adoption of novel urban systems. More: http://bit.ly/TEDxp</p>",,--Choose Location,2017-03-31 20:52:58.972,True,2015-09-01,Persuasive Cities,PUBLIC,http://cp.media.mit.edu/agnis-stibe,True,City Science,False
andorra-mobility,kll,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile</a></p><p>With no airport or train service, most of the 8 million tourists who visit Andorra each year arrive by car, making traffic management and parking some of the country's most important challenges. We are currently developing different projects spanning from data science to the deployment of autonomous vehicles to help address these issues.<br></p>",,,2017-10-25 05:56:26.309,True,2016-09-01,Andorra | Mobility,PUBLIC,,True,City Science,False
CityscopeBostonbrt,kll,False,"<p>The&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://mfc.mit.edu/"">Mobility Futures Collaborative</a>&nbsp;in the MIT Department of Urban Studies and Planning (DUSP) and the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://cp.media.mit.edu/"">Changing Places group</a>&nbsp;at the MIT Media Lab have developed new interactive tools aimed to better communicate the possible impacts of new transit systems. The Media Lab and DUSP have partnered with the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""https://www.barrfoundation.org/"">Barr Foundation</a>&nbsp;to test these tools in a series of community engagement workshops to examine the impacts of Bus Rapid Transit (BRT) systems in greater Boston. These tools include the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://cp.media.mit.edu/city-simulation/"">CityScope</a>&nbsp;— an interactive platform that utilizes physical models (built from LEGO bricks) and 3-D projection — to enable community members to engage in neighborhood and street-level decisions including alternative bus corridor designs and station-level variations (such as pre-pay boarding). The second tool,&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://mfc.mit.edu/innovations-participatory-design-brt-systems"">CoAXs</a>&nbsp;is a new interactive platform for collaborative transit planning that builds on open-source urban analytics tools such as&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://conveyal.com/projects/analyst/"">Conveyal Transport Analyst</a></p>",,--Choose Location,2017-10-16 18:32:03.412,True,2015-01-01,CityScope Boston BRT,PUBLIC,,True,City Science,False
finding-places,kll,False,"<h1><b>What is FindingPlaces?</b></h1><p>In reaction to the sudden arrival of tens of thousands of refugees in the city of Hamburg (DE) in 2015, the Lord Mayor requested the CityScienceLab (CSL) at HafenCity University to facilitate a public discussion and decision-making process on locations for refugee accommodation in Hamburg neighborhoods. With highly sensitive socio-political implications, this project demanded a well-designed technological and procedural approach. CSL employed an innovative Human-Computer Interaction tool, CityScope, to facilitate public participation and urban decision-making. A workshop process was also designed to help multiple participants and stakeholders interact effectively. Running from May to July 2016, the FindingPlaces (FP) project enabled about 400 participants to identify 160 locations accepted by Hamburg’s citizens, out of which 44 passed legal confirmation by the authorities. Overall, on a qualitative level, the project facilitated surprisingly constructive and collaborative interaction, raising awareness and a sense of ownership among participants.</p>",,,2019-02-12 19:05:58.708,True,2016-01-01,FindingPlaces,PUBLIC,https://findingplaces.hamburg/,True,City Science,False
receptive-skins,kll,False,"<p><br>In architecture, the building skin is the primary interface for mediating the environment of the external with the internal. But today, this mediation is mechanical, deterministic, and static—often seeing the human as a generalizable and problematic input. With advances in material science however, there is great potential to disrupt these traditional manufactured environments of architecture and turn them into responsive mediated environments. What this thesis aims to explore is this idea of the receptive skin—a sensate and dynamic multi-material interface for environmental mediation. This suggests that by departing from the view that buildings are static artifacts, we may instead begin to see buildings as organic, living entities.</p><p>Through the development of a working prototype, this project explores how such an interface may manifest itself, through dynamic material composites, instead of mechanical and electronic means. The final prototype is a “proof of concept,” a built example of this novel design methodology, which unites material performance with sensate technologies, as a way to enable new interactions between building and environment.&nbsp;</p>",,,2018-06-26 21:12:49.214,True,2017-05-01,Receptive Skins: The Breathing Wall,PUBLIC,http://www.chrisoulakapelonis.com,True,City Science,False
urban-swarms,kll,False,"<p>Modern cities have to respond to the growing demands of more efficient and sustainable urban development, as well as an increased quality of life. In this context, the cities of the future will need the ability to gain insight about current urban conditions and react dynamically to them. According to this view, ""smart cities"" can be seen as cybernetic urban environments in which different agents (e.g., citizens) and actuators (e.g., robots) exploit the city-wide infrastructure as a medium to operate synergistically.<b><i> Urban Swarms</i></b> explores the feasibility of swarm robotics systems in urban environments. By using bio-inspired methods, a swarm of robots is able to handle important urban systems and infrastructures, improving their efficiency and autonomy. A diverse set of simulation experiments were designed and conducted using real-world GIS data. Results show that the proposed combination is able to outperform current approaches. <i><b>Urban Swarms</b></i> not only aims to show the efficiency of our proposed solution, but also to give insights about how to design and customize these systems.&nbsp;<a href=""https://www.media.mit.edu/projects/cityscope-volpe/overview/"" style=""font-size: 18px; font-weight: 400;"">CityScope</a><span style=""font-size: 18px; font-weight: 400;"">&nbsp;Volpe ABM model has been customized to integrate Swarm behavior using the </span><a href=""https://gama-platform.github.io/"" style=""font-size: 18px; font-weight: 400;"">Gama Platform</a><span style=""font-size: 18px; font-weight: 400;""> as an </span><a href=""https://github.com/mitmedialab/UrbanSwarms"" style=""font-size: 18px; font-weight: 400;"">open source project</a><span style=""font-size: 18px; font-weight: 400;"">.&nbsp;</span></p>",,,2019-03-12 15:37:36.573,True,2018-10-01,Urban Swarms,PUBLIC,http://www.eduardocastello.com,True,City Science,False
cities-network-unpublished,kll,False,"<h2>We propose that fundamentally new strategies must be found for creating the places where people live and work, and the mobility systems that connect these places, in order to meet the profound challenges of the future.</h2><p><span style=""font-size: 18px; font-weight: normal;"">Building on current work at the Media Lab, City Science researchers will initially focus on the following project themes. Additional project themes will be added in response to the priorities of corporate members, MIT researchers, and the City Science advisory board. These six initial themes represent a cross section of the interdisciplinary research that will be undertaken to address the major challenges associated with global urbanization.</span><br></p><p>The world is experiencing a period of extreme urbanization. In China alone, 300 million rural inhabitants will move to urban areas over the next 15 years. This will require building an infrastructure equivalent to the one housing the entire population of the United States in a matter of a few decades.</p><p>In the future, cities will account for nearly 90% of global population growth, 80% of wealth creation, and 60% of total energy consumption. Developing better strategies for the creation of new cities, is therefore, a global imperative.</p><p>Our need to improve our understanding of cities, however, is pressed not only by the social relevance of urban environments, but also by the availability of new strategies for city-scale interventions that are enabled by emerging technologies. Leveraging advances in data analysis, sensor technologies, and urban experiments, City Science will provide new insights into creating a data-driven approach to urban design and planning. To build the cities that the world needs, we need a scientific understanding of cities that considers our built environments and the people who inhabit them. Our future cities will desperately need such understanding.</p>",,,2019-04-17 17:26:57.486,False,2017-08-01,Cities Network (UNPUBLISHED),PUBLIC,http://cities.media.mit.edu/,True,City Science,False
cityscope_playground,kll,False,"<p>This project depicts the design, deployment and operation of a Tangible Regulation Platform, a physical-technological apparatus made for the distilment of regulations. The platform is set to exemplify the effects of regulations on a designated territory, allowing planners, designers, stakeholders and community members a common ground for discussion and decision making. An accessible and self-explanatory tool, this platform illustrates the relationship between urban form and regulations, offering a seamless and transparent process of regulation-based urban design. Lastly, projecting on the foreseen future of law and urbanism, this project proposes an alternative data and performance-based approach for the making of new regulations. Beyond excelling the processes of design under regulations, this platform and other new tools are offered to help facilitate a discussion on the way future regulations will be devised, improving both the design processes and their final outcome.</p>",,--Choose Location,2019-04-09 14:40:07.692,True,2014-09-01,CityScope PlayGround: MIT East Campus,PUBLIC,http://ArielNoyman.com,True,City Science,False
city-science-lab-shanghai,kll,False,"<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>",,,2019-05-16 20:43:38.677,True,2017-02-14,City Science Lab Shanghai,PUBLIC,,True,City Science,False
city-science-guadalajara,kll,False,"<p>The University of Guadalajara, referred to as UdeG, is a university network composed of 15 campuses within the state of Jalisco and one online system. The University offers undergraduate and graduate studies to around 130,000 students. UdeG strives to understand urban performance metrics using evidence-based decision making tools, facilitated through a collaboration with the MIT Media Lab City Science group.</p>",,,2019-05-10 19:44:20.937,True,2018-11-01,City Science Collaboration Guadalajara,PUBLIC,,True,City Science,False
andorra-tourism,kll,False,"<p><span style=""font-size: 18px; font-weight: 400;""></span><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/""><span style=""font-size: 18px; font-weight: 400;"">View the main City Science Andorra project profile.</span></a><br></p><p><span style=""font-size: 18px; font-weight: 400;"">With more than eight million visitors a year, tourism represents almost 30% of the economy of Andorra. By gathering and analyzing data from social media, call detail records, and wifi, we can understand the country's dynamics of tourism and commerce as well as design interventions that can improve the experience for tourists, encouraging them to visit Andorra more frequently, stay longer, and increase spending.&nbsp;</span><br></p><h2><b>Current Projects</b></h2><ul><li>Event Analysis<br></li><li>Social Network<br></li><li>Location Recommendation system<br></li></ul><p> </p><h2><b>EVENT ANALYSIS</b></h2><p>Based on the analysis of call detail records and social media, the goal of this project is to understand the tourist behaviors in Andorra.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">After mining those anonymized data, we have been able to learn different patterns and behaviors of the tourism in Andorra thanks to an agent-based model developed in order to represent the flow of people. This simulation is also coupled with an interactive table called CityMatrix.</span></p>",,,2019-02-25 15:33:28.936,True,2015-08-01,Andorra | Tourism,PUBLIC,,True,City Science,False
andorra-energy-environment,kll,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p>",,,2018-10-22 21:46:23.783,True,2016-09-02,Andorra | Energy + Environment,PUBLIC,,True,City Science,False
aalto-saas,kll,False,"<h2>How can we get more value from the same buildings?&nbsp;</h2><p>Cities contain many different resources and spaces and typically, these resources operate as products with&nbsp; a single function and a single owner and/or renter. However, the owner's demand&nbsp;for space often varies daily or seasonally, meaning that many buildings tend to be underutilized and are often vacant or partially vacant for large portions of each day.&nbsp;</p><p>Meanwhile, the ""sharing economy"" has been one of the most significant economic shifts in the last 10 years, with companies like Uber and Airbnb experiencing explosive growth. Along these lines, Aalto University—a member of the MIT Media Lab City&nbsp;Science network—has developed the concept&nbsp;of City-as-a-Service, where building space and other resources are shared among institutions, businesses, and citizens in a community. Aalto has already begun experimenting with School-as-a-Service, as a prototype of City-as-a-Service on their campus in Espoo.</p>",,,2019-05-07 20:00:40.076,True,2017-07-01,Aalto Campus-as-a-Service Simulations,PUBLIC,,True,City Science,False
the-trade-off-between-the-utility-and-privacy-risks-of-location-data-and-implications-for-data-as-a-public-good,kll,False,"<p><i>Paper presented at the ""Connected Life 2019: Data &amp; Disorder"" conference at the Oxford Internet Institute.</i></p><p>High-resolution individual geolocation data passively collected from mobile phones is increasingly sold in private markets and shared with researchers.</p><p>This data poses significant security, privacy, and ethical risks: it’s been shown that users can be re-identified in such datasets, and its collection rarely involves their full consent or knowledge. This data is valuable to private firms (e.g. targeted marketing) but also presents clear value as a public good. Recent public interest research has demonstrated that high-resolution location data can more accurately measure segregation in cities and provide inexpensive transit modeling. But as data is aggregated to mitigate its re-identifiability risk, its value as a good diminishes. How do we rectify the clear security and safety risks of this data, its high market value, and its potential as a resource for public good? We extend the recently proposed concept of a tradeoff curve that illustrates the relationship between dataset utility and privacy. We then hypothesize how this tradeoff differs between private market use and its potential use for public good. We further provide real-world examples of how high resolution location data, aggregated to varying degrees of privacy protection, can be used in the public sphere and how it is currently used by private firms.</p>",,,2019-05-24 15:26:51.045,False,2019-03-01,The Tradeoff Between the Utility and Risk of Location Data and Implications for Public Good,LAB,,True,City Science,False
escape-pod-1,kll,False,"<p>The esc-Pod&nbsp; (or Escape Pod) is an exploratory platform for researchers investigating moments of refuge within our bustling work lives.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">The core of the esc-Pod consists of actuated work and rest surfaces. This allows for moments of productivity and relaxation to occur within a single space.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The outer skin provides variable transparency, enabling a spectrum of visibility settings according to privacy requirements.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The inner skin provides an infrastructure for the modulation of spatial experiences. Each panel is a pixel, connecting itself to the skin network, and can embody an array of senses.</span></p>",,,2019-04-08 17:01:14.555,True,2016-08-01,Escape Pod,PUBLIC,,True,City Science,False
cityscope-livingline-shanghai,kll,False,"<p>College of Design and Innovation of Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform to support the urban decision making that promotes urban vibrancy and innovation potential. </p><p>The “NICE2035 LivingLine” project in Shanghai, China, is a design-driven, community-based urban innovation initiated by Professor Yongqi Lou, Dean of College of Design of Innovation. LivingLine is a crowdsourcing and co-creation project aiming at building an ecosystem of innovation and entrepreneurship on the internal street of a typical gated residential neighborhood. By introducing radical programs such as living labs, co-working space, and startup-incubators into underutilized storefront space, LivingLine’s goal is to revitalize the urban space and to prototype diverse future lifestyles.<br></p>",,,2019-06-06 16:01:44.891,True,2018-03-01,CityScope LivingLine Shanghai,PUBLIC,,True,City Science,False
basic,kll,False,"<p>Autonomous vehicles (AVs), drones, and robots will revolutionize our way of traveling and understanding urban space. In order to operate, all of these devices are expected to collect and analyze a lot of sensitive data about our daily activities. However, current operational models for these devices have extensively relied on centralized models of managing these data. The security of these models unveiled significant issues.</p><p>This project&nbsp; proposes BASIC, the Blockchained Agent-based Simulator for Cities. This tool aims to verify the feasibility of the use of blockchain in simulated urban scenarios by considering the communication between agents through&nbsp;<i>smart contracts</i>. In order to test the proposed tool, we implemented a car-sharing model within the city of Cambridge (Massachusetts, USA). In this research, the relevant literature was explored, new methods were developed, and different solutions were designed and tested. Finally, conclusions about the feasibility of the combination between blockchain technology and agent-based simulations were drawn.</p><p>Developed using&nbsp;<a href=""https://gama-platform.github.io/"">Gama Platform</a>.&nbsp;&nbsp;</p><p>Click <a href=""https://github.com/mitmedialab/Basic"">here</a> for the Open Source Repository.</p>",,,2019-04-23 20:08:21.338,True,2018-09-03,BASIC: Blockchained Agent-based Simulator for Cities,PUBLIC,,True,City Science,False
cityscope-cooper-hewitt,kll,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
piccolo-kitchen,kll,False,"<p>&nbsp;This project aims to create a modular platform for exploring micro-kitchens that are culture specific. Cooking is a personal experience that has cultural attributes.&nbsp; This project explores new modes of cooking using robotically enabled cabinets and appliances to minimize the footprint of the kitchen, while maximizing the ability for users to cook large meals, socialize, and utilize the same space during non-meal times for work. Piccolo kitchen is one of the components of the micro-units that are currently under development as part of the CityHome 02 projects.</p>",,,2019-04-19 13:39:55.600,True,2018-09-01,Piccolo Kitchen,PUBLIC,,True,City Science,False
city-science-andorra,kll,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
cityscope,kll,False,"<p>City Science researchers are developing a slew of tangible and digital platforms dedicated to solving spatial design and urban planning challenges. The tools range from simulations that quantify the impact of disruptive interventions in cities to communicable collaboration applications. We develop and deploy these tools around the world and maintain open source repositories for the majority of deployments. ""CityScope"" is a concept for shared, interactive computation for urban planning.</p><p>All current CityScope development, tools, and software are open source <a href=""https://cityscope.github.io/"">here</a>.&nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2019-05-16 20:42:57.474,True,2017-08-01,Theme | CityScope,PUBLIC,,True,City Science,False
hybrid-autonomous-shared-bike-fleet-deployment-simulator,ptinn,False,"<p>Cities around the world are striving to improve livability by way of reducing dependency on fossil-fuels cars. How might we leverage the autonomous technology to help fulfill this vision, while ensuring the flow of people and goods across the city? The Persuasive Electric Vehicle (PEV) is a small, on-demand, shared, agile, autonomous, and functionally hybrid tricycle. We believe it will become a critical platform in the constellation of emerging mobility systems. The PEV will be a shared bike platform for people's inner-urban and last-mile travel needs, and for delivering goods on-demand around the clock. To deploy it in the real world, it is necessary to match the fleet supply with its demand. This simulator enables cities around the world to forecast the fleet size based on proxy demands from taxis, shared bikes, shared car services, and Call Detail Records (CDR).</p>",2016-01-01,--Choose Location,2016-12-08 17:15:49.102,True,2015-01-01,Hybrid Autonomous Shared Bike Fleet Deployment Simulator,PUBLIC,,False,City Science,False
mod,ptinn,False,"<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>’s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>",,,2019-04-23 14:03:41.132,True,2017-08-01,Theme   |   Mobility On-Demand,PUBLIC,,True,City Science,False
fleet-simulation,ptinn,False,"<h2><i>Forecasting the supply of fleets to meet emerging travel demands and service needs in cities</i></h2><p>The availability of vehicles is a critical factor behind successful shared-use mobility services. Proper management of supply-demand dynamics is paramount for achieving viability in a new mobility service, as achieving scale often requires a large capital investment. Under-supplying the fleet would result in low service availability and user dissatisfaction; over-supplying results in inefficient use of capital. In addition, as a new shared mobility platform diversifies its service across both passenger and freight delivery, its required scale of operation and investment becomes more difficult to estimate. &nbsp;</p><p>In this fleet deployment and optimization research, the City Science group aims to create an accessible simulation tool to enable cities to forecast the size of deployment of new shared mobility services using the Persuasive Electric Vehicle delivering passengers and packages as an initial test case. The simulation tool also provides a platform for testing fleet rebalancing and service-hub strategies.</p>",,,2019-04-17 19:44:17.413,True,2016-04-01,Service deployment simulation and optimization for mixed-use delivery fleets,PUBLIC,,True,City Science,False
hmi,ptinn,False,"<h2><i>Facilitating coexistence, trust-building, and collaboration among people and machines.</i></h2><p>New modes of 21st century urban transportation are becoming increasingly lightweight, electrified, connected, shared, and autonomous. Cohabitation of humans and machines is an increasingly important question, and one which requires careful attention and design.&nbsp; We strive to enable new forms of human-machine co-existence, trust, and collaboration.</p><h2>This work focuses on enabling:</h2><ol><li>Intuitive and effective two-way communication between vehicles and pedestrians;</li><li>Street safety and traffic-yielding mechanisms; and</li><li>Behavior change related to the adoption of active mobility mode, or electric assist.</li></ol>",,,2019-04-17 19:49:25.786,True,2016-11-01,Human-machine cooperation (HMC) for lightweight autonomous robots,PUBLIC,,True,City Science,False
pev,ptinn,False,"<h1><b>An Alternative Autonomous Revolution&nbsp;</b></h1><h2><i>System design for emerging urban contexts and societal aspirations</i></h2><p>The Persuasive Electric Vehicle (PEV) aims to solve urban mobility challenges with a healthy, convenient, sustainable alternative to cars. The PEV is a low-cost, agile, shared-use autonomous bike that can be either an electrically assisted tricycle for passenger commuting or an autonomous carrier for package delivery.</p><p>The PEV uses standard bicycle components and is lightweight (&lt;50kg) yet robust. Its sensors are easy to reconfigure and it has a 250W mid-drive electric motor and 10Ah battery pack that provides 25 miles of travel per charge and a top speed of 20 miles per hour.</p><p>Our vision for the PEV: a rider summons the PEV through a phone app, and the nearest available PEV arrives autonomously to meet the rider. Upon completing the trip, the PEV simply moves on to its next passenger or package pickup.&nbsp; The PEV can be autonomous, operated by the rider, or provide the rider with an electric assist. PEV's operate in bike lanes, avoiding the congestion and adding incentives to make more bikeable cities.</p>",,--Choose Location,2018-07-19 21:12:27.919,True,2014-09-01,Persuasive Electric Vehicle (PEV),PUBLIC,,True,City Science,False
smart-infra,ptinn,False,"<h1>Scalable Urban Infrastructure for Human-Machine Cohabitation</h1><h2><i>New infrastructure to help sustain public-sector participation and operation, and maximize public interest and safety.</i></h2><p>Advancements in autonomous technology have led automobile makers and tech companies to focus on reinventing the automobile—increasing computational capability and enhancing sensor systems. But due to strict road-safety regulations, this vehicle-centric, inside-out approach may take years to materialize, and when it does, restricting “autonomy” to selected vehicles will limit autonomy’s impact on street safety and accessibility.</p><p>To address current issues, The City Science group focuses on ways to offload often-heavy computational requirements from the vehicle through affordable interventions to street infrastructure by creating human-machine readable traffic signs and urban markers.</p><p>With the support of a new genre of smart urban infrastructure, we believe this “autonomy-lite” approach will soon allow lightweight autonomous vehicles to be widely deployed and navigate smoothly in most urban environments.&nbsp;</p>",,,2018-03-28 22:18:15.132,True,2017-05-01,Urban Tattoo,PUBLIC,,True,City Science,False
andorra-mobility,ptinn,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile</a></p><p>With no airport or train service, most of the 8 million tourists who visit Andorra each year arrive by car, making traffic management and parking some of the country's most important challenges. We are currently developing different projects spanning from data science to the deployment of autonomous vehicles to help address these issues.<br></p>",,,2017-10-25 05:56:26.309,True,2016-09-01,Andorra | Mobility,PUBLIC,,True,City Science,False
CityscopeBostonbrt,ptinn,False,"<p>The&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://mfc.mit.edu/"">Mobility Futures Collaborative</a>&nbsp;in the MIT Department of Urban Studies and Planning (DUSP) and the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://cp.media.mit.edu/"">Changing Places group</a>&nbsp;at the MIT Media Lab have developed new interactive tools aimed to better communicate the possible impacts of new transit systems. The Media Lab and DUSP have partnered with the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""https://www.barrfoundation.org/"">Barr Foundation</a>&nbsp;to test these tools in a series of community engagement workshops to examine the impacts of Bus Rapid Transit (BRT) systems in greater Boston. These tools include the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://cp.media.mit.edu/city-simulation/"">CityScope</a>&nbsp;— an interactive platform that utilizes physical models (built from LEGO bricks) and 3-D projection — to enable community members to engage in neighborhood and street-level decisions including alternative bus corridor designs and station-level variations (such as pre-pay boarding). The second tool,&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://mfc.mit.edu/innovations-participatory-design-brt-systems"">CoAXs</a>&nbsp;is a new interactive platform for collaborative transit planning that builds on open-source urban analytics tools such as&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://conveyal.com/projects/analyst/"">Conveyal Transport Analyst</a></p>",,--Choose Location,2017-10-16 18:32:03.412,True,2015-01-01,CityScope Boston BRT,PUBLIC,,True,City Science,False
city-science-lab-shanghai,ptinn,False,"<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>",,,2019-05-16 20:43:38.677,True,2017-02-14,City Science Lab Shanghai,PUBLIC,,True,City Science,False
torque,ptinn,False,"<h1>Open-Source Autonomous Platform for Educational and Service Design Applications</h1><h2><i>How can new technologies respond to society’s diverse industrial, socio-economic, and educational needs?</i></h2><p>Despite AI and robotics being widely trumpeted as keys to the new Industrial Revolution, access to their development remains largely restricted to companies and institutions that are rich in capital and/or data, potentially further deepening the socio-economic disparity observed across continents. As a likely result, these new technologies generate limited positive externalities. For instance, are automobiles really the most critical area in need of self-driving technology? Where else might AI and robotics be applied to lead to increased urban livability, socioeconomic equity, and the vibrancy of local businesses?</p><p>Building upon the architecture of MIT’s open-source race car platform, the City Science group introduces&nbsp;a new open-ended and heavy-duty self-driving platform.&nbsp;&nbsp;Torque is intended to be used by educators and makers and is ideal for hackathons and classroom instruction. Torque will soon allow rapid prototyping of usage scenarios and services for various contexts and needs.&nbsp;</p>",,,2018-05-04 15:23:33.802,True,2017-07-01,Torque,PUBLIC,,True,City Science,False
city-science-andorra,ptinn,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
an-ultrasonic-sensing-and-indentation-apparatus-for-assessment-of-tissue-geometry-and-mechanical-properties,kmoerman,False,"<p>Measuring the distance from the skin to the bone and soft tissue mechanical proper- ties is important to custom designing prosthetic sockets for amputee patients using a computer aided method. The current state-of-the-art method to obtain such informa- tion is via MRI scans. However, MRI scans are expensive, not widely accessible, and may not be as accurate if there is a time gap between when the MRI scan is taken and when the design process takes place. In this project, I designed and implemented a hand-held apparatus which measures both the skin-to-bone depth and soft tissue mechanical properties. With a PC interface, this method involves gathering and pro- cessing data from an ultrasound transducer, a force sensor, and an accelerometer. The procedure of use involves rotating the apparatus around the limb while main- taining a light contact to acquire skin-to-bone depth, and indenting the apparatus into the limb to acquire soft tissue mechanical properties. Here I show that a minia- turized apparatus as such can measure tissue boundaries and tissue indentation with sub-millimeter precision and out performs a commercial ultrasound imaging system in my case study, which makes custom computer prosthetic socket design easier, more affordable, and more accessible.
                    
                </p>",2017-08-25,,2017-08-16 19:47:11.126,True,2016-09-01,An Ultrasonic Sensing and Indentation Apparatus for Assessment of Tissue Geometry and Mechanical Properties,LAB,,False,Biomechatronics,False
ultrasound-prosthetic-interface-design,kmoerman,False,"​<p>In the United States, there are an estimated 1.7 million people living with amputation, with that number expected to double by 2050. Complications of prosthetic leg use in persons with lower extremity amputation (LEA) include delayed wound healing, recurrent skin ulcerations, and pressure damage to soft tissues. This can result in limited mobility, which further contributes to conditions such as obesity, musculoskeletal pathologies (e.g., osteoarthritis, osteopenia, and osteoporosis), as well as cardiovascular disease. Traditionally, fabrication of prosthetic sockets remains a fundamentally artisanal process with limited input of quantitative data. Even with advances in computer-aided design and manufacturing (CAD/CAM), prosthetists often modify sockets using non-quantitative craft processes requiring substantial human hours and financial cost. The goal of this research is to develop and validate musculoskeletal ultrasound imaging techniques for creating predictive biomechanical models of residual limbs that will reduce the barrier for and cost of computer aided design (CAD)-driven prosthetic socket design in the US and in low-and middle-income countries.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>​",,,2019-04-17 19:24:01.508,True,2016-10-18,Ultrasound imaging for transtibial prosthetic interface design,PUBLIC,,True,Biomechatronics,False
multidic-a-matlab-toolbox-for-multi-view-3d-digital-image-correlation,kmoerman,False,"<p>&nbsp;Three-dimensional Digital Image Correlation (3D-DIC) is a non-contact optical-numerical&nbsp;technique for evaluating the dynamic mechanical behavior at the surface of structures and materials,&nbsp;including biological tissues. 3D-DIC can be used to extract shape and full-field displacements and strains&nbsp;with high resolution, at various length scales. While various commercial and academic 3D-DIC software&nbsp;exist, the field lacks 3D-DIC packages which offer straightforward calibration and data-merging solutions for&nbsp;multi-view analysis, which is particularly desirable in biomedical applications. To address these limitations,&nbsp;we present MultiDIC, an open-source MATLAB toolbox, featuring the first 3D-DIC software specifically&nbsp;dedicated to multi-view setups. MultiDIC integrates robust two-dimensional subset-based DIC software with&nbsp;specially tailored calibration procedures, to reconstruct the dynamic behavior of surfaces from multiple&nbsp;stereo-pairs. MultiDIC contains novel algorithms to automatically merge meshes from multiple stereo-pairs,&nbsp;and to compute and visualize 3D shape and full-field motion, deformation, and strain. User interfaces&nbsp;provide capabilities to perform 3D-DIC analyses without interacting with MATLAB syntax, while standalone&nbsp;functions also allow proficient MATLAB users to write custom scripts for specific experimental&nbsp;requirements. This paper discusses the challenges underlying multi-view 3D-DIC, details the proposed&nbsp;solutions, and describes the algorithms implemented in MultiDIC. The performance of MultiDIC is tested&nbsp;using a low-cost experimental system featuring a 360-deg 12-camera setup. The software and system are&nbsp;evaluated using measurement of a cylindrical object with known geometry subjected to rigid body motion&nbsp;and measurement of the lower limb of a human subject. The findings confirm that shape, motion, and full-field deformations and strains can be accurately measured, and demonstrate the feasibility of MultiDIC in&nbsp;multi-view in-vivo biomedical applications.</p>",,,2019-04-26 19:00:50.702,True,2017-05-15,MultiDIC: a MATLAB Toolbox for Multi-View 3D Digital Image  Correlation,PUBLIC,,True,Biomechatronics,False
analysis-of-residual-limb-changes-using-digital-image-correlation-and-finite-element-modelling,kmoerman,False,"<p>Local changes in the volume, shape, and mechanical properties of the residual limb can be caused by adjacent joint motion, muscle activation, hydration, atrophy, and more. These changes affect socket fit quality and might cause inefficient load distribution, discomfort, and dermatological problems. Analyzing these effects is an important step in considering their influence on socket fit, and in accounting for their contribution within the socket design process. </p><p>In this study, a 360° 3D digital image correlation (3D-DIC) system was developed for the full-field deformation measurements of the residuum. A multi-camera rig was designed for capturing synchronized image sets as well as force measurements from a hand-held indenter. Custom camera calibration and data-processing procedures were specifically designed to transform image data into 3D point clouds, and automatically merge data obtained from multiple views into continuous surfaces. Moreover, a specially developed data-analysis procedure was applied for correlating pairs of largely deformed images of speckled surfaces, from which displacements, deformation gradients, and strains were calculated.&nbsp;Characterization of the full-field deformations using 3D-DIC provides insight into the patterns and sources of the phenomena.&nbsp;</p><p>In addition, local and subject-specific soft tissue mechanical properties were obtained by analyzing surface deformation and force measurement during indentation using inverse FE analysis. These data can be used to accurately describe the residuum’s biomechanical behavior. Consequently, prosthetic socket designs that take into account these effects can be considered.</p>",,,2019-04-26 19:04:59.543,True,2017-05-01,Analysis of residual limb changes using digital image correlation and finite element modeling,PUBLIC,,True,Biomechatronics,False
mechanical-interfaces,kmoerman,False,"<p>This research track focuses on the use of computational (and experimental) techniques to understand the biomechanical behavior of human tissue as well as the musculoskeletal system.&nbsp; This knowledge feeds into novel methods for computational modeling based design of biomechatronic devices which in turn aim to restore or improve the human body. These devices include prosthetic and orthotic devices, and exoskeletons. <br></p>",,,2019-04-26 19:03:38.901,True,2015-11-01,Computational Biomechanics,PUBLIC,,True,Biomechatronics,False
an-ultrasonic-sensing-and-indentation-apparatus-for-assessment-of-tissue-geometry-and-mechanical-properties,srkeyes,False,"<p>Measuring the distance from the skin to the bone and soft tissue mechanical proper- ties is important to custom designing prosthetic sockets for amputee patients using a computer aided method. The current state-of-the-art method to obtain such informa- tion is via MRI scans. However, MRI scans are expensive, not widely accessible, and may not be as accurate if there is a time gap between when the MRI scan is taken and when the design process takes place. In this project, I designed and implemented a hand-held apparatus which measures both the skin-to-bone depth and soft tissue mechanical properties. With a PC interface, this method involves gathering and pro- cessing data from an ultrasound transducer, a force sensor, and an accelerometer. The procedure of use involves rotating the apparatus around the limb while main- taining a light contact to acquire skin-to-bone depth, and indenting the apparatus into the limb to acquire soft tissue mechanical properties. Here I show that a minia- turized apparatus as such can measure tissue boundaries and tissue indentation with sub-millimeter precision and out performs a commercial ultrasound imaging system in my case study, which makes custom computer prosthetic socket design easier, more affordable, and more accessible.
                    
                </p>",2017-08-25,,2017-08-16 19:47:11.126,True,2016-09-01,An Ultrasonic Sensing and Indentation Apparatus for Assessment of Tissue Geometry and Mechanical Properties,LAB,,False,Biomechatronics,False
an-ultrasonic-sensing-and-indentation-apparatus-for-assessment-of-tissue-geometry-and-mechanical-properties,zixiliu,False,"<p>Measuring the distance from the skin to the bone and soft tissue mechanical proper- ties is important to custom designing prosthetic sockets for amputee patients using a computer aided method. The current state-of-the-art method to obtain such informa- tion is via MRI scans. However, MRI scans are expensive, not widely accessible, and may not be as accurate if there is a time gap between when the MRI scan is taken and when the design process takes place. In this project, I designed and implemented a hand-held apparatus which measures both the skin-to-bone depth and soft tissue mechanical properties. With a PC interface, this method involves gathering and pro- cessing data from an ultrasound transducer, a force sensor, and an accelerometer. The procedure of use involves rotating the apparatus around the limb while main- taining a light contact to acquire skin-to-bone depth, and indenting the apparatus into the limb to acquire soft tissue mechanical properties. Here I show that a minia- turized apparatus as such can measure tissue boundaries and tissue indentation with sub-millimeter precision and out performs a commercial ultrasound imaging system in my case study, which makes custom computer prosthetic socket design easier, more affordable, and more accessible.
                    
                </p>",2017-08-25,,2017-08-16 19:47:11.126,True,2016-09-01,An Ultrasonic Sensing and Indentation Apparatus for Assessment of Tissue Geometry and Mechanical Properties,LAB,,False,Biomechatronics,False
an-ultrasonic-sensing-and-indentation-apparatus-for-assessment-of-tissue-geometry-and-mechanical-properties,danask,False,"<p>Measuring the distance from the skin to the bone and soft tissue mechanical proper- ties is important to custom designing prosthetic sockets for amputee patients using a computer aided method. The current state-of-the-art method to obtain such informa- tion is via MRI scans. However, MRI scans are expensive, not widely accessible, and may not be as accurate if there is a time gap between when the MRI scan is taken and when the design process takes place. In this project, I designed and implemented a hand-held apparatus which measures both the skin-to-bone depth and soft tissue mechanical properties. With a PC interface, this method involves gathering and pro- cessing data from an ultrasound transducer, a force sensor, and an accelerometer. The procedure of use involves rotating the apparatus around the limb while main- taining a light contact to acquire skin-to-bone depth, and indenting the apparatus into the limb to acquire soft tissue mechanical properties. Here I show that a minia- turized apparatus as such can measure tissue boundaries and tissue indentation with sub-millimeter precision and out performs a commercial ultrasound imaging system in my case study, which makes custom computer prosthetic socket design easier, more affordable, and more accessible.
                    
                </p>",2017-08-25,,2017-08-16 19:47:11.126,True,2016-09-01,An Ultrasonic Sensing and Indentation Apparatus for Assessment of Tissue Geometry and Mechanical Properties,LAB,,False,Biomechatronics,False
multidic-a-matlab-toolbox-for-multi-view-3d-digital-image-correlation,danask,False,"<p>&nbsp;Three-dimensional Digital Image Correlation (3D-DIC) is a non-contact optical-numerical&nbsp;technique for evaluating the dynamic mechanical behavior at the surface of structures and materials,&nbsp;including biological tissues. 3D-DIC can be used to extract shape and full-field displacements and strains&nbsp;with high resolution, at various length scales. While various commercial and academic 3D-DIC software&nbsp;exist, the field lacks 3D-DIC packages which offer straightforward calibration and data-merging solutions for&nbsp;multi-view analysis, which is particularly desirable in biomedical applications. To address these limitations,&nbsp;we present MultiDIC, an open-source MATLAB toolbox, featuring the first 3D-DIC software specifically&nbsp;dedicated to multi-view setups. MultiDIC integrates robust two-dimensional subset-based DIC software with&nbsp;specially tailored calibration procedures, to reconstruct the dynamic behavior of surfaces from multiple&nbsp;stereo-pairs. MultiDIC contains novel algorithms to automatically merge meshes from multiple stereo-pairs,&nbsp;and to compute and visualize 3D shape and full-field motion, deformation, and strain. User interfaces&nbsp;provide capabilities to perform 3D-DIC analyses without interacting with MATLAB syntax, while standalone&nbsp;functions also allow proficient MATLAB users to write custom scripts for specific experimental&nbsp;requirements. This paper discusses the challenges underlying multi-view 3D-DIC, details the proposed&nbsp;solutions, and describes the algorithms implemented in MultiDIC. The performance of MultiDIC is tested&nbsp;using a low-cost experimental system featuring a 360-deg 12-camera setup. The software and system are&nbsp;evaluated using measurement of a cylindrical object with known geometry subjected to rigid body motion&nbsp;and measurement of the lower limb of a human subject. The findings confirm that shape, motion, and full-field deformations and strains can be accurately measured, and demonstrate the feasibility of MultiDIC in&nbsp;multi-view in-vivo biomedical applications.</p>",,,2019-04-26 19:00:50.702,True,2017-05-15,MultiDIC: a MATLAB Toolbox for Multi-View 3D Digital Image  Correlation,PUBLIC,,True,Biomechatronics,False
analysis-of-residual-limb-changes-using-digital-image-correlation-and-finite-element-modelling,danask,False,"<p>Local changes in the volume, shape, and mechanical properties of the residual limb can be caused by adjacent joint motion, muscle activation, hydration, atrophy, and more. These changes affect socket fit quality and might cause inefficient load distribution, discomfort, and dermatological problems. Analyzing these effects is an important step in considering their influence on socket fit, and in accounting for their contribution within the socket design process. </p><p>In this study, a 360° 3D digital image correlation (3D-DIC) system was developed for the full-field deformation measurements of the residuum. A multi-camera rig was designed for capturing synchronized image sets as well as force measurements from a hand-held indenter. Custom camera calibration and data-processing procedures were specifically designed to transform image data into 3D point clouds, and automatically merge data obtained from multiple views into continuous surfaces. Moreover, a specially developed data-analysis procedure was applied for correlating pairs of largely deformed images of speckled surfaces, from which displacements, deformation gradients, and strains were calculated.&nbsp;Characterization of the full-field deformations using 3D-DIC provides insight into the patterns and sources of the phenomena.&nbsp;</p><p>In addition, local and subject-specific soft tissue mechanical properties were obtained by analyzing surface deformation and force measurement during indentation using inverse FE analysis. These data can be used to accurately describe the residuum’s biomechanical behavior. Consequently, prosthetic socket designs that take into account these effects can be considered.</p>",,,2019-04-26 19:04:59.543,True,2017-05-01,Analysis of residual limb changes using digital image correlation and finite element modeling,PUBLIC,,True,Biomechatronics,False
mechanical-interfaces,danask,False,"<p>This research track focuses on the use of computational (and experimental) techniques to understand the biomechanical behavior of human tissue as well as the musculoskeletal system.&nbsp; This knowledge feeds into novel methods for computational modeling based design of biomechatronic devices which in turn aim to restore or improve the human body. These devices include prosthetic and orthotic devices, and exoskeletons. <br></p>",,,2019-04-26 19:03:38.901,True,2015-11-01,Computational Biomechanics,PUBLIC,,True,Biomechatronics,False
eyeglasses-free-displays,raskar,False,"<p>Millions of people worldwide need glasses or contact lenses to see or read properly. We introduce a computational display technology that predistorts the presented content for an observer, so that the target image is perceived without the need for eyewear. We demonstrate a low-cost prototype that can correct myopia, hyperopia, astigmatism, and even higher-order aberrations that are difficult to correct with glasses.</p>",2016-09-01,--Choose Location,2016-12-05 00:16:24.496,True,2014-01-01,Eyeglasses-Free Displays,PUBLIC,,False,Camera Culture,False
barmaks-untitled-project-2,raskar,False,"<p>We exploit the sub-picosecond time resolution along with spectral resolution provided by terahertz time-domain spectroscopy to extract occluding content from layers whose thicknesses are wavelength comparable. The method uses the statistics of the THz E-field at subwavelength gaps to lock into each layer position and then uses a time-gated spectral kurtosis to tune to highest spectral contrast of the content on that specific layer. To demonstrate, occluding textual content was successfully extracted from a sample similar to a closed book down to nine pages without human supervision. The method provides over an order of magnitude enhancement in the signal contrast and can impact inspection of structural defects in wooden objects, plastic components, composites, drugs, and especially cultural artifacts with subwavelength or wavelength comparable layers.</p>",,--Choose Location,2016-10-24 19:44:56.028,False,2015-09-01,Reading through Closed Books: THz Time-Gated Spectral Imaging for Content Extraction through Layered Structures,PUBLIC,,True,Camera Culture,False
single-photon-sensitive-ultrafast-imaging,raskar,False,"<p>The ability to record images with extreme temporal resolution enables a diverse range of applications, such as time-of-flight depth imaging and characterization of ultrafast processes. Here we present a demonstration of the potential of single-photon detector arrays for visualization and rapid characterization of events evolving on picosecond time scales. The single-photon sensitivity, temporal resolution, and full-field imaging capability enables the observation of light-in-flight in air, as well as the measurement of laser-induced plasma formation and dynamics in its natural environment. The extreme sensitivity and short acquisition times pave the way for real-time imaging of ultrafast processes or visualization and tracking of objects hidden from view. </p>",,--Choose Location,2016-12-05 00:17:02.706,True,2015-01-01,Single-Photon Sensitive Ultrafast Imaging,PUBLIC,,True,Camera Culture,False
beyond-the-self-driving-car,raskar,False,<p>This concept gallery shows the chain of startups and ideas that will follow after the emergence of self-driving cars.</p>,,--Choose Location,2016-12-05 00:16:14.777,True,2016-01-01,Beyond the Self-Driving Car ,PUBLIC,,True,Camera Culture,False
nashik-smart-citizen-collaboration-with-tcs,raskar,False,"<p>We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: What billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, Big Data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.</p>",,--Choose Location,2016-12-05 00:17:18.564,True,2016-01-01,Nashik Smart Citizen Collaboration with TCS,PUBLIC,,True,Camera Culture,False
the-next-30-years-of-vr,raskar,False,"<p>In this visual brainstorming, we present the next 30 years of VR in a set of concept designs.</p>",,--Choose Location,2016-12-05 00:16:55.169,True,2016-01-01,The Next 30 Years of VR ,PUBLIC,,True,Camera Culture,False
streetscore,raskar,False,"<p>StreetScore is a machine learning algorithm that predicts the perceived safety of a streetscape. StreetScore was trained using 2,920 images of streetscapes from New York and Boston and their rankings for perceived safety obtained from a crowdsourced survey. To predict an image's score, StreetScore decomposes this image into features and assigns the image a score based on the associations between features and scores learned from the training dataset. We use StreetScore to create a collection of map visualizations of perceived safety of street views from cities in the United States. StreetScore allows us to scale up the evaluation of streetscapes by several orders of magnitude when compared to a crowdsourced survey. StreetScore can empower research groups working on connecting urban perception with social and economic outcomes by providing high-resolution data on urban perception.</p>",,--Choose Location,2016-12-05 00:17:03.647,True,2014-01-01,StreetScore,PUBLIC,,True,Camera Culture,False
skin-perfusion-photography,raskar,False,"<p>Skin and tissue perfusion measurements are important parameters for diagnosis of wounds and burns, and for monitoring plastic and reconstructive surgeries. In this project, we use a standard camera and a laser source in order to image blood-flow speed in skin tissue. We show results of blood-flow maps of hands, arms, and fingers. We combine the complex scattering of laser light from blood with computational techniques found in computer science.</p>",,--Choose Location,2016-12-14 20:46:59.692,True,2014-09-01,Skin Perfusion Photography,PUBLIC,,True,Camera Culture,False
time-of-flight-microwave-camera,raskar,False,"<p>Our architecture takes a hybrid approach to microwaves and treats them like waves of light. Most other work places antennas in a 2D arrangement to directly sample the RF reflections that return. Instead of placing antennas in a 2D arrangment, we use a single, passive, parabolic reflector (dish) as a lens. Think of every point on that dish as an antenna with a fixed phase-offset. This means that the lens acts as a fixed set of 2D antennas which are very dense and spaced across a large aperture. We then sample the focal-plane of that lens. This architecture makes it possible for us to capture higher resolution images at a lower cost.</p>",,--Choose Location,2016-12-05 00:16:55.816,True,2014-09-01,Time-of-Flight Microwave Camera,PUBLIC,,True,Camera Culture,False
terahertz-time-gated-spectroscopic-imaging-for-content-extraction-through-layered-structures,raskar,False,"Terahertz time-domain spectroscopy (THz-TDS) is a leading method for inspection in the frequency range of 0.1- 10 THz. In contrast to IR-based time-of-flight cameras, coherent techniques, and X-ray, THz-TDS provides both fine time resolution and broadband spectral information. We exploit both of these to extract occluding content from layers whose thicknesses are wavelength comparable. The method uses the statistics of the THz E-field at subwavelength gaps to lock into each layer position and then uses a time-gated spectral kurtosis to tune to highest spectral contrast of the content on that specific layer. To demonstrate, occluding textual content was successfully extracted from a sample similar to a closed book down to 9 pages without human supervision. The study impacts inspection of structural defects in wooden objects, plastic components, composites; drugs, and specially cultural artifacts with subwavelength or wavelength comparable layers.",,--Choose Location,2016-12-05 00:17:23.179,True,2014-09-01,Terahertz Time-gated Spectroscopic Imaging for Content Extraction through Layered Structures,LAB,,True,Camera Culture,False
ultrasound-tomography,raskar,False,"<p>Traditional medical ultrasound assumes that we are imaging ideal liquids. We are interested in imaging muscle and bone as well as measuring elastic properties of tissues, all of which are places where this assumption fails quite miserably. Interested in cancer detections, Duchenne muscular dystrophy, and prosthetic fitting, we use tomographic techniques as well as ideas from seismic imaging to deal with these issues.</p>",,--Choose Location,2016-12-05 00:17:24.963,True,2014-01-01,Ultrasound Tomography,PUBLIC,,True,Camera Culture,False
imaging-behind-diffusive-layers,raskar,False,"<h2><span style=""font-weight: normal;"">Locating and classifying florescent tags behind turbid layers using time-resovled inversion&nbsp;</span></h2><p>Using time resolved and sparse optimization framework to locate and classify fluorescent markers hidden behind turbid layer:&nbsp;<span style=""font-size: 18px; font-weight: normal;"">The use of fluorescent probes and the recovery of their lifetimes allow for significant advances in many imaging systems, in particular medical imaging systems. Here, we propose and experimentally demonstrate reconstructing the locations and lifetimes of fluorescent markers hidden behind a turbid layer. This opens the door to various applications for non-invasive diagnosis, analysis, flowmetry, and inspection. The method is based on a time-resolved measurement which captures information about both fluorescence lifetime and spatial position of the probes. To reconstruct the scene, the method relies on a sparse optimization framework to invert time-resolved measurements. This wide-angle technique does not rely on coherence, and does not require the probes to be directly in line of sight of the camera, making it potentially suitable for long-range imaging.</span></p><p>More details:<br>http://web.media.mit.edu/~guysatat/project_scattering.html&nbsp;<br>http://web.media.mit.edu/~guysatat/fl/<br></p>",,--Choose Location,2017-04-05 01:48:57.383,True,2015-01-01,Imaging Behind Diffusive Layers,PUBLIC,,True,Camera Culture,False
imaging-with-all-photons,raskar,False,"<h2><span style=""font-weight: normal;"">How to see through tissue</span></h2><p>We demonstrate a new method to image through scattering materials like tissue and fog. The demonstration includes imaging an object hidden behind 1.5cm of tissue; it's like imaging through the palm of a hand. Our optical method is based on measuring and using all photons in the signal (as opposed to traditional methods, which use only part of the signal). Specifically, we use a time-resolved method that allows us to distinguish between photons that travel different paths in the tissue. Combining this unique measurement process with novel algorithms allows us to recover the hidden objects. This technique can be used in biomedical imaging, as well as imaging through fog and clouds.</p>",,--Choose Location,2017-04-05 01:49:57.109,True,2015-09-01,Imaging with All Photons,PUBLIC,,True,Camera Culture,False
streetchange,raskar,False,"<p>Computer vision uncovers predictors of physical urban change
                    
                </p>",,,2017-07-07 18:51:23.160,True,2015-06-01,Streetchange,PUBLIC,,True,Camera Culture,False
aneye-extending-the-reach-of-anterior-segment-ophthalmic-imaging,raskar,False,"<p>Eye exams via a slit lamp are critical in early diagnosis of diseases such as cataracts, corneal injury, and pterygia, in order to avert vision loss. The slit lamp is one of the most versatile tools in an ophthalmologist's clinic, but is big, expensive, and is designed with specialized ophthalmic clinics in mind. AnEye is a suite of portable, computationally driven solutions that leverage modern optics and commercially available consumer electronics to extend the reach of examinations of the anterior segment of the eye well beyond large hospitals and clinics, into resource-constrained settings such as rural mass-screening camps, mobile ophthalmology clinics, and even primary care.</p>",,--Choose Location,2019-04-19 17:46:56.865,True,2015-09-01,AnEye: Extending the reach of anterior segment ophthalmic imaging,PUBLIC,,True,Camera Culture,False
blind-and-reference-free-fluorescence-lifetime-estimation-via-consumer-time-of-flight-sensors,raskar,False,"<p>Fluorescence lifetime imaging is a significant bio-imaging tool that finds important applications in life-sciences. Widely known applications include cancer detection and DNA sequencing. To that end, fluorescence microscopy which is at the heart of bio-imaging is an electronically and optically sophisticated device which is prohibitively expensive. Our work is demonstrates the fluorescence microscopy like functionality can be achieved by a simple, consumer sensor such as the Microsoft Kinect which costs about $100. This is done by trading-off the precision in optics and electronics for sophistication in computational methods. Not only this allows for massive cost reduction but leads to several advances in the area. For example, our method is calibration-free in that we do not assume sample's relative placement with respect to the sensor. Furthermore, our work opens new pathways of interaction between bio-imaging, optics and computer vision communities.</p>",,--Choose Location,2019-04-19 17:48:42.345,True,2015-01-01,Blind and reference-free fluorescence lifetime estimation via consumer time-of-flight sensors,PUBLIC,,True,Camera Culture,False
imaging-without-a-lens-and-only-a-few-pixels,raskar,False,"<h2>Lensless imaging with compressive ultrafast sensing</h2><p>Traditional cameras require a lens and a mega-pixel sensor to capture images. The lens focuses light from the scene onto the sensor. We demonstrate a new imaging method that is lensless and requires only a single pixel for imaging. Compared to previous single pixel cameras our system allows significantly faster and more efficient acquisition. This is achieved by using ultrafast time-resolved measurement with compressive sensing. The time-resolved sensing adds information to the measurement, thus fewer measurements are needed and the acquisition is faster. Lensless and single pixel imaging computationally resolves major constraints in imaging systems design. Notable applications include imaging in challenging parts of the spectrum (like infrared and THz), and in challenging environments where using a lens is problematic.
                    
                </p>",,,2019-04-19 17:50:37.233,True,2015-01-01,Efficient lensless imaging with a femto-pixel,PUBLIC,http://web.media.mit.edu/~guysatat/,True,Camera Culture,False
seeing-through-fog,raskar,False,"<h2>Seeing through dense, dynamic, and heterogeneous fog conditions. The technique, based on visible light, uses hardware that is similar to LIDAR to recover the target depth and reflectance.</h2><p>The system relies on ultrafast measurements, used to computationally remove inclement weather conditions such as fog, and produce a photo and depth map as if the fog weren’t there (with contrast improved by 6.5x in dense fog conditions).&nbsp;&nbsp;<br></p><h2>Applications</h2><ul><li>Autonomous and augmented driving in challenging weather.</li><li>Airplanes and helicopters take off, landing and low level flight in dense fog conditions.</li><li>Trains traveling at normal speeds during inclement weather conditions.</li></ul>",,,2018-08-22 17:18:20.368,True,2017-03-01,Seeing Through Realistic Fog,PUBLIC,http://web.media.mit.edu/~guysatat/,True,Camera Culture,False
thermal-nlos-imaging,raskar,False,"<h2><b>Seeing around corners with a thermal camera. Our technique exploits unique properties of long-wave IR surface reflectance to see around corners.</b></h2><p>Seeing around corners is challenging in the visible spectrum as photons scatter at diffuse surfaces. However, the surface reflectance of common materials in the long-wave IR spectrum has a strong specular component, making it easier for us to see around corners. Furthermore, any heat sources are long-wave IR sources while common objects are not visible light source. We exploit these to recover 2D shape and 3D location of objects around corners with completely passive sensing.<br></p><p><b>Key Idea</b></p><p>Long-wave IR has much stronger specular surface reflectance on common surfaces than visible light. Any heat sources act as light sources in the long-wave IR spectrum, and their emission can be estimated if the temperature is known. We exploit these two properties of long-wave IR to see around corners. We first estimate the bidirectional reflectance distribution function (BRDF) of the wall at a corner. The estimated BRDF gives the mapping between the hidden object's long-wave IR emission and the measurements of a thermal camera. Using this mapping, our technique recovers the 2D shape and the 3D location of the hidden heat sources around corners. This method is completely passive and does not require occlusion geometries that other passive techniques require.&nbsp;</p>",,,2019-04-18 13:40:06.043,True,2018-10-01,Seeing around corners with thermal imaging,LAB-INSIDERS,,True,Camera Culture,False
health-tech-innovations-with-tata-trusts-mumbai,raskar,False,"<p>We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: what billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, big data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.</p>",,--Choose Location,2019-04-19 17:52:03.224,True,2016-01-01,"Health-tech innovations with Tata Trusts, Mumbai",PUBLIC,,True,Camera Culture,False
high-frequency-lidar-using-beat-notes,raskar,False,"<p>Time of Flight 3D cameras like the Microsoft Kinect are prevalent in computer vision and computer graphics. In such devices, the power of an integrated laser is amplitude modulated at megahertz (MHz) frequencies and demodulated using a specialized imaging sensor to obtain sub-cm range precision. To use a similar architecture and obtain micron range precision, this paper incorporates beat notes. To bring telecommunications ideas to correlation ToF imaging, we study a form of ""cascaded Time of Flight"" that uses a Hertz-scale intermediate frequency to encode high-frequency pathlength information. We show synthetically and experimentally that a bulk implementation of opto-electronic mixers offers: (a) robustness to environmental vibrations; (b) programmability; and (c) stability in frequency tones. A fiberoptic prototype is constructed, which demonstrates three micron range precision over a range of two meters. A key contribution of this paper is to study and evaluate the proposed architecture for use in machine vision.<br></p>",,,2019-04-19 17:53:06.677,True,2017-12-21,High-frequency LIDAR using beat notes,PUBLIC,,True,Camera Culture,False
hyderabad-eye-health-collaboration-with-lvp,raskar,False,"<p>We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: What billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, big data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.</p>",,--Choose Location,2019-04-19 17:54:21.753,True,2016-01-01,Hyderabad eye health collaboration with LVP,PUBLIC,,True,Camera Culture,False
identi-wheez-a-device-for-in-home-diagnosis-of-asthma,raskar,False,"<p>Asthma is the most common chronic illness among children. The skills required to diagnose it make it an even greater concern. Our solution is a child-friendly wearable device that allows in-home diagnosis of asthma. The device acquires simultaneous measurements from multiple stethoscopes. The recordings are then sent to a specialist who uses assistive diagnosis algorithms that enable auscultation (listening to lung sounds with a stethoscope). Sound refocusing algorithms enable the specialist to listen to any location in the lungs. The specialist also has access to a sound ""heat map"" that shows the location of sound sources in the lungs.</p>",,--Choose Location,2019-04-19 18:25:32.160,True,2016-01-01,Identi-Wheez: A device for in-home diagnosis of asthma,PUBLIC,,True,Camera Culture,False
unbounded-high-dynamic-range-photography-using-a-modulo-camera,raskar,False,"<p>We present a novel framework to extend the dynamic range of images called Unbounded High Dynamic Range (UHDR) photography with a modulo camera. A modulo camera could theoretically take unbounded radiance levels by keeping only the least significant bits. We show that with limited bit depth, very high radiance levels can be recovered from a single modulus image with our newly proposed unwrapping algorithm for natural images. We can also obtain an HDR image with details equally well preserved for all radiance levels by merging the least number of modulus images. Synthetic experiments and experiments with a real modulo camera show the effectiveness of the proposed approach.</p>",,--Choose Location,2019-04-19 18:38:01.471,True,2014-09-01,Unbounded high dynamic range photography using a modulo camera,PUBLIC,,True,Camera Culture,False
ajdass-untitled-project,raskar,False,"<p>A smartphone based spectrometer design that is standalone and supported on a wireless platform. The device is low-cost and the power consumption is minimal making it portable to perform a range of studies in the field. Essential components of the device like the light source, spectrometer, filters, microcontroller and wireless circuits have been assembled in a housing that fits into a pocket and the entire device weighs 48 g. The device has a dedicated app on the smartphone to communicate, receive, plot and analyze spectral data. Validations of the device were carried out by demonstrating non-destructive ripeness testing in fruits. Ultra-Violet fluorescence from Chlorophyll present in the skin was measured across various apple varieties during the ripening process and correlated with destructive firmness tests. This demonstration is a step towards possible consumer, bio-sensing and diagnostic applications that can be carried out in a rapid manner.</p>",,--Choose Location,2017-03-31 23:01:55.075,True,2015-01-01,Smartphone spectrometer for food sensing,PUBLIC,http://www.mit.edu/~ajdas,True,Camera Culture,False
towards-in-vivo-biopsy,raskar,False,"<p>A new method to detect and distinguish between different types of fluorescent materials. The suggested technique has provided a dramatically larger depth range compared to previous methods; thus it enables medical diagnosis of body tissues without removing the tissue from the body, which is the current medical standard. It uses fluorescent probes, which are commonly used in medical diagnosis. One of these parameters is the fluorescence lifetime, that is the average time the fluorescence emission lasts. The new method can distinguish between different fluorescence lifetimes, which allows diagnosis of deep tissues. Locating fluorescence probes in the body using this method can, for example, indicate the location of a tumor in deep tissue, and classify it as malignant or benign according to the fluorescence lifetime, thus eliminating the need for X-ray or biopsy.</p>",,--Choose Location,2018-03-29 20:34:54.336,True,2014-09-01,Towards In-Vivo Biopsy,PUBLIC,,True,Camera Culture,False
time-folded-optics,raskar,False,<h2>Rethinking photography optics in the time dimension</h2><p><i>What if we could design optics in time instead of space?</i></p>,,,2018-09-24 18:15:42.266,True,2018-08-01,Time-folded optics,PUBLIC,http://web.media.mit.edu/~barmak/Time-folded.html,True,Camera Culture,False
calibration-invariant-i,raskar,False,"<p><b>Object Classification through Scattering Media&nbsp;with Deep Learning</b></p><p>A method for classifying objects hidden behind a scattering layer with a neural network. Training on synthetic data with variations in calibration parameters allows the network to learn a model that doesn't require calibration during lab experiments.<br></p><p>Traditional techniques to see through scattering media rely on a physical model that needs to be precisely calibrated. Computationally overcoming the scattering relies heavily on accurately calibrated physical models. Thus, such systems are extremely sensitive to a precise and lengthy calibration process. </p><p>In this work we overcome this bottleneck by utilizing neural networks and their ability to learn models that are invariant to data transformation. In our case, the transformations are variations in the imaging system calibration parameters. To that end, we create a synthetic dataset that contains variations in all calibration parameters (we use a Monte Carlo forward model to render the measurements). The system is then tested on actual lab experiments without specific calibration or tuning.</p>",,,2018-10-20 00:40:19.319,True,2016-09-01,Calibration Invariant Imaging,PUBLIC,,True,Camera Culture,False
distributed-learning-and-collaborative-learning-1,raskar,False,"<h1>Split Learning: Distributed deep learning without sharing raw data</h1><p><strong>Abstract:</strong>&nbsp;Can a server utilize deep learning models for training or inference without accessing raw data from clients?&nbsp;Split learning naturally allows for various configurations of cooperating entities to train (and infer from) machine learning models without sharing any raw data or detailed information about the model.&nbsp;</p><p><strong>Key idea:</strong>&nbsp;In the simplest of configurations of split learning, each client (for example, radiology center) trains a partial deep network up to a specific layer known as the cut layer. The outputs at the cut layer are sent to another entity (server/another client) which completes the rest of the training without looking at raw data from any client that holds the raw data. This completes a round of forward propagation without sharing raw data. The gradients are now back propagated again from its last layer until the cut layer in a similar fashion. The gradients at the cut layer (and only these gradients) are sent back to radiology client centers. The rest of back propagation is now completed at the radiology client centers. This process is continued until the distributed split learning network is trained without looking at each others raw data.</p><p><b>Split Learning Papers:</b></p><p><b>1.) Split learning for health: Distributed deep learning without sharing raw patient data, Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, Ramesh Raskar,&nbsp;<a href=""https://arxiv.org/pdf/1812.00564.pdf"">(PDF)</a>&nbsp;(2018)</b></p><p><b>2.) “Distributed learning of deep neural network over multiple agents”, Otkrist Gupta and Ramesh Raskar, In: Journal of Network and Computer Applications 116,&nbsp;<a href=""https://www.sciencedirect.com/science/article/pii/S1084804518301590"">(PDF)</a>&nbsp;(2018)</b></p><p><b>3.) Survey paper: No Peek: A Survey of private distributed deep learning, Praneeth Vepakomma, Tristan Swedish, Ramesh Raskar, Otkrist Gupta, Abhimanyu Dubey,&nbsp;<a href=""https://arxiv.org/pdf/1812.03288.pdf"">(PDF)</a>&nbsp;(2018)</b></p><h2>Split learning’s computational and communication efficiency on clients:</h2><p>Client-side communication costs are significantly reduced as the data to be transmitted is restricted to initial layers of the split learning network (splitNN) prior to the split. The client-side computation costs of learning the weights of the network are also significantly reduced for the same reason. In terms of model performance, the accuracies of Split NN remained competitive to other distributed deep learning methods like federated learning and large batch synchronous SGD with a drastically smaller client side computational burden when training on a larger number of clients as shown below in terms of teraflops of computation and gigabytes of communication when split learning is used to train Resnet and VGG architectures over 100 and 500 clients with CIFAR 10 and CIFAR 100 datasets.</p><p><b>Versatile plug-and-play configurations of split learning</b></p><p>Versatile configurations of split learning configurations cater to various practical settings of&nbsp;i) multiple entities holding different modalities of patient data, ii) centralized and local health entities collaborating on multiple tasks, iii) learning without sharing labels, iv) multi-task split learning, v) multi-hop split learning&nbsp;and other hybrid possibilities to name a few as shown below and further detailed in our paper here&nbsp;<a href=""https://arxiv.org/pdf/1812.00564.pdf"">(PDF)</a>&nbsp;</p><h2>News stories about this work</h2><p><strong><a href=""https://www.technologyreview.com/the-download/612567/a-new-ai-method-can-train-on-medical-records-without-revealing-patient-data/"">MIT Technology Review</a></strong></p>",,,2018-12-12 20:53:48.275,True,2018-08-27,Distributed and collaborative learning,PUBLIC,,True,Camera Culture,False
eyeglasses-free-displays,gordonw,False,"<p>Millions of people worldwide need glasses or contact lenses to see or read properly. We introduce a computational display technology that predistorts the presented content for an observer, so that the target image is perceived without the need for eyewear. We demonstrate a low-cost prototype that can correct myopia, hyperopia, astigmatism, and even higher-order aberrations that are difficult to correct with glasses.</p>",2016-09-01,--Choose Location,2016-12-05 00:16:24.496,True,2014-01-01,Eyeglasses-Free Displays,PUBLIC,,False,Camera Culture,False
eyeglasses-free-displays,naik,False,"<p>Millions of people worldwide need glasses or contact lenses to see or read properly. We introduce a computational display technology that predistorts the presented content for an observer, so that the target image is perceived without the need for eyewear. We demonstrate a low-cost prototype that can correct myopia, hyperopia, astigmatism, and even higher-order aberrations that are difficult to correct with glasses.</p>",2016-09-01,--Choose Location,2016-12-05 00:16:24.496,True,2014-01-01,Eyeglasses-Free Displays,PUBLIC,,False,Camera Culture,False
streetscore,naik,False,"<p>StreetScore is a machine learning algorithm that predicts the perceived safety of a streetscape. StreetScore was trained using 2,920 images of streetscapes from New York and Boston and their rankings for perceived safety obtained from a crowdsourced survey. To predict an image's score, StreetScore decomposes this image into features and assigns the image a score based on the associations between features and scores learned from the training dataset. We use StreetScore to create a collection of map visualizations of perceived safety of street views from cities in the United States. StreetScore allows us to scale up the evaluation of streetscapes by several orders of magnitude when compared to a crowdsourced survey. StreetScore can empower research groups working on connecting urban perception with social and economic outcomes by providing high-resolution data on urban perception.</p>",,--Choose Location,2016-12-05 00:17:03.647,True,2014-01-01,StreetScore,PUBLIC,,True,Camera Culture,False
time-of-flight-microwave-camera,naik,False,"<p>Our architecture takes a hybrid approach to microwaves and treats them like waves of light. Most other work places antennas in a 2D arrangement to directly sample the RF reflections that return. Instead of placing antennas in a 2D arrangment, we use a single, passive, parabolic reflector (dish) as a lens. Think of every point on that dish as an antenna with a fixed phase-offset. This means that the lens acts as a fixed set of 2D antennas which are very dense and spaced across a large aperture. We then sample the focal-plane of that lens. This architecture makes it possible for us to capture higher resolution images at a lower cost.</p>",,--Choose Location,2016-12-05 00:16:55.816,True,2014-09-01,Time-of-Flight Microwave Camera,PUBLIC,,True,Camera Culture,False
ultrasound-tomography,naik,False,"<p>Traditional medical ultrasound assumes that we are imaging ideal liquids. We are interested in imaging muscle and bone as well as measuring elastic properties of tissues, all of which are places where this assumption fails quite miserably. Interested in cancer detections, Duchenne muscular dystrophy, and prosthetic fitting, we use tomographic techniques as well as ideas from seismic imaging to deal with these issues.</p>",,--Choose Location,2016-12-05 00:17:24.963,True,2014-01-01,Ultrasound Tomography,PUBLIC,,True,Camera Culture,False
streetchange,naik,False,"<p>Computer vision uncovers predictors of physical urban change
                    
                </p>",,,2017-07-07 18:51:23.160,True,2015-06-01,Streetchange,PUBLIC,,True,Camera Culture,False
identi-wheez-a-device-for-in-home-diagnosis-of-asthma,naik,False,"<p>Asthma is the most common chronic illness among children. The skills required to diagnose it make it an even greater concern. Our solution is a child-friendly wearable device that allows in-home diagnosis of asthma. The device acquires simultaneous measurements from multiple stethoscopes. The recordings are then sent to a specialist who uses assistive diagnosis algorithms that enable auscultation (listening to lung sounds with a stethoscope). Sound refocusing algorithms enable the specialist to listen to any location in the lungs. The specialist also has access to a sound ""heat map"" that shows the location of sound sources in the lungs.</p>",,--Choose Location,2019-04-19 18:25:32.160,True,2016-01-01,Identi-Wheez: A device for in-home diagnosis of asthma,PUBLIC,,True,Camera Culture,False
unbounded-high-dynamic-range-photography-using-a-modulo-camera,naik,False,"<p>We present a novel framework to extend the dynamic range of images called Unbounded High Dynamic Range (UHDR) photography with a modulo camera. A modulo camera could theoretically take unbounded radiance levels by keeping only the least significant bits. We show that with limited bit depth, very high radiance levels can be recovered from a single modulus image with our newly proposed unwrapping algorithm for natural images. We can also obtain an HDR image with details equally well preserved for all radiance levels by merging the least number of modulus images. Synthetic experiments and experiments with a real modulo camera show the effectiveness of the proposed approach.</p>",,--Choose Location,2019-04-19 18:38:01.471,True,2014-09-01,Unbounded high dynamic range photography using a modulo camera,PUBLIC,,True,Camera Culture,False
glance,vdiep,False,"<p>We address two critical elements of news: that it informs, and that it is trustworthy. Glance creates dynamic, real-time, semantic control over news presentation that reveals the inherent slant that underlies coverage of an event. The goal is to empower readers to understand their news intake through a visualization of metadata that empowers readers to choose their news source based on computed metrics rather than sensationalized headlines. Relevant additional information, such as sentiment of text and public reaction, is gathered on each topic to further give readers a richer news-scape.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:26.560,True,2014-09-01,Glance,PUBLIC,,False,Viral Communications,False
media-matrix,vdiep,False,"<p>We present two scalable ways to explore and distribute media in all forms: video, text, and graphics; published and conversational. The first presentation has been demonstrated as an interactive, dynamic time/source array where one can see the pulse of publication and suggest media for friends. A revision organizes content as 3D �stacks� that correspond to people and topics. The Matrix dissolves media silos and types and assembles it in a data- and socially driven way. �Glue� is the engine that drives assembly.</p>",2014-09-01,--Choose Location,2016-12-05 00:17:17.472,True,2014-01-01,Media Matrix,PUBLIC,,False,Viral Communications,False
glance,lip,False,"<p>We address two critical elements of news: that it informs, and that it is trustworthy. Glance creates dynamic, real-time, semantic control over news presentation that reveals the inherent slant that underlies coverage of an event. The goal is to empower readers to understand their news intake through a visualization of metadata that empowers readers to choose their news source based on computed metrics rather than sensationalized headlines. Relevant additional information, such as sentiment of text and public reaction, is gathered on each topic to further give readers a richer news-scape.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:26.560,True,2014-09-01,Glance,PUBLIC,,False,Viral Communications,False
glyph,lip,False,"<p>Wearable devices and ambient displays need to atomize video to evocative excerpts as glanceable as a still image. Glyph is a web-based tool for generating expressive GIFs from video. The tool integrates scene detection, video stabilization, video manipulation, and loop detection into a simple, web-based authoring interface. Glyph allows for creating GIFs from video with more editorial control than just choosing a clip's start and end time�diminishing some regions of movement in the clip, and highlighting others; erasing a jarring jump between the start and end of the GIF; imbuing a still image with just enough dynamism to hold our eyes and pique our interest. The result is a subtle, dynamic moving image that's lightweight, transmissible, and immediately engaging.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:26.820,True,2014-09-01,Glyph,PUBLIC,,False,Viral Communications,False
helios,lip,False,<p>Helios provides an automatic way of socializing one's video interactions. It is a Chrome browser plug-in that records user's encounters with embedded videos on the web. This data is contributed to a group collection so that one can readily see what is trending among friends and where the outliers are. In addition the data is processed by Glue for metadata tagging. </p>,2015-01-01,--Choose Location,2016-12-05 00:16:28.961,True,2014-01-01,Helios,PUBLIC,,False,Viral Communications,False
plethora,lip,False,"<p>Plethora creates simultaneous, localized, personal broadcasting networks that allow audiences to form on-the-fly and build their own media streams. The display ranges from personal devices such as phones, to embedded screens in kitchens. It uses Bluetooth LE beacons to emulate local broadcast stations that signal the proximity. Plethora inverts the relationship between mobility and the multiplicity of screens with which we interact on a moment-to-moment basis. It also considers media as migrating away from larger, high definition screens to a universe of personal, anytime, anywhere representations.</p>",2016-01-01,--Choose Location,2016-12-05 00:16:44.818,True,2014-09-01,Plethora,PUBLIC,,False,Viral Communications,False
quantify,lip,False,"<p>QUANTIFY is a generalized framework and JavaScript library to allow rapid multi-dimensional ""measurement"" of subjective qualities of media. The goal is to make qualitative metrics quantized. For everything from measuring emotional responses of content to the cultural importance of world landmarks, QUANTIFY helps to elicit the raw human subjectivity that fills much of our lives, and makes it programmatically actionable. </p>",2015-09-01,--Choose Location,2016-12-05 00:16:46.569,True,2014-01-01,QUANTIFY,PUBLIC,,False,Viral Communications,False
recap,lip,False,"<p>Recap uses Glue to automatically create video casts, allowing users to specify how much time they have available and then intelligently filling that chunk of time with the top trending stories of the day.</p>",2014-09-01,--Choose Location,2016-12-05 00:16:47.091,True,2014-01-01,Recap,PUBLIC,,False,Viral Communications,False
newsclouds,lip,False,"<p>NewsClouds presents a visual exploration of how the news reporting of an event evolves over time. Each ""cloud"" represents a publication and each competing news organization usually emphasizes different aspects of that same story. Using the time sliders, that evolution becomes evident. In addition, each word or phrase can be expanded to show its links and context. We are building an archive of events associated with ongoing US election developments.</p>",2018-06-06,--Choose Location,2018-10-09 01:51:14.404,True,2014-09-01,NewsClouds,PUBLIC,,False,Viral Communications,False
super-cut-notes,lip,False,"<p>A large portion of popular media is remixed: existing media content is spliced and re-ordered in a manner that serves a specific narrative. Super Cut Notes is a semi-comical content remix tool that allows a user to splice and combine the smallest bits of media: words. By tapping into the dataset of our group's <a href=""https://www-prod.media.mit.edu/projects/superglue/overview/"">SuperGlue platform</a>, it has access to a huge dictionary of words created by SuperGlue's transcription module. Users are able to input a text of any length, choose video-bits of individual words that match their text, and create a video of their combination—in the style of cut-and-pasted ransom notes.</p>",2017-06-06,--Choose Location,2018-10-09 01:52:24.513,True,2016-09-01,Super Cut Notes,PUBLIC,,False,Viral Communications,False
worldlens,lip,False,"<p>World Lens informs users about newsworthy events that are both popular and obscure. It is a front page that is both navigable and scalable, allowing one to discover as well as track ongoing events. We array in-depth news information across a large multitouch display organized by time, coverage, and geography. Elements are drawn from blogs, the web, newspapers, magazines, and television. Each is presented by a front page that tells the literal story. Readers can fly through the news space, mark items for interest, and activate each. News data is gathered and analyzed by our Glue system, which generates frame-by-frame metadata for video and page analysis for other online material.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:59.435,True,2014-01-01,WorldLens,PUBLIC,,False,Viral Communications,False
internet-as-an-object,lip,False,"<p>In Cuba, the Internet has another name, and it fits in the palm of your hand. It is a hard drive called “El Paquete Semanal” (the Weekly Package)—a collection of one terabyte of information: shows, movies, music, PDFs, downloaded onto hard drives and distributed door-to-door and operated by its users. In the rest of the world we have Facebook.  For the past thirty years, social networks have demonstrated their power and usefulness to link us together and place communications in everyone’s hands.  At the same time, they have matured from being operated by the people who use them (e.g., Cuba, the WELL, BBSes), to large-scale commercial organizations operated on those users’ behalf (e.g., Facebook).  As Nicholas Negroponte said: in the early days of media, the users were the inventors—now they are two separate classes.&nbsp;</p><p>The central question addressed in this thesis is whether the design and underlying technology of entry points to a network change the way people interact with it and the experience they have. To explore this question, we designed and engineered a set of playful physical objects which function as nodes of a hyper-local network. Information bestowed upon this network remains within these nodes, cryptographically secure, and accessible only to local community members who are aware of the network's existence and mode of operation. This network was tested by deploying the node-objects in four real world locations, where participants could leave and retrieve audio messages from and to the nodes.</p>",2019-05-08,,2019-05-13 13:46:04.394,True,2018-10-15,Topophonia,LAB-INSIDERS,https://kalli-retzepi.com/,False,Viral Communications,False
radio-days,lip,False,"<p>Following the 2016 election, the entirety of the nation became conscious of its polarization. According to a study by the National Bureau of Economic Research*, polarization has increased among Americans since 1990. The study observes, however, that in eight of the nine measures of polarization, older individuals (70+ age group) show higher rates of increase in polarization than other age groups. This age group also utilizes social media less than other age-groups. Could it be that social media is not the root cause of polarization?</p><p>In order to explore this further, we looked at polarization through talk radio, which is commonly thought to have political influence.&nbsp;</p>",2018-12-31,,2019-04-11 15:26:19.908,True,2018-10-12,Radio Days,PUBLIC,,False,Viral Communications,False
solar-micro-mining,lip,False,"<p>Bitcoin generates net-new value from ""mining"" in a distributed network. In this work, we explore solar micro-mining rigs that transform excess energy capacity from renewable energy (hard to trade) into money (fungible). Each rig runs a small Bitcoin miner and produces Bitcoin dust for micropayments. We envision these micro-miners populating a highly distributed network, across rooftops, billboards, and other outdoor spaces. Where systematic or environmental restrictions limit the ability to freely trade the underlying commodity, micro-mining produces new economic viability. Renewable energy-based, micropayment mining systems can broaden financial inclusion in the Bitcoin network, particularly among populations that need a currency for temporary store of value and must rely on flexible electricity off the grid (e.g., unbanked populations in the developing world). This exploration seeds a longer-term goal to enable open access to digital currency via account-free infrastructure for the public good.</p>",2017-06-06,--Choose Location,2018-10-08 01:32:51.238,True,2015-09-01,Solar Micro-Mining,PUBLIC,,False,Viral Communications,False
watch-people-watch,lip,False,"<p>Recording your reaction to a short video is becoming the new gossip; famous watchers get as many as 750,000 views. We attempt to transform this utterly useless and talentless event into a socially constructive alternative to simultaneous, synchronized, group viewing. Any user can opt in to be recorded and added to the shared, collective viewing experience. No talent or skills required.</p>",2017-06-30,--Choose Location,2018-10-09 18:03:44.000,True,2016-01-01,Watch People Watch,PUBLIC,,False,Viral Communications,False
votomosaic,lip,False,"<p>A tapestry where each pixel represents a pledge by an individual to vote. Anyone can participate and watch the growing and changing image that emerges as others agree to vote. The tapestry evolves and will encourage repeated attention. This tests local reinforcement to support actions.</p><p>Follow this <a href=""https://votomosaic.media.mit.edu/"">link</a> for more information.</p>",2018-12-31,,2019-04-18 15:05:48.332,True,2018-08-01,votoMosaic,PUBLIC,,False,Viral Communications,False
8k-time-into-space,lip,False,"<p>8K Time into Space is a user interface for a video exploration system with an 8K display. 8K is an ultra high-definition video system and it can present a huge amount of visual content on one display. In our system, video thumbnails with shifted playback time in chronological order are spaced out like tiles. The time range of a scene that a viewer wants to check can be adjusted with a touch interface, and resolution of the thumbnails is changed depending of the range. 8K Time into Space aims to provide responsive and intuitive experiences for video consumption.</p>",2018-12-31,--Choose Location,2019-04-17 22:34:57.493,True,2015-09-01,8K Time into Space,PUBLIC,,False,Viral Communications,False
election-arg,lip,False,"<p>Only 40% of the eligible population votes in the typical US midterm election, and among young people turnout is even lower. In this experiment, we develop a game that encourages people to influence their friends to physically go to the polls. The system is reminiscent of <a href=""https://www.media.mit.edu/projects/fiftynifty/overview/"">Fifty Nifty</a>, where people competed to amass points by both calling representatives and spreading the message to others. In addition to awarding points, vote.lol aims to motivate players by allowing them influence over the outcome of a shared narrative that develops in real-time before (and during) the election. Interactive stories with real-world game mechanics are characteristic of alternate reality games (ARGs), which have received scholarly attention for their potential to instigate viral communications among players who self-organize to solve complex problems. The purpose of this study is to test whether ARG techniques can motivate gamers to solve the intractable problem of getting their peers to vote.</p>",2018-12-31,,2019-04-18 00:52:49.802,True,2018-10-01,Election ARG,PUBLIC,,False,Viral Communications,False
perspectives,lip,False,"<p>The news is probably one of the first things people check in the morning, but how much does what you know and understand about the world depend on your news source? Will you view the world differently if you head over to CNN instead of BBC?</p><p>“Perspectives” presents top news stories from many points of view. The viewer can easily see the different perspectives and get the whole story.</p>",2018-06-06,,2018-10-17 23:15:04.211,True,2016-08-01,Perspectives,PUBLIC,,False,Viral Communications,False
media-matrix,lip,False,"<p>We present two scalable ways to explore and distribute media in all forms: video, text, and graphics; published and conversational. The first presentation has been demonstrated as an interactive, dynamic time/source array where one can see the pulse of publication and suggest media for friends. A revision organizes content as 3D �stacks� that correspond to people and topics. The Matrix dissolves media silos and types and assembles it in a data- and socially driven way. �Glue� is the engine that drives assembly.</p>",2014-09-01,--Choose Location,2016-12-05 00:17:17.472,True,2014-01-01,Media Matrix,PUBLIC,,False,Viral Communications,False
invisible-ink,lip,False,"<p>Invisible Ink is a certified mail application that demonstrates the utility of the blockchain for maintaining a public ledger of transactions while keeping the content of those transactions private. In this case, the idea is a method for guaranteeing the delivery, receipt and existence of email messages. It archives the transaction in the Bitcoin blockchain and uses secure off-chain storage for the other details. Invisible Ink demonstrates the extensibility of this distributed technology for contracts, audits, and recovery of sensitive information. It is an evolution of work begun as the Ethos project. Since Bitcoin has shown that a distributed system of trust can be workable for irreversibly storing time-stamped information, these extensions and applications are potentially important for a wide variety of cases from finance to personal information.</p>",2015-01-01,--Choose Location,2017-06-25 16:35:11.855,True,2015-01-01,Invisible Ink,PUBLIC,,False,Viral Communications,False
ethos,lip,False,"<p>Ethos is a decentralized, Bitcoin-like network for storing and sharing valuable information. We provide transparency, control, and ownership over personal data and its distribution. Validation and maintenance is distributed throughout the data community and automatically maintained without needing a safe deposit box or a commercial site. What Bitcoin has done for currency and BitTorrent for media, Ethos does for personal data. Nodes in the network are incentivized by collecting transaction fees, coinbase transactions (""finding blocks""), and proof-of-storage fees to sustain the distribution of personal data. Fees are paid with the underlying cryptocurrency represented by the network, also known as ""PrivacyCoin."" The role of nodes, besides the usual proof-of-work, which protects against ""double spending,"" is to maintain shredded pieces of information and present them to the network on-demand. </p>",2015-09-01,--Choose Location,2017-06-25 16:32:52.708,True,2014-01-01,Ethos,PUBLIC,,False,Viral Communications,False
panorama,lip,False,"<p>An interface for smashing filter bubbles,&nbsp;<a href=""http://panorama.um-dokku.media.mit.edu/"">Panorama</a> is built to allow open, transparent, and collaborative exploration of news from all across the political map. It presents different perspectives and encourages serendipity in news exploration, versus getting all of our news from one single source.&nbsp;Panorama is a human-in-the-loop interface.&nbsp;The computer processes more than 10,000 news stories each day, both broadcast and written, and it uses machine learning algorithms to decide  what topics each story is talking about and if the stories are positive, subjective, or trending.&nbsp;The machine learning process pours over massive datasets and learns to generalize in smart ways, but not in the same smart ways that humans generalize. As a result, it can be brilliant and also get very confused. With Panorama, some of the training data was a large open set of movie reviews, and while this is a great dataset to start with, it is not mapped so well to news stories.&nbsp;As humans interact with Panorama, they are encouraged to give better labels to stories; those labels are fed back into the algorithm to make it better.</p><p>Having a lot of information about each news story and all stories together allows us to create an open-box news aggregator. With most aggregators we use today (like the Facebook News feed), the user has no idea what are the algorithms and filters that decide what s/he will see. Panorama is open: the user can decide to view everything, or filter only to specific things that he s/he is interested in, by playing with the sliders and seeing in real time how the news feed changes accordingly.&nbsp;For example, you could easily get all stories about animals, from the right side of the political map, that are also positive and objective.&nbsp;Panorama also exposes interesting patterns, such as the topics that different news sources focus on every day, and what sources had many objective versus subjective stories.</p>",2018-06-06,,2018-10-17 23:55:32.429,True,2016-10-01,Panorama,PUBLIC,,False,Viral Communications,False
as-you-need-it,lip,False,"<p>Video or broadcast news is viewed in a far wider set of circumstances than it ever has been before. It is composed with the assumption of a complete, situated viewing, but in fact it is often grabbed on-the-fly as a momentary experience. As You Need It is a semantic summarizer that deconstructs a multi-part segment for presentation as ""chunks of importance."" We are learning if a story can be cut down to a useful update that takes less time than a traffic light, or as much time as a given user has. This project uses and contributes to another group project, SuperGlue.</p>",,--Choose Location,2016-12-05 00:16:49.848,True,2016-01-01,As You Need It,PUBLIC,,True,Viral Communications,False
dbdb,lip,False,"<p>DbDb (pronounced DubDub) is a collaborative, visually based analysis and simulation platform. We promote open distribution of experimental data by allowing researchers to present a graphical representation of their data and processing techniques that collaborators can build on and augment. This helps test the reproducibility of results and allows others to learn and apply their own techniques. Our intention is for the research community as a whole to benefit from a growing body of open, analytical techniques. DbDb provides an interface for archiving data, executing code, and visualizing a tree of forked analyses. It is part of the Viral initiative on open, author-driven publishing, collaboration, and analysis. It is intended to be linked to PubPub, the main project.</p>",,--Choose Location,2016-12-05 00:16:20.776,True,2015-01-01,DbDb,PUBLIC,,True,Viral Communications,False
nuestra-vista,lip,False,,,,2019-04-17 19:53:30.162,False,2018-01-01,Nuestra Vista,PUBLIC,http://nv.mit.edu,True,Viral Communications,False
this-is-how,lip,False,"<p>This Is How is a platform for connecting makers with small businesses through stories. Small businesses share their stories in the form of video bytes in which they explain what they do and why, what their requirements and constraints are, and what kinds of issues they have. Makers can then annotate the video, ask further questions, and propose solutions for issues. The video is passed through SuperGlue for annotation and to categorize and find commonalities among requests.</p>",,--Choose Location,2016-12-05 00:16:55.625,True,2016-09-01,This Is How,PUBLIC,,True,Viral Communications,False
boycott,lip,False,"<p>A web browser extension that reveals less well-known aspects of corporate public behavior such as environmental respect and political bias. When one engages in a search, we place an image next to the link to a corporate site that graphically reveals relevant information. It might be a donkey versus an elephant, or a measure of ""greenness.""&nbsp; We seed the system with public information and allow users to contribute to the database.&nbsp;</p><p>Data collected can be further explored and mapped out using data visualizations, allowing&nbsp; perception of network distributions and polarizations.</p>",2020-01-01,,2019-04-18 01:20:40.082,True,2018-10-01,Boycott!,PUBLIC,,True,Viral Communications,False
plusnudge,lip,False,"<p>+nudge helps people to become their imagined future self one&nbsp;nudge&nbsp;at a time.</p><p>Distracted by all of the demands on our time from urgent notifications, reminders, and advertising on our phones and laptops, it is increasingly challenging to align our day to day actions with what we believe matters most inside. +nudge&nbsp;creates specifically tuned, subtle reminders throughout your day to reflect and be mindful of the things in life that really matter to you, and as a consequence assist you in making better, more holistic decisions.</p>",,,2019-04-19 16:09:59.016,True,2019-01-01,+nudge,PUBLIC,,True,Viral Communications,False
civic-link,lip,False,"<p>CivicLink is an online organization in a box. A leader or moderator plugs it in, invites members, and the tools needed to build community action are in place. It is grassroots mobilization recursed to the lower level: we envision extremely large networks of extremely local, single-issue orgs. Core elements are an events calendar and forum for each event. The link is a private server that retains all communications and personal information within it; there is no contribution to an online cloud. The architecture is extensible to add features such as mapping, canvassing, etc. It is designed to be used where groups physically meet and for access via a smartphone app.</p><p>Related work tests whether privacy is important to users, whether games can be used to promote actions, how web sites can be distributed offline via QR codes, and how this link can merge culturally unique resonances.</p>",,,2019-04-18 01:23:05.324,True,2018-09-04,CivicLink,PUBLIC,,True,Viral Communications,False
captions,lip,False,"<p>Modern web presentations such as Youtube feature videos with commentary appended at the bottom. In our new imagining of <b>Videotext,</b> we put the two together: comments appear as active bubbles along the playback time line. We thereby associate the commentary with the place in the video to which it refers. It gains context. This project is in the early test stage and is presented for discussion and further development in summer 2016.</p>",,--Choose Location,2016-12-05 00:17:03.626,True,2016-01-01,Captions++,PUBLIC,,True,Viral Communications,False
ultimate-media,lip,False,"<p>We concentrate on media that informs, unifies, and defines society such as news, sports, and public events. These are characterized by simultaneity, synchronicity, immersion, multiple perspectives, and new ways of framing discussions as stories. We design structures that allow for civic participation in media creation and distribution yet retain the editorial imperative to create a shared reality based on trust and truth.</p><p>Our current emphasis is on validating broadcast television news and creating applications that make it easier to broaden ones views than it is to remain in a bubble.&nbsp; This is based on ""Unspoken News,"" an AI-driven engine for revealing cues such as emotions, peripheral text, set layout and other non-literal influencers. This extends our fully functional video recorder and analysis engine, Superglue. Related work also provides for adding confidence indications to messages that one passes on.</p>",,,2019-04-18 01:24:17.283,True,2018-10-01,Theme | Ultimate Media,PUBLIC,,True,Viral Communications,False
iot-recorder,lip,False,"<p>The physical world is increasingly coming online. We have things that measure, sense, and broadcast to the rest of the world. We call this the Internet of Things (IoT). But our cameras are blind to this new layer of metadata on reality. The IoT recorder is a camera that understands what IoT devices it sees and what data they are streaming, thus creating a rich information ""caption-track"" for the videos it records. Using this meta-data, we intend to explore how this enables new video applications, starting with cooking.</p>",,--Choose Location,2016-12-05 00:16:32.661,True,2015-01-01,IoT Recorder,PUBLIC,,True,Viral Communications,False
gifgif,lip,False,"<p>An animated GIF is a magical thing. It has the power to compactly convey emotion, empathy, and context in a subtle way that text or emoticons often miss. GIFGIF is a project to combine that magic with quantitative methods. Our goal is to create a tool that lets people explore the world of GIFs by the emotions they evoke, rather than by manually entered tags. A web site with 200,000 users maps the GIFs to an emotion space and lets you peruse them interactively.</p>",,--Choose Location,2016-12-14 14:00:48.619,True,2014-01-01,GIFGIF,PUBLIC,,True,Viral Communications,False
layer,lip,False,"<p><b>layer</b> decentralizes recommendation systems and intersects third-party recommendations with your locally stored, personal information to result in privacy-respecting, relevant recommendations.</p><p>We envision growing the repertoire of personal information beyond purchase choices.</p>",,,2019-04-18 17:13:41.858,True,2019-01-01,layer,PUBLIC,,True,Viral Communications,False
what-s-america-listening-to,lip,False,,,,2018-04-21 00:41:22.039,True,2017-06-01,What's America Listening To?,PUBLIC,,True,Viral Communications,False
viralcasting,lip,False,"<p>Ditch the truck. Live, collaborative broadcasting through mixed reality.</p>",,,2019-04-18 01:30:20.552,True,2016-09-06,Broadercasting,PUBLIC,,True,Viral Communications,False
pubpub,lip,False,"<p>PubPub reinvents publication to align with the way the web was designed: collaborative, evolving, and open. PubPub uses a graphical format that is deliberately simple and allows illustrations and text that are programs as well as static PDFs. The intention is to create an author-driven,  distributed alternative to academic journals that is tuned to the dynamic nature of many of our modern experiments and discoveries. It is optimized for public discussion and academic journals, and is being used for both.  It is equally useful for a newsroom to develop a story that is intended for both print and online distribution.</p>",,--Choose Location,2018-05-31 18:16:31.248,True,2015-01-01,PubPub,PUBLIC,,True,Viral Communications,False
fiftynifty,lip,False,"<p>This is a grassroots challenge to get friends to participate in democracy by making calls to congresspeople in all 50 states. Live phone calls are the best way to directly express your opinion on an issue to your elected officials. Your mission is to pass message this along to friends who will make calls and also pass the message/link along to others who will do the same. It's a social chain letter and a call to action for a better participatory democracy. &nbsp;<span style=""font-size: 18px; font-weight: normal;"">We help you make your call and you pass on an invitation for your friends to do the same. Your invite can stress your opinion on a given issue.&nbsp;</span></p><p>The winners are the first ten chains to reach 50 states and accumulate the most challenge points. You get 250 points for making a call, 125 points for a call that your friend makes, 65 points for the call their friend makes, on and on. Everyone on the chain earns points. Points count for your first call to each of your two senators and your representative. You get a bonus for a ""grand slam""—a network that reaches all 435 representatives and 100 senators.</p><p>There is a leaderboard and a network view so you can track how you are doing. You can also see how much of the country your chain is covering.</p>",,,2019-06-04 20:46:21.258,True,2017-02-13,FiftyNifty,PUBLIC,https://fiftynifty.org,True,Viral Communications,False
let-s-see-a-game,lip,False,"<p><b>In ""Let's see a game!"" we&nbsp;expose the different perspectives in&nbsp;TV&nbsp;sports and news in order to build broadcasting systems that unify rather than divide. We use the galvanizing impact of sports and live events as a forum, and then we add production and viewing opportunities to distinguish fact from opinion and to challenge the basis of those opinions.</b></p><p>In 1951, when the Dartmouth football team played against Princeton, there was deep disagreement between the two schools as to what had happened during the game. In ""They Saw a Game: A Case Study,"" the psychologists Albert Hastorf and Hadley Cantril found that when the <i>same </i>motion picture of the game was shown to a sample of undergraduates at each school,&nbsp; each individual <i>perceived a different game</i>, and their versions of the game was just as ""real"" as other versions were to other people.&nbsp;</p><p><b>However,&nbsp; little is known about whether and how broadcasting media are adding fuel to the fire. In order to study the relationship between storytelling/perspectives and opinion formation, we built the following two applications: ""Let's see a game!"" and ""Let's watch news!""</b></p><p>In the first step, we built an interactive application&nbsp;that exposes different perspectives in sports broadcasting.&nbsp;The application plays two broadcasts of the same game, created for each team's home audience.&nbsp;The user can tune into an audio channel by moving the slider. Additional buttons allow the user to take other actions.</p>",,,2019-04-18 14:04:57.014,True,2017-09-11,Let's see a game!,PUBLIC,,True,Viral Communications,False
decentralized-systems,lip,False,"<p>Here we include a suite of projects that migrate information to personal devices and systems instead of centralizes silos. In part this take you personal device seriously—it is more than a window into the cloud—it can store your information for your purposes.</p><p>Blockchains have spawned new ways of looking at security, trust, and consensus. These are now design variables that allow diverse communities to develop networks with permanence&nbsp; and agreement that have no central authority. We explore trust as a variable, building blockchain-based systems that separate transactions and currency from the utility of a shared, uneditable ledger.</p>",,,2019-04-18 02:42:59.862,True,2018-10-01,Theme | Decentralized Systems,PUBLIC,,True,Viral Communications,False
enlightened,lip,False,"<p>Media filter bubbles sacrifice shared reality amongst US citizens. We aim to burst these echo chambers by presenting short, automatically summarized news clips to users through a mobile app. The user watches these short clips in sequence and has the option to press a lightbulb that signifies whether the news segment have been enlightening. These clips are generated from SuperGlue metadata and is based on&nbsp;<a href=""https://www.media.mit.edu/projects/news-2/overview/"">News*2</a>. It uses an “Anti-recommender system” that actively expands the user’s horizon—contrary to traditional recommender systems that aim to thicken the walls of echo chambers.</p><p>Follow this <a href=""https://viral.pubpub.org/pub/enlightened/"">link</a> for more information.</p>",,,2019-04-18 01:36:42.924,True,2019-01-01,Enlightened: Broaden Your Views,PUBLIC,,True,Viral Communications,False
superglue,lip,False,"<p>SuperGlue is a core news research initiative that is a ""digestion system"" and metadata generator for mass media. An evolving set of analysis modules annotate 14 DirecTV live news broadcast channels as well as web pages and tweets. The video is archived and synchronized with the analysis. Currently, the system provides named-entity extraction, audio expression markers, face detectors, scene/edit point locators, excitement trackers, and thumbnail summarization. We use this to organize material for presentation, analysis, and summarization. SuperGlue supports other news-related experiments.</p><p>SuperGlue is a framework for media digestion and metadata generation. The digestion work flow also has applications for media more broadly including conversational ecommerce.</p>",,--Choose Location,2019-04-08 16:48:12.894,True,2014-09-01,SuperGlue,PUBLIC,,True,Viral Communications,False
newssense,lip,False,"<p>How something is presented can be as important as the message itself. We use various artificial intelligence techniques to model the ""subcarriers of information"" present in a TV newscast, to automatically detect and understand visual and auditory cues beyond the spoken word including the layout of the set, the affect of the participants, the nature of the motion, and other cues. This would enable a broad-range, comprehensive analysis of <b>how news presentation is trying to shape the public debate</b>. Insights in this area are of vital importance in the age of political polarization and lead directly to applications that can break our bubbles.</p><p>See:&nbsp; viral.media.mit.edu/pub/tbd</p>",,,2019-04-18 16:50:14.749,False,2018-02-05,Unspoken News,PUBLIC,,True,Viral Communications,False
holonews,lip,False,"<p>&nbsp;News reporting today suffers from sensationalism. News agencies are constantly fighting for attention and clicks, leading to headlines and photos that exaggerate a single perspective.&nbsp;</p><p>What if you could get a full perspective on certain news topics by exploring the news in AR? The spatial nature of AR allows a user to gain a more complete perspective on a story.&nbsp; Having a constant holographic widget on your desk also allows you to follow developing stories, such as a foreign conflict. In addition, the interactive nature of AR means that users can explore the news in a delightful way.</p>",,,2018-10-22 16:52:05.905,True,2017-09-28,Holonews,PUBLIC,,True,Viral Communications,False
defacto,lip,False,"<p>Despite recent efforts, current fact-checking organizations cannot keep up with the amount of information that is produced and spread throughout the internet [1]. One of the biggest challenges is to minimize the time it takes to verify the claims of a story. We are building a decentralized, crowd-sourced news verification system that aims to leverage the ""wisdom of crowds"" to generate timely labels that can be used to put a badge on news articles. The labels are stored on <a href=""https://ipfs.io/"">IPFS</a> using <a href=""https://underlay.mit.edu/"">The Underlay</a>—a protocol developed by the Knowledge Futures Group at MIT. Governance mechanisms and incentive structures are implemented to hold all parties accountable and to prevent unbalanced concentration of power.</p><p>Follow this&nbsp;<a href=""https://viral.pubpub.org/pub/defacto/"">link</a>&nbsp;for more information.</p><p>[1] <a href=""https://www.washingtonpost.com/news/fact-checker/wp/2018/06/25/rapidly-expanding-fact-checking-movement-faces-growing-pains/?utm_term=.069157def6eb"">https://www.washingtonpost.com/news/fact-checker/wp/2018/06/25/rapidly-expanding-fact-checking-movement-faces-growing-pains/?utm_term=.069157def6eb</a>.</p>",,,2019-04-18 14:34:43.198,True,2019-02-01,Defacto: Decentralized crowdsourced news verification system,PUBLIC,,True,Viral Communications,False
unspoken-news,lip,False,"<p>How something is presented can be as important as the message itself.</p><p>In the age of political polarization and election meddling, it is of vital importance to understand which factors contribute to the formation of public opinions. Television is one of the main sources of information for a large portion of the general population.</p><p>In order to understand discrepancies in news perception, what they are caused by and which implications they might have on shaping public political debate, we first need to understand how television news are constructed and define presentational aspects of the news. Given that, we can build tools that analyze news consumption by the public.</p><p>We aim to use various artificial intelligence techniques to model the ""subcarriers of information"" present in a TV newscast, to automatically detect and understand visual and auditory cues beyond the spoken word including the layout of the set, the affect of the participants, the nature of the motion, and other cues. Our goal is to develop an algorithmic understanding of journalistic choices in the way news content is presented. We also attempt develop an understanding of higher-level characteristics of television news such as television set atmosphere or political bias. This altogether would enable a broad-range, comprehensive algorithmic analysis of <b>how news presentation is trying to shape the public political debate</b>.</p><p><a href=""https://viral.media.mit.edu/pub/unspoken"">Project updates</a> via PubPub</p>",,,2019-04-18 16:51:12.445,True,2018-11-01,Unspoken News,PUBLIC,,True,Viral Communications,False
youtune,lip,False,"<p>&nbsp;Seemingly an old TV box which turns out to offer an abundance of control over content selection: each aspect of news can be selected for using old-style controls. The main goal is to make people aware of their choices and biases and to make them curious to tune the controls and to watch something different. Hitting the TV resets the controls to random values and lets user discover completely new unexpected content.&nbsp;</p><p>The main underlying assumption behind the idea of YouTune is that while it is very difficult to influence people’s opinions and change their beliefs explicitly without a willingness on their part to change these beliefs, we could attempt to influence people implicitly by providing a tool that, first, would make users aware of the choices they take: to watch what they want or are used to, the user has to explicitly set controls. Unlike traditional broadcast and online media platforms, YouTune does not allow users to select content based on channel or program, so in order to find content that the person is usually interested in, with YouTune they would need to manually select characteristics of the content they want to watch. And second, we believe that YouTune will make people curious to change the control values they are used to and therefore to discover unexpected content that could potentially influence their opinions in an implicit way.&nbsp;</p><p>With YouTune the user can choose between different characteristics and tune into some value of the chosen characteristic to see short news stories. The idea is to make statistics experienceable without showing graphs: a person using YouTune would subconsciously aggregate statistical patterns and understand which tricks and stylistic tools channels tend to use to cover certain topics.</p><p><a href=""https://viral.media.mit.edu/pub/youtune"">Project updates</a> via PubPub</p>",,,2019-04-18 16:51:54.465,True,2019-01-01,YouTune,PUBLIC,,True,Viral Communications,False
self-advertising,lip,False,"<p>Self-Advertising&nbsp;reclaims and repurposes attention from&nbsp;advertising&nbsp;media online. As we use the Internet, we often feel advertising following us around, distracting our focus, and leading us down misaligned paths, or we block it out entirely. Here we explore repurposing the advertising space online to be aligned with our goals and desires.</p>",,,2019-06-04 19:35:02.989,True,2019-01-01,Self-Advertising,PUBLIC,,True,Viral Communications,False
viral-political-action,lip,False,"<p>Friends don’t let friends vote alone. 2018 featured a suite of grassroots mobilization ideas for voting; 2019 generalizes that theme for civic organizations at any scale or number.</p><p>We construct bottom-up ways to get people civically engaged. The idea is independent, culturally and community-based ways to reinforce participation. We replace centrally radiated messaging and mobilization with local, iterative, and potentially orthogonal cues that diffuse through a population. The core project is CivicLink, a, private, local org-in-a-box, owned by a moderator who mediates member participation and networking with other similar orgs.&nbsp; Related work tests and validates the design.</p>",,,2019-04-19 16:05:13.425,True,2018-10-01,Theme | Viral Civic Action,PUBLIC,,True,Viral Communications,False
qr-sites,lip,False,"<p>QR Websites serves websites without the need for an Internet connection. The approach uses a minified site, encoded as a Data URI on a QR code, to enable a smooth interaction: scan the QR code, and open the URI in the address bar of any browser. Applications that involve sending data, such as webforms or quizzes, work by using a response formatting engine to send the data as SMS messages. Using this approach, developers can create forms, quizzes, educational tools, games, and demonstrations without a connection to the Internet, all while using what people likely have in their pockets: a mobile phone with a browser, and any QR code scanning app.&nbsp;</p>",2019-09-30,,2019-04-17 14:21:29.384,True,2019-04-01,QR Sites,PUBLIC,,True,Viral Communications,False
youtune-1,lip,False,"<p>A web app that is a television with topical tuning rather than channel tuning. You can select the subject, the slant, and the emotional intensity of the people and the captions. Our question is whether these controls will broaden people's perspective and help create a common framework with which we can discuss our attitudes, sentiments, and positions on public issues. This project is a visualization of the infrastructure of Unspoken News and Superglue. These are an evolving resource for broadcast news understanding.</p>",,,2019-04-18 16:50:41.771,False,2018-10-15,YouTune,LAB-INSIDERS,,True,Viral Communications,False
search-intent-optimization,lip,False,"<p>Search Intent Optimization unwinds SEO by externalizing intent. Users select their intent during a search’s initiation. We offer a set of example controls to distinguish between shopping and information access. Adding your intent suppresses irrelevant noise results—no more ""I'm feeling lucky.""</p>",,,2019-04-18 17:12:22.057,True,2019-01-01,Search Intent Optimization,PUBLIC,,True,Viral Communications,False
news-graph,lip,False,"<p>This project aims to show a different picture of the data behind the news, looking at how we analyze, represent, and interact with it.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">Video content is constantly created and added to the public archives, but there is never time to watch it all. News Graph explores a new method for interacting with news media.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">By analyzing the words that are said, extracting entities that appear, and finding the connections between them, we are able to map connections between video segments. Each connection represents two entities that were mentioned in the same video segment, and a video segment can be mapped to a number of connections.</span></p>",2019-09-30,,2019-04-18 01:15:48.619,True,2016-04-01,News Graph,PUBLIC,http://news-graph.um-dokku.media.mit.edu,True,Viral Communications,False
wall-of-now,lip,False,"<p>Wall of Now is a multi-dimensional media browser of recent news items. It attempts to address our need to know everything by presenting a deliberately overwhelming amount of media, while simplifying the categorization of the content into single entities. Every column in the wall represents a different type of entity: people, countries, states, companies, and organizations. Each column contains the top-trending stories of that type in the last 24 hours. Pressing on an entity will reveal a stream of video that relates to that specific entity. The Wall of Now is a single-view experience that challenges previous perceptions of screen space utilization towards a future of extremely large, high-resolution displays.</p>",,--Choose Location,2019-04-05 20:29:14.403,True,2015-01-01,Wall of Now,PUBLIC,,True,Viral Communications,False
relativoty,lip,False,"<p>Many people think their vote doesn't count—that a single vote would not change an election's outcome—and they stay home on Election Day. However, your vote is a public statement of beliefs, amplified because many people in your voting district may agree with you, but do not (and in many cases cannot) vote. Your vote gives a voice to those many others.</p><p>For example, congressional elections affect everyone in the district, regardless of whether they are eligible to vote. The same senators represent everyone in your state. The same ballot questions affect everyone in your community.</p><p>Using data we've gathered from the United States Census Bureau, we have calculated just how many people you're representing with your vote. We directly compare the number of votes in recent congressional elections (by district) to the total population of the district.</p><p>When you vote, you're not only voting for your own best interest, but you're also voting for the best interest of those around you.</p><p>Visit&nbsp;<a href=""https://www.relativotey.org/"" style=""font-size: 18px; font-weight: 400;"">https://www.relativotey.org</a>&nbsp;and<span style=""font-size: 18px; font-weight: 400;"">&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">recognize your relative voting power!</span></p>",,,2019-04-18 01:18:25.012,True,2018-09-04,RelatiVotey,PUBLIC,,True,Viral Communications,False
visible-communities,koehrsen,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Vast regions of the world are unmapped by com­mercial services, and communities living there are digitally invisible. Visible Communities is a system that combines what local people using smartphones see on the ground with what computers can detect from satellite images, to create an interactive map at a fine resolution that continuously improves. The map captures both spatial and social data: houses and the paths connecting them, and the households living there and their relationships.</span><br></p><p>Enabling communities to put themselves on the map is a powerful way to increase their own visibility, and in turn serves institutional needs to improve infrastructure planning and humanitarian aid delivery. Existing approaches to do community-driven mapping either require outside experts to facilitate, or the results are lower-tech and not easy to keep up to date. In collaboration with Partners in Health (PIH), and supported by the MIT Tata Center, we are piloting this social machine in a sparsely populated, hilly region with a Community Health Worker (CHW) network in Burera, Rwanda.</p><p>The smartphone app enables CHWs to self-map their communities. We are intentionally designing an intuitive pre-literacy touch interface, enabling a wide range of users to participate without training. By removing barriers for people at the base of the socio-economic pyramid and designing with social dynamics in mind, we hope to unlock existing, self-motivated human potential.</p>",2017-05-31,--Choose Location,2017-10-16 15:35:47.765,True,2016-01-01,Visible Communities,PUBLIC,,False,Social Machines,True
visible-communities,schaad,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Vast regions of the world are unmapped by com­mercial services, and communities living there are digitally invisible. Visible Communities is a system that combines what local people using smartphones see on the ground with what computers can detect from satellite images, to create an interactive map at a fine resolution that continuously improves. The map captures both spatial and social data: houses and the paths connecting them, and the households living there and their relationships.</span><br></p><p>Enabling communities to put themselves on the map is a powerful way to increase their own visibility, and in turn serves institutional needs to improve infrastructure planning and humanitarian aid delivery. Existing approaches to do community-driven mapping either require outside experts to facilitate, or the results are lower-tech and not easy to keep up to date. In collaboration with Partners in Health (PIH), and supported by the MIT Tata Center, we are piloting this social machine in a sparsely populated, hilly region with a Community Health Worker (CHW) network in Burera, Rwanda.</p><p>The smartphone app enables CHWs to self-map their communities. We are intentionally designing an intuitive pre-literacy touch interface, enabling a wide range of users to participate without training. By removing barriers for people at the base of the socio-economic pyramid and designing with social dynamics in mind, we hope to unlock existing, self-motivated human potential.</p>",2017-05-31,--Choose Location,2017-10-16 15:35:47.765,True,2016-01-01,Visible Communities,PUBLIC,,False,Social Machines,False
the-electome-measuring-responsiveness-in-the-2016-election,schaad,False,"<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “<a href=""http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/"">Horse Race of Ideas</a>”).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf"">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/""> share of conversation</a> or coverage that <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/"">any given issue </a>or candidate commands on Twitter and in the news media, respectively—and how the two platforms are aligned<br></li><li>which issues are <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/"">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of “<a href=""http://fusion.net/story/304384/election-incivility-on-twitter/"">incivility</a>” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=""http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/""> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM’s deployment of Electome analytics has been <a href=""http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will"">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets—including the <a href=""https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/"">Washington Post</a>, <a href=""http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps"">Bloomberg News</a>, <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf"">CNN Politics</a> and Fusion—as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=""http://www.debates.org/"">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates’ moderators and credentialed journalists<br></li><li>also collaborated with the <a href=""http://ropercenter.cornell.edu/"">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center’s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>",,--Choose Location,2019-04-19 18:40:58.879,True,2015-09-01,The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,True,Social Machines,False
deepview-computational-tools-for-chess-spectatorship,gregab,False,"<p>Competitive chess is an exciting spectator sport. It is fast-paced, dynamic, and deeply psychological. Unfortunately, most of the game's drama is only visible to spectators who are themselves expert chess players. DeepView seeks to use computational tools to make the drama of high-level chess accessible to novice viewers. There is a long tradition of software trying to beat human players at chess; DeepView takes advantage of algorithmic tools created in the development of advanced chess engines such as Deep Blue, but instead uses them to understand and explain the styles of individual players and the dynamics of a given match. It puts into the hands of chess commentators powerful data science tools that can calculate player position preferences and likely game outcomes, helping commentators to better explain the exciting human story inside every match.</p>",2015-07-01,--Choose Location,2016-12-05 00:16:27.884,True,2014-01-01,DeepView: Computational Tools for Chess Spectatorship,PUBLIC,,False,Playful Systems,False
sneak-a-hybrid-digital-physical-tabletop-game,gregab,False,"<p>Sneak is a hybrid digital tabletop game for two-to-four players about deception, stealth, and social intuition. Each player secretly controls one agent in a procedurally generated supervillain lair. Their mission is to find the secret plans and escape without getting discovered, shot, or poisoned by another player. To accomplish this, players must interact and blend in with a series of computer-controlled henchmen while keeping a close eye on their human opponents for any social cues that might reveal their identity. Sneak introduces a number of systems that are common in video games, but were impractical in tabletop games that did not deeply integrate a smartphone app. These include procedural map generation, NPC pathfinding, dynamic game balancing, and the use of sound.</p>",,--Choose Location,2016-12-05 00:16:11.193,True,2015-01-01,Sneak: A Hybrid Digital-Physical Tabletop Game,PUBLIC,,True,Playful Systems,False
hybrid-objects,heun,False,<p>A web technology-based update of Smarter Objects and the reality editor project.</p>,2015-09-01,--Choose Location,2016-12-05 00:16:30.529,True,2014-01-01,Hybrid Objects,PUBLIC,,False,Fluid Interfaces,False
reality-editor-20,heun,False,"<p>The Reality Editor is a web browser for the physical world: Point your phone or tablet at a physical object and an interface pops up with information about that object as well as services related to that object. The Reality Editor platform is open and entirely based on web standards making it easy for anyone to create Reality Editor enabled objects as well as Reality Editor applications that integrate the physical and digital world in one experience.<br></p><p>Reality Editor version 2.0<br></p><p>&nbsp;<b><a href=""http://realityeditor.org"">Reality Editor &nbsp;version 2.0</a>&nbsp;</b>is now available for download and adds the following features:</p><ul><li><b>World Wide Web</b> conform content creation.<br></li><li><span style=""font-size: 18px;""><b>Spatial Search</b> -&nbsp;</span>Instantly browse through relevant information in the physical world around you. you to browse reality.<br></li><li><span style=""font-size: 18px;""><b>Bi-Directional AR</b> - A real-time interactions system.</span></li><li><b style=""font-size: 18px;"">Private and Decentralized</b><span style=""font-size: 18px;""> infrastructure for connecting the IoT objects.</span><br></li><li><span style=""font-size: 18px;""><b>Logic Crafting</b> - A visual programming language designed for Augmented Reality.<br></span></li></ul><p>The &nbsp;Reality Editor works on iOS and you can get it <a href=""https://itunes.apple.com/us/app/reality-editor/id997820179""><b>here</b></a>.&nbsp;<span style=""font-size: 18px;"">Try</span><span style=""font-size: 18px;"">&nbsp;it out with our <a href=""https://www.dropbox.com/s/3vd1d2v9bmm7e7s/Reality%20Editor.dmg?dl=1""><b>Starter App</b></a> and some Philips Hue Lights or the Lego WeDo 2.0. Learn more about Logic Crafting in our <a href=""http://realityeditor.org/getting-started/""><b>User Interface 101</b></a>.</span></p><p></p>",2017-12-31,,2018-10-11 18:46:45.692,True,2017-05-22,Reality Editor 2.0,PUBLIC,http://www.valentinheun.com,False,Fluid Interfaces,False
open-hybrid,heun,False,"<p>Open Hybrid is an open source augmented reality platform for physical computing and Internet of Things. It is based on the web and Arduino.</p><p>This platform allows you to:</p><ul><li>Create augmented reality content with HTML tools</li><li>Create augmented reality without any knowledge of 3D programming</li><li>Connect the functionality of objects with a simple drag and drop paradigm</li><li>Program your physical hybrid objects and connect them to the AR-UI using Arduino</li></ul><p>Learn more about this project at: <a href=""https://web.archive.org/web/20180314110710/http://www.openhybrid.org/"">http://www.openhybrid.org</a></p>",2017-06-30,--Choose Location,2018-10-20 23:06:14.400,True,2015-01-01,Open Hybrid,PUBLIC,,False,Fluid Interfaces,False
reality-editor,heun,False,"<p>The Reality Editor is a new kind of tool for empowering you to connect and manipulate the functionality of physical objects. Just point the camera of your smartphone at an object and its invisible capabilities will become visible for you to edit. Drag a virtual line from one object to another and create a new relationship between these objects. With this simplicity, you are able to master the entire scope of connected objects.</p>",2017-06-30,--Choose Location,2018-04-30 14:19:37.322,True,2014-01-01,Reality Editor,PUBLIC,http://www.realityeditor.org,False,Fluid Interfaces,False
thursday-morning-screenshare-project-creation,heun,False,,,,2016-10-06 14:24:24.211,False,2016-10-06,Thursday morning screenshare project creation,PUBLIC,,True,Fluid Interfaces,False
pop-roach,sputniko,False,"<p>Facing issues of food crisis by overpopulation, this project explores a possible future where a small community of activists arises to design an edible cockroach that can survive in harsh environments. These genetically modified roaches are designed to pass their genes to the next generations; thus the awful black and brown roaches will be pushed to extinction by the newly designed, cute, colorful, tasty, and highly nutritional ""pop roach."" The color of these ""pop roaches"" corresponds to a different flavor, nutrition, and function, while the original ones remain black or brown, and not recommended to be eaten. How will genetic engineering shift our perception of food and eating habits? Pop Roach explores how we can expand our perception of cuisine to solve some of the world's most pressing problems.</p>",2017-08-31,--Choose Location,2017-10-11 20:31:47.421,True,2015-01-01,Pop Roach,PUBLIC,,False,Design Fiction,False
impossible-baby,sputniko,False,"<p>(Im)possible Baby is a speculative design project that aims to stimulate discussions about the social, cultural, and ethical implications of emerging biotechnologies that could enable same-sex couples to have their own, genetically related children. Delivering a baby from same-sex parents is not a sci-fi dream anymore, due to recent developments in genetics and stem cell research. In this project, the DNA data of a lesbian couple was analyzed using 23andme to simulate and visualize their potential children, and then we created a set of fictional, ""what if"" future family photos using this information to produce a hardcover album which was presented to the couple as a gift. To achieve more public outreach, we worked with the Japanese national television service, NHK, to create a 30-minute documentary film following the whole process, which aired in October 2015.</p>",2017-08-31,--Choose Location,2017-10-11 20:32:40.918,True,2014-01-01,(Im)possible Baby,PUBLIC,,False,Design Fiction,False
crematebot-transform-reborn-free,sputniko,False,"<p>
                    CremateBot is an apparatus that takes in human-body samples, such as fingernails, hair, or dead skin, and turns them into ashes through the cremation process. The process of converting human remains to ashes becomes a critical experience for observers, causing witnesses to question their sense of existence and physical self through the conversion process. CremateBot transforms our physical self and celebrates our rebirth through self-regeneration. The transformation and rebirth open our imagination to go beyond our physical self and cross the span of time. Similar to Theseus' paradox, the dead human cells–which at one point were considered part of our physical selves and helped to define our sense of existence–are continually replaced with newly generated cells. With recent advancements in implants, biomechatronics, and bioengineered organs, how we define ourselves is increasingly blurred.
                </p>",2017-08-31,--Choose Location,2017-10-11 20:33:19.419,True,2014-09-01,"CremateBot: Transform, Reborn, Free",LAB,,False,Design Fiction,False
biome-botany-for-sensorial-memory-2,sputniko,False,"<p>Nothing triggers memories like smell.  Momentary, fleeting, and at times unexpected, one scent can conjure up the warmth of a grandparent, or the heat of a first kiss.   </p><p>Certain botanicals are known for their olfactive properties.  Evolved to seduce pollinators and to proliferate the plant's own genes, the fragrance of flowers have also become entangled in our human dance of seduction.  </p><p>In the art of perfumery, we have long extracted the scents of flowers to apply to ourselves- what if we did the contrary and engineered a plant to emit the odor profile of a person instead?   Could we design new rituals for mourning, new biologies for remembering? </p><p>This project is the speculative design of a plant that smells like a person who is emotionally significant to me, but has passed away.  </p><p>Commercial agendas often drive the progress of certain trajectories of engineering.  This project explores the an alternative design of plants that is not driven nor thoroughly integrated in capitalist production.  Exploring emotions such as loneliness, isolation, and feelings of guilt and anxiety towards human impacts on the environment, the function of these inquiries is to reflect on past, current, and future trajectories of human influences on plant life.</p>",2017-08-31,--Choose Location,2017-10-11 20:31:21.574,True,2016-01-01,Forget Me Not: The Botany of Desire & Loss,PUBLIC,,False,Design Fiction,False
digital-pregnancy-through-domestic-objects,sputniko,False,"<p>
                    Driven female professionals often choose to pursue their careers in lieu of having children. For many of them, strategies of surrogacy or freezing eggs are popular options not only because of available technological advancements, but also because of shifts in cultural perspective enabled by a new biotechnical regime. The dichotomy that forces an ""either/or"" divide between motherhood and career can be seen as a modern form of regulatory control on women. The question of reproduction becomes a matter of our bio-techno-capitalist society as a confine of women's voices and freedom. Companies such as Facebook and Apple have recently offered to pay female employees to freeze their eggs so they can continue with their careers, without interrupting their dreams of having children. However, there still remain many ethical, social, and political dilemmas which exist with surrogacy, questions that must be posed to the public. 
                </p>",2017-08-31,--Choose Location,2017-10-11 20:30:51.528,True,2016-01-01,-,LAB,,False,Design Fiction,False
open-source-estrogen,sputniko,False,"<p>Biomolecules to biopolitics: hormones with institutional biopower! Open Source Estrogen combines do-it-yourself science, body and gender politics, and ethics of hormonal manipulation. The goal of the project is to create an open source protocol for estrogen biosynthesis. The kitchen is a politically charged space, prescribed to women as their proper dwelling, making it the appropriate place to prepare an estrogen synthesis recipe. With recent developments in the field of synthetic biology, the customized kitchen laboratory may be a ubiquitous possibility in the near future. Open-access estrogen would allow women and transgender females to exercise greater control over their bodies by circumventing governments and institutions. We want to ask: What are the biopolitics governing our bodies? More importantly, is it ethical to self-administer self-synthesized hormones?</p>",2017-10-31,--Choose Location,2017-10-11 20:29:44.233,True,2015-09-01,Open Source Estrogen,PUBLIC,http://maggic.ooo/Open-Source-Estrogen-2015,False,Design Fiction,False
nostalgic-touch,sputniko,False,"Nostalgic Touch proposes a new ritual for remembering the deceased in the digital and multicultural age. It is an apparatus that captures hand motions and attempts to replicate the sensation of intimacy or affection by playing back the comforting gestures. It stores gesture data of the people you cared about, then plays them back after they are gone. Similar to rituals in all religions, it gives us a sense of comfort in coping with the death. People in Japan, Singapore, and China live with high standards of technology, but many embrace religious rituals and superstitions as an important part of their wellbeing and decision-making. Nostalgic Touch explores how emerging technologies could be used to enrich the experience of these rituals. How could we augment these rituals to give an even better sense of comfort and intimacy? ",,--Choose Location,2016-12-05 00:16:41.544,True,2015-01-01,Nostalgic Touch,LAB,,True,Design Fiction,False
teshima-8-million-lab,sputniko,False,"<p>Teshima 8 Million Lab is the first Shinto shrine worshipping a genetically engineered life—a silkworm created in Sputniko!'s new work Red Silk of Fate—Tamaki's Crush. In the Shinto religion, ""Yaoyorozu"" (which literally means ""8 Million"") is a word used to describe the myriad of gods believed to reside in almost anything, such as the wind, the ocean, trees, and animals. Conceived by artist Sputniko!, Teshima 8 Million Lab sets out to create new members of Yaoyorozu, forming a mythology from emerging science and art. Far from the big city and located on a site blessed with an abundance of nature, the facility invites the exploration of alternative perspectives on our future of nature and beliefs, as science continues to move forward. </p>",,--Choose Location,2017-04-05 18:32:01.509,True,2016-01-01,Teshima 8 Million Lab,PUBLIC,,True,Design Fiction,False
tranceflora-amys-glowing-silk,sputniko,False,"<p>We collaborated with NIAS (National Institute of Agricultural Science) to genetically engineer silkworms to develop new kinds of silk for future fashion. For an exhibition at Tokyo's Gucci Gallery, we designed a Nishijin-Kimono dress, working with NIAS's glowing silk (created by injecting the genes of a glowing coral and jellyfish into silkworm eggs) .&nbsp;</p>",,--Choose Location,2017-04-05 18:33:46.371,True,2015-01-01,Tranceflora—Amy's Glowing Silk,PUBLIC,http://sputniko.com/2015/04/amyglowingsilk,True,Design Fiction,False
red-silk-of-fate-tamakis-crush,sputniko,False,"<p>Red String of Fate is an East Asian mythology in which gods tie an invisible red string between those that are destined to be together. Sputniko! has collaborated with scientists from NIAS to genetically engineer silkworms to spin this mythical ""Red String of Fate"" by inserting genes that produce oxytocin, a social-bonding ""love"" hormone, and the genes of a red-glowing coral into silkworm eggs. Science has long challenged and demystified the world of mythologies: from Galileo's belief that the Earth revolved around the sun, to Darwin's theory of evolution and beyond—but in the near future, could science be recreating our mythologies? The film unravels a story around the protagonist Tamaki, an aspiring genetic engineer, who engineers her own ""Red Silk of Fate"" in the hopes of winning the heart of her crush, Sachihiko. However, strange, mythical powers start to inhabit her creation....</p>",,--Choose Location,2017-04-05 18:35:25.694,True,2014-01-01,Red Silk of Fate—Tamaki's Crush,PUBLIC,,True,Design Fiction,False
pop-roach,aih,False,"<p>Facing issues of food crisis by overpopulation, this project explores a possible future where a small community of activists arises to design an edible cockroach that can survive in harsh environments. These genetically modified roaches are designed to pass their genes to the next generations; thus the awful black and brown roaches will be pushed to extinction by the newly designed, cute, colorful, tasty, and highly nutritional ""pop roach."" The color of these ""pop roaches"" corresponds to a different flavor, nutrition, and function, while the original ones remain black or brown, and not recommended to be eaten. How will genetic engineering shift our perception of food and eating habits? Pop Roach explores how we can expand our perception of cuisine to solve some of the world's most pressing problems.</p>",2017-08-31,--Choose Location,2017-10-11 20:31:47.421,True,2015-01-01,Pop Roach,PUBLIC,,False,Design Fiction,False
impossible-baby,aih,False,"<p>(Im)possible Baby is a speculative design project that aims to stimulate discussions about the social, cultural, and ethical implications of emerging biotechnologies that could enable same-sex couples to have their own, genetically related children. Delivering a baby from same-sex parents is not a sci-fi dream anymore, due to recent developments in genetics and stem cell research. In this project, the DNA data of a lesbian couple was analyzed using 23andme to simulate and visualize their potential children, and then we created a set of fictional, ""what if"" future family photos using this information to produce a hardcover album which was presented to the couple as a gift. To achieve more public outreach, we worked with the Japanese national television service, NHK, to create a 30-minute documentary film following the whole process, which aired in October 2015.</p>",2017-08-31,--Choose Location,2017-10-11 20:32:40.918,True,2014-01-01,(Im)possible Baby,PUBLIC,,False,Design Fiction,False
crematebot-transform-reborn-free,dkc,False,"<p>
                    CremateBot is an apparatus that takes in human-body samples, such as fingernails, hair, or dead skin, and turns them into ashes through the cremation process. The process of converting human remains to ashes becomes a critical experience for observers, causing witnesses to question their sense of existence and physical self through the conversion process. CremateBot transforms our physical self and celebrates our rebirth through self-regeneration. The transformation and rebirth open our imagination to go beyond our physical self and cross the span of time. Similar to Theseus' paradox, the dead human cells–which at one point were considered part of our physical selves and helped to define our sense of existence–are continually replaced with newly generated cells. With recent advancements in implants, biomechatronics, and bioengineered organs, how we define ourselves is increasingly blurred.
                </p>",2017-08-31,--Choose Location,2017-10-11 20:33:19.419,True,2014-09-01,"CremateBot: Transform, Reborn, Free",LAB,,False,Design Fiction,False
digital-pregnancy-through-domestic-objects,dkc,False,"<p>
                    Driven female professionals often choose to pursue their careers in lieu of having children. For many of them, strategies of surrogacy or freezing eggs are popular options not only because of available technological advancements, but also because of shifts in cultural perspective enabled by a new biotechnical regime. The dichotomy that forces an ""either/or"" divide between motherhood and career can be seen as a modern form of regulatory control on women. The question of reproduction becomes a matter of our bio-techno-capitalist society as a confine of women's voices and freedom. Companies such as Facebook and Apple have recently offered to pay female employees to freeze their eggs so they can continue with their careers, without interrupting their dreams of having children. However, there still remain many ethical, social, and political dilemmas which exist with surrogacy, questions that must be posed to the public. 
                </p>",2017-08-31,--Choose Location,2017-10-11 20:30:51.528,True,2016-01-01,-,LAB,,False,Design Fiction,False
nostalgic-touch,dkc,False,"Nostalgic Touch proposes a new ritual for remembering the deceased in the digital and multicultural age. It is an apparatus that captures hand motions and attempts to replicate the sensation of intimacy or affection by playing back the comforting gestures. It stores gesture data of the people you cared about, then plays them back after they are gone. Similar to rituals in all religions, it gives us a sense of comfort in coping with the death. People in Japan, Singapore, and China live with high standards of technology, but many embrace religious rituals and superstitions as an important part of their wellbeing and decision-making. Nostalgic Touch explores how emerging technologies could be used to enrich the experience of these rituals. How could we augment these rituals to give an even better sense of comfort and intimacy? ",,--Choose Location,2016-12-05 00:16:41.544,True,2015-01-01,Nostalgic Touch,LAB,,True,Design Fiction,False
field-experimentation-in-boston-s-intertidal-zone,devora,False,"<p><b><i>The Intertidal Experimentation Workshop will take place September 29 and 30 (9am to 2pm) at the MIT Media Lab, open to students ages 8-14. To register, please visit <a href=""https://docs.google.com/forms/d/e/1FAIpQLScK6OlnRFis3YNc2ijTHh6QHu_KtDUIGGdQbG3jsVusJrUdEQ/viewform"">this link</a>.&nbsp;</i></b></p><p><i>Field Exploration in Boston's Intertidal Zone </i>is a two-day, hands-on educational workshop for neurodiverse youth in the Greater Boston area, in which participants will use the city of Boston as a classroom, laboratory, and creative playground. Together, scientists, engineers, and artists will take to the field as explorers in order to answer questions related to ecology, biology, chemistry, art, and more.&nbsp;</p><p>Workshop sessions will take place from 9am to 2pm on September 29 and 30, 2018. We will gather at the MIT Media Lab (75 Amherst Street, Cambridge) prior to traveling to field sites located within Greater Boston.&nbsp;</p><p><b>Day I. Introduction to Intertidal Ecology and Experimental Design. <br></b>Day I of this workshop will take place at the MIT Media Lab, where students will learn about the ecology of rocky intertidal zones as well as experimental design. Students will additionally work collaboratively in small groups to develop hypotheses about the phenomena occurring in Boston's urban intertidal zones which they will subsequently test on-site during Day II.&nbsp;</p><p><b>Day II. Data Collection, Interpretation, and Presentation</b>. <br>On Day II, following the field investigation, students will regroup at the Media Lab to get a crash course on data interpretation and visualization. Thereafter, they will present their work to classmates as well as parents and community members through text, graphics, and more.&nbsp;</p><p><i>Participation is free of charge, and all materials and meals are provided.&nbsp;</i><i>For questions, or opportunities for involvement, please contact Avery Normandin (ave@media.mit.edu).</i>&nbsp;</p><p><b>Why Urban Oceans?<br></b>Presently, over 40% of the world’s population lives within 100 kilometers of the coastline, often in seaside megalopolises. While it is known that urban-adjacent marine ecosystems are subjected to unique stressors—namely unparalleled amounts of pollution stemming from urban runoff—efforts related to ocean conservation, as well as marine ecological investigation, most frequently concern the open sea, beyond the immediate reaches of urban ecosystems.</p><p>To better inform regulatory actions related to urban ocean protection, we must understand the unique qualities of these ecological bodies—the seas of cities—particularly as global changes (climate change, rapid urbanization) increase strain on these fragile systems. </p><p>In parallel, given technological-driven paradigmatic shifts in our ability to characterize the unknown world, we are driven to generate innovative and novel platforms for education in the environmental sciences: experiential, instructional excursions which will empower and inspire urban populations to spearhead efforts to sculpt the future of their territories. </p><p>Ideally, these sorts of experiences will cater to all individuals, regardless of gender, race, or cognitive differences.</p><p><b>A Workshop for Neuroinclusivity&nbsp;<br></b>""Citizen science"" (or Open Science) movements have&nbsp;generated robust momentum for allowing communities to delineate the natural world—or speculate on its future—in hands-on and creative ways. As part of a larger effort to cultivate a future generation of environmentally engaged and justice-focused citizen scientists—and in line with the&nbsp;outreach efforts of the Media Lab's&nbsp;<a href=""https://www.media.mit.edu/groups/open-ocean/overview/"">Open Ocean Initiative</a>—we have developed&nbsp;<i>Field Experimentation in Boston's Intertidal Zone:</i>&nbsp;a two-day pilot workshop for Boston-area&nbsp;<a href=""https://medium.com/mit-media-lab/hands-on-ecology-fostering-neuroinclusivity-in-stem-education-c4d8cab61b7"">neurodivergent</a>&nbsp;(e.g., autistic, dyslexic, dyspraxic, ADD, ADHD) youth, in which participants will learn about the ecology of rocky and intertidal systems, develop a hypothesis surrounding these bodies, and subsequently execute a field investigation to test this hypothesis.&nbsp;Students will have the option of approaching fieldwork as a scientist (or engineer), an artist, or a writer (poet, journalist).</p><p>We envision that use of easy-to-access, public sites for the pilot workshop will further democratize the potential to recapitulate similar endeavors in ecological exploration and immersive learning.</p>",2018-12-31,,2019-04-22 17:58:25.679,True,2018-04-02,Field Experimentation in Boston's Intertidal Zone,PUBLIC,,False,Responsive Environments,False
oceancultures,devora,False,"<p><i>Māori</i>&nbsp;have a long and deep connection to their island and ocean ecosystem. The <i>Māori</i> concept of <i>Rahui</i> focuses on traditional methods of ocean protection that long predate marine protected areas.&nbsp;Ocean Cultures hopes to support the younger generation of islanders to understand and monitor their own ocean surroundings at a time when it is critical. &nbsp;</p><p>We hope to do this through a dual set of tools, monitoring through science as well as culture. &nbsp;Monitoring through science will consist of educational hands-on workshops teaching participants fundamental concepts within the marine microbiome,  the invisible but critical foundation of the ocean’s ecosystem, which governs the health, biodiversity, and innumerable processes that occur on our planet. Additional workshops on low cost sensors and remotely operated vehicles (ROVs)&nbsp; may be explored as well. To monitor through culture, we hope to collaborate with <i>Māori</i>&nbsp;<i>kaumātua</i>&nbsp;to teach participants traditional and cultural knowledge about their ocean ecosystems and how best to preserve that though generations.</p>",,,2019-01-25 04:49:18.572,True,2018-04-02,Ocean Cultures,PUBLIC,,True,Responsive Environments,False
reducing-suffering-in-laboratory-animals,devora,False,"<p>The world uses an estimated 20 million mice in laboratory research experiments each year. These experiments are monitored and regulated to protect animal welfare whenever possible. However, analgesics cannot completely eliminate suffering, and many studies cannot use opiates or anti-inflammatory drugs because they would interfere with the biological process being studied. The benefits of animal research may outweigh the cost in animal suffering, but it would be better to perform these experiments without animal suffering. This project seeks to develop strains of mice that experience far less pain and suffering than current animals, but that are equally suited to laboratory and medical research. If successful, widespread adoption of these mice could drastically reduce animal suffering in laboratories worldwide.</p>",,--Choose Location,2017-10-03 18:17:03.994,True,2016-01-01,Reducing Suffering in Laboratory Animals,PUBLIC,,True,Responsive Environments,False
empowered-brains,devora,False,"<h2><b>EEEeb Spring 2019:&nbsp;&nbsp;Urban Oceans</b></h2><p>March 24,&nbsp;April 7 and 21,&nbsp;May 19,&nbsp;June 2&nbsp;<br><span style=""font-size: 18px; font-weight: 400;"">To register, please </span><a href=""https://www.eventbrite.com/e/ecology-evolution-and-engineering-for-empowered-brains-spring-2019-tickets-54681164836"" style=""font-size: 18px; font-weight: 400;"">visit this link</a><span style=""font-size: 18px; font-weight: 400;"">.&nbsp;</span></p><p>Sponsored and run by members of the MIT Media Lab and the <a href=""http://www.empoweredbrain.org/"">Empowered Brain Institute</a>,&nbsp;<i>Ecology, Evolution, and Engineering for Empowered Brains</i> is an eight-week, sensory-friendly series of related educational workshops for neurodiverse individuals (ages 8 - 14) which aims to hone skills in understanding, interpreting, and protecting the natural environment. Through creative, hands-on teaching exercises and field visits, participants become comfortable with basic ecological principles, as well as emerging technologies used to sculpt ecological and evolutionary processes. We discuss contemporary issues related to conservation and highlight engineering strategies with which to address these obstacles. Through project-based learning, students will have the opportunity to develop understanding by experimentation—or play—and workshops will emphasize immersion, rather than memorization.  Wholly, we seek to foster a safe and creative learning space in which students are able to develop the necessary technical literacy to become future leaders in the myriad realms of environmental science.&nbsp;</p><p>For questions, please contact Avery Normandin (ave@media.mit.edu).<br></p>",,,2019-01-10 16:02:03.396,True,2019-01-10,"Ecology, Evolution, and Engineering for Empowered Brains",PUBLIC,,True,Responsive Environments,False
field-experimentation-in-boston-s-intertidal-zone,ave,False,"<p><b><i>The Intertidal Experimentation Workshop will take place September 29 and 30 (9am to 2pm) at the MIT Media Lab, open to students ages 8-14. To register, please visit <a href=""https://docs.google.com/forms/d/e/1FAIpQLScK6OlnRFis3YNc2ijTHh6QHu_KtDUIGGdQbG3jsVusJrUdEQ/viewform"">this link</a>.&nbsp;</i></b></p><p><i>Field Exploration in Boston's Intertidal Zone </i>is a two-day, hands-on educational workshop for neurodiverse youth in the Greater Boston area, in which participants will use the city of Boston as a classroom, laboratory, and creative playground. Together, scientists, engineers, and artists will take to the field as explorers in order to answer questions related to ecology, biology, chemistry, art, and more.&nbsp;</p><p>Workshop sessions will take place from 9am to 2pm on September 29 and 30, 2018. We will gather at the MIT Media Lab (75 Amherst Street, Cambridge) prior to traveling to field sites located within Greater Boston.&nbsp;</p><p><b>Day I. Introduction to Intertidal Ecology and Experimental Design. <br></b>Day I of this workshop will take place at the MIT Media Lab, where students will learn about the ecology of rocky intertidal zones as well as experimental design. Students will additionally work collaboratively in small groups to develop hypotheses about the phenomena occurring in Boston's urban intertidal zones which they will subsequently test on-site during Day II.&nbsp;</p><p><b>Day II. Data Collection, Interpretation, and Presentation</b>. <br>On Day II, following the field investigation, students will regroup at the Media Lab to get a crash course on data interpretation and visualization. Thereafter, they will present their work to classmates as well as parents and community members through text, graphics, and more.&nbsp;</p><p><i>Participation is free of charge, and all materials and meals are provided.&nbsp;</i><i>For questions, or opportunities for involvement, please contact Avery Normandin (ave@media.mit.edu).</i>&nbsp;</p><p><b>Why Urban Oceans?<br></b>Presently, over 40% of the world’s population lives within 100 kilometers of the coastline, often in seaside megalopolises. While it is known that urban-adjacent marine ecosystems are subjected to unique stressors—namely unparalleled amounts of pollution stemming from urban runoff—efforts related to ocean conservation, as well as marine ecological investigation, most frequently concern the open sea, beyond the immediate reaches of urban ecosystems.</p><p>To better inform regulatory actions related to urban ocean protection, we must understand the unique qualities of these ecological bodies—the seas of cities—particularly as global changes (climate change, rapid urbanization) increase strain on these fragile systems. </p><p>In parallel, given technological-driven paradigmatic shifts in our ability to characterize the unknown world, we are driven to generate innovative and novel platforms for education in the environmental sciences: experiential, instructional excursions which will empower and inspire urban populations to spearhead efforts to sculpt the future of their territories. </p><p>Ideally, these sorts of experiences will cater to all individuals, regardless of gender, race, or cognitive differences.</p><p><b>A Workshop for Neuroinclusivity&nbsp;<br></b>""Citizen science"" (or Open Science) movements have&nbsp;generated robust momentum for allowing communities to delineate the natural world—or speculate on its future—in hands-on and creative ways. As part of a larger effort to cultivate a future generation of environmentally engaged and justice-focused citizen scientists—and in line with the&nbsp;outreach efforts of the Media Lab's&nbsp;<a href=""https://www.media.mit.edu/groups/open-ocean/overview/"">Open Ocean Initiative</a>—we have developed&nbsp;<i>Field Experimentation in Boston's Intertidal Zone:</i>&nbsp;a two-day pilot workshop for Boston-area&nbsp;<a href=""https://medium.com/mit-media-lab/hands-on-ecology-fostering-neuroinclusivity-in-stem-education-c4d8cab61b7"">neurodivergent</a>&nbsp;(e.g., autistic, dyslexic, dyspraxic, ADD, ADHD) youth, in which participants will learn about the ecology of rocky and intertidal systems, develop a hypothesis surrounding these bodies, and subsequently execute a field investigation to test this hypothesis.&nbsp;Students will have the option of approaching fieldwork as a scientist (or engineer), an artist, or a writer (poet, journalist).</p><p>We envision that use of easy-to-access, public sites for the pilot workshop will further democratize the potential to recapitulate similar endeavors in ecological exploration and immersive learning.</p>",2018-12-31,,2019-04-22 17:58:25.679,True,2018-04-02,Field Experimentation in Boston's Intertidal Zone,PUBLIC,,False,Initiatives,False
oceancultures,ave,False,"<p><i>Māori</i>&nbsp;have a long and deep connection to their island and ocean ecosystem. The <i>Māori</i> concept of <i>Rahui</i> focuses on traditional methods of ocean protection that long predate marine protected areas.&nbsp;Ocean Cultures hopes to support the younger generation of islanders to understand and monitor their own ocean surroundings at a time when it is critical. &nbsp;</p><p>We hope to do this through a dual set of tools, monitoring through science as well as culture. &nbsp;Monitoring through science will consist of educational hands-on workshops teaching participants fundamental concepts within the marine microbiome,  the invisible but critical foundation of the ocean’s ecosystem, which governs the health, biodiversity, and innumerable processes that occur on our planet. Additional workshops on low cost sensors and remotely operated vehicles (ROVs)&nbsp; may be explored as well. To monitor through culture, we hope to collaborate with <i>Māori</i>&nbsp;<i>kaumātua</i>&nbsp;to teach participants traditional and cultural knowledge about their ocean ecosystems and how best to preserve that though generations.</p>",,,2019-01-25 04:49:18.572,True,2018-04-02,Ocean Cultures,PUBLIC,,True,Initiatives,False
engineering-microbial-ecosystems,ave,False,"<p>We are developing methods of controlling the genetic and cellular composition of microbial communities in the gut. Stably colonized microbes could be engineered to sense disease, resist pathogen invasion, and release appropriate therapeutics in situ.</p><p><br></p>",,--Choose Location,2017-07-11 00:30:42.637,True,2016-01-01,Engineering Microbial Ecosystems,PUBLIC,,True,Initiatives,False
empowered-brains,ave,False,"<h2><b>EEEeb Spring 2019:&nbsp;&nbsp;Urban Oceans</b></h2><p>March 24,&nbsp;April 7 and 21,&nbsp;May 19,&nbsp;June 2&nbsp;<br><span style=""font-size: 18px; font-weight: 400;"">To register, please </span><a href=""https://www.eventbrite.com/e/ecology-evolution-and-engineering-for-empowered-brains-spring-2019-tickets-54681164836"" style=""font-size: 18px; font-weight: 400;"">visit this link</a><span style=""font-size: 18px; font-weight: 400;"">.&nbsp;</span></p><p>Sponsored and run by members of the MIT Media Lab and the <a href=""http://www.empoweredbrain.org/"">Empowered Brain Institute</a>,&nbsp;<i>Ecology, Evolution, and Engineering for Empowered Brains</i> is an eight-week, sensory-friendly series of related educational workshops for neurodiverse individuals (ages 8 - 14) which aims to hone skills in understanding, interpreting, and protecting the natural environment. Through creative, hands-on teaching exercises and field visits, participants become comfortable with basic ecological principles, as well as emerging technologies used to sculpt ecological and evolutionary processes. We discuss contemporary issues related to conservation and highlight engineering strategies with which to address these obstacles. Through project-based learning, students will have the opportunity to develop understanding by experimentation—or play—and workshops will emphasize immersion, rather than memorization.  Wholly, we seek to foster a safe and creative learning space in which students are able to develop the necessary technical literacy to become future leaders in the myriad realms of environmental science.&nbsp;</p><p>For questions, please contact Avery Normandin (ave@media.mit.edu).<br></p>",,,2019-01-10 16:02:03.396,True,2019-01-10,"Ecology, Evolution, and Engineering for Empowered Brains",PUBLIC,,True,Initiatives,False
lego-wayfinder,ave,False,"<p>The LEGO Wayfinder project combines LEGO, robotics, and seawater into a playground of project-based learning and citizen science for budding engineers and explorers. As part of this outreach program, our team has developed a first prototype of a buildable LEGO marine exploration vehicle kit—addressing some of the design challenges of building for the underwater context.</p><p>Our aim is to build an awareness of the state of the aquatic environment and instill a greater responsibility in shaping our interactions with the environment. To do so, young people will view underwater wonders of the world with their robots and get outside to explore their local waterway. Our approach embraces Seymour Papert’s model of ""low floors"" (where getting started is easy), and ""high ceilings,"" where students can pour their time and collaborative work efforts into creative engineering solutions to carry out a marine science experiment of their own design in the field.</p>",,,2019-04-22 17:15:27.304,True,2018-04-01,LEGO Wayfinder,PUBLIC,,True,Initiatives,False
influx,udayan,False,"<p>We present a stiffness-changing interface based on a magneto-rheological (MR) fluid. The device consists of a material&nbsp;surface with electromagnetically&nbsp;induced visco-elasticity,&nbsp;which acts as a proxy for stiffness during tangible interaction with the material. We present several advantages of&nbsp;this enabling technology and outline potential applications&nbsp;and routes for future development.</p>",2015-12-15,,2017-12-07 04:43:01.290,True,2015-11-01,inFlux,PUBLIC,,False,Tangible Media,False
droplet-io,udayan,False,"<p>DropletIO uses aqueous droplets as a form of programmable material for human-material interaction. We demonstrate how fluids in our environment can be &nbsp;programmed droplet-wise to move, merge, and split. Through these operations we can then digitally regulate fluid properties—smell, color, chemical, and biological characteristics. When seamlessly integrated into a range of everyday objects and spaces, droplets become ubiquitous displays and other interactive elements aiding creative activity such as painting, storytelling, and art. Beyond this, programmable droplets have applications in digital biology and chemistry.</p>",2017-04-06,,2017-10-05 21:52:41.593,True,2016-10-01,Droplet IO: Programmable Droplets for Human-material Interaction,LAB-INSIDERS,http://udayan-u.com,False,Tangible Media,False
animastage,udayan,False,"<p>We present AnimaStage: a hands-on animated craft platform based on an actuated stage. Utilizing a pin-based shape display, users can animate their crafts made from various materials. Through this system, we intend to lower the barrier for artists and designers to create actuated objects and to contribute to interaction design using shape-changing interfaces for inter-material interactions.</p><p>We introduce a three-phase design process for AnimaStage with examples of animated crafts. We implemented the system with several control modalities that allow users to manipulate the motion of the crafts so that they could easily explore their desired motion through an iterative process. Dynamic landscapes can also be rendered to complement the animated crafts. We conducted a user study to observe the subject and process by which people make crafts using AnimaStage. We invited participants with different backgrounds to design and create crafts using multiple materials and craft techniques. A variety of outcomes and application spaces were found in this study.</p><p><a href=""http://tangible.media.mit.edu/project/animastage/"">Project Page</a></p>",,,2017-06-21 18:35:35.551,True,2017-06-12,AnimaStage,PUBLIC,http://tangible.media.mit.edu/project/animastage,True,Tangible Media,False
inflatables,udayan,False,"<p>Printflatables is a design and fabrication system for human-scale, functional and dynamic inflatable objects. The user begins with specifying an intended 3D model which is decomposed to two dimensional fabrication geometry. This forms the input for a numerically controlled contact iron that seals layers of thermoplastic fabric. </p><p>In this project, we showcase the system design in detail, the pneumatic primitives that this technique enables and merits of being able to make large, functional and dynamic pneumatic artifacts. We demonstrate the design output through multiple objects which could motivate fabrication of inflatable media and pressure-based interfaces.</p><p><a href=""http://tangible.media.mit.edu/project/printflatables/"">Project Website</a></p>",,,2019-04-17 19:31:11.620,True,2017-05-10,"Printflatables: Printing human-scale, functional, and dynamic inflatable objects",PUBLIC,http://tangible.media.mit.edu/project/printflatables/,True,Tangible Media,False
droplets,udayan,False,"<p>DropletIO proposes aqueous droplets as a programmable material for biology, art, and design. The DropletIO system can actuate and sense macro-scale droplets&nbsp;(nano-liter to micro-liter)&nbsp;on planar surfaces. The system can precisely move, merge, split, oscillate, and change the shape of droplets. We built custom printed circuit boards that integrate actuation and sensing, which act as building blocks for droplet control on devices of various form factors. We show how DropletIO boards can be integrated into a range of tools for biology, everyday objects as ubiquitous information displays and as an interaction medium for art and entertainment.&nbsp;&nbsp;</p><h2>Droplets in Biology&nbsp;&nbsp;</h2><p>Droplet-based microfluidics is extensively used in biology and chemistry.&nbsp;With DropletIO as the core technology, we are building a desktop machine to automate small volume liquid handling. The programmable system is capable of manipulating tiny droplets of biological samples/reagents with precise volume control. Our desktop machine will reduce lab equipment cost, eliminate human errors, and allow for the scaling of complex biological experiments from lab to production with ease. With the machine we want to bring down the cost of running assays from $10,000 to $10 to bring healthcare to billions of people. </p><p>Our solution replaces current liquid handling built on leaky tubes and unreliable pumping mechanisms with a solid-state device. Our digital device is entirely electronic and compact, a system that inexpensively scales to address complex experiments with small volume liquids. Due to its digital nature, a biologist using our system could define biological protocols by programming, executing, and sharing them. Thus, operations would scale digitally from lab to production. The system furthers cost savings by producing significantly less disposable waste such as pipette tips.</p>",,,2018-01-16 21:51:37.104,True,2017-01-01,DropletIO,LAB-INSIDERS,http://udayan-u.com,True,Tangible Media,False
programmable-droplets,udayan,False,"​<p>State-of-the art liquid handling systems are generally pump-driven systems connected with valves and tubes. These systems are manually assembled, expensive, and unreliable. With the growth of the genomic and drug industries, we are moving toward increasingly complex biological processes requiring very small volume liquid manipulation capability. </p><p>Manually assembled mechanical systems do not scale to parallel manipulation of large amounts of small volume liquids. However, the electronics industry has demonstrated how to build robust integrated systems for information manipulation. With this as our motivation, we look toward electronics and integrated circuits to bring miniaturization, complexity, and integration to enable the next generation of biology.</p>",,,2018-10-19 15:44:35.016,True,2016-01-01,Programmable Droplets,PUBLIC,,True,Tangible Media,False
influx,sareen,False,"<p>We present a stiffness-changing interface based on a magneto-rheological (MR) fluid. The device consists of a material&nbsp;surface with electromagnetically&nbsp;induced visco-elasticity,&nbsp;which acts as a proxy for stiffness during tangible interaction with the material. We present several advantages of&nbsp;this enabling technology and outline potential applications&nbsp;and routes for future development.</p>",2015-12-15,,2017-12-07 04:43:01.290,True,2015-11-01,inFlux,PUBLIC,,False,Fluid Interfaces,False
inflatables,sareen,False,"<p>Printflatables is a design and fabrication system for human-scale, functional and dynamic inflatable objects. The user begins with specifying an intended 3D model which is decomposed to two dimensional fabrication geometry. This forms the input for a numerically controlled contact iron that seals layers of thermoplastic fabric. </p><p>In this project, we showcase the system design in detail, the pneumatic primitives that this technique enables and merits of being able to make large, functional and dynamic pneumatic artifacts. We demonstrate the design output through multiple objects which could motivate fabrication of inflatable media and pressure-based interfaces.</p><p><a href=""http://tangible.media.mit.edu/project/printflatables/"">Project Website</a></p>",,,2019-04-17 19:31:11.620,True,2017-05-10,"Printflatables: Printing human-scale, functional, and dynamic inflatable objects",PUBLIC,http://tangible.media.mit.edu/project/printflatables/,True,Fluid Interfaces,False
elowan-a-plant-robot-hybrid,sareen,False,"<p>Elowan is a cybernetic lifeform, a plant in direct dialogue with a machine. Using its own internal electrical signals, the plant is interfaced with a robotic extension that drives it toward light.</p>",,,2018-12-06 19:07:07.895,True,2017-04-01,Elowan: A plant-robot hybrid,PUBLIC,http://harpreetsareen.com,True,Fluid Interfaces,False
cyborg-botany,sareen,False,"<p>Plants can sense the environment, other living entities and regenerate, actuate or grow in response.&nbsp;Our interaction and communication channels with plant organisms in nature are subtle - whether it be looking at their color, orientation, moisture, position of flowers, leaves and such. This subtlety stands in contrast to our interactions with artificial electronic devices&nbsp;that are centered in&nbsp;and around the screens, requiring full attention and induce cognitive load.&nbsp; We envision bringing such interaction out from the screens back into natural world around us.&nbsp;</p><p>Beyond external indicators, plants also have electrochemical signals and response mechanisms inside them that make them very similar to our electronic devices.&nbsp;To tap into such capacities already built in nature, we propose a new convergent view of interaction design. Our goal is to&nbsp;<i>merge and power our electronic functionalities with existing biological functions of living plant</i>s.&nbsp;Through Cyborg Botany, we re-appropriate some of these natural capabilities of plants for our interactive functions.&nbsp;</p>",,,2019-05-09 14:53:38.490,True,2017-01-03,"Cyborg Botany: Augmented plants as sensors, displays, and actuators",PUBLIC,http://harpreetsareen.com,True,Fluid Interfaces,False
argus-sensors-inside-plants,sareen,False,"<p>Argus is a plant with DNA nanosensors inside it that detects lead (Pb2+ Heavy Metal). Irregularities in industrial waste management has lead to depletion of water quality in rivers in many regions. Current processes of monitoring the water quality are not realtime (taking a week or longer) or require special systems that are not off-the-shelf.</p><p>Through Argus, we propose a novel water monitoring method where plants provide an optical readout of lead in water. A DNAZyme is used as sensor assay—double stranded DNA that breaks into single stranded on contact with Pb2+ and binds to single walled carbon nanotubes. This sensor assay can be injected inside the leaf of a plant and stays within the intercellular space.</p>",,,2019-05-20 17:32:10.412,True,2017-06-01,Argus: Water monitoring through nanosensors inside plants,PUBLIC,http://www.harpreetsareen.com,True,Fluid Interfaces,False
floral-cosmonauts,sareen,False,"<p>Biological neuronal networks are highly complex and interconnected with superior information processing capabilities. Such networks have previously served as a model for creating inorganic synapses [1] through silver compounds. However, efficiently generating such complex networks through conventional fabrication remains a challenge. Our mission objective was to grow nanometer/sub-micrometer scale silver dendrite networks in microgravity and characterize them for nanoscale manufacturing on Earth.&nbsp;</p>",2019-06-30,,2019-05-08 18:01:27.669,True,2018-09-01,Floral Cosmonauts: Self-assembling silver dendrite networks in microgravity,PUBLIC,http://www.harpreetsareen.com,True,Fluid Interfaces,False
influx,andresc,False,"<p>We present a stiffness-changing interface based on a magneto-rheological (MR) fluid. The device consists of a material&nbsp;surface with electromagnetically&nbsp;induced visco-elasticity,&nbsp;which acts as a proxy for stiffness during tangible interaction with the material. We present several advantages of&nbsp;this enabling technology and outline potential applications&nbsp;and routes for future development.</p>",2015-12-15,,2017-12-07 04:43:01.290,True,2015-11-01,inFlux,PUBLIC,,False,Speech + Mobility,False
snowballvr,andresc,False,"<p><a href=""http://dl.acm.org/citation.cfm?id=2984779&amp;CFID=856493995&amp;CFTOKEN=37757229"">Multiplayer virtual reality</a> games introduce the problem of variations in the physical size and shape of each user’s space for mapping into a shared virtual space. We designed an asymmetric approach to solve the spatial variation problem, by allowing people to choose roles based on the size of their space. We demonstrate this design through the implementation of a virtual snowball fight where players can choose from multiple roles, namely the shooter, the target, or an onlooker depending on whether the game is played remotely or together in one large space. In the co-located version, the target stands behind an actuated cardboard fort that responds to events in VR, providing non-VR spectators a way to participate in the experience.
                    
                </p>",,,2017-04-20 08:59:00.349,True,2016-01-01,SnowballVR,PUBLIC,,True,Speech + Mobility,False
2ndskin,andresc,False,"<p>DuoSkin is a fabrication process that enables anyone to create customized functional devices that can be attached directly to the skin. Using gold metal leaf, a material that is cheap, skin-friendly, and robust for everyday wear, we demonstrate three types of on-skin interfaces: sensing touch input, displaying output, and wireless communication. DuoSkin draws from the aesthetics found in metallic jewelry-like temporary tattoos to create on-skin devices which resemble jewelry. DuoSkin devices enable users to control their mobile devices, display information, and store information on their skin while serving as a statement of personal style. We believe that in the future, on-skin electronics will no longer be black-boxed and mystified; instead, they will converge towards the user friendliness, extensibility, and aesthetics of body decorations, forming a DuoSkin integrated to the extent that it has seemingly disappeared.</p><p><span style=""font-size: 18px; font-weight: normal;"">Credits:<br></span><span style=""font-size: 18px; font-weight: normal;"">Cindy Hsin-Liu Kao, Asta Roseway*, Christian Holz*, Paul Johns*, Andres Calvo, Chris Schmandt<br></span><span style=""font-size: 18px; font-weight: 400;"">MIT Media Lab in collaboration with Microsoft Research*</span></p>",,--Choose Location,2017-10-17 18:38:36.804,True,2015-09-01,DuoSkin,PUBLIC,http://duoskin.media.mit.edu,True,Speech + Mobility,False
para,jacobsj,False,"<p>Procedural representations, enabled through programming, are a powerful tool for digital illustration, but writing code conflicts with the intuitiveness and immediacy of direct manipulation. Para is a digital illustration tool that uses direct manipulation to define and edit procedural artwork. Through creating and altering vector paths, artists can define iterative distributions and parametric constraints. Para makes it easier for people to create generative artwork, and creates an intuitive workflow between manual and procedural drawing methods.</p>",2016-09-01,--Choose Location,2016-12-11 15:31:18.256,True,2014-09-01,Para,PUBLIC,,False,Lifelong Kindergarten,False
active-drawing,jacobsj,False,"<p>Computation is a powerful artistic medium. The introduction of computers as a tool for making art has established new forms of art which are dynamic: able to actively change in response to an artist’s actions. Tools for dynamic art, like programming languages, offer artists new creative capabilities, but can often be difficult to learn and use in expressive ways.  In my dissertation work I'm developing two systems for supporting Active Dynamic Drawing:  Para and Dynamic Brushes. </p><p>Para is a direct-manipulation parametric tool that supports accessible but expressive procedural graphic art through a direct-manipulation interface. Dynamic Brushes is a system for enabling artists to create their own dynamic drawing tools. Dynamic Brushes builds on lessons gained through evaluating Para to support the combination of drawing by hand with procedural manipulation and augmentation. </p><p>The development of Para and Dynamic Brushes is informed through in-depth interviews with professional artists, and evaluated through a series of open-ended studies where professional artists create their own artwork with the tools.  These studies demonstrate  how dynamic mediums can extend manual art practice by supporting exploration, enabling gradual learning, and allowing manual artists to leverage existing skills.</p>",2017-08-31,,2017-10-10 22:34:46.711,True,2014-09-01,Active Drawing,PUBLIC,http://web.media.mit.edu/,False,Lifelong Kindergarten,False
encuestacdmx,emreiser,False,"<p>Everyone in the city is an expert on their own experience of that city. So how might we integrate new forms of citizen input into the planning and transformation of public spaces around Mexico City, using both digital and non-digital strategies? EncuestaCDMX is a civic technology platform developed with the Laboratorio para la Ciudad that combines in-person surveys and responses from a version of the Action Path location-based survey app to inform city planning decisions. The survey responses power a real-time public dashboard of the feedback available to both city planners and residents for accountability.</p>",2015-10-01,--Choose Location,2016-12-05 00:16:34.976,True,2015-09-01,EncuestaCDMX,PUBLIC,http://erhardtgraeff.com,False,Civic Media,False
openscope,emreiser,False,"<p>OpenScope is an open source project that combines three components for anyone to explore the micro world anytime, anywhere. The 3D-printable open hardware turns your smartphone into a 200x microscope, the image processing application helps you recognize specific objects, and the online community allows you to share and contribute your findings from the microscope. OpenScope is expanding microscopy technologies beyond research laboratories and transforming the way we interact with the micro world.</p>",2017-01-01,--Choose Location,2018-05-04 10:52:12.271,True,2016-01-01,OpenScope,PUBLIC,http://oi7.me,False,Civic Media,False
our-2-sense,emreiser,False,"<p>Innovation and investment in Smart City infrastructure has led to an increasing number of sensors in our urban environment and greater arsenals of data, stored in the cloud and viewed primarily by experts (if at all). &nbsp;We are interested in shifting the current Smart City paradigm to one where sensor data is explored in the physical places where it is being collected, by the communities that inhabit the space. &nbsp;Building on existing sensor infrastructure deployed through Chicago’s Array of Things, we are partnering with the School of the Art Institute of Chicago &nbsp;to explore tools and processes that integrate local residents in gathering and analyzing data &nbsp;in order to spark dialogue, learning, and collaborative revisioning of our cities. </p>",,--Choose Location,2016-12-05 00:17:19.600,True,2016-01-01,Our 2 Sense,LAB,,True,Civic Media,False
encuestacdmx,erhardt,False,"<p>Everyone in the city is an expert on their own experience of that city. So how might we integrate new forms of citizen input into the planning and transformation of public spaces around Mexico City, using both digital and non-digital strategies? EncuestaCDMX is a civic technology platform developed with the Laboratorio para la Ciudad that combines in-person surveys and responses from a version of the Action Path location-based survey app to inform city planning decisions. The survey responses power a real-time public dashboard of the feedback available to both city planners and residents for accountability.</p>",2015-10-01,--Choose Location,2016-12-05 00:16:34.976,True,2015-09-01,EncuestaCDMX,PUBLIC,http://erhardtgraeff.com,False,Civic Media,False
empowering-civic-tech,erhardt,False,"<p>Civic technology should empower us as citizens. But despite its breadth as a field, civic technology often takes its lead from Silicon Valley companies that espouse design goals potentially hazardous to participatory democracy. As an example, Facebook has been used to help organize democratic social movements around the world, but it has also allowed undemocratic actors to inflame partisanship and hate at the same time. I explore: How might we design civic technologies for citizen empowerment and evaluate their impact on this goal?&nbsp;</p><p>With their increasingly important role as mediators of democracy, it is insufficient for civic technology designers to evaluate their designs in terms of ease of use and increased engagement with their platform. Research from political and developmental psychology shows the importance to lifelong civic engagement of learning experiences that cultivate a citizen's perception they can make change (political efficacy) and their belief in having responsibilities to the public good (civic identity). To achieve these positive feedback loops, we need a richer framework for civic technology design.&nbsp;</p><p>This project proposes two solutions: 1) empowerment-based design principles for civic technology, and 2) a prototype toolkit for evaluating the impact of civic technology on political efficacy. Because empowerment is contextual, the proposals here focus on tools and platforms built to support ""monitorial citizenship,"" an increasingly popular form of civic engagement aimed at holding institutions accountable.</p>",,,2019-04-19 18:57:20.471,True,2017-09-01,Empowerment-based design and evaluation of civic technology,PUBLIC,,True,Civic Media,False
making-with-stories,nrusk,False,"<p>We are developing a set of participatory ""maker"" activities to engage youth in creating tangible projects that depict stories about themselves and their worlds. These activities introduce electronics and computational tools as a medium to create, connect, express, and derive meaning from personal narratives. For example, we are offering workshops where participants design sewable circuits and bring them together to create a collaborative Story Quilt. Through the Making with Stories project we are exploring how story-based pedagogy can inspire youth participation in arts and engineering within formal and informal learning environments.</p>",2015-09-01,--Choose Location,2016-12-05 00:16:35.326,True,2014-01-01,Making with Stories,PUBLIC,,False,Lifelong Kindergarten,False
start-making,nrusk,False,"<p>The Lifelong Kindergarten group is collaborating with the Museum of Science in Boston to develop materials and workshops that engage young people in ""maker"" activities in Computer Clubhouses around the world, with support from Intel. The activities introduce youth to the basics of circuitry, coding, crafting, and engineering. In addition, graduate students are testing new maker technologies and workshops for Clubhouse staff and youth. The goal of the initiative is to help young people from under-served communities gain experience and confidence in their ability to design, create, and invent with new technologies.</p>",2016-08-31,--Choose Location,2016-12-05 00:17:01.293,True,2014-01-01,Start Making!,PUBLIC,,False,Lifelong Kindergarten,False
scratch-data-blocks,nrusk,False,"<p>Scratch Community Blocks is an NSF-funded project that extends the Scratch programming language to enable youth to analyze and visualize their own learning and participation in the Scratch online community. With Scratch Community Blocks, youth in the Scratch community can easily access, analyze, and represent data about the ways they program, share, and discuss Scratch projects.</p>",,--Choose Location,2016-12-05 00:17:22.736,True,2014-01-01,Scratch Community Blocks,PUBLIC,,True,Lifelong Kindergarten,False
microworlds,nrusk,False,"<p>The MIT Scratch Team is exploring ways to make it easier for newcomers to get started creating with coding. We are designing ""microworlds""— customized versions of the Scratch editor that contain a small set of blocks for making projects based on a theme. </p><p>Microworlds offer a more creative entry point to coding. While many introductory coding experiences focus on engaging children in puzzles with one right answer, microworlds provide an open-ended experience, enabling children to explore, experiment, and create, while still providing a more simplified and scaffolded entry point into coding.</p><p>Each microworld includes subset of the Scratch programming blocks that are most relevant and useful for the particular interest area, along with specialized graphical assets related to the interest area. In addition to aligning with a particular interest area, each microworld highlights how coding can enable young people to create projects and express ideas with code. For example, by tinkering with the music microworld, young people can see how they can use code to make musical melodies and beats; by tinkering with the soccer microworld, young people can see how they can use coding to make objects move and start building their own game. </p><p>The project is part of the <a href=""http://scratch.mit.edu/info/codingforall"">Coding for All project</a>. The Coding for All project brings together an interdisciplinary research team from the MIT Media Lab, the Digital Media and Learning Hub at University of California Irvine, and Harvard University’s Berkman Center for Internet and Society to develop new online tools and activities to engage more young people in developing computational fluency, particularly youth from groups currently underrepresented in computing.&nbsp;</p>",,,2017-04-05 01:53:28.281,True,2015-09-01,Microworlds,PUBLIC,,True,Lifelong Kindergarten,False
scratch-pad,nrusk,False,"<p>ScratchBit is an effort to enable children to create more seamlessly in both the physical and digital world by creating a dedicated physical interface for the&nbsp;<a href=""https://scratch.mit.edu"">Scratch</a>&nbsp;programming language and environment. Designed to be rugged, low cost, and highly composable, the ScratchBit allows children to take the materials around them—such as cardboard, clothes, skateboards, and trees—and &nbsp;transform them into inputs to their digital creations on Scratch. Unlike the <a href=""http://makeymakey.com/"">Makey Makey</a> which was designed to make these connections electronically, the ScratchBit is designed to create these connections through motion and mechanism.</p>",,,2018-11-03 16:11:24.635,True,2016-09-01,ScratchBit,PUBLIC,,True,Lifelong Kindergarten,False
getting-started-with-scratch,nrusk,False,"<p>Every day, young people around the world use the Scratch programming language to create and share thousands of interactive projects on the <a href=""https://scratch.mit.edu/"">Scratch website</a>. Yet many students aren’t sure how to get started coding their own projects.  </p><p>To address this, we have launched a new set of free resources to help students learn to create with code. The <a href=""https://scratch.mit.edu/go"">Things to Try</a> page offers a variety of project ideas, such as creating an animated story, making a pong game, or designing a virtual pet. For each theme, students can use step-by-step tutorials or printable activity cards. In addition, the site offers educator guides you can use to organize a class or workshop based on the theme.</p><p>The <a href=""https://scratch.mit.edu/info/cards/"">Scratch Activity Cards</a> is a collection of more than 80 colorful cards with 11 project themes. The front of each card illustrates an activity students can do with Scratch, such as animating a character or keeping score in a game. The back of the card shows how to snap together blocks of code to make their projects come to life.  </p><p>These resources are designed to let students learn at their own pace and personalize their projects. Students can work individually or pair up to make projects together.&nbsp;</p>",,,2017-04-19 17:58:02.568,True,2015-09-01,Getting Started with Scratch,PUBLIC,,True,Lifelong Kindergarten,False
computational-tinkering,nrusk,False,"<p>As children tinker with materials in the world, they are constantly putting things together and taking them apart. They are learning through play—trying out new ideas, exploring alternate paths, making adjustments, imagining new possibilities, expressing themselves creatively. In the process, they learn about the creative process and develop as creative thinkers.</p><p>As digital technologies enter the lives of children, there is risk that they will crowd out tinkering, with children spending more time watching screens than tinkering with materials. Yet, in our work, we have seen how digital technologies can also be used to open up new opportunities for tinkering.</p><p>Working in collaboration with the Tinkering Studio at the Exploratorium, Fondazione Reggio Children and the LEGO Foundation, we are developing a new generation of tools, activities, and spaces to support playful investigation and experimentation, integrating digital and physical materials.&nbsp;</p><p>The new activities will enable children to engage in new types of inquiry into light, sound, motion, and storytelling. In the initial set of activities, called ""light play,"" children can program colored lights and moving objects to make dynamic patterns of shadows.<br></p>",,,2018-03-27 10:44:38.668,True,2016-09-01,Computational Tinkering,PUBLIC,,True,Lifelong Kindergarten,False
teen-summit,nrusk,False,"<p>Teen Summit is a biennial week-long Youth Leadership event that brings Clubhouse youth together from each of the 100 Clubhouses internationally. Youth leaders explore issues relevant to them and propose solutions through the creative use of innovative, high-end technologies. The 2018 Teen Summit will take place in late July at Boston University, featuring a college and career fair, collaborative cross-cultural activities, and many other opportunities for educational, career, and personal growth.</p>",,,2018-11-03 16:33:08.852,True,2017-10-02,Clubhouse Teen Summit,PUBLIC,http://www.computerclubhouse.org/teensummit,True,Lifelong Kindergarten,False
sociabowl,joaleong,False,"<p>We introduce SociaBowl, a dynamic table centerpiece to promote positive social dynamics in 2-way cooperative conversations. A centerpiece such as a bowl of food, a decorative flower arrangement, or a container of writing tools, is commonly placed on a table around which people have conversations. We explore the design space for an augmented table and centerpiece to influence how people may interact with one another. We present an initial functional prototype to explore different choices in materiality of feedback, interaction styles, and animation and motion patterns. These aspects are discussed with respect to how it may impact people’s awareness of their turn taking dynamics as well as provide an additional channel for expression. Potential enhancements for future iterations in its design are then outlined based on these findings.</p>",2019-01-07,,2019-05-04 23:42:45.225,True,2018-11-12,SociaBowl: A Dynamic Table Centerpiece to Mediate Group Conversations,PUBLIC,,False,Tangible Media,False
wordsense-learning-language-in-the-wild,cdvm,False,"<p>As more powerful and spatially aware Augmented Reality devices become available, we can leverage the user’s context to embed reality with audio-visual content that enables learning in the wild. Second-language learners can explore their environment to acquire new vocabulary relevant to their current location. Items are identified, ""labeled"" and spoken out loud, allowing users to make meaningful connections between objects and words. As time goes on, word groups and sentences can be customized to the user's current level of competence. When desired, a remote expert can join in real-time for a more interactive ""tag-along"" learning experience.<br></p>",2018-05-31,,2018-08-20 16:21:23.964,True,2016-09-15,WordSense,PUBLIC,,False,Fluid Interfaces,False
holobits,cdvm,False,"<p>The onset of Mixed Reality as a platform offers the opportunity to create new, playful paradigms for building and fostering creativity. The Holobits application leverages the tried and tested features of physical block building platforms like LEGO and introduces the benefits of building in mixed environments to support making and storytelling. The proposed system combines the hand tracking capabilities of the Leap Motion with the spatial mapping of Hololens to enable hands-on building experiences with virtual blocks, denoted as “bits.” These blocks have different attributes and characteristics that determine how they look and behave within the mixed reality building space. The platform also allows users to share their creative building process in a frame-by-frame fashion that enables remixing and reflection on every play session. Holobits allows users to record their interactions with their creations to make animated environments with ease and support storytelling. Another way to enable collaboration is to let kids share models (or download someone else's creation in your space), allowing multiple users to build in a shared physical space or over distances, where a player can “hop” into the physical space of the Hololens user using virtual reality. Last but not least, we intend to integrate the Scratch visual programming toolkit into the Holobits platform to allow users to orchestrate their virtual creations and create the ultimate interactive stories.</p>",2018-05-31,,2018-11-30 17:21:40.954,True,2017-02-13,Holobits: Creativity and fun in Mixed Reality,PUBLIC,,False,Fluid Interfaces,False
words-in-motion,cdvm,False,"<p>Embodied theories of language propose that the way we communicate verbally is grounded in our bodies. Nevertheless, the way a second language is conventionally taught does not capitalize on kinesthetic modalities. The tracking capabilities of room-scale virtual reality systems afford a way to incorporate kinesthetic learning in language education.&nbsp; Words in Motion is a virtual reality language learning system that reinforces associations between word-action pairs by recognizing a student’s movements and presenting the corresponding name of the performed action in the target language. Experiments with Words in Motion suggest that the kinesthetic approach in virtual reality has less immediate learning gain in comparison to a text-only condition. However, virtual kinesthetic learners showed significantly higher retention rates after a week of exposure. Positive correlation between the times a word-action pair was executed and the times a word was remembered by the subjects, supports the premise that virtual reality can impact language learning by leveraging kinesthetic elements.</p>",2018-05-31,,2018-08-20 16:26:03.600,True,2017-02-13,Words in Motion,PUBLIC,,False,Fluid Interfaces,False
inner-child,cdvm,False,"<p>Children can be better at learning than adults. Socio-psychological factors often inhibit adults in learning environments; they are afraid to make mistakes or look silly around their peers and as a result don’t engage with the material as a child would. In language learning, this becomes even more prominent, as children are less afraid of making weird sounds or mispronouncing new words, whereas older students hesitate to speak out loud.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Not only are children less constrained by socio-psychological pressures, but they tend to be naturally more curious and open to new ideas when learning. They approach the task of learning, in many ways, differently than an adult would.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: 400;"">Inner Child intends to tap into the potential benefits of changing the way we perceive ourselves by altering our body and environment in virtual reality.&nbsp;Inner Child is a virtual reality experience developed on the HTC Vive that embodies users in the virtual avatar of a child. The platform leverages the tracking capabilities of room-scale virtual reality to create the illusion of feeling younger, so that learners can approach learning with a more childlike mindset.&nbsp;</span></p><p>Results from our user study suggest that simply having a virtual body impacts the acquisition of vocabulary in virtual reality in a positive way. Furthermore, the illusion of being younger in VR improved participants’ performance in subsequent recall tests both immediately and one week after exposure to the system, which was dependent on the type of body a participant inhabited in the virtual scenario. These results can radically change the way we learn in the classrooms of the future, suggesting that we can alter our notion of self to enhance the way we approach learning on a subconscious level.&nbsp;</p>",2018-05-31,,2018-08-20 16:27:41.575,True,2018-01-01,Inner Child,LAB-INSIDERS,,False,Fluid Interfaces,False
drift-bottle,vmb,False,"<p>How can emotions be conveyed, expressed, and felt? Drift Bottle is a project exploring interfaces that allow users to ""feel"" others� emotions to promote their communication. We have developed a voice message-exchange web service. Based on that, we design and develop several terminals with different interfaces which convey emotions via media such as light, smell, and motion. One solution is to convey the emotions in voice messages via different colors of light. Our latest effort is conveying emotions via smells, with the intention of arousing the same emotions in the receivers.</p>",2014-09-01,--Choose Location,2016-12-05 00:16:38.605,True,2014-01-01,Drift Bottle,PUBLIC,,False,Object Based Media,False
living-materials-library,vmb,False,"<p>The control of living systems as part of design interfaces is of interest to both the scientific and design communities due to the ability of living organisms to sense and respond to their environments.  They may, for example, detect and break down harmful environmental agents, or create beneficial products when environmental levels dropped below a certain threshold.  However, it is also important for these systems to be reversible, so that the biological components are only active when their functionality is necessary, and the system can remain dormant otherwise.&nbsp;</p><p>The Living Material Library is an exploration of tunable hybrid systems. Our work in this area demonstrates the means through which intrinsic material properties may be functionally changed through environmental factors and, in turn, serve as dynamic substrates for living systems. Nearly all organisms have highly developed sensing capabilities, and have been shown to behaviorally respond to changes in substrate properties. By creating a tunable and reversible material system, we explore how cell behavior such as adhesion, patterning, and differentiation may be influenced via an active interface.
                    
                </p><p>In this iteration, we propose a reversible material system that allows for control of living interactions (much like a light switch). We are particularly interested in fluid material systems (such as electrorheological fluids) that transition from a liquid-like to a solid-like state when exposed to electric fields and currents.&nbsp;</p><p>This endeavor brings to light the complex relationship between dynamic materials and living systems. While other methods of cell intervention often rely on light, chemicals, or temperature, here we explore substrate material properties as inputs for organisms. &nbsp;Our library may allow for more directed inquiry into processes such as collective cell durotaxis, general mechanotaxis, and active sensing. This marks an initial foray into establishing candidate design methods for responsive applications.<br></p>",,,2017-08-04 19:50:13.221,True,2017-03-01,Living Materials Library,PUBLIC,,True,Object Based Media,False
pigmented-lumens,vmb,False,"<p>This pedagogy of experiments seeks to accurately model and manufacture a new form of lighting fixtures.</p><p>These new fixtures are made of polyester resin and a particle distribution called a colloid. Unlike traditional light bulb models, rather than having illumination come from individual lighting fixtures that are wired to a common electric source, we are using one lighting source in the form of a laser to illuminate each fixture.</p><p>By using controlled resin casting alongside numerical simulation of structured colloids, we can program and evaluate different substances held in distribution with a 9 dimensionality lighting model. This allows us to understand the type of lighting output that can be achieved with various materials and shapes, as well as predict the aesthetic properties that these lighting sources may allow us. From various colors to different glittery reflects, this framework allows both technical and artistic framework.&nbsp;</p>",,,2019-02-07 23:14:01.769,True,2018-08-01,Pigments and Lumens,PUBLIC,https://ninalutz.github.io,True,Object Based Media,False
printed-wearable-holographic-display,vmb,False,"<p>Holographic displays offer many advantages, including comfort and maximum realism. In this project we adapt our guided-wave light-modulator technology to see-through lenses to create a wearable 3D display suitable for augmented or virtual reality applications. As part of this work we also are developing a femtosecond-laser-based process that can fabricate the entire device by ""printing.""</p>",,--Choose Location,2016-12-05 00:16:56.157,True,2015-09-01,Printed Wearable Holographic Display,PUBLIC,,True,Object Based Media,False
smell-narratives,vmb,False,"<p>We are adding an olfactory dimension to storytelling in order to create more immersive and evocative experiences. Smell Narratives allows the authoring of a ""smell track,"" involving individual or proportionally mixed fragrance components. </p>",,--Choose Location,2016-12-05 00:16:51.129,True,2014-09-01,Smell Narratives,PUBLIC,,True,Object Based Media,False
4k8k-comics,vmb,False,"<p>4K/8K Comics applies the affordances of ultra-high-resolution screens to traditional print media such as comic books, graphic novels, and other sequential art forms. The comic panel becomes the entry point to the corresponding moment in the film adaptation, while scenes from the film indicate the source frames of the graphic novel. The relationships among comics, films, social media, parodies, and other support materials can be navigated using native touch screens, gestures, or novel wireless control devices. Big data techniques are used to sift, store, and explore vast catalogs of long-running titles, enabling sharing and remixing among friends, fans, and collectors.</p>",,--Choose Location,2018-06-20 17:07:10.273,True,2014-01-01,4K/8K Comics,PUBLIC,,True,Object Based Media,False
emotive-materials,vmb,False,"<p>The design process is no longer limited to one group of individuals, as number, level, and cost make tools ever more accessible. As we move towards tools that allow us to create our own materials, having a set of rules with which to evaluate, interpret, and design them will become increasingly important. One way of approaching this problem is by unpacking the ways in which materials create meaning. This project explores the more emotive aspects of materials, such as haptic responses to, cognitive evaluation of, and emotive perception of materials to understand how materials communicate meaning.The development of an effective methodology aims to lower the barriers of fabrication of engaging objects. By incorporating qualities that were not previously quantifiable, we aim to encourage a more interactive design process that allows for the production of experiences tailored to individual preference, and a framework for conversations around material issues.</p>",,--Choose Location,2016-12-05 00:17:01.184,True,2015-09-01,Emotive Materials,PUBLIC,,True,Object Based Media,False
live-objects,vmb,False,"<p>A Live Object is a small device that can stream media content wirelessly to nearby mobile devices without an Internet connection. Live Objects are associated with real objects in the environment, such as an art piece in a museum, a statue in a public space, or a product in a store. Users exploring a space can discover nearby Live Objects and view content associated with them, as well as leave comments for future visitors. The mobile device retains a record of the media viewed (and links to additional content), while the objects can retain a record of who viewed them. Future extensions will look into making the system more social, exploring game applications such as media ""scavenger hunts"" built on top of the platform, and incorporating other types of media such as live and historical data from sensors associated with the objects.</p>",,--Choose Location,2016-12-14 13:53:33.699,True,2015-01-01,Live Objects,PUBLIC,,True,Object Based Media,False
8k-time-machine,vmb,False,"<p>Archived TV programs evoke earlier times. This application combines a video and music archive with an immersive screen and a simple user interface suitable for everyone, from children to the elderly, to create a ""Time Machine"" effect. The only key for exploring is the user's age. People can enjoy over 1,300 TV programs from the last seven decades without having to do tedious text searches. This  catalogue intuitively guides the user with an image array (64 different videos on one screen at the same time) that simplifies navigation and makes it immediate, rather than referencing it to previous screens.</p>",,--Choose Location,2016-12-05 00:17:05.553,True,2016-01-01,8K Time Machine,PUBLIC,,True,Object Based Media,False
neaq-2069,vmb,False,"<p>The New England Aquarium was&nbsp;one of the world’s first modern aquariums when it opened its doors in Boston in 1969.&nbsp; Throughout its history, the aquarium has been a leader in innovative ways to share the ocean with the public, including the creation of the&nbsp;&nbsp;Giant Ocean Tank, the largest circular saltwater tank in the world when it opened in 1970.&nbsp;</p><p>Approaching its 50th anniversary, the New England Aquarium is working with the Open Ocean initiative and&nbsp;MIT Design Lab to develop future scenarios depicting what the experience of the aquarium will be in the next 50 years.</p>",,,2018-04-30 16:04:10.248,True,2018-01-22,NEAQ 2069: Envisioning the Future Aquarium Experience,PUBLIC,,True,Object Based Media,False
8k-brain-tour,vmb,False,"<p>We present an 8K (7680 x 4320 pixels) visualization system for terabyte-scale, three-dimensional microscopy images of a brain slice that can facilitate neuroscience research. High resolution, large format (85” or 188 cm x 106 cm) rendering allows the viewer to dive into the massive dataset of 700 billion voxels capturing thousands of neurons and to investigate nanoscale and macroscale structures of the neurons simultaneously.</p>",,,2019-04-17 18:31:38.208,True,2017-02-23,8K Brain Tour,PUBLIC,,True,Object Based Media,False
project-prometheus,vmb,False,"<p>Most of the underwater world remains far off the map. For many of the most exciting exploration challenges—from Maya cenotes to urban aquifers to archaeological treasures to coral reefs—map-making remains largely pre-industrial and time consuming. The difficulty and expense of mapping these spaces is a major barrier to storytelling for science, conservation, and stewardship. While many tools exist for open-ocean bathymetry (such as multibeam sonars), cost-effective diver-deployable tools for rapidly mapping complex and enclosed spaces are sorely lacking. Our goal is to create diver-deployable tools that are orders of magnitude faster, more precise, and less expensive than current practice–to enable mapping and imaging of these underwater resources at a societal scale.</p><p>To this end we are developing low-cost, high-precision, diver-deployable underwater LIDAR and Depth-Imaging systems—3D scanning and navigation systems with which to quickly, safely, and beautifully map caves, aquifers, coral reefs, sunken cities, and other large-scale underwater spaces. To satisfy scientific and storytelling needs, these devices must be easy to use, have fine spatial resolution, map at swimming speed, produce data in industry-standard formats, and be completely open source at both hardware and software levels.</p>",,,2019-04-22 17:55:43.363,True,2018-04-02,Project Prometheus,PUBLIC,,True,Object Based Media,False
dusk,vmb,False,"<p>DUSK was created as part of the Media Lab's Advancing Wellbeing initiative (supported by the Robert Wood Johnson Foundation) to create private, restful spaces for people in the workplace. DUSK promotes a vision of a new type of ""nap pod,"" where workers are encouraged to use the structure on a daily basis for regular breaks and meditation. The user is provided with the much-needed privacy to take a phone call, focus, or rest inside the pod for short periods during the day. The inside can be silent, or filled with binaural beats audio; pitch black, or illuminated by a sunlamp; whatever works for users to get the rest and relaxation needed to continue to be healthy and productive. DUSK is created with a parametric press-fit design, making it scalable and suitable for fabrication customizable on a per-user basis.</p>",,--Choose Location,2017-04-05 01:19:27.607,True,2014-09-01,DUSK,PUBLIC,,True,Object Based Media,False
airtap,vmb,False,"<p>We demonstrate a method for augmenting existing visual interfaces, including 3D and conventional displays, with haptic feedback capabilities, by utilizing a large number of closely spaced vortex-ring generators mounted along the periphery of the display. We present our first prototype of a multimodal interactive interface platform with 16 independently-controlled air-vortex ring&nbsp; generators with one angular degree of freedom each. Our system has applications as an interactive interface, as a research tool, as an automotive control interface, and as a platform for creative expressions.</p>",,,2018-05-04 11:46:32.043,True,2017-02-02,AirTap,PUBLIC,,True,Object Based Media,False
free-space-haptic-feedback-for-3d-displays,vmb,False,"<p>What if you could not only see but also feel virtual objects as you interacted with them? This would enable richer and more realistic user experiences. We have designed a low-cost air-vortex generator to provide midair haptic feedback when a user touches virtual objects displayed on holographic, aerial, and other 3D displays. The system consists of a 3D-printed chamber and nozzle, five low-frequency transducers, and a custom-designed driver board. The air-vortex generator can provide localized haptic feedback to a range of over 100cm. With increased driving power and a more optimized nozzle design, this range could be extended to several meters. </p>",,--Choose Location,2018-05-01 01:37:57.436,True,2016-01-01,Free-Space Haptic Feedback for 3D Displays,PUBLIC,,True,Object Based Media,False
modesense,vmb,False,"<p>ModeSense is a full stack system that enables indoor environments to become aware of what is happening in them, and then enables the environment to locally inform (offline) all nearby phones and other electronic devices about the most appropriate operating mode in the present time and space.&nbsp; We have developed an ultra-low cost, $5 device that can be installed in conference rooms, lecture halls, movie theaters, homes, and cars, which can dynamically determine the contexts in those areas and then locally broadcast the corresponding mode. Any phones in those areas are then aware of the mode in which they ought to be at that time, and can change their behavior accordingly.</p>",,,2018-05-01 02:35:01.568,True,2016-10-01,ModeSense,PUBLIC,,True,Object Based Media,False
networked-playscapes-dig-deep,vmb,False,"<p>Networked Playscapes re-imagines outdoor play by merging the flexibility of the digital world with the tangible, sensorial properties of physical play to create hybrid interactions for the urban environment.&nbsp;</p><p>Dig Deep takes the classic sandbox found in children's playgrounds and merges it with the common fantasy of ""digging your way to the other side of the world"" to create a networked interaction in tune with child cosmogony. </p>",,--Choose Location,2017-10-13 18:29:16.756,True,2014-01-01,Networked Playscapes: Dig Deep,PUBLIC,,True,Object Based Media,False
programmable-synthetic-hallucinations,vmb,False,<p>We are creating consumer-grade appliances and authoring methodologies that will allow hallucinatory phenomena to be programmed and utilized for information display and narrative storytelling.</p>,,--Choose Location,2019-04-16 16:26:42.218,True,2015-01-01,Programmable Synthetic Hallucinations,PUBLIC,,True,Object Based Media,False
synthetic-shape-motion,vmb,False,<p>Our visual realities are flooded with complex and robust natural scenes. New findings in curvature estimation illustrate where our expectation of reality can break down under very simple mechanisms.</p>,,,2018-10-23 19:51:45.892,True,2018-05-05,Synthetic Shape Motion,LAB-INSIDERS,,True,Object Based Media,False
quiesense,vmb,False,"<p>What if our mobile devices could sense and then adapt to the spatial, temporal, and social context of their local environments? Imagine if your smartphone was smart enough to know that it should not be ringing loudly when you are in an important meeting, or that it should not be in silent mode when you are trying to find where you have misplaced it at home. We have created an inexpensive secure system that delivers this goal by embedding contextual information into the environment rather than the phone.  In that way, all mobile devices at a given location can detect the broadcasted contextual information using Wi-Fi and change their behavior accordingly,  without requiring any handshake or internet connection. By leveraging the latest and most inexpensive Wi-Fi modules on the market, and by building our own embedded firmware, server-side software, and mobile app, we are able to deploy this system in a secure and massively scalable way. <br></p>",,,2019-04-17 18:20:16.620,True,2016-10-21,QuieSense:  Distributed context-awareness system for Wi-Fi enabled mobile devices,PUBLIC,,True,Object Based Media,False
thermal-fishing-bob-in-place-environmental-data-visualization,vmb,False,"<p>Two of the most important traits of environmental hazards today are their invisibility and the fact that they are experienced by communities, not just individuals. Yet we don't have a good way to make hazards like chemical pollution visible and intuitive.&nbsp; SeeBoat and the thermal fishing bob seek to visceralize rather than simply visualize data by creating a physical data experience that makes water pollution data present in communities.&nbsp;</p><p>SeeBoat is a remote control boat with sensors (temperature, turbidity, conductivity, pH) that measure local water quality and LEDs that&nbsp;display the data on site by changing color in real time. Data is also logged to be physically displayed elsewhere and can be further recorded using long-exposure photos.&nbsp;Making environmental data experiential and interactive will help both communities and researchers better understand pollution and its implications.</p><p>The Thermal Fishing Bob is an early version of this tool that has a spherical form factor and focuses on measuring water temperature as a marker for combined sewer overflows (CSOs) that may pollute rivers.&nbsp;&nbsp;</p><p>This project began in partnership with Sara Wylie (Northeastern University) in Spring 2015. Early work included Thermal Fishing Bob workshops, design iteration, prototyping, system testing with users in the Mystic River and Charles River, long exposure photography events, and further concept development. In Spring of 2017, Perovich and Wylie began a collaboration with Roseann Bongiovanni of GreenRoots, an&nbsp;environmental justice community group in Chelsea, MA, to test and iterate on the devices so they best suit the environmental and social context in the local community.&nbsp; As part of this process,&nbsp;Perovich continued to develop the technical side of the project to create SeeBoat, a remote control boat based system, including sensors for&nbsp;turbidity, conductivity, pH, radio based data communication, and designs for and early implementation of an Android app for collecting and viewing quantitative sensor data. Perovich, Wylie, and Bongiovanni are also pursuing related routes of research and community engagement around open access environmental data, the politics of space, community based data installations, and evaluating individual and group learning through extended participatory action research projects.&nbsp;<br></p><p>A publication describing their first year of collaboration can be found in their paper:&nbsp;&nbsp;</p><p>Laura J. Perovich, Sara Wylie, Roseann Bongiovanni (2018)&nbsp;Pokémon Go, pH, and projectors: Applying transformation design and participatory action research to an environmental justice collaboration in Chelsea, MA,&nbsp;Cogent Arts &amp; Humanities,&nbsp;5:1,&nbsp;1-22. (<a href=""https://www.tandfonline.com/doi/pdf/10.1080/23311983.2018.1483874"">Link to PDF</a>.)&nbsp;</p><p>In July of 2018, the team began to collaborate with high school students and staff at the Microsoft Garage Makerspace to test the ease of fabrication of SeeBoat in a more general audience and to continue development of the SeeBoat Android app for numeric data display.&nbsp;</p><p>Thanks to&nbsp;ECO, David Ortiz, Adela Gonzalez, Leo Martinez, GreenRoots staff, Don Blair, Catherine D’Ignazio, the Boston University Law Clinic, and Dr. Sharon Harlan for their support and input on this project. Thanks to MIT undergraduates Sophia Struckman, Rod Bayliss, Robert Henning, and Claudia Chen who contributed to the technical aspect of these workshops and citizen science tool development, photographers Jorge Valdez and Shirin Adhami,&nbsp; the Wylie Lab at Northeastern University, Dr. V. Michael Bove and members of the Object-Based Media group at the MIT Media Lab, the MIT Arts Scholars, the Public Lab community,&nbsp; Mare Librum, the MIT Sailing Pavilion, and the Council for the Arts at MIT.</p>",,--Choose Location,2019-04-17 18:22:32.202,True,2015-01-01,SeeBoat (Thermal Fishing Bob): In-place environmental data visualization,PUBLIC,,True,Object Based Media,False
aerial-light-field-display,vmb,False,"<p>Suitable for anywhere a ""Pepper's Ghost"" display could be deployed, this display adds 3D with motion parallax, as well as optically relaying the image into free space such that gestural and haptic interfaces can be used to interact with it. The current version is able to display a person at approximately full-size.  </p>",,--Choose Location,2019-02-18 02:25:17.276,True,2015-01-01,Aerial Light-Field Display,PUBLIC,,True,Object Based Media,False
cosmetics-empowered-identities,vmb,False,"<p>What if we could leverage technology and modern cosmetics for good? To empower people to explore identities? To give anyone, regardless of skill level, the tools to bend, express, and explore various gender identities and traits?</p><p>The goal of this project is to create a tool that will allow a user to add more feminine, masculine, or androgynous traits to themselves with cosmetics. After interacting with renders and the tool's preferences, the user will receive a list of cosmetics and then follow along with the tool in real time while they apply their makeup.</p><p>This tool is meant to be accessible, to be free and open source technology, and made for users of all skin tones, shapes, and preferences. To allow anyone to express their identities.</p><p>This work is ongoing and will have regular updates, beta releases, and user surveys coming soon.&nbsp;</p>",,,2019-02-07 16:07:17.285,True,2018-08-01,Cosmetics-Empowered Identities,PUBLIC,https://ninalutz.github.io,True,Object Based Media,False
drift-bottle,slysun,False,"<p>How can emotions be conveyed, expressed, and felt? Drift Bottle is a project exploring interfaces that allow users to ""feel"" others� emotions to promote their communication. We have developed a voice message-exchange web service. Based on that, we design and develop several terminals with different interfaces which convey emotions via media such as light, smell, and motion. One solution is to convey the emotions in voice messages via different colors of light. Our latest effort is conveying emotions via smells, with the intention of arousing the same emotions in the receivers.</p>",2014-09-01,--Choose Location,2016-12-05 00:16:38.605,True,2014-01-01,Drift Bottle,PUBLIC,,False,Object Based Media,True
tagme,xavib,False,"<p>TagMe is an end-user toolkit for easy creation of responsive objects and environments. It consists of a wearable device that recognizes the object or surface the user is touching. The user can make everyday objects come to life through the use of RFID tag stickers, which are read by an RFID bracelet whenever the user touches the object. We present a novel approach to create simple and customizable rules based on emotional attachment to objects and social interactions of people. Using this simple technology, the user can extend their application interfaces to include physical objects and surfaces into their personal environment, allowing people to communicate through everyday objects in very low-effort ways.</p>",2018-01-01,--Choose Location,2018-10-11 18:48:35.308,True,2014-09-01,TagMe,PUBLIC,,False,Fluid Interfaces,False
showme-immersive-remote-collaboration-system-with-3d-hand-gestures,xavib,False,"<p>ShowMe is an immersive mobile collaboration system that allows remote users to communicate with peers using video, audio, and gestures. With this research, we explore the use of head-mounted displays and depth sensor cameras to create a system that (1) enables remote users to be immersed in another person's view, and (2) offers a new way of sending and receiving the guidance of an expert through 3D hand gestures. With our system, both users are surrounded in the same physical environment and can perceive real-time inputs from each other. </p>",2017-08-31,--Choose Location,2018-10-12 16:45:04.639,True,2014-01-01,ShowMe: Immersive Remote Collaboration System with 3D Hand Gestures,PUBLIC,,False,Fluid Interfaces,False
psychicvr,xavib,False,"<p>We present PsychicVR, a proof-of-concept system that integrates a brain-computer interface device and virtual reality headset to improve mindfulness while enjoying a playful immersive experience.&nbsp;The fantasy that any of us could have superhero powers has always inspired us, and by using virtual reality and real-time brain activity sensing we are moving one step closer to making this dream real. We non-invasively monitor and record electrical activity of the brain and incorporate this data into the VR experience using an Oculus Rift and the MUSE headband. By sensing brain waves using a series of EEG sensors, the level of activity is fed back to the user via 3D content in the virtual environment. When users are focused, they are able to make changes in the 3D environment and control their powers. Our system increases mindfulness and helps achieve higher levels of concentration while entertaining the user.</p>",,--Choose Location,2019-04-18 17:06:09.570,True,2015-09-01,PsychicVR,PUBLIC,,True,Fluid Interfaces,False
intrepid,manisham,False,"<p>Every 98 seconds, a person in the United States is sexually abused. Every 16 hours,&nbsp;<span style=""font-size: 18px; font-weight: 400;"">a woman in the United States is murdered by her romantic partner or ex-partner.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">Sexual abuse, assault, and harassment are regarded as some of the most common&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">human rights violations in the world by the United Nations. Our work examines&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">methods to prevent sexual assault, from pre-historic times to latest technologies,&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">to inform contemporary designs. Intrepid investigates multiple methods to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">detect initial signs of assault and develop methods for communication and prevention&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">of assault. We also explore olfactory stimuli as a potential means to prevent sexual&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">assault in real-time.</span></p>",2017-06-30,,2018-04-20 17:21:38.623,True,2016-01-04,Intrepid,PUBLIC,,False,Speech + Mobility,False
caps-curbing-assault-to-protect-society,manisham,False,"<p>We present CAPS, wearable on-body capsules which produce repulsive odor to deter sexual&nbsp;abuse. The capsules can be triggered by self-actuation,&nbsp;i.e. by pressure or when an act of forceful removal of clothing is identified. &nbsp;The formulation of odor involves compounds&nbsp;like civet reconstruction, 1-4, butadiene and more.</p>",,--Choose Location,2018-04-20 16:49:41.304,True,2016-01-01,CAPS: Curbing Assault to Protect Society,PUBLIC,,True,Speech + Mobility,False
smell-camera2,manisham,False,"<p>We present Smell Camera: an innovative way of capturing memories in the form of smell that can be recorded, stored, and played in the future.</p><p><span style=""font-size: 18px; font-weight: normal;"">The device consists of a hand-held pneumatic pump which is controlled by the user's phone, through which the user can record memories. The smells are encapsulated in a gelatin capsule which can be preserved in an air-tight personalized accessory. Whenever the user wants to experience the same environment or feeling, or evoke the same emotions, the user can play the smell and relive the &nbsp;moments.&nbsp;</span><br></p>",,,2018-03-24 23:06:36.139,True,2016-03-30,"Smell Camera: Record, Play, Rewind",PUBLIC,,True,Speech + Mobility,False
cultural-lens,manisham,False,"<p>Cultural Lens is a speculative design project that reverses traditional gender responsibilities around clothing and personal appearance, interrogating the status of prevalent historical arguments around modesty and etiquette. Every society, culture, and religion has implicit or explicit expectations for women's clothing and public appearance, which has recently led to debates around issues for human rights, freedom, and self-expression.</p><p>Cultural Lens asks society-at-large to share the burden of enforcing public appearance for women. Instead of forcing women to wear clothing styles that are deemed acceptable by the public, Cultural Lens allows members of the public that might be offended–by, for example, perceived immodesty or improper etiquette–to have the freedom to selectively filter the appearance of the women they see in public to conform  literally to their view of how women should look.&nbsp;</p><p>The system uses a Microsoft HoloLens to implement an Augmented Reality visual field for the user.  The system can identify people and faces in the view, classify gender, and apply visual filters to their appearance according to the user's preference. For example, Cultural Lens can add digital veil to the faces of all women the user observes.&nbsp;<br></p>",,,2017-03-29 20:15:11.938,True,2017-01-03,Cultural Lens,PUBLIC,,True,Speech + Mobility,False
conforming-materials,manisham,False,"<h2>Combining the art of two worlds–fashion and biology–into one.&nbsp;</h2><p>Conforming Materials is working towards designing fully recyclable clothing. Through the development of a biomaterial that changes from a crystallized powder into a solid and back, at just the right temperatures, we have harnessed a breakthrough technology that could change fashion as we know it. This biomaterial can be manipulated in a multitude of ways due to its hydrophilic and hydrophobic properties. For example, imagine having a dress that can be tailored to encapsulate smell, eliminating the need for perfume. Imagine wanting a new wardrobe and being able to 100% recycle the biomaterial due to it being made solely of biological components. Imagine going for a run and your clothes trap the odor of your sweat and release a pleasant scent in its place. The possibilities of this material to be used in clothing design are endless. Electronics can be integrated into the design process due to the ability of the biomaterial to be conductive and overlaid with conductive ink. This component would enable analysis on biometrics and human data, with a wide variety of applications including sports clothing.</p>",,,2018-04-20 16:47:40.289,True,2014-12-15,Conforming Materials,PUBLIC,,True,Speech + Mobility,False
chromoskin,manisham,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Makeup has long been used as a body decoration process for self-expression and for the transformation of one's appearance. While the material composition and processes for creating makeup products have evolved, they still remain static and non-interactive. But our social contexts demand different representations of ourselves; thus, we propose ChromoSkin, a dynamic color-changing makeup system that gives the wearer ability to alter seamlessly their appearance. We prototyped an interactive eye shadow tattoo composed of thermochromic pigments activated by electronics or ambient temperature conditions. We present the design and fabrication of these interactive cosmetics, and the challenges in creating skin interfaces that are seamless, dynamic, and fashionable.</span></p>",,--Choose Location,2018-02-08 20:50:38.533,True,2015-12-01,ChromoSkin,PUBLIC,https://vimeo.com/155460417,True,Speech + Mobility,False
beautiful-me,manisham,False,"<p>Beautiful Me&nbsp;has been designed to address the problem of&nbsp;<a href=""https://en.wikipedia.org/wiki/Acid_throwing"">acid attacks</a>&nbsp;in southeast Asian countries and parts of Europe. This form of assault is done primarily to disfigure someone's body or in an attempt to kill them. As there are very few legal checks pertaining to the buying and selling of acid, we have developed a foundation (topical cream) which can act as a barrier against the acid. The foundation formula creates a thin film, which reacts with the acid to reduce its effects and prevents the acid from penetrating to the skin. This first layer of defense is oleophilic,&nbsp; hence the acid can be washed off easily.&nbsp;</p>",,,2019-05-22 16:48:35.447,True,2016-01-01,Beautiful Me,PUBLIC,,True,Speech + Mobility,False
dancing-control-system-for-bionic-ankle-prosthesis,bevinlin,False,"<p>Professional ballroom dancer Adrianne Haslet-Davis lost her natural ability to dance when her left leg was amputated below the knee following the Boston Marathon bombings in April 2013. Hugh Herr was introduced to Adrianne while meeting with bombing survivors at Boston's Spaulding Rehabilitation Hospital. For Professor Herr, this meeting generated a research challenge: build Adrianne a bionic ankle prosthesis, and restore her ability to dance. The research team for this project spent some 200 days studying the biomechanics of dancing and designing the bionic technology based on their investigations. The control system for Adrianne was implemented on a customized BiOM bionic ankle prosthesis.  </p>",2013-03-31,--Choose Location,2016-12-05 00:16:43.994,True,2014-01-01,Dancing Control System for Bionic Ankle Prosthesis,PUBLIC,,False,Biomechatronics,False
dancing-control-system-for-bionic-ankle-prosthesis,hherr,False,"<p>Professional ballroom dancer Adrianne Haslet-Davis lost her natural ability to dance when her left leg was amputated below the knee following the Boston Marathon bombings in April 2013. Hugh Herr was introduced to Adrianne while meeting with bombing survivors at Boston's Spaulding Rehabilitation Hospital. For Professor Herr, this meeting generated a research challenge: build Adrianne a bionic ankle prosthesis, and restore her ability to dance. The research team for this project spent some 200 days studying the biomechanics of dancing and designing the bionic technology based on their investigations. The control system for Adrianne was implemented on a customized BiOM bionic ankle prosthesis.  </p>",2013-03-31,--Choose Location,2016-12-05 00:16:43.994,True,2014-01-01,Dancing Control System for Bionic Ankle Prosthesis,PUBLIC,,False,Biomechatronics,False
closed-loop-optogenetics-for-peripheral-nerve-control,hherr,False,"<p>Electrical stimulation (FES) is the current clinical stimulation modality used to restore function and provide therapy in a variety of clinical applications.&nbsp;However, its clinical implementation is riddled with challenges of fatigue, reverse order recruitment of motor units, and diffuse/non-specific excitation of surrounding tissues.&nbsp;</p><p>Optogenetics is a recently evolving field, which involves genetically altering cells so that they can be activated with light. Optogenetic techniques have largely been used to probe neural circuits and study the brain's function. Work in our lab has focused on implementing optogenetics as a stimulation modality for peripheral tissues. Under optogenetic stimulation, many of the challenges associated with electrical stimulation are overcome.&nbsp;</p><p>Most recently, we have utilized optogenetics in a closed-loop system to control a murine hind limb to follow desired movement patterns, mimicking climbing stairs and walking.&nbsp;&nbsp;In an ideal future, similar techniques may be used to restore functional movement in patients with paralysis or other motor impairments. We demonstrate that our methods outperform traditional electrical stimulation methods by having less fatigue and smoother movement.&nbsp; This system is the first proof-of-principle for peripheral limb control using closed-loop optogenetics and can perform with greater than 95% accuracy.</p>",2018-12-31,,2019-01-31 15:21:42.923,True,2017-09-01,Closed-Loop Optogenetics for Peripheral Nerve Control,PUBLIC,,False,Biomechatronics,False
ultrasound-prosthetic-interface-design,hherr,False,"​<p>In the United States, there are an estimated 1.7 million people living with amputation, with that number expected to double by 2050. Complications of prosthetic leg use in persons with lower extremity amputation (LEA) include delayed wound healing, recurrent skin ulcerations, and pressure damage to soft tissues. This can result in limited mobility, which further contributes to conditions such as obesity, musculoskeletal pathologies (e.g., osteoarthritis, osteopenia, and osteoporosis), as well as cardiovascular disease. Traditionally, fabrication of prosthetic sockets remains a fundamentally artisanal process with limited input of quantitative data. Even with advances in computer-aided design and manufacturing (CAD/CAM), prosthetists often modify sockets using non-quantitative craft processes requiring substantial human hours and financial cost. The goal of this research is to develop and validate musculoskeletal ultrasound imaging techniques for creating predictive biomechanical models of residual limbs that will reduce the barrier for and cost of computer aided design (CAD)-driven prosthetic socket design in the US and in low-and middle-income countries.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>​",,,2019-04-17 19:24:01.508,True,2016-10-18,Ultrasound imaging for transtibial prosthetic interface design,PUBLIC,,True,Biomechatronics,False
3d-neuromuscular-model-of-the-human-ankle-foot-complex,hherr,False,"<p>During the gait cycle, the human ankle complex serves as a primary power generator while simultaneously stabilizing the entire limb. These actions are controlled by an intricate interplay of several lower leg muscles that cannot be fully uncovered using experimental methods alone. A combination of experiments and mathematical modeling may be used to estimate aspects of neuromusculoskeletal functions that control human gait. In this research, a three-dimensional neuromuscular model of the human ankle-foot complex based on biplanar fluoroscopy gait analysis is presented.&nbsp;Driven by kinematics, kinetics, and electromyography (EMG), the model seeks to solve the redundancy problem, individual muscle-tendon contributions to net joint torque, in ankle and subtalar joint actuation during overground gait. An optimization approach was employed to calculate sets of morphological parameters that simultaneously maximize the neuromuscular model’s metabolic efficiency and fit to experimental joint torques. Optimal morphological parameter sets produce estimates of force contributions and states for individual muscles.</p>",,,2019-04-26 18:59:57.781,True,2018-08-01,3D neuromuscular model of the human ankle-foot complex,PUBLIC,,True,Biomechatronics,False
multidic-a-matlab-toolbox-for-multi-view-3d-digital-image-correlation,hherr,False,"<p>&nbsp;Three-dimensional Digital Image Correlation (3D-DIC) is a non-contact optical-numerical&nbsp;technique for evaluating the dynamic mechanical behavior at the surface of structures and materials,&nbsp;including biological tissues. 3D-DIC can be used to extract shape and full-field displacements and strains&nbsp;with high resolution, at various length scales. While various commercial and academic 3D-DIC software&nbsp;exist, the field lacks 3D-DIC packages which offer straightforward calibration and data-merging solutions for&nbsp;multi-view analysis, which is particularly desirable in biomedical applications. To address these limitations,&nbsp;we present MultiDIC, an open-source MATLAB toolbox, featuring the first 3D-DIC software specifically&nbsp;dedicated to multi-view setups. MultiDIC integrates robust two-dimensional subset-based DIC software with&nbsp;specially tailored calibration procedures, to reconstruct the dynamic behavior of surfaces from multiple&nbsp;stereo-pairs. MultiDIC contains novel algorithms to automatically merge meshes from multiple stereo-pairs,&nbsp;and to compute and visualize 3D shape and full-field motion, deformation, and strain. User interfaces&nbsp;provide capabilities to perform 3D-DIC analyses without interacting with MATLAB syntax, while standalone&nbsp;functions also allow proficient MATLAB users to write custom scripts for specific experimental&nbsp;requirements. This paper discusses the challenges underlying multi-view 3D-DIC, details the proposed&nbsp;solutions, and describes the algorithms implemented in MultiDIC. The performance of MultiDIC is tested&nbsp;using a low-cost experimental system featuring a 360-deg 12-camera setup. The software and system are&nbsp;evaluated using measurement of a cylindrical object with known geometry subjected to rigid body motion&nbsp;and measurement of the lower limb of a human subject. The findings confirm that shape, motion, and full-field deformations and strains can be accurately measured, and demonstrate the feasibility of MultiDIC in&nbsp;multi-view in-vivo biomedical applications.</p>",,,2019-04-26 19:00:50.702,True,2017-05-15,MultiDIC: a MATLAB Toolbox for Multi-View 3D Digital Image  Correlation,PUBLIC,,True,Biomechatronics,False
embedded-neural-interface-electronics-for-advanced-prosthesis-research,hherr,False,"<p>This project aims to develop embedded neural interface electronics for integration on mobile, external prosthetic devices. By pushing the technological limits of every layer within the stack, from semiconductor components to high-level signal processing and classification, we hope to provide practical benefit to people's daily lives in the near future.</p><p>As a first step, we are actively developing a high-fidelity electromyography (EMG) device used to observe human muscle activity. Several prosthetic ankles have already been demonstrated using this portable EMG device as a control system input.&nbsp;</p>",,,2019-04-26 19:01:33.105,True,2017-09-01,Embedded Systems for Neural Interfaces,LAB-INSIDERS,,True,Biomechatronics,False
ankle-foot-prosthesis-for-rock-climbing,hherr,False,"<p>Lower extremity amputation leads to limitations of biological function of individuals, which leads to challenges remaining physically active and participating in athletic activities. Physical activity is very important for cardiovascular health, weight management, and mental health. Designing devices aimed at increasing accessibility to sports will encourage individuals with amputations to continue or begin participating in athletic pursuits such as rock climbing.&nbsp;</p><p>This research presents the design and evaluation of a 2-degree-of-freedom powered ankle-foot prosthesis for rock climbing. The aim of this device is to restore function of the ankle and subtalar joints for trans-tibial amputees during rock climbing, providing the user with myoelectric position control of the foot. Precise positional control of the foot is especially important while climbing, as the climber’s ability to successfully scale a route requires them to reliably reorient the foot to various shapes and orientations of holds. Passive prostheses do not allow the user to reposition the foot, and current powered prostheses are too bulky and heavy to provide benefit during rock climbing.&nbsp;</p><p>The design requirements for this device are that it must be lightweight (&lt; 1.5 kg), low profile, robust, with 2 degrees of freedom of electromyographically controlled movement. The custom designed device consists of 2 non-backdrivable linear actuators in a differential pair, allowing for powered motion in plantarflexion/dorsiflexion and inversion/eversion. Load cells aligned axially with each actuator are used to provide force feedback to the device, allowing for position control during free-space motion, and powering off the actuators when the device is loaded, relying on the non-backdrivable transmission to maintain ankle and foot position while loaded. This control scheme reduces the power requirements of the device, allowing for lighter batteries as well as smaller motors and transmission.&nbsp;</p>",,,2019-04-26 19:02:57.902,True,2017-09-15,Design of a 2-Degree-of-Freedom Powered Ankle-Foot Prosthesis for Rock Climbing,LAB-INSIDERS,,True,Biomechatronics,False
electrical-interfaces,hherr,False,"<h2>Nerve-Muscle Graft Chamber and micro-channel arrays tor interface to peripheral nerves for prosthesis control.&nbsp;</h2><p>This research effort consists of two sub-projects with the goal to develop a small implantable device for achieving bi-directional communication with the amputated nerves in a prosthesis user’s residuum. The nerve-muscle graft chamber (NMGC) is a small implanted device which contains one or more electrically isolated chambers (ca. 20mm&nbsp;<i>l&nbsp;</i>x 4mm&nbsp;<i>h&nbsp;</i>x 4mm&nbsp;<i>w )</i>&nbsp;that can be filled with muscle or cutaneous tissue. The electrical activities of the components of a compound peripheral nerve that in the intact limb sub-served different motor functions can be separated by mechanically dividing the nerve and placing each isolated nerve segment into apposition with a small piece of muscle tissue in each of the separate chambers of the NMGC.&nbsp; For example, the muscle filled chambers can be ganged together in a modular design so that a single implanted device containing three chambers would interface to motor nerve fascicles that provide prosthesis command signals for three different motor functions. For a mixed peripheral nerve that is known to contain cutaneous fascicles as well as motor fascicles, an additional compartment could be added that contains cutaneous tissue. This would be done to provide an appropriate target for regenerating cutaneous nerve fibers to prevent the cutaneous axons from competing with regenerating motor nerve fibers and errantly taking up residence in the muscle tissues. Also, by provide cutaneous&nbsp; target tissue, regenerating sensory afferent nerve fiber are less likely to result in the formation of potentially painful &nbsp;neuromas.</p><p>The second sub-project aims to develop a micro-channel array into which peripheral nerve fibers will grow into. Because the micro-channels are on the order of 100 to 200 um I.D., only a small number of nerve fibers will be present in an individual micro-channel. This can potentially provide greater separation of axons by their functionality. Such separation by function is important when seeking to provide cutaneous and proprioceptive feedback by means of direct electrical activation of the sensory components of the interfaced peripheral nerves.&nbsp;&nbsp;</p>",,,2019-04-26 19:04:39.951,True,2016-01-04,Neural Interfaces,PUBLIC,,True,Biomechatronics,False
analysis-of-residual-limb-changes-using-digital-image-correlation-and-finite-element-modelling,hherr,False,"<p>Local changes in the volume, shape, and mechanical properties of the residual limb can be caused by adjacent joint motion, muscle activation, hydration, atrophy, and more. These changes affect socket fit quality and might cause inefficient load distribution, discomfort, and dermatological problems. Analyzing these effects is an important step in considering their influence on socket fit, and in accounting for their contribution within the socket design process. </p><p>In this study, a 360° 3D digital image correlation (3D-DIC) system was developed for the full-field deformation measurements of the residuum. A multi-camera rig was designed for capturing synchronized image sets as well as force measurements from a hand-held indenter. Custom camera calibration and data-processing procedures were specifically designed to transform image data into 3D point clouds, and automatically merge data obtained from multiple views into continuous surfaces. Moreover, a specially developed data-analysis procedure was applied for correlating pairs of largely deformed images of speckled surfaces, from which displacements, deformation gradients, and strains were calculated.&nbsp;Characterization of the full-field deformations using 3D-DIC provides insight into the patterns and sources of the phenomena.&nbsp;</p><p>In addition, local and subject-specific soft tissue mechanical properties were obtained by analyzing surface deformation and force measurement during indentation using inverse FE analysis. These data can be used to accurately describe the residuum’s biomechanical behavior. Consequently, prosthetic socket designs that take into account these effects can be considered.</p>",,,2019-04-26 19:04:59.543,True,2017-05-01,Analysis of residual limb changes using digital image correlation and finite element modeling,PUBLIC,,True,Biomechatronics,False
transdermal-optogenetic-peripheral-nerve-stimulation,hherr,False,"<p>
                    Optogenetic techniques have recently been applied to peripheral nerves as a scientific tool with the translatable goal of alleviating a variety of disorders, including chronic pain, muscle fatigue, glucose-related pathologies, and others.  When compared to the electrical stimulation of peripheral nerves, there are numerous advantages: the ability to target molecularly defined subtypes, access to opsins engendering neural inhibition, and optical recruitment of motor axons in a fashion that mimics natural recruitment, which eliminates the fatigue roadblock inherent to functional electrical stimulation. The ability to control peripheral nerves situated under deep tissue structures with transdermal, optical signals would be of enormous benefit, integrating all of the advantages conferred by optogenetics while averting the drawbacks associated with implantable devices, such as mechanical failure, device tissue heating, and a chronic foreign body response.&nbsp;</p><p>We work to develop novel molecular and optical methods in an effort to enable this transdermal optogenetic peripheral nerve control. A further example of a potential clinical application involves optogenetically targeting the vagus nerve, a peripheral cranial nerve implicated in numerous ailments, including epilepsy, migraines, obesity, hypertension, fibromyalgia, Crohn’s disease, asthma, depression, and obsessive-compulsive disorder.  An efficient method of stimulating the vagus nerve with minimal side-effects and high target specificity, such as described here, may have profound implications to the study of various illnesses and disabilities.</p>",,,2019-04-26 19:05:38.186,True,2015-01-01,Transdermal Optogenetic Peripheral Nerve Stimulation,PUBLIC,,True,Biomechatronics,False
amputation,hherr,False,"<p>Lower-extremity amputation surgery has not seen significant change since the Civil War. This research is focused on the development of novel amputation paradigms that leverage native biological end organs to interpret efferent motor commands and to provide meaningful neural feedback from an artificial limb. Surgical replication of natural agonist-antagonist muscle pairings within the residuum allow us to use biomimetic constructs to communicate joint state and torque from the prosthesis directly to the peripheral nervous system. We hypothesize that these architectures will facilitate control of advanced prosthetic systems to improve gait and reduce metabolic cost of transport.
                    
                </p>",,,2019-04-26 19:05:58.968,True,2016-08-15,Revolutionizing amputation surgery for the restoration of natural neural sensation and mobility,PUBLIC,,True,Biomechatronics,False
osseo,hherr,False,"<p>Recent advancements in orthopedic implants have made way for a new generation of bionic limbs that attach directly to the skeleton. Leveraging these ""osseointegrated"" implants to pass wires out of the body enables robust, long-term communication with residual muscles and the nervous system. We are exploring the ways in which the improved neural communication afforded by osseointegration can impact the experience of controlling a limb prosthesis.
                    
                </p>",,,2019-04-26 19:06:19.611,True,2016-10-19,An osseointegrated prosthesis with bi-directional neural communication,PUBLIC,,True,Biomechatronics,False
terrain-adaptive-lower-limb-prosthesis,hherr,False,"<p>Although there have been great advances in the control of lower extremity prostheses, transitioning between terrains such as ramps or stairs remains a major challenge for the field. The mobility of leg amputees is thus limited, impacting their quality of life and independence. This projects aims to solve this problem by designing, implementing, and integrating a combined terrain-adaptive and volitional controller for powered lower limb prostheses. The controller will be able to predict terrain changes using data from both intrinsic sensors and electromyography (EMG) signals from the user; adapt the ankle position before footfall in a biologically accurate manner; and provide a torque profile consistent with biological ankle kinetics during stance. The result will allow amputees to traverse and transition among flat ground, stairs, and slopes of varying grade with lower energy and pain, greater balance, and without manually changing the walking mode of their prosthesis.</p>",,--Choose Location,2019-04-26 19:07:36.576,True,2014-09-01,Terrain-Adaptive Lower Limb Prosthesis,PUBLIC,,True,Biomechatronics,False
metabolic-and-biomechanical-evaluation-of-walking-with-an-autonomous-exoskeleton-on-sloped-terrain,hherr,False,"<p>We have developed an autonomous powered exoskeleton capable of providing a significant metabolic benefit to the user when walking on level ground. To date, this device has demonstrated the greatest published metabolic benefit for an exoskeleton, even outperforming some exoskeletons that rely on a tethered power supply. To gain a better sense of the practicality and versatility of this system, an assessment of metabolic and biomechanical performance on uneven ground is warranted. To accomplish this, the performance of the Biomechatronics exoskeleton will be evaluated through measurement of human respiratory rate while walking on an instrumented treadmill. Furthermore, EMG and motion-capture data will be collected to provide insight as to the biomechanical basis of observed metabolic changes. This study aims to further demonstrate the practicality of an autonomous powered exoskeleton for military, recreational, rehabilitative, or other use.</p>",,--Choose Location,2019-04-26 19:06:47.182,True,2016-02-01,Metabolic and Biomechanical Evaluation of Walking with an Autonomous Exoskeleton on Sloped Terrain,LAB-INSIDERS,,True,Biomechatronics,False
mechanical-interfaces,hherr,False,"<p>This research track focuses on the use of computational (and experimental) techniques to understand the biomechanical behavior of human tissue as well as the musculoskeletal system.&nbsp; This knowledge feeds into novel methods for computational modeling based design of biomechatronic devices which in turn aim to restore or improve the human body. These devices include prosthetic and orthotic devices, and exoskeletons. <br></p>",,,2019-04-26 19:03:38.901,True,2015-11-01,Computational Biomechanics,PUBLIC,,True,Biomechatronics,False
agonist-antagonist-myoneural-interface-ami,hherr,False,"<h2><b>Humans can accurately sense the position, speed, and torque of their limbs, even with their eyes shut. This sense, known as proprioception, allows humans to precisely control their body movements. </b></h2><p>Today’s conventional prosthetic limbs do not provide feedback to the nervous system. Because of this, people with amputated limbs cannot feel the position, speed, and torque of their prosthetic joints without looking at them, making it difficult to control their movement. In order to create a more complete prosthetic control experience, researchers at the Center for Extreme Bionics at the MIT Media Lab invented the&nbsp;<b>agonist-antagonist myoneural interface (AMI)</b>. The AMI is a method to restore proprioception to persons with amputation.</p>",,,2018-08-17 16:20:19.891,True,2014-06-01,Agonist-antagonist Myoneural Interface (AMI),PUBLIC,,True,Biomechatronics,False
physical-telepresence,daniell,False,"<p>We propose a new approach to physical telepresence, based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In the research paper, we describe the concept of shape transmission, and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of users' body parts can be altered to amplify their capabilities for teleoperation. A preliminary evaluation found that users were able to manipulate simple objects remotely, and found many different techniques for manipulation that highlight the expressive nature of our system.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:44.043,True,2014-09-01,Physical Telepresence,PUBLIC,,False,Tangible Media,True
animastage,daniell,False,"<p>We present AnimaStage: a hands-on animated craft platform based on an actuated stage. Utilizing a pin-based shape display, users can animate their crafts made from various materials. Through this system, we intend to lower the barrier for artists and designers to create actuated objects and to contribute to interaction design using shape-changing interfaces for inter-material interactions.</p><p>We introduce a three-phase design process for AnimaStage with examples of animated crafts. We implemented the system with several control modalities that allow users to manipulate the motion of the crafts so that they could easily explore their desired motion through an iterative process. Dynamic landscapes can also be rendered to complement the animated crafts. We conducted a user study to observe the subject and process by which people make crafts using AnimaStage. We invited participants with different backgrounds to design and create crafts using multiple materials and craft techniques. A variety of outcomes and application spaces were found in this study.</p><p><a href=""http://tangible.media.mit.edu/project/animastage/"">Project Page</a></p>",,,2017-06-21 18:35:35.551,True,2017-06-12,AnimaStage,PUBLIC,http://tangible.media.mit.edu/project/animastage,True,Tangible Media,True
transform-adaptive-and-dynamic-furniture,daniell,False,"<p>Introducing TRANSFORM, a shape-changing desk. TRANSFORM is an exploration of how shape display technology can be integrated into our everyday lives as interactive, transforming furniture. These interfaces not only serve as traditional computing devices, but also support a variety of physical activities. By creating shapes on demand or by moving objects around, TRANSFORM changes the ergonomics and aesthetic dimensions of furniture, supporting a variety of use cases at home and work: it holds and moves objects like fruit, game tokens, office supplies, and tablets, creates dividers on demand, and generates interactive sculptures to convey messages and audio.</p>",,--Choose Location,2019-04-17 19:36:00.162,True,2014-09-01,TRANSFORM: Adaptive and dynamic furniture,PUBLIC,,True,Tangible Media,True
materiable-rendering-dynamic-material-properties-with-shape-changing-interfaces,daniell,False,"<p>Shape-changing interfaces give physical shape to digital data so that users can feel and manipulate data with their hands and body. Combining techniques from haptics with the field of shape-changing interfaces, we propose a technique to build a perceptive model of material properties by taking advantage of the shape display's ability to dynamically render flexibility, elasticity, and viscosity in response to the direct manipulation of any computationally rendered physical shape. Using a computer-generated relationship between the manipulated pins and nearby pins in the shape display, we can create human proprioception of various material properties. Our results show that users can identify varying material properties in our simulations through direct manipulation, and that this perception is gathered mainly from their physical relationship (touch) with the shape display and its dynamic movements.</p>",,--Choose Location,2016-12-16 20:08:16.037,True,2015-09-01,Materiable,PUBLIC,,True,Tangible Media,True
physical-telepresence,olwal,False,"<p>We propose a new approach to physical telepresence, based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In the research paper, we describe the concept of shape transmission, and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of users' body parts can be altered to amplify their capabilities for teleoperation. A preliminary evaluation found that users were able to manipulate simple objects remotely, and found many different techniques for manipulation that highlight the expressive nature of our system.</p>",2015-01-01,--Choose Location,2016-12-05 00:16:44.043,True,2014-09-01,Physical Telepresence,PUBLIC,,False,Tangible Media,False
plethora,weller,False,"<p>Plethora creates simultaneous, localized, personal broadcasting networks that allow audiences to form on-the-fly and build their own media streams. The display ranges from personal devices such as phones, to embedded screens in kitchens. It uses Bluetooth LE beacons to emulate local broadcast stations that signal the proximity. Plethora inverts the relationship between mobility and the multiplicity of screens with which we interact on a moment-to-moment basis. It also considers media as migrating away from larger, high definition screens to a universe of personal, anytime, anywhere representations.</p>",2016-01-01,--Choose Location,2016-12-05 00:16:44.818,True,2014-09-01,Plethora,PUBLIC,,False,Viral Communications,False
super-cut-notes,weller,False,"<p>A large portion of popular media is remixed: existing media content is spliced and re-ordered in a manner that serves a specific narrative. Super Cut Notes is a semi-comical content remix tool that allows a user to splice and combine the smallest bits of media: words. By tapping into the dataset of our group's <a href=""https://www-prod.media.mit.edu/projects/superglue/overview/"">SuperGlue platform</a>, it has access to a huge dictionary of words created by SuperGlue's transcription module. Users are able to input a text of any length, choose video-bits of individual words that match their text, and create a video of their combination—in the style of cut-and-pasted ransom notes.</p>",2017-06-06,--Choose Location,2018-10-09 01:52:24.513,True,2016-09-01,Super Cut Notes,PUBLIC,,False,Viral Communications,False
watch-people-watch,weller,False,"<p>Recording your reaction to a short video is becoming the new gossip; famous watchers get as many as 750,000 views. We attempt to transform this utterly useless and talentless event into a socially constructive alternative to simultaneous, synchronized, group viewing. Any user can opt in to be recorded and added to the shared, collective viewing experience. No talent or skills required.</p>",2017-06-30,--Choose Location,2018-10-09 18:03:44.000,True,2016-01-01,Watch People Watch,PUBLIC,,False,Viral Communications,False
this-is-how,weller,False,"<p>This Is How is a platform for connecting makers with small businesses through stories. Small businesses share their stories in the form of video bytes in which they explain what they do and why, what their requirements and constraints are, and what kinds of issues they have. Makers can then annotate the video, ask further questions, and propose solutions for issues. The video is passed through SuperGlue for annotation and to categorize and find commonalities among requests.</p>",,--Choose Location,2016-12-05 00:16:55.625,True,2016-09-01,This Is How,PUBLIC,,True,Viral Communications,False
g3p-II,weller,False,"<h2><p><span style=""font-size: 18px; font-weight: normal;"">Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods—such as blowing, pressing, and forming—have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.</span><br></p></h2>",,--Choose Location,2018-10-23 15:19:17.532,True,2015-09-01,Glass II,PUBLIC,,True,Viral Communications,False
captions,weller,False,"<p>Modern web presentations such as Youtube feature videos with commentary appended at the bottom. In our new imagining of <b>Videotext,</b> we put the two together: comments appear as active bubbles along the playback time line. We thereby associate the commentary with the place in the video to which it refers. It gains context. This project is in the early test stage and is presented for discussion and further development in summer 2016.</p>",,--Choose Location,2016-12-05 00:17:03.626,True,2016-01-01,Captions++,PUBLIC,,True,Viral Communications,False
superglue,weller,False,"<p>SuperGlue is a core news research initiative that is a ""digestion system"" and metadata generator for mass media. An evolving set of analysis modules annotate 14 DirecTV live news broadcast channels as well as web pages and tweets. The video is archived and synchronized with the analysis. Currently, the system provides named-entity extraction, audio expression markers, face detectors, scene/edit point locators, excitement trackers, and thumbnail summarization. We use this to organize material for presentation, analysis, and summarization. SuperGlue supports other news-related experiments.</p><p>SuperGlue is a framework for media digestion and metadata generation. The digestion work flow also has applications for media more broadly including conversational ecommerce.</p>",,--Choose Location,2019-04-08 16:48:12.894,True,2014-09-01,SuperGlue,PUBLIC,,True,Viral Communications,False
wall-of-now,weller,False,"<p>Wall of Now is a multi-dimensional media browser of recent news items. It attempts to address our need to know everything by presenting a deliberately overwhelming amount of media, while simplifying the categorization of the content into single entities. Every column in the wall represents a different type of entity: people, countries, states, companies, and organizations. Each column contains the top-trending stories of that type in the last 24 hours. Pressing on an entity will reveal a stream of video that relates to that specific entity. The Wall of Now is a single-view experience that challenges previous perceptions of screen space utilization towards a future of extremely large, high-resolution displays.</p>",,--Choose Location,2019-04-05 20:29:14.403,True,2015-01-01,Wall of Now,PUBLIC,,True,Viral Communications,False
snapshot-expose,picard,False,"<p>We are applying learnings from the SNAPSHOT study to the problem of changing behavior, exploring the design of user-centered tools which can harness the experience of collecting and reflecting on personal data to promote healthy behaviors--including stress management and sleep regularity. We draw on commonly used theories of behavior change as the inspiration for distinct conceptual designs for a behavior-change application based on the SNAPSHOT study. This approach will enable us to compare the types of visualization strategies that are most meaningful and useful for acting on each theory.</p>",2016-12-01,--Choose Location,2018-03-08 15:48:06.097,True,2015-01-01,SNAPSHOT Expose,PUBLIC,,False,Affective Computing,False
objective-asessment-of-depression-and-its-improvement,picard,False,"<p>Current methods to assess depression and then ultimately select appropriate treatment have many limitations. They are usually based on having a clinician rate scales, which were developed in the 1960s. Their main drawbacks are lack of objectivity, being symptom-based and not preventative, and requiring accurate communication. This work explores new technology to assess depression, including its increase or decrease, in an automatic, more objective, pre-symptomatic, and cost-effective way using wearable sensors and smart phones for 24/7 monitoring of different personal parameters such as physiological data, voice characteristics, sleep, and social interaction. We aim to enable early diagnosis of depression, prevention of depression, assessment of depression for people who cannot communicate, better assignment of a treatment, early detection of treatment remission and response, and anticipation of post-treatment relapse or recovery.</p>",2019-01-01,--Choose Location,2018-05-01 21:02:35.703,True,2014-01-01,Objective Asessment of Depression and its Improvement,PUBLIC,,False,Affective Computing,False
smiletracker,picard,False,"<p>SmileTracker is a system designed to capture naturally occurring instances of positive emotion during the course of normal interaction with a computer. A facial expression recognition algorithm is applied to images captured with the user's webcam. When the user smiles, both a photo and a screenshot are recorded and saved to the user's profile for later review. Based on positive psychology research, we hypothesize that the act of reviewing content that led to smiles will improve positive affect, and consequently, overall wellbeing.</p>",2015-09-01,--Choose Location,2016-12-05 00:17:02.371,True,2014-09-01,SmileTracker,PUBLIC,,False,Affective Computing,False
unlocking-sleep,picard,False,"<p>Despite a vast body of knowledge about the importance of sleep, exercise, and healthy eating, our daily schedules are often planned around work and social events, not  healthy behaviors. We're prompted to plan&nbsp;throughout the day by devices and people, and we think about our schedules in terms of things to do; but sleep is rarely considered until we're tired and it's late. This project proposes a way that our everyday use of technology can help improve sleep habits and estimate mood.</p><p>A smartphone's unlock screen is an unobtrusive way of prompting user reflection throughout the day by posing ""microquestions"" as users unlock their phone. The questions are easily answered with a single swipe. Since we unlock our phones 50 to 200 times per day, microquestions can collect information with minimal intrusiveness to the user's daily life.&nbsp;</p><p>Can these swipe-questions help users mentally plan their day around wellbeing, and trigger healthier behaviors?</p>",2018-01-01,--Choose Location,2018-05-06 22:38:11.891,True,2015-01-01,Unlocking Wellbeing,LAB-INSIDERS,,False,Affective Computing,False
valinor-mathematical-models-to-understand-and-predict-self-harm,picard,False,"<p>We are developing statistical tools for understanding, modeling, and predicting self-harm by using advanced probabilistic graphical models and fail-soft machine learning in collaboration with Harvard University and Microsoft Research.</p>",,--Choose Location,2019-04-19 17:37:54.819,True,2014-01-01,Valinor: Mathematical models to understand and predict self-harm,PUBLIC,,True,Affective Computing,False
predicting-bonding,picard,False,"<p>We show that using thin slices (&lt; 1 minute) of facial expression and body language data, we can train a deep neural network to predict whether two people in a conversation will bond with each other. Bonding is measured using the Bonding subscale of the Working Alliance Inventory. We show that participants who experience bonding perceive their conversational partner as interesting, charming, and friendly, and do not perceive them as distant or annoying. </p><p>The data are collected from a user study of naturalistic conversations, in which participants were asked to interact for 20 minutes, and were recorded using cameras, microphones, and Microsoft Kinects. To ensure participants did not become self-conscious of their non-verbal cues, they were told the purpose of the study was to train machine learning algorithms to read lips. </p><p>We show that not only can we accurately predict bonding from participants' personality, disposition, and traits, but that we can predict whether the participant will experience bonding up to 20 minutes later, using only one-minute thin slices of facial expression and body language data. This ability could be extremely useful to an intelligent virtual agent, because if it could detect at one-minute intervals whether it was bonding with its user, it could make course corrections to promote enjoyment and foster bonding. We provide an analysis of the facial expression and body language cues associated with higher bonding, and show how this information could be used by an agent to synthesize the appropriate non-verbal cues during conversation.</p>",,,2019-02-08 16:15:38.454,True,2015-10-01,Predicting Bonding in Conversations,PUBLIC,,True,Affective Computing,False
improving-sleep-wake-schedule-using-sleep-behavior-visualization-and-a-bedtime-alarm,picard,False,"<p><a href=""http://dl.acm.org/citation.cfm?id=2897442.2897469"">Humans need sleep</a>, along with food, water, and oxygen, to survive. With about one-third of our lives spent sleeping, there has been increased attention and interest in understanding sleep and the overall state of our ""sleep health."" The rapid adoption of smartphones, along with a growing number of sleep tracking applications for these devices, presents an opportunity to use phones to encourage better sleep hygiene. Procrastinating going to bed and being unable to stick to a consistent bedtime can lead to inadequate sleep time, which in turn affects quality of life and overall wellbeing. To help address this problem, we developed two applications, Lights Out and Sleep Wallpaper, which provide a sensor-based bedtime alarm and a connected peripheral display on the wallpaper of the user's mobile phone to promote awareness with sleep data visualization.</p>",,--Choose Location,2016-12-05 17:35:08.047,True,2015-09-01,Improving Sleep-Wake Schedule Using Sleep Behavior Visualization and a Bedtime Alarm,PUBLIC,,True,Affective Computing,False
an-eeg-and-motion-capture-based-expressive-music-interface-for-affective-neurofeedback,picard,False,"<p>This project examines how the expression granted by new musical interfaces can be harnessed to create positive changes in health and wellbeing. We are conducting experiments to measure EEG dynamics and physical movements performed by participants who are using software designed to invite physical and musical expression of the basic emotions. The present demonstration of this system incorporates an expressive gesture sonification system using a Leap Motion device, paired with an ambient music engine controlled by EEG-based affective indices. Our intention is to better understand affective engagement, by creating both a new musical interface to invite it, and a method to measure and monitor it. We are exploring the use of this device and protocol in therapeutic settings in which mood recognition and regulation are a primary goal.</p>",,--Choose Location,2019-04-19 14:56:44.273,True,2014-09-01,An EEG and motion-capture based expressive music interface for affective neurofeedback,PUBLIC,,True,Affective Computing,False
eda-explorer,picard,False,"<p>Electrodermal Activity (EDA) is a physiological indicator of stress and strong emotion. While an increasing number of wearable devices can collect EDA, analyzing the data to obtain reliable estimates of stress and emotion remains a difficult problem. We have built a graphical tool that allows anyone to upload their EDA data and analyze it. Using a highly accurate machine learning algorithm, we can automatically detect noise within the data. We can also detect skin conductance responses, which are spikes in the signal indicating a ""fight or flight"" response. Users can visualize these results and download files containing features calculated on the data to be used in their own analysis. Those interested in machine learning can also view and label their data to train a machine learning classifier. We are currently adding active learning, so the site can intelligently select the fewest possible samples for the user to label. </p>",,--Choose Location,2016-12-05 00:17:11.264,True,2015-01-01,EDA Explorer,PUBLIC,http://eda-explorer.media.mit.edu/,True,Affective Computing,False
learning-via-social-awareness-improving-sketch-representations-with-facial-feedback,picard,False,"<p>In the quest towards general artificial intelligence (AI), researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards. This paper argues that such research has overlooked an important and useful intrinsic motivator: social interaction. We posit that making an AI agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations, and could potentially impact AI safety. We collect social feedback in the form of facial expression reactions to samples from Sketch RNN, an LSTM-based variational autoencoder (VAE) designed to produce sketch drawings. We use a Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small group of viewers, and then show in an independent evaluation with 76 users that this model produced sketches that lead to significantly more positive facial expressions. Thus, we establish that implicit social feedback can improve the output of a deep learning model.</p>",,,2019-04-19 17:14:55.804,True,2018-03-20,Learning via Social Awareness: Improving sketch representations with facial feedback,PUBLIC,,True,Affective Computing,False
modulating-peripheral-and-cortical-arousal-using-a-musical-motor-response-task,picard,False,"<p>We are conducting EEG studies to identify the musical features and musical interaction patterns that universally impact measures of arousal. We hypothesize that we can induce states of high and low arousal using electrodermal activity (EDA) biofeedback, and that these states will produce correlated differences in concurrently recorded skin conductance and EEG data, establishing a connection between peripherally recorded physiological arousal and cortical arousal as revealed in EEG. We also hypothesize that manipulation of musical features of a computer-generated musical stimulus track will produce changes in peripheral and cortical arousal. These musical stimuli and programmed interactions may be incorporated into music technology therapy, designed to reduce arousal or increase learning capability by increasing attention. We aim to provide a framework for the neural basis of emotion-cognition integration of learning that may shed light on education and possible applications to improve learning by emotion regulation.</p>",,--Choose Location,2019-04-19 17:19:56.174,True,2015-09-01,Modulating peripheral and cortical arousal using a musical motor response task,PUBLIC,,True,Affective Computing,False
deep-gif,picard,False,"<p>
                    Animated GIFs are widely used on the Internet to express emotions, but automatic analysis of their content is largely unexplored. To help with the search and recommendation of GIFs, we aim to predict &nbsp;how their emotions will be perceived by humans based on their content. Since previous solutions to this problem only utilize image-based features and lose all the motion information, we propose to use 3D convolutional neural networks (CNNs) to extract spatiotemporal features from GIFs. We evaluate our methodology on a crowdsourcing platform called GIFGIF with more than 6,000 animated GIFs, and achieve better accuracy than any previous approach in predicting crowdsourced intensity scores of 17 emotions. We have also found that our trained model can be used to distinguish and cluster emotions in terms of valence and risk perception.</p>",,,2019-04-19 17:22:36.655,True,2016-10-18,Predicting perceived emotions in animated GIFs with 3D convolutional neural networks,PUBLIC,,True,Affective Computing,False
predicting-students-wellbeing-from-physiology-phone-mobility-and-behavioral-data,picard,False,"<p>The goal of this project is to apply machine learning methods to model the wellbeing of MIT undergraduate students. Extensive data is obtained from the SNAPSHOT study, which monitors participating students on a 24/7 basis, collecting data on their location, sleep schedule, phone and SMS communications, academics, social networks, and even physiological markers like skin conductance, skin temperature, and acceleration.&nbsp;</p><p>We extract features from this data and apply a variety of machine learning algorithms, including Gaussian mixture models and Multi-task Multi-Kernel Learning; we are currently working to apply Bayesian hierarchical multi-task learning and Deep Learning as well.</p><p>Interesting findings include: when participants visit novel locations they tend to be happier; when they use their phones or stay indoors for long periods they tend to be unhappy; and when several dimensions of wellbeing (including stress, happiness, health, and energy) are learned together, classification accuracy improves. The biggest classification accuracy improvements come when we use multi-tasking algorithms to leverage group data while customizing a model for each participant.</p>",,--Choose Location,2019-04-19 17:23:56.302,True,2015-01-01,"Predicting students' wellbeing from physiology, phone, mobility, and behavioral data",PUBLIC,,True,Affective Computing,False
real-time-assessment-of-suicidal-thoughts-and-behaviors,picard,False,"<p>Depression correlated with anxiety is one of the key factors leading to suicidal behavior, and is among the leading causes of death worldwide. Despite the scope and seriousness of suicidal thoughts and behaviors, we know surprisingly little about what suicidal thoughts look like in nature (e.g., How frequent, intense, and persistent are they among those who have them? What cognitive, affective/physiological, behavioral, and social factors trigger their occurrence?). The reason for this lack of information is that historically researchers have used retrospective self-report to measure suicidal thoughts, and have lacked the tools to measure them as they naturally occur. In this work we explore use of wearable devices and smartphones to identify behavioral, affective, and physiological predictors of suicidal thoughts and behaviors.</p>",,--Choose Location,2019-04-19 17:25:15.168,True,2015-01-01,Real-time assessment of suicidal thoughts and behaviors,PUBLIC,,True,Affective Computing,False
the-challenge,picard,False,"<p>Mental wellbeing is intimately tied to both social support and physical activity. The Challenge is a tool aimed at promoting social connections and decreasing sedentary activity in a workplace environment. Our system asks participants to sign up for short physical challenges and pairs them with a partner to perform the activity. Social obligation and social consensus are leveraged to promote participation. Two experiments were conducted in which participants' overall activity levels were monitored with a fitness tracker. In the first study, we show that the system can improve users' physical activity, decrease sedentary time, and promote social connection. As part of the second study, we provide a detailed social network analysis of the participants, demonstrating that users' physical activity and participation depends strongly on their social community.</p>",,--Choose Location,2016-12-05 00:16:31.390,True,2015-01-01,The Challenge,PUBLIC,,True,Affective Computing,False
the-entrain-study,picard,False,"<p>Individuals with autism are known to have difficulties connecting with other people, reciprocating social interactions, and being emotionally regulated by others. Yet, until recently, very little attention has been given to the way people interact together, in a system, rather than by themselves. We propose a new way to collect data on how caregivers and their children, with and without autism, affect and are affected by each other (i.e., how they ""sync up"" with one another), both in their behavior and in their physiology. We also introduce a customizable digital-physical smart toy platform that will allow us to test hypotheses and collect data about patterns of caregiver-child synchrony in a naturalistic and engaging environment. MIT and Northeastern are forging a new collaboration between smart toy technology and autism research that will help uncover how the social brain develops.</p>",,--Choose Location,2019-04-19 17:28:26.918,True,2016-09-01,The enTRAIN Study: Physiological synchrony in children with autism,PUBLIC,,True,Affective Computing,False
affective-response-to-haptic-signals,picard,False,"<p>This study attempts to examine humans' affective responses to superimposed sinusoidal signals. These signals can be perceived either through sound, in the case of electronically synthesized musical notes, or through vibro-tactile stimulation, in the case of vibrations produced by vibrotactile actuators. This study is concerned with the perception of superimposed vibrations, whereby two or more sinusoisal signals are perceived simultaneously, producing a perceptual impression that is substantially different than of each signal alone, owing to the interactions between perceived sinusoidal vibrations that give rise to a unified percept of a sinusoidal chord. The theory of interval affect was derived from systematic analyses of Indian, Chinese, Greek, and Arabic music theory and tradition, and proposes a universal organization of affective response to intervals organized using a multidimensional system. We hypothesize that this interval affect system is multi-modal and will transfer to the vibrotactile domain.</p>",,--Choose Location,2016-12-05 00:17:06.039,True,2015-09-01,Affective Response to Haptic Signals,PUBLIC,,True,Affective Computing,False
large-scale-pulse-analysis,picard,False,"<p>This study aims to bring objective measurement to the multiple ""pulse"" and ""pulse-like"" measures made by practitioners of traditional Chinese medicine (TCM). The measurements are traditionally made by manually palpitating the patient's inner wrist in multiple places, and relating the sensed responses to various medical conditions. Our project brings several new kinds of objective measurement to this practice, compares their efficacy, and examines the connection of the measured data to various other measures of health and stress. Our approach includes the possibility of building a smartwatch application that can analyze stress and health information from the point of view of TCM.</p>",,--Choose Location,2019-04-19 17:36:58.241,True,2015-09-01,Traditional Chinese medicine-inspired pulse analysis,PUBLIC,,True,Affective Computing,False
wavelet-based-motion-artifact-removal-for-electrodermal-activity,picard,False,"<p>Electrodermal activity (EDA) recording is a powerful, widely used tool for monitoring psychological or physiological arousal. However, analysis of EDA is hampered by its sensitivity to motion artifacts. We propose a method for removing motion artifacts from EDA, measured as skin conductance (SC), using a stationary wavelet transform (SWT). We modeled the wavelet coefficients as a Gaussian mixture distribution corresponding to the underlying skin conductance level (SCL) and skin conductance responses (SCRs). The goodness-of-fit of the model was validated on ambulatory SC data. We evaluated the proposed method in comparison with three previous approaches. Our method achieved a greater reduction of artifacts while retaining motion-artifact-free data.</p>",,--Choose Location,2019-04-19 17:39:31.969,True,2015-01-01,Wavelet-based motion artifact removal for electrodermal activity,PUBLIC,,True,Affective Computing,False
worldbeat,picard,False,"<p>Can we modulate the way we hear the world around us to make it more calming or to induce focus? </p><p>While technology is usually associated with causing stress, technology also&nbsp;has the potential to bring about calm. In particular, breathing usually&nbsp;speeds up with higher stress, but it can be slowed through a manipulation,&nbsp;and in so doing, it can help the person calm down. We are exploring a range&nbsp;of interventions to influence breathing without requiring any focused&nbsp;attention in order to be effective. In multiple projects, we have looked at&nbsp;dynamic composition of music, modulation of screen brightness, and headphone volume to create a seamless pulsating behavior, similar to breathing biofeedback, to indirectly influence breathing. Our preliminary analyses show promising results that such seamless modulation indeed have an influence on breathing rate and pattern.</p><p>In this project, we explore modulating insertion gain on a headphone in harmony with affective signals, particularly breathing rate. We study the influence of this dynamic change between “inside” and “outside” sources of sound to induce a sense of calmness. We experiment in simulated environments that resemble different situations such as a library, a busy street, and a fireplace.</p><p>We would like to thank Dan Gauger for giving us equipment and his thoughtful suggestions, including the project name. We would also like to thank Bose for making this project happen.&nbsp;</p>",,,2019-04-19 17:40:48.922,True,2017-12-01,WorldBeat: Hearing the world differently,PUBLIC,http://www.ghandeharioun.com,True,Affective Computing,False
brightbeat,picard,False,"<p>The relationship between breathing and self-reported stress is bidirectional. Respiration pattern is an indicator of stress, but it can also be manipulated to induce calmness.&nbsp;</p><p>In this project we explore this relationship via novel means of interaction. BrightBeat is a set of seamless visual, auditory, and tactile interventions that mimic a calming breathing oscillation, with the aim of influencing physiological syncing and consequently bringing a sense of focus and calmness.&nbsp;</p><p>The animation above shows an exaggerated version of BrightBeat. These interventions are designed to run easily on commonplace personal electronic devices, respect the user's privacy, and not to require constant focus or attention in order to be effective.&nbsp;<br></p>",,--Choose Location,2019-04-19 14:59:31.588,True,2015-09-01,BrightBeat: Effortlessly influencing breathing for cultivating calmness and focus,PUBLIC,,True,Affective Computing,False
automated-tongue-analysis,picard,False,"<p>A common practice in Traditional Chinese Medicine (TCM) is visual examination of the patient's tongue. This study will examine ways to make this process more objective and to test its efficacy for understanding stress- and health-related changes in people over time. We start by developing an app that makes it comfortable and easy for people to collect tongue data in daily life together with other stress- and health-related information. We will obtain assessment from expert practitioners of TCM, and also use pattern analysis and machine learning to attempt to create state-of-the-art algorithms able to help provide better insights for health and prevention of sickness.</p>",,--Choose Location,2017-09-21 15:02:57.578,True,2015-09-01,Automated Tongue Analysis,PUBLIC,,True,Affective Computing,False
behavioral-indications-of-depression-severity,picard,False,"<p>In collaboration with Massachusetts General Hospital, we are conducting a clinical trial exploring objective methods for assessing depression and its severity.&nbsp;</p><p>We are challenging the assessment methods that were created decades ago and which rely mostly on self-reported measures. We are including information from wearable sensors and regular sensors in mobile phones to collect information about sleep, social interaction, and location changes to find behavioral patterns that are associated with depressive symptoms. 
                    
                </p>",,,2018-04-09 01:37:18.568,True,2015-01-01,Behavioral Indications of Depression Severity,PUBLIC,,True,Affective Computing,False
relational-ai,picard,False,"<h2>Creating long-term interpersonal interaction and shared experiences with social robots&nbsp;<br></h2><p>Many of our current projects explore the use of social robots as a technology to support young children's early language development. In this project, instead of focusing on <i>how</i> to make social robots effective as an educational tools, we ask <i>why</i> they are effective. Based on our prior work, we hypothesize that a key aspect of why social robots can benefit children's learning is their nature as a<i> relational technology</i>—that is, a technology that can build long-term, social-emotional relationships with users. </p><p>Thus, in this project, our goals are twofold. First, we aim to understand how children conceptualize social robots as relational agents in learning contexts, and how children relate to these robots through time. Second, we explore the core nature of autonomous relational technologies, that is, relational AI. We will examine how adding features of relational AI to a social robot impacts longitudinal child-robot learning interactions, including children's learning, engagement, and relationships.</p><p>As part of this project, we are taking a second look at work we have done so far, this time through the lens of children's relationships. We are creating assessments for measuring young children's relationships. We are developing a computational relational AI model, which we will test during a longitudinal study with a social robot.</p><p><a href=""https://www.media.mit.edu/posts/making-new-robot-friends/"">Read more about children's relationships with robots here!</a><br></p>",,,2018-10-19 15:23:25.170,True,2016-09-01,Relational AI,PUBLIC,,True,Affective Computing,False
lensing-cardiolinguistics-for-atypical-angina,picard,False,"<p>Conversations between two individuals—whether between doctor and patient, mental health therapist and client, or between two people romantically involved with each other—are complex. Each participant contributes to the conversation using her or his own ""lens."" This project involves advanced probabilistic graphical models to statistically extract and model these dual lenses across large datasets of real-world conversations, with applications that can improve crisis and psychotherapy counseling and patient-cardiologist consultations. We're working with top psychologists, cardiologists, and crisis counseling centers in the United States.</p>",,--Choose Location,2019-04-18 03:29:15.605,True,2014-01-01,Lensing: Cardiolinguistics for Atypical Angina,PUBLIC,,True,Affective Computing,False
kind-and-grateful-promoting-kindness-and-gratitude-with-pervasive-technology,picard,False,"<p>We have designed a novel system to promote kindness and gratitude. We leverage pervasive technologies to naturally embed gratitude inspiration in everyday life. Mobile sensor data is utilized to infer optimal moments for stimulating contextually relevant thankfulness and appreciation. We analyze the interplay between mood, contextual cues, and gratitude expressions.</p>",,--Choose Location,2018-10-22 19:48:44.018,True,2015-01-01,"""Kind and Grateful"": Promoting kindness and gratitude with pervasive technology",PUBLIC,,True,Affective Computing,False
quantifyme,picard,False,"<p>Unlike traditional randomized controlled trials that generalize relationships in large groups of people, single-case experiments seek to quantify an individual's reaction to an intervention by measuring an independent variable's effect on a dependent variable (i.e., an intervention's effect on an outcome behavior). These single-case experiments are then combined back together using Bayesian Statistics methods in order to learn more general patterns about a population. We are interested in single-case experiments that test the causal relationships between behaviors that have been observed to be correlated with higher wellbeing.</p><p>Thus, instead of using an RCT to find what works for the imaginary ""average"" person, we can learn what works for each individual and then carefully combine data to generalize the results to other real individuals.</p><p>To our knowledge, single-case experiments have not been implemented in a smartphone app format. We believe that a successful app will allow researchers to dramatically scale the number of participants in these studies.</p><p>Code available on <a href=""https://github.com/mitmedialab/AffectiveComputingQuantifyMeAndroid"">GitHub</a>!&nbsp;</p>",,,2018-05-01 18:27:59.835,True,2016-06-01,QuantifyMe,PUBLIC,,True,Affective Computing,False
physio-freefall,picard,False,"<p>This project seeks to examine the effects of altered gravity on an individual’s physiology during parabolic flight. Specifically, we will collect flight participants’ heart rate, heart rate variability, breathing rate, skin temperature, and skin conductance measurements using wearable, wireless sensors in order to determine the response of these biosignals to zero/hyper/microgravity and feelings of nausea. </p><p>The results of this research will have both significant scientific and civilian value. To our knowledge, this experiment will be the first to investigate the new Multiple Arousal Theory in the context of motion sickness, as well as altered gravity. This theory was developed in the Affective Computing group at the MIT Media Lab and examines asymmetry in skin conductance signals from right and left wrists as differing metrics of emotional arousal and intensity. The parabolic flight configuration provides an inimitable circumstance to systematically analyze the evolution of these signals over the course of the repeated parabolic flight path. For example, we expect to see globally heightened stress and emotional arousal on the first pass, with maximal skin conductance peaks from both wrists just before the first moment of weightlessness. We expect these peaks to monotonically decrease over time with each pass, but to remain more elevated (relative to an individual’s baseline) for participants experiencing more self-reported nausea during flight. For individuals not experiencing extreme nausea, we expect to see a much higher skin conductance signal from their right wrists compared to their left (for right-handed participants) during the first few passes, with this difference decreasing steadily as the participant habituates to the flight pattern and sensations. </p><p>Note that NASA and other researchers—including the Boston-local scientists at the Ashton Graybiel Spatial Orientation Lab at Brandeis University—have investigated spatial orientation and motion sickness, but they are just beginning to add the use of physiological sensors to their work. Not only does this demonstrate that the proposed experiment is at the forefront of scientific inquiry, but it also facilitates potential collaboration with world-renowned experts in the Boston area!</p><p>In addition to sensor data, we intend to collect pre- and post-flight surveys recording participant reactions to different levels of gravity, including points at which they experienced nausea or discomfort. Pre-flight surveys will include nausea sensitivity metrics, designed to determine how likely a person is to feel nausea (i.e., separating those who feel carsick on a drive through town versus those who approach rollercoasters without hesitation). It will also ask about each participant’s feelings of anxiety, nausea, and excitement in anticipation of flying. Note that while these feelings may be experienced simultaneously, each one has a different effect on one’s physiology. </p><p>After the flight, we will ask participants to rank which sections of the flight (e.g., beginning, middle, end) prompted the greatest sensations of anxiety, nausea, and excitement and to what degree. We will also annotate the flight video recordings to denote periods of high anxiety, nausea, or excitement.</p><p>Then, we will use the survey, annotation, and sensor information to build a model that predicts when an individual might experience distress in altered gravity environments. This aspect of the study will leverage our research group’s unique expertise building machine learning algorithms for physiological data, but the results could have widespread impact. For example, such a system could be deployed to space travelers to help them monitor their physiology and anticipate or prevent feelings of discomfort during flight. As access to space travel becomes more pervasive, it is critical to understand the physiological effects of altered gravity on a population that does not solely include astronauts or specially trained individuals. Our models, along with the use of low-cost, commercially available sensors, would enable “space hacking” by tourists and other non-technical personnel, allowing them to measure and track their biosignals to achieve optimal wellness during space travel.&nbsp;</p>",,,2018-10-22 19:51:21.995,True,2017-05-11,Physio FreeFall,PUBLIC,,True,Affective Computing,False
human-adherence,picard,False,"<p>The Guardians project aims to use the same game design principles used in mobile game platforms to create greater engagement with individuals. The Affective Computing group is developing a custom video game with an independent patient reporting outcome tool to increase adherence to completion of&nbsp;patient reported outcomes.</p><p>Forming positive health habits can be difficult. Whether it’s taking medication, sticking to a diet, or going to the gym, it’s tough to commit to a new schedule long enough to form a habit. It is even more difficult when a person is asked to do something regularly that does not directly and immediately benefit them. This is an&nbsp;issue when clinical researchers need study participants to report outcomes regularly over a long period of time. Adherence is lost, resulting in suboptimal clinical outcomes and the loss of important data.</p><p>Mobile video&nbsp;games, on the other hand, generate an increased amount of adherence (<a href=""https://venturebeat.com/2017/02/01/superdata-mobile-games-hit-40-6-billion-in-2016-matching-world-box-office-numbers/"">as seen by an&nbsp;estimated market revenue of over $40.6 billion in 2016</a>**). Mobile video games have captured the attention of a wide variety of demographics and are often targeted to specific subgroups in order to increase engagement with a number of in-game features. These games use common design techniques and mechanics to produce a loop that draws players to return&nbsp;on a regular schedule and encourages them to watch ads, share on social media, or pay a fee for special rewards within the game.</p><p>By using the same game design principles, we aim to replace typical video game behaviors, like watching ads or sharing on social, with new behaviors that help improve the player’s wellbeing.</p><p>This project is a collaboration between the Affective Computing&nbsp;research group and Media Lab member company Takeda Pharmaceuticals.</p><p>**according to research by SuperData Research and Unity Technologies</p>",,,2019-05-16 15:17:42.342,True,2017-04-01,The Guardians,PUBLIC,,True,Affective Computing,False
emotion-recognition-in-context,picard,False,"<p>The goal of this project is providing machines with the ability of understanding what a person is experiencing from her frame of reference, taking into account the scene context: where is this person, what is this person doing, how does this person look, etc.&nbsp;</p><p>You can find more information about this project on this <a href=""http://sunai.uoc.edu/emotic/"">website</a>.</p>",,,2018-10-20 16:59:10.142,True,2017-01-02,Emotion Recognition in Scene Context,PUBLIC,,True,Affective Computing,False
personalized-machine-learning-for-future-health,picard,False,"<p>The view on Alzheimer’s Disease (AD) diagnosis has shifted towards a more dynamic process in which clinical and pathological markers evolve gradually before diagnostic criteria are met. Given the&nbsp;wide variability in data available per subject, inherent per-person differences, and the slowly changing nature of the disease, accurate prediction of AD progression is a significant, difficult challenge. The goal of this project is to devise novel Personalized Machine Learning Models that can accurately capture future changes in the key biomarkers and cognitive scores related to AD and other neurological conditions. As the basis for our framework, we use the&nbsp;Alzheimer’s Disease Neuroimaging Initiative (ADNI)&nbsp;dataset–the largest publicly available dataset&nbsp; for AD research.&nbsp;&nbsp;These data are highly heterogeneous and multi-modal, and include imaging (MRI, PET), cognitive scores, CSF biomarkers, genetics, and demographics (e.g. age, gender, race). The developed models are the break-through in machine learning for health-care as they allow&nbsp; personalized forecasting of the diseases' progression - in contrast to the traditional ""one-size-fits-all"" approaches. This capability is of&nbsp; great importance to both clinicians and those at risk of AD since it is critical to early identification of at-risk subjects, construction of informative clinical trials, and timely detection of AD.</p>",,,2019-02-14 19:49:51.362,True,2017-11-01,Personalized Machine Learning for Future Health,PUBLIC,,True,Affective Computing,False
engageme,picard,False,"<h2>EngageME: Personalized machine learning and humanoid robots for measuring affect and engagement of children with autism</h2><p>EngageME is a project aimed at building a new technology to enable automatic monitoring of affect and engagement of children with ASC (Autism Spectrum Conditions) in communication-centered activities.</p><p>This <a href=""https://www.media.mit.edu/publications/personalized-machine-learning-for-robot-perception-of-affect-and-engagement-in-autism-therapy/"">work</a> has been published in&nbsp;<a href=""http://robotics.sciencemag.org/content/3/19/eaao6760"">Science Robotics</a>, June 2018.</p>",,,2019-02-14 19:50:51.284,True,2016-10-01,Personalized Machine Learning for Autism Therapy,PUBLIC,,True,Affective Computing,False
emotional-navigation-system,picard,False,"<p>Before automobiles were invented and widely adopted, animals like horses were the most common mode of transportation. While this change brought significant improvements in terms of reliability and efficiency, it also removed a core component: the emotional relationship that existed between the person and the animal.</p><p>While largely ignored, the emotional states of drivers are quite important, as they influence not only driving behavior but also the safety of all road users. For instance, driving can be quite an emotionally stressful experience and, while certain amounts of stress help the driver to remain alert and attentive, too much or too little can negatively impact driving performance and safety. Furthermore, stress in large doses has been linked to a large array of adverse health conditions such as depression and various forms of cardiovascular disease.</p><p>The&nbsp;Emotion Navigation&nbsp;special interest group is led by Dr. Javier Hernandez with the goal of stimulating research efforts at the intersection of&nbsp;Automotive and&nbsp;Affective Computing.</p>",,,2019-04-19 19:34:00.344,True,2018-04-02,Emotion Navigation,PUBLIC,http://enavigation.media.mit.edu,True,Affective Computing,False
personlized-animation,picard,False,"<p>Storytelling is a fundamental way in which human beings understand the world. Imagine watching a movie telling the story of your life, how would you respond to it and how would it change your perception of your own memories? Personalized animated movies are generated from Unity, customized to each user's mood and behavior date collected through self-reports. Our study shows that personalized animations can elicit strong emotional responses from participants and lengthier writing of self-reflection compared to a non-personalized control. Moving forward, we're looking at using personalized animation to encourage cognitive reappraisal and positive thinking.<br></p>",,,2019-06-05 18:33:04.324,True,2016-10-01,Personalized Animated Movies,PUBLIC,,True,Affective Computing,False
spring,picard,False,"<p>SPRING is a custom-built hardware and software platform for children with neuro-differences. The system automates data acquisition, optimizes learning progressions, and encourages social, cognitive, and motor development in a positive, personalized, child-led play environment. The quantitative data and developmental trajectories captured by this platform enable systematic, mutli-modal, long-term studies of different therapeutic and educational approaches to autism and other developmental disorders, as well as a better understanding of motivation, engagement, and learning for the general population.</p>",,--Choose Location,2019-04-19 17:26:39.652,True,2016-01-01,"SPRING: A Smart Platform for Research, Intervention, and Neurodevelopmental Growth",PUBLIC,,True,Affective Computing,False
affective-network,picard,False,"<p><a href=""https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US"">Try Affective Network!</a></p><p><span style=""font-size: 18px;"">Emotional contagion in online social networks has been of great interest over the past years. Previous studies have mainly focused on finding evidence of affection contagion in homophilic atmospheres. However, these studies have overlooked users' awareness of the sentiments they share and consume online. In this work, we present an experiment with Twitter users that aims to help them better understand which emotions they experience on this social network. We introduce&nbsp;</span><a href=""https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US"" style=""font-size: 18px;""><b>Affective Network</b></a><span style=""font-size: 18px;"">&nbsp;(Aff-Net), a Google Chrome extension that enables Twitter users to filter and make explicit (through colored visual marks) the emotional content in their news feed.</span><br></p><p>The extension is powered by machine learning algorithms that classify tweets into different sentiment categories: positive posts tend to use happy or surprising language; negative posts tend to use sad, angry, or disgusting language; and posts without strong emotional language are classified as neutral.</p><p>Affective Network aims to help social media users better understand which emotions they tend to consume on social media, and how these emotions can spread through their social networks. It was built by researchers at the <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">Laboratory for Social Machines&nbsp;</a>and the&nbsp;<a href=""https://www.media.mit.edu/groups/affective-computing/overview/"">Affective Computing</a> group at the <a href=""https://www.media.mit.edu/"">MIT Media Lab</a>.</p><p>Note that Affective Network does not necessarily reflect the official position of the MIT Media Lab regarding the benefits and drawbacks of filtering out specific emotional content.</p><p><a href=""https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US"">Try Affective Network!</a></p>",,,2019-05-17 19:51:18.500,True,2018-12-01,Affective Network,PUBLIC,https://affectivenetwork.media.mit.edu,True,Affective Computing,False
adaptive-music-for-affect-improvement,picard,False,<p>Adaptive Music for Affect Improvement (AMAI) is a music generation and playback system with the goal of steering the listener toward a state of more positive affect. AMAI utilizes techniques from game music in order to adjust elements of the music being heard; such adjustments are made adaptively in response to the valence levels of the listener as measured via facial expression and emotion detection.</p>,,,2019-01-22 15:44:58.691,True,2017-10-01,Adaptive Music for Affect Improvement,PUBLIC,,True,Affective Computing,False
personalized-emotional-wellness-coach,picard,False,"<p>The diagnosis and tracking of mood disorders still rely on clinical assessments, originating more than 50 years ago, of self-reported depressive symptoms via surveys and interviews. Such methods are known to provide limited accuracy and reliability in addition to being costly to track and scale. Once a problem is detected, providing personalized daily intervention and support is also too costly and does not scale. The goal of this pilot project is to develop a proof of concept of personalized emotional wellness coach focusing on key technology modules to create an emotionally intelligent social robot for this targeted domain. We shall also conduct a pilot evaluation with a five-week user study to evaluate the robot coach with respect to its ability to successfully sustain a user long-term adherence (i.e., daily self-report and efficacious advice)—with the expected result that it is more effective than state-of-the-art, gamified mobile apps currently used.&nbsp;</p>",2019-09-30,,2019-04-22 15:26:39.440,True,2018-09-01,Personalized Emotional Wellness Coach,PUBLIC,,True,Affective Computing,False
elsa,picard,False,"<h2><b><i>What is ELSA?</i></b></h2><p>ELSA is an AI-powered chatbot that acts as an empathetic companion, encouraging users to talk about their day through a form of interactive journaling.</p><p>You can try some of the current ELSA bots in this online&nbsp;<a href=""http://elsaneural.net"">demo</a>.&nbsp;</p><h2><b><i>How does ELSA work?</i></b></h2><p>Our project goal is to build a more empathetic neural network conversational AI by incorporating a deeper understanding of both the affective content of the conversation and the topic.&nbsp; More specifically, we build hierarchical recurrent neural network models that can converse like people &nbsp;and use transfer learning of topic and emotional tone recognition models to improve our final model.</p><h2><b><i>What are the applications of ELSA?</i></b></h2><p>Beyond the development of chatbots that act as an empathetic companion, we have a more ambitious and longer term goal: deploy the empathetic companion bots to support mental health.&nbsp; In particular, &nbsp;we aim to make ELSA useful for:</p><ul><li>Eliciting journaling</li><li>Suggesting behavioura interventions</li><li>Using Cognition Behavioral Therapy</li><li>Detecting individuals at risk of depression or suicide</li></ul><h2><b><i>Work in progress</i></b></h2><p>ELSA is a recently started project in the Affective Computing group. You can see an example of ELSA bot conversations below. You can also try our online <a href=""http://elsaneural.net"">demo</a>. &nbsp;&nbsp;</p>",,,2019-04-22 19:06:31.702,True,2019-03-03,"ELSA: Empathy learning, socially-aware agents",PUBLIC,,True,Affective Computing,False
brainbeat,picard,False,"<p>Can we sonfiy calming breathing and passively influence a state of calm?</p><p>Deep breathing has been scientifically proven to affect the heart, brain, digestive system, and the immune system. We believe designing a technology to promote deep breathing can facilitate transition into a calm state.&nbsp;</p><p>Nowadays, many people are spending a significant amount of time listening to music while working or studying. This makes music a good means for providing auditory breathing cues. While it has been shown that liminal auditory cues can be effective in encouraging a healthy breathing pattern, we are examining the use of subliminal encouragement of &nbsp;breathing modulation using music. Auditory ambient feedback has long been studied and is proved to be effective. It has been explored in concert settings, interactive installations, and smartphone applications. However, our aim is to design an intervention that is unobtrusive and doesn't keep people from doing their primary work. </p><p>In order to find the best auditory feedback design, we have designed a controlled study comparing an interactive rhythmic ambient music track that responds to a user's current breathing patterns to a fixed rate music track whose speed of playback is pegged to a rate slightly below the user's natural resting breathing rate. A control condition with no music is also included. We will compare the resulting breathing patterns, heart rate, EEG signals, and self-reported measures to determine if the ambient music feedback has any effect on the user's state of mind and body. If successful, a musical system to subliminally encourage calming breathing patterns may be integrated into workplace environments, hospitals, and other places where it is necessary to promote less stressful and healthy environments.</p><p>Our preliminary results significant shift in multiple physiological measures that indicate a state of calmness.</p>",,,2019-04-23 04:44:31.353,True,2016-12-01,BrainBeat: Breath-based music therapy,PUBLIC,,True,Affective Computing,False
improving-well-being-prediction-performance-using-temporal-machine-learning-models,picard,False,"<p>This project aims to improve the prediction accuracy of wellbeing (stress, mood, and health levels) using temporal machine learning models. We extend our previous approach using Long Short-Term Memory models and time series data from the <a href=""https://www.media.mit.edu/projects/snapshot-study/overview/"">SNAPSHOT study</a>. In addition, we consider adaptive methods to fill in missing data with time series information. We also develop the model using modifiable behavioral features such as bedtime, and examine how these contribute to wellbeing, so that people can get better control over how to improve their personal well-being.</p>",,,2019-05-24 18:28:52.701,True,2018-04-02,Improving wellbeing prediction performance using temporal machine learning models,PUBLIC,,True,Affective Computing,False
snapshot-expose,sataylor,False,"<p>We are applying learnings from the SNAPSHOT study to the problem of changing behavior, exploring the design of user-centered tools which can harness the experience of collecting and reflecting on personal data to promote healthy behaviors--including stress management and sleep regularity. We draw on commonly used theories of behavior change as the inspiration for distinct conceptual designs for a behavior-change application based on the SNAPSHOT study. This approach will enable us to compare the types of visualization strategies that are most meaningful and useful for acting on each theory.</p>",2016-12-01,--Choose Location,2018-03-08 15:48:06.097,True,2015-01-01,SNAPSHOT Expose,PUBLIC,,False,Affective Computing,False
unlocking-sleep,sataylor,False,"<p>Despite a vast body of knowledge about the importance of sleep, exercise, and healthy eating, our daily schedules are often planned around work and social events, not  healthy behaviors. We're prompted to plan&nbsp;throughout the day by devices and people, and we think about our schedules in terms of things to do; but sleep is rarely considered until we're tired and it's late. This project proposes a way that our everyday use of technology can help improve sleep habits and estimate mood.</p><p>A smartphone's unlock screen is an unobtrusive way of prompting user reflection throughout the day by posing ""microquestions"" as users unlock their phone. The questions are easily answered with a single swipe. Since we unlock our phones 50 to 200 times per day, microquestions can collect information with minimal intrusiveness to the user's daily life.&nbsp;</p><p>Can these swipe-questions help users mentally plan their day around wellbeing, and trigger healthier behaviors?</p>",2018-01-01,--Choose Location,2018-05-06 22:38:11.891,True,2015-01-01,Unlocking Wellbeing,LAB-INSIDERS,,False,Affective Computing,False
eda-explorer,sataylor,False,"<p>Electrodermal Activity (EDA) is a physiological indicator of stress and strong emotion. While an increasing number of wearable devices can collect EDA, analyzing the data to obtain reliable estimates of stress and emotion remains a difficult problem. We have built a graphical tool that allows anyone to upload their EDA data and analyze it. Using a highly accurate machine learning algorithm, we can automatically detect noise within the data. We can also detect skin conductance responses, which are spikes in the signal indicating a ""fight or flight"" response. Users can visualize these results and download files containing features calculated on the data to be used in their own analysis. Those interested in machine learning can also view and label their data to train a machine learning classifier. We are currently adding active learning, so the site can intelligently select the fewest possible samples for the user to label. </p>",,--Choose Location,2016-12-05 00:17:11.264,True,2015-01-01,EDA Explorer,PUBLIC,http://eda-explorer.media.mit.edu/,True,Affective Computing,False
predicting-students-wellbeing-from-physiology-phone-mobility-and-behavioral-data,sataylor,False,"<p>The goal of this project is to apply machine learning methods to model the wellbeing of MIT undergraduate students. Extensive data is obtained from the SNAPSHOT study, which monitors participating students on a 24/7 basis, collecting data on their location, sleep schedule, phone and SMS communications, academics, social networks, and even physiological markers like skin conductance, skin temperature, and acceleration.&nbsp;</p><p>We extract features from this data and apply a variety of machine learning algorithms, including Gaussian mixture models and Multi-task Multi-Kernel Learning; we are currently working to apply Bayesian hierarchical multi-task learning and Deep Learning as well.</p><p>Interesting findings include: when participants visit novel locations they tend to be happier; when they use their phones or stay indoors for long periods they tend to be unhappy; and when several dimensions of wellbeing (including stress, happiness, health, and energy) are learned together, classification accuracy improves. The biggest classification accuracy improvements come when we use multi-tasking algorithms to leverage group data while customizing a model for each participant.</p>",,--Choose Location,2019-04-19 17:23:56.302,True,2015-01-01,"Predicting students' wellbeing from physiology, phone, mobility, and behavioral data",PUBLIC,,True,Affective Computing,False
wavelet-based-motion-artifact-removal-for-electrodermal-activity,sataylor,False,"<p>Electrodermal activity (EDA) recording is a powerful, widely used tool for monitoring psychological or physiological arousal. However, analysis of EDA is hampered by its sensitivity to motion artifacts. We propose a method for removing motion artifacts from EDA, measured as skin conductance (SC), using a stationary wavelet transform (SWT). We modeled the wavelet coefficients as a Gaussian mixture distribution corresponding to the underlying skin conductance level (SCL) and skin conductance responses (SCRs). The goodness-of-fit of the model was validated on ambulatory SC data. We evaluated the proposed method in comparison with three previous approaches. Our method achieved a greater reduction of artifacts while retaining motion-artifact-free data.</p>",,--Choose Location,2019-04-19 17:39:31.969,True,2015-01-01,Wavelet-based motion artifact removal for electrodermal activity,PUBLIC,,True,Affective Computing,False
kind-and-grateful-promoting-kindness-and-gratitude-with-pervasive-technology,sataylor,False,"<p>We have designed a novel system to promote kindness and gratitude. We leverage pervasive technologies to naturally embed gratitude inspiration in everyday life. Mobile sensor data is utilized to infer optimal moments for stimulating contextually relevant thankfulness and appreciation. We analyze the interplay between mood, contextual cues, and gratitude expressions.</p>",,--Choose Location,2018-10-22 19:48:44.018,True,2015-01-01,"""Kind and Grateful"": Promoting kindness and gratitude with pervasive technology",PUBLIC,,True,Affective Computing,False
quantifyme,sataylor,False,"<p>Unlike traditional randomized controlled trials that generalize relationships in large groups of people, single-case experiments seek to quantify an individual's reaction to an intervention by measuring an independent variable's effect on a dependent variable (i.e., an intervention's effect on an outcome behavior). These single-case experiments are then combined back together using Bayesian Statistics methods in order to learn more general patterns about a population. We are interested in single-case experiments that test the causal relationships between behaviors that have been observed to be correlated with higher wellbeing.</p><p>Thus, instead of using an RCT to find what works for the imaginary ""average"" person, we can learn what works for each individual and then carefully combine data to generalize the results to other real individuals.</p><p>To our knowledge, single-case experiments have not been implemented in a smartphone app format. We believe that a successful app will allow researchers to dramatically scale the number of participants in these studies.</p><p>Code available on <a href=""https://github.com/mitmedialab/AffectiveComputingQuantifyMeAndroid"">GitHub</a>!&nbsp;</p>",,,2018-05-01 18:27:59.835,True,2016-06-01,QuantifyMe,PUBLIC,,True,Affective Computing,False
physio-freefall,sataylor,False,"<p>This project seeks to examine the effects of altered gravity on an individual’s physiology during parabolic flight. Specifically, we will collect flight participants’ heart rate, heart rate variability, breathing rate, skin temperature, and skin conductance measurements using wearable, wireless sensors in order to determine the response of these biosignals to zero/hyper/microgravity and feelings of nausea. </p><p>The results of this research will have both significant scientific and civilian value. To our knowledge, this experiment will be the first to investigate the new Multiple Arousal Theory in the context of motion sickness, as well as altered gravity. This theory was developed in the Affective Computing group at the MIT Media Lab and examines asymmetry in skin conductance signals from right and left wrists as differing metrics of emotional arousal and intensity. The parabolic flight configuration provides an inimitable circumstance to systematically analyze the evolution of these signals over the course of the repeated parabolic flight path. For example, we expect to see globally heightened stress and emotional arousal on the first pass, with maximal skin conductance peaks from both wrists just before the first moment of weightlessness. We expect these peaks to monotonically decrease over time with each pass, but to remain more elevated (relative to an individual’s baseline) for participants experiencing more self-reported nausea during flight. For individuals not experiencing extreme nausea, we expect to see a much higher skin conductance signal from their right wrists compared to their left (for right-handed participants) during the first few passes, with this difference decreasing steadily as the participant habituates to the flight pattern and sensations. </p><p>Note that NASA and other researchers—including the Boston-local scientists at the Ashton Graybiel Spatial Orientation Lab at Brandeis University—have investigated spatial orientation and motion sickness, but they are just beginning to add the use of physiological sensors to their work. Not only does this demonstrate that the proposed experiment is at the forefront of scientific inquiry, but it also facilitates potential collaboration with world-renowned experts in the Boston area!</p><p>In addition to sensor data, we intend to collect pre- and post-flight surveys recording participant reactions to different levels of gravity, including points at which they experienced nausea or discomfort. Pre-flight surveys will include nausea sensitivity metrics, designed to determine how likely a person is to feel nausea (i.e., separating those who feel carsick on a drive through town versus those who approach rollercoasters without hesitation). It will also ask about each participant’s feelings of anxiety, nausea, and excitement in anticipation of flying. Note that while these feelings may be experienced simultaneously, each one has a different effect on one’s physiology. </p><p>After the flight, we will ask participants to rank which sections of the flight (e.g., beginning, middle, end) prompted the greatest sensations of anxiety, nausea, and excitement and to what degree. We will also annotate the flight video recordings to denote periods of high anxiety, nausea, or excitement.</p><p>Then, we will use the survey, annotation, and sensor information to build a model that predicts when an individual might experience distress in altered gravity environments. This aspect of the study will leverage our research group’s unique expertise building machine learning algorithms for physiological data, but the results could have widespread impact. For example, such a system could be deployed to space travelers to help them monitor their physiology and anticipate or prevent feelings of discomfort during flight. As access to space travel becomes more pervasive, it is critical to understand the physiological effects of altered gravity on a population that does not solely include astronauts or specially trained individuals. Our models, along with the use of low-cost, commercially available sensors, would enable “space hacking” by tourists and other non-technical personnel, allowing them to measure and track their biosignals to achieve optimal wellness during space travel.&nbsp;</p>",,,2018-10-22 19:51:21.995,True,2017-05-11,Physio FreeFall,PUBLIC,,True,Affective Computing,False
human-adherence,sataylor,False,"<p>The Guardians project aims to use the same game design principles used in mobile game platforms to create greater engagement with individuals. The Affective Computing group is developing a custom video game with an independent patient reporting outcome tool to increase adherence to completion of&nbsp;patient reported outcomes.</p><p>Forming positive health habits can be difficult. Whether it’s taking medication, sticking to a diet, or going to the gym, it’s tough to commit to a new schedule long enough to form a habit. It is even more difficult when a person is asked to do something regularly that does not directly and immediately benefit them. This is an&nbsp;issue when clinical researchers need study participants to report outcomes regularly over a long period of time. Adherence is lost, resulting in suboptimal clinical outcomes and the loss of important data.</p><p>Mobile video&nbsp;games, on the other hand, generate an increased amount of adherence (<a href=""https://venturebeat.com/2017/02/01/superdata-mobile-games-hit-40-6-billion-in-2016-matching-world-box-office-numbers/"">as seen by an&nbsp;estimated market revenue of over $40.6 billion in 2016</a>**). Mobile video games have captured the attention of a wide variety of demographics and are often targeted to specific subgroups in order to increase engagement with a number of in-game features. These games use common design techniques and mechanics to produce a loop that draws players to return&nbsp;on a regular schedule and encourages them to watch ads, share on social media, or pay a fee for special rewards within the game.</p><p>By using the same game design principles, we aim to replace typical video game behaviors, like watching ads or sharing on social, with new behaviors that help improve the player’s wellbeing.</p><p>This project is a collaboration between the Affective Computing&nbsp;research group and Media Lab member company Takeda Pharmaceuticals.</p><p>**according to research by SuperData Research and Unity Technologies</p>",,,2019-05-16 15:17:42.342,True,2017-04-01,The Guardians,PUBLIC,,True,Affective Computing,False
improving-well-being-prediction-performance-using-temporal-machine-learning-models,sataylor,False,"<p>This project aims to improve the prediction accuracy of wellbeing (stress, mood, and health levels) using temporal machine learning models. We extend our previous approach using Long Short-Term Memory models and time series data from the <a href=""https://www.media.mit.edu/projects/snapshot-study/overview/"">SNAPSHOT study</a>. In addition, we consider adaptive methods to fill in missing data with time series information. We also develop the model using modifiable behavioral features such as bedtime, and examine how these contribute to wellbeing, so that people can get better control over how to improve their personal well-being.</p>",,,2019-05-24 18:28:52.701,True,2018-04-02,Improving wellbeing prediction performance using temporal machine learning models,PUBLIC,,True,Affective Computing,False
snapshot-expose,akanes,False,"<p>We are applying learnings from the SNAPSHOT study to the problem of changing behavior, exploring the design of user-centered tools which can harness the experience of collecting and reflecting on personal data to promote healthy behaviors--including stress management and sleep regularity. We draw on commonly used theories of behavior change as the inspiration for distinct conceptual designs for a behavior-change application based on the SNAPSHOT study. This approach will enable us to compare the types of visualization strategies that are most meaningful and useful for acting on each theory.</p>",2016-12-01,--Choose Location,2018-03-08 15:48:06.097,True,2015-01-01,SNAPSHOT Expose,PUBLIC,,False,Affective Computing,False
eda-explorer,akanes,False,"<p>Electrodermal Activity (EDA) is a physiological indicator of stress and strong emotion. While an increasing number of wearable devices can collect EDA, analyzing the data to obtain reliable estimates of stress and emotion remains a difficult problem. We have built a graphical tool that allows anyone to upload their EDA data and analyze it. Using a highly accurate machine learning algorithm, we can automatically detect noise within the data. We can also detect skin conductance responses, which are spikes in the signal indicating a ""fight or flight"" response. Users can visualize these results and download files containing features calculated on the data to be used in their own analysis. Those interested in machine learning can also view and label their data to train a machine learning classifier. We are currently adding active learning, so the site can intelligently select the fewest possible samples for the user to label. </p>",,--Choose Location,2016-12-05 00:17:11.264,True,2015-01-01,EDA Explorer,PUBLIC,http://eda-explorer.media.mit.edu/,True,Affective Computing,False
predicting-students-wellbeing-from-physiology-phone-mobility-and-behavioral-data,akanes,False,"<p>The goal of this project is to apply machine learning methods to model the wellbeing of MIT undergraduate students. Extensive data is obtained from the SNAPSHOT study, which monitors participating students on a 24/7 basis, collecting data on their location, sleep schedule, phone and SMS communications, academics, social networks, and even physiological markers like skin conductance, skin temperature, and acceleration.&nbsp;</p><p>We extract features from this data and apply a variety of machine learning algorithms, including Gaussian mixture models and Multi-task Multi-Kernel Learning; we are currently working to apply Bayesian hierarchical multi-task learning and Deep Learning as well.</p><p>Interesting findings include: when participants visit novel locations they tend to be happier; when they use their phones or stay indoors for long periods they tend to be unhappy; and when several dimensions of wellbeing (including stress, happiness, health, and energy) are learned together, classification accuracy improves. The biggest classification accuracy improvements come when we use multi-tasking algorithms to leverage group data while customizing a model for each participant.</p>",,--Choose Location,2019-04-19 17:23:56.302,True,2015-01-01,"Predicting students' wellbeing from physiology, phone, mobility, and behavioral data",PUBLIC,,True,Affective Computing,False
large-scale-pulse-analysis,akanes,False,"<p>This study aims to bring objective measurement to the multiple ""pulse"" and ""pulse-like"" measures made by practitioners of traditional Chinese medicine (TCM). The measurements are traditionally made by manually palpitating the patient's inner wrist in multiple places, and relating the sensed responses to various medical conditions. Our project brings several new kinds of objective measurement to this practice, compares their efficacy, and examines the connection of the measured data to various other measures of health and stress. Our approach includes the possibility of building a smartwatch application that can analyze stress and health information from the point of view of TCM.</p>",,--Choose Location,2019-04-19 17:36:58.241,True,2015-09-01,Traditional Chinese medicine-inspired pulse analysis,PUBLIC,,True,Affective Computing,False
wavelet-based-motion-artifact-removal-for-electrodermal-activity,akanes,False,"<p>Electrodermal activity (EDA) recording is a powerful, widely used tool for monitoring psychological or physiological arousal. However, analysis of EDA is hampered by its sensitivity to motion artifacts. We propose a method for removing motion artifacts from EDA, measured as skin conductance (SC), using a stationary wavelet transform (SWT). We modeled the wavelet coefficients as a Gaussian mixture distribution corresponding to the underlying skin conductance level (SCL) and skin conductance responses (SCRs). The goodness-of-fit of the model was validated on ambulatory SC data. We evaluated the proposed method in comparison with three previous approaches. Our method achieved a greater reduction of artifacts while retaining motion-artifact-free data.</p>",,--Choose Location,2019-04-19 17:39:31.969,True,2015-01-01,Wavelet-based motion artifact removal for electrodermal activity,PUBLIC,,True,Affective Computing,False
automated-tongue-analysis,akanes,False,"<p>A common practice in Traditional Chinese Medicine (TCM) is visual examination of the patient's tongue. This study will examine ways to make this process more objective and to test its efficacy for understanding stress- and health-related changes in people over time. We start by developing an app that makes it comfortable and easy for people to collect tongue data in daily life together with other stress- and health-related information. We will obtain assessment from expert practitioners of TCM, and also use pattern analysis and machine learning to attempt to create state-of-the-art algorithms able to help provide better insights for health and prevention of sickness.</p>",,--Choose Location,2017-09-21 15:02:57.578,True,2015-09-01,Automated Tongue Analysis,PUBLIC,,True,Affective Computing,False
emotion-wellbeing-x-skincare-cosmetics,akanes,False,"<p>The project aims to investigate the relationships between emotion, wellbeing, skin, and skincare cosmetics.&nbsp;In our first study, we measured emotion/wellbeing, heart rate, and respiration using a mobile phone and a wearable sensor&nbsp;during consumer in-use test.</p>",,,2018-05-01 19:59:13.741,True,2017-11-01,Emotion/wellbeing x skincare cosmetics,PUBLIC,,True,Affective Computing,False
quantifyme,akanes,False,"<p>Unlike traditional randomized controlled trials that generalize relationships in large groups of people, single-case experiments seek to quantify an individual's reaction to an intervention by measuring an independent variable's effect on a dependent variable (i.e., an intervention's effect on an outcome behavior). These single-case experiments are then combined back together using Bayesian Statistics methods in order to learn more general patterns about a population. We are interested in single-case experiments that test the causal relationships between behaviors that have been observed to be correlated with higher wellbeing.</p><p>Thus, instead of using an RCT to find what works for the imaginary ""average"" person, we can learn what works for each individual and then carefully combine data to generalize the results to other real individuals.</p><p>To our knowledge, single-case experiments have not been implemented in a smartphone app format. We believe that a successful app will allow researchers to dramatically scale the number of participants in these studies.</p><p>Code available on <a href=""https://github.com/mitmedialab/AffectiveComputingQuantifyMeAndroid"">GitHub</a>!&nbsp;</p>",,,2018-05-01 18:27:59.835,True,2016-06-01,QuantifyMe,PUBLIC,,True,Affective Computing,False
improving-well-being-prediction-performance-using-temporal-machine-learning-models,akanes,False,"<p>This project aims to improve the prediction accuracy of wellbeing (stress, mood, and health levels) using temporal machine learning models. We extend our previous approach using Long Short-Term Memory models and time series data from the <a href=""https://www.media.mit.edu/projects/snapshot-study/overview/"">SNAPSHOT study</a>. In addition, we consider adaptive methods to fill in missing data with time series information. We also develop the model using modifiable behavioral features such as bedtime, and examine how these contribute to wellbeing, so that people can get better control over how to improve their personal well-being.</p>",,,2019-05-24 18:28:52.701,True,2018-04-02,Improving wellbeing prediction performance using temporal machine learning models,PUBLIC,,True,Affective Computing,False
powers-sensor-chair,tod,False,"<p>The Powers Sensor Chair gives visitors a special glimpse into Tod Machover�s robotic opera ""Death and the Powers,"" by providing a new way to explore the sonic world of the opera. A solo participant sitting in a chair discovers that when she moves her hands and arms, the air in front of her becomes an instrument. With a small, delicate gesture, a sharp and energetic thrust of her hand, or a smooth caress of the space around her, she can use her expressive movement and gesture to play with and sculpt a rich sound environment drawn from the opera, including vocal outbursts and murmurs and the sounds of the show�s special Hyperinstruments. This installation explores the body as a subtle and powerful instrument, providing continuous control of continuous expression, and incorporates alum Elena Jessop�s high-level analysis frameworks for recognition and extension of expressive movement.</p>",2014-09-01,--Choose Location,2016-12-05 00:16:45.403,True,2014-01-01,Powers Sensor Chair,PUBLIC,,False,Opera of the Future,False
mmodm,tod,False,"<p>MMODM is an online drum machine based on the Twitter streaming API, using tweets from around the world to create and perform musical sequences together in real time. Users anywhere can express 16-beat note sequences across 26 different instruments, using plain-text tweets from any device. Meanwhile, users on the site itself can use the graphical interface to locally DJ the rhythm, filters, and sequence blending. By harnessing this duo of website and Twitter network, MMODM enables a whole new scale of synchronous musical collaboration between users locally, remotely, across a wide variety of computing devices, and across a variety of cultures.</p>",2017-06-01,--Choose Location,2017-05-17 22:49:11.048,True,2015-09-01,MMODM: Massively Multiplayer Online Drum Machine,PUBLIC,http://mmodm.co/,False,Opera of the Future,False
fragile-instruments,tod,False,"<p>We introduce a family of fragile electronic musical instruments designed to be “played” through the act of destruction. Each Fragile Instrument consists of an analog synthesizing circuit with embedded sensors that detect the destruction of an outer shell, which is destroyed and replaced for each performance. Destruction plays an integral role in both the spectacle and the generated sounds.</p>",2017-12-01,,2017-05-17 23:43:56.207,True,2016-06-01,Fragile Instruments,PUBLIC,,False,Opera of the Future,False
kinephone,tod,False,"<p>This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.</p>",2016-08-01,,2017-05-18 01:07:33.691,True,2016-05-01,Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display,PUBLIC,,False,Opera of the Future,False
maestro-myth-exploring-the-impact-of-conducting-gestures-on-musicians-body-and-sounding-result,tod,False,"<p>Expert or fraud, the powerful person in front of an orchestra or choir attracts both hate and admiration. But what is the actual influence a conductor has on the musician and the sounding result? To throw light on the fundamental principles of this special gestural language, we try to prove a direct correlation between the conductor's gestures, muscle tension, and the physically measurable reactions of musicians in onset-precision, muscle tension, and sound quality. We also measure whether the mere form of these gestures causes different levels of stress or arousal. With this research we aim not only to contribute to the development of a theoretical framework on conducting, but also to enable a precise mapping of gestural parameters in order to develop and demonstrate a new system to the optional enhancement of musical learning, performance, and expression.</p>",2016-06-30,--Choose Location,2016-12-05 00:17:16.936,True,2015-01-01,Maestro Myth: Exploring the Impact of Conducting Gestures on Musician's Body and Sounding Result,PUBLIC,,False,Opera of the Future,False
sound-cycles,tod,False,"<p>Sound Cycles is a new interface for exploring, re-mixing, and composing with large volumes of audio content. The project presents a simple and intuitive interface for scanning through long audio files or pre-recorded music. Sound Cycles integrates with the existing Digital Audio Workstation for on-the-fly editing, audio analysis, and feature extraction.</p>",,--Choose Location,2016-12-05 00:17:04.683,True,2015-01-01,Sound Cycles,PUBLIC,,True,Opera of the Future,False
music-visualization-via-musical-information-retrieval,tod,False,"<p>In a study of human perception of music in relation to different representations of video graphics, this project explores the automatic synchronization in real time between audio and image. This aims to make the relationship seem smaller and more consistent. The connection is made using techniques that rely on audio signal processing to automatically extract data from the music, which subsequently are mapped to the visual objects. The visual elements are influenced by data obtained from various Musical Information Retrieval (MIR) techniques. By visualizing music, one can stimulate the nervous system to recognize different musical patterns and extract new features.</p>",,--Choose Location,2016-12-05 00:17:18.424,True,2015-01-01,Music Visualization via Musical Information Retrieval,PUBLIC,,True,Opera of the Future,False
breathing-window,tod,False,"<p>Breathing Window is a tool for non-verbal dialogue that reflects on your own breathing while also offering a window on another person's respiration. This prototype is an example of shared human experiences (SHEs) crafted to improve the quality of human understanding and interactions. Our work on SHEs focuses on first encounters with strangers. We meet strangers every day, and without prior background knowledge of the individual we often form opinions based on prejudices and differences. In this work, we bring respiration to the foreground as one common experience of all living creatures.</p>",,--Choose Location,2016-12-05 00:16:16.294,True,2015-01-01,Breathing Window,PUBLIC,,True,Opera of the Future,False
fablur,tod,False,"<p>Fablur explores the limit of the self in its relationship to others through the medium of clothing. The augmented gown uses a rear dome projection system on the surface of the fabric. The system comprises laser projectors and mirror structures talking wirelessly with a computer, within which is contained both content and warp projection mapping software. This novel technological interface presents both a performative element and a seamless integration in a woman's life experience. This wearable project questions the boundary between the self and others, the boundary between the individual and society, and the boundary between the body and nature.</p>",,--Choose Location,2016-12-05 00:16:26.876,True,2016-01-01,Fablur,PUBLIC,,True,Opera of the Future,False
sidr-deep-learning-based-real-time-speaker-identification,tod,False,"<p>Consider each of our individual voices as a flashlight to illuminate how we project ourselves in society and how much sonic space we give ourselves or others. Thus, turn-taking computation through speaker recognition systems has been used as a tool to understand social situations or work meetings. We present SIDR, a deep learning-based, real-time speaker recognition system designed to be used in real-world settings. The system is resilient to noise, and adapts to room acoustics, different languages, and overlapping dialogues. While existing systems require the use of several microphones for each speaker or the need to couple video and sound recordings for accurate recognition of a speaker, SIDR only requires a medium-quality microphone or computer-embedded microphone.</p>",,--Choose Location,2016-12-05 00:17:02.465,True,2016-01-01,SIDR: Deep Learning-Based Real-Time Speaker Identification,PUBLIC,,True,Opera of the Future,False
ambisonic-surround-sound-audio-compression,tod,False,"<p>Traditional music production and studio engineering depends on dynamic range compression audio signal processors that precisely and dynamically control the gain of an audio signal in the time domain. This project expands on the traditional dynamic range compression model by adding a spatial dimension. Ambisonic Compression allows audio engineers to dynamically control the spatial properties of a three-dimensional sound field, opening new possibilities for surround-sound design and spatial music performance.</p>",,--Choose Location,2019-04-17 19:58:29.909,True,2015-01-01,Ambisonic surround-sound audio compression,PUBLIC,,True,Opera of the Future,False
empathy-and-the-future-of-experience,tod,False,"<p>Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of–as well as long-term commitment to–empathic communication.</p>",,--Choose Location,2019-04-17 19:59:42.795,True,2015-01-01,Empathy and the future of experience,PUBLIC,,True,Opera of the Future,False
fensadense,tod,False,"<p>Fensadense is a new work for 10-piece ensemble composed by Tod Machover, commissioned for the Lucerne Festival in summer 2015. The project represents the next generation of hyperinstruments, involving the measurement of relative qualities of many performers where previous systems only looked at a single performer. Off-the-shelf components were used to collect data about movement and muscle tension of each musician. The data was analyzed using the Hyperproduction platform to create meaningful production control for lighting and sound systems based on the connection of the performers, with a focus on qualities such as momentum, connection, and tension of the ensemble as a whole. The project premiered at the Lucerne Festival, and a spring European tour just concluded this May 2016.</p><p class=""""><a style=""font-size: 18px; font-weight: normal;"" href=""http://garrettparrish.com/about/fensadense/"">Fensadense site</a><span style=""font-size: 18px; font-weight: normal;""> created by our former UROPer, Garrett Parrish.</span><br></p><p class="""">Listen to a complete recording of the Lucerne performance <a href=""http://www.wqxr.org/#!/story/listen-tod-machovers-fensadense-hyperinstruments-and-interactive-electronics/"">here</a>.<br></p>",,--Choose Location,2017-04-03 19:36:35.564,True,2015-01-01,Fensadense,PUBLIC,,True,Opera of the Future,False
learning-empathy,tod,False,"<p>As part of its broader work around learning, this project is exploring both individualized and community-based models for promoting empathy by designing training methods and developing complementary &nbsp;technologies. The initiative launched 20 Day Stranger app with Playful Systems . The initiative is also working with the Opera of the Future group on a Vocal Vibrations/ Finding Your Voice interactive exhibition targeted towards awareness, empathy, and empowerment. The exhibition debuted in Paris and Cambridge with plans for Mexico City in 2017.
                    
                </p>",,,2018-12-11 18:23:20.052,True,2017-05-01,"Strangers, Voices, and Society",PUBLIC,,True,Opera of the Future,False
philadelphia-voices,tod,False,"<p><i><a href=""https://citysymphonies.media.mit.edu/philadelphia.html"">Philadelphia Voices</a></i>&nbsp;is the latest in the series of <a href=""https://www.media.mit.edu/projects/city-symphonies-massive-musical-collaboration/overview/"">City Symphonies</a> projects that Tod Machover and the Opera of the Future group	have created since 2012. Previous City Symphonies have centered on&nbsp;Toronto, Edinburgh, Perth, Lucerne, and	Detroit.</p><p>Each project paints a musical portrait of a city—using “traditional” musical elements as well as real sounds recorded by residents—to portray the essence of their city's history and future. Everyone living in that city is invited to collaborate to create the symphony,	resulting in an unprecedented creative collaboration around music, sound, and storytelling.&nbsp;</p><p><i style=""font-size: 18px; font-weight: 400;"">Philadelphia Voices</i>&nbsp;has been in progress	since spring 2017 and will culminate in performances in Philadelphia (Kimmel Center) and New York (Carnegie Hall) in April	2018. A special mobile app has been developed to allow anyone with a smartphone to collect sounds and video and to upload those files to a communal database for listening and morphing.&nbsp;</p><p>Opera of the Future researchers have created new software that enables anyone to contribute their voice to a specially-designed sonic landscape from Philadelphia. Workshops and special activities have been organized	with local singers from every age and background, and Tod Machover has chosen several hundred of them to sing in the final performances with The Philadelphia Orchestra under the baton of its music director, Yannick Nézet-Séguin.&nbsp;</p><p>Since Philadelphia is considered the birthplace of&nbsp;	American democracy,&nbsp;<i style=""font-size: 18px; font-weight: 400;"">Philadelphia Voices</i>&nbsp;will investigate the current state of democracy from a Philly	perspective. The project will also consider the society in which we want to live, and what we are willing to do to achieve that ideal.</p>",,,2018-10-19 19:39:16.909,True,2017-10-13,Philadelphia Voices,PUBLIC,,True,Opera of the Future,False
symphony-for-the-koreas,tod,False,"<p><a href=""http://koreajoongangdaily.joins.com/news/article/article.aspx?aid=3060788&amp;cloc=joongangdaily%7Chome%7Conline""><i>Symphony for the Koreas</i></a> will be the latest installment of the celebrated <a href=""https://www.youtube.com/watch?v=Cmmk6hDj7do"">City Symphony</a> series. Over the next few years, Tod Machover and his team will collaborate with citizens from both South and North Korea to create a symphony that reflects what both sides have in common, where conflicts remain, and what might be effective, realistic, and peaceful ways to resolve conflicts through music. Through collaboration with the <a href=""http://lindenbaumschool.com/index.php/Main"">Lindenbaum</a> orchestra and festival, a South Korean organization dedicated to bringing peace to the Korean Peninsula through music, Machover and his team plan to invite musicians from both Koreas to participate in ongoing creative activities—as well as to live performances—of <i>Symphony for the Koreas</i>. The Lindenbaum organization has been granted unprecedented permission by the South Korean government to communicate and collaborate with the North Korean Government. It has also secured an MOU with the North Korean Ministry of Culture to hold a joint concert between the two Koreas. The final performance is expected to take place at or near the Korean Demilitarized Zone, and will then be toured worldwide.</p><br><p>Since 2012, Tod Machover and his Opera of the Future Group have created <a href=""https://citysymphonies.media.mit.edu/"">City Symphonies</a> for <a href=""https://www.nytimes.com/2018/04/09/arts/music/cheese-steak-philadelphia-orchestra-carnegie-hall.html"">Philadelphia</a>, <a href=""https://www.nytimes.com/2015/11/15/arts/music/detroit-symphony-tackles-an-adventurous-premiere.html"">Detroit</a>, Lucerne, Perth, Edinburgh, Miami and Toronto. In addition to <i>Symphony for the Koreas</i>, Machover is currently working on new City Symphonies for Chennai (India) and for Boston (in collaboration with <a href=""https://hubweek.org/"">HUBweek</a>).</p><br>",,,2019-04-18 14:21:02.865,True,2019-04-17,Symphony for the Koreas,PUBLIC,,True,Opera of the Future,False
schoenberg-in-hollywood,tod,False,"<p><i>“Can a man know the truth and tell it to the greatest number and still be misunderstood? Can one man be of the many and still be known?” </i></p><br><p><i>Schoenberg in Hollywood </i>is the most recent opera by composer-inventor-professor Tod Machover that explores the complex relationship between uncompromising art and mass appeal, and of whether—and how—art can change the world. Arnold Schoenberg was a man of extraordinary contradictions: now considered one of the twentieth century’s greatest composers, during much of his lifetime Schoenberg was known for—and excelled at—composing music hated by the public and critics; a man whose only compass was his pursuit of pure ideas, Schoenberg also yearned for popularity; Schoenberg’s music absorbs tradition, but it is not hampered by it and always points forward. What happened—and might have happened—when such an uncompromising spirit settled in Hollywood, the epicenter of American popular culture, after he fled Hitler's Europe in 1935? <br></p><p><i>Schoenberg in Hollywood </i>begins with a meeting (one that did occur in history) between the legendary producer Irving G. Thalberg of Metro Goldwyn Mayer and Arnold Schoenberg. Thalberg asks Schoenberg to compose music for a film based on Pearl S. Buck’s <i>The Good Earth</i>, a best-seller about the life of peasants in a Chinese village. Although Schoenberg disdained the idea of composing music to please the public, the prospect of writing a Hollywood film score that would reach millions appealed to him greatly. In reality, Schoenberg was not offered the job when he demanded a $50,000 fee—an astronomical sum at the time—from Thalberg. However, <i>Schoenberg in Hollywood</i> exploits—and explores—the hypothetical scenario of what would have happened if Schoenberg had indeed composed for Hollywood. In the opera, Schoenberg imagines the events of his life through the lens of different film genres: silents, noir mysteries, Disney cartoons, musicals, and Westerns, making the movie—and projecting his vision well into the future—that Hollywood never allowed him to do.</p><p>Commissioned and presented by Boston Lyric Opera, with much visionary  technology for sound, image, and staging created at the MIT Media Lab, <i>Schoenberg in Hollywood</i> is based on a scenario by the late Braham Murray, with a libretto by Simon Robson and directed by Karole Armitage. <i>Schoenberg in Hollywood</i> premiered at the Boston Lyric Opera in November 2018 and travels to the Vienna Volksoper in the 2019/2020 season.</p>",,,2019-04-18 00:45:48.433,True,2018-09-01,Schoenberg in Hollywood: A new opera by Tod Machover,PUBLIC,,True,Opera of the Future,False
quantify,kzh,False,"<p>QUANTIFY is a generalized framework and JavaScript library to allow rapid multi-dimensional ""measurement"" of subjective qualities of media. The goal is to make qualitative metrics quantized. For everything from measuring emotional responses of content to the cultural importance of world landmarks, QUANTIFY helps to elicit the raw human subjectivity that fills much of our lives, and makes it programmatically actionable. </p>",2015-09-01,--Choose Location,2016-12-05 00:16:46.569,True,2014-01-01,QUANTIFY,PUBLIC,,False,Collective Learning,False
fold,kzh,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Some readers require greater context to understand complex stories.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">FOLD (</span><a href=""http://fold.cm"" style=""font-size: 18px; font-weight: normal;"">fold.cm</a><span style=""font-size: 18px; font-weight: normal;"">) is an open publishing platform with a unique structure that lets writers link media cards to the text of their stories.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">Media cards can contain videos, maps, tweets, music, interactive visualizations, and more.&nbsp;</span></p><p>FOLD is used by journalists, educators, and storytellers around the world.&nbsp;<br></p>",,--Choose Location,2016-12-15 02:27:31.751,True,2014-01-01,FOLD,PUBLIC,,True,Collective Learning,False
gifgif,kzh,False,"<p>An animated GIF is a magical thing. It has the power to compactly convey emotion, empathy, and context in a subtle way that text or emoticons often miss. GIFGIF is a project to combine that magic with quantitative methods. Our goal is to create a tool that lets people explore the world of GIFs by the emotions they evoke, rather than by manually entered tags. A web site with 200,000 users maps the GIFs to an emotion space and lets you peruse them interactively.</p>",,--Choose Location,2016-12-14 14:00:48.619,True,2014-01-01,GIFGIF,PUBLIC,,True,Collective Learning,False
pubpub,kzh,False,"<p>PubPub reinvents publication to align with the way the web was designed: collaborative, evolving, and open. PubPub uses a graphical format that is deliberately simple and allows illustrations and text that are programs as well as static PDFs. The intention is to create an author-driven,  distributed alternative to academic journals that is tuned to the dynamic nature of many of our modern experiments and discoveries. It is optimized for public discussion and academic journals, and is being used for both.  It is equally useful for a newsroom to develop a story that is intended for both print and online distribution.</p>",,--Choose Location,2018-05-31 18:16:31.248,True,2015-01-01,PubPub,PUBLIC,,True,Collective Learning,False
dive,kzh,False,"<p><a href=""https://dive.media.mit.edu""><b>DIVE</b></a> is a web-based data exploration system that lets non-technical users create stories from their data without writing code.&nbsp;DIVE combines semantic data ingestion, recommendation-based visualization and analysis, and dynamic story sharing into a unified workflow.&nbsp;</p><p><b>Links</b><br></p><ul><li>Public version:&nbsp;<a href=""https://dive.media.mit.edu/"">dive.media.mit.edu</a></li><li>Demo video:&nbsp;<a href=""https://dive.media.mit.edu/video"">dive.media.mit.edu/video</a></li><li>Front-end repository:&nbsp;<a href=""https://github.com/MacroConnections/dive-frontend"">github.com/MacroConnections/dive-frontend</a></li><li>Back-end repository:&nbsp;<a href=""http://github.com/MacroConnections/dive-backend"">github.com/MacroConnections/dive-backend</a></li></ul>",,,2018-06-28 14:01:39.643,True,2015-01-01,DIVE,PUBLIC,,True,Collective Learning,False
quantify,trich,False,"<p>QUANTIFY is a generalized framework and JavaScript library to allow rapid multi-dimensional ""measurement"" of subjective qualities of media. The goal is to make qualitative metrics quantized. For everything from measuring emotional responses of content to the cultural importance of world landmarks, QUANTIFY helps to elicit the raw human subjectivity that fills much of our lives, and makes it programmatically actionable. </p>",2015-09-01,--Choose Location,2016-12-05 00:16:46.569,True,2014-01-01,QUANTIFY,PUBLIC,,False,Other,False
dbdb,trich,False,"<p>DbDb (pronounced DubDub) is a collaborative, visually based analysis and simulation platform. We promote open distribution of experimental data by allowing researchers to present a graphical representation of their data and processing techniques that collaborators can build on and augment. This helps test the reproducibility of results and allows others to learn and apply their own techniques. Our intention is for the research community as a whole to benefit from a growing body of open, analytical techniques. DbDb provides an interface for archiving data, executing code, and visualizing a tree of forked analyses. It is part of the Viral initiative on open, author-driven publishing, collaboration, and analysis. It is intended to be linked to PubPub, the main project.</p>",,--Choose Location,2016-12-05 00:16:20.776,True,2015-01-01,DbDb,PUBLIC,,True,Other,False
gifgif,trich,False,"<p>An animated GIF is a magical thing. It has the power to compactly convey emotion, empathy, and context in a subtle way that text or emoticons often miss. GIFGIF is a project to combine that magic with quantitative methods. Our goal is to create a tool that lets people explore the world of GIFs by the emotions they evoke, rather than by manually entered tags. A web site with 200,000 users maps the GIFs to an emotion space and lets you peruse them interactively.</p>",,--Choose Location,2016-12-14 14:00:48.619,True,2014-01-01,GIFGIF,PUBLIC,,True,Other,False
pubpub,trich,False,"<p>PubPub reinvents publication to align with the way the web was designed: collaborative, evolving, and open. PubPub uses a graphical format that is deliberately simple and allows illustrations and text that are programs as well as static PDFs. The intention is to create an author-driven,  distributed alternative to academic journals that is tuned to the dynamic nature of many of our modern experiments and discoveries. It is optimized for public discussion and academic journals, and is being used for both.  It is equally useful for a newsroom to develop a story that is intended for both print and online distribution.</p>",,--Choose Location,2018-05-31 18:16:31.248,True,2015-01-01,PubPub,PUBLIC,,True,Other,False
fiftynifty,trich,False,"<p>This is a grassroots challenge to get friends to participate in democracy by making calls to congresspeople in all 50 states. Live phone calls are the best way to directly express your opinion on an issue to your elected officials. Your mission is to pass message this along to friends who will make calls and also pass the message/link along to others who will do the same. It's a social chain letter and a call to action for a better participatory democracy. &nbsp;<span style=""font-size: 18px; font-weight: normal;"">We help you make your call and you pass on an invitation for your friends to do the same. Your invite can stress your opinion on a given issue.&nbsp;</span></p><p>The winners are the first ten chains to reach 50 states and accumulate the most challenge points. You get 250 points for making a call, 125 points for a call that your friend makes, 65 points for the call their friend makes, on and on. Everyone on the chain earns points. Points count for your first call to each of your two senators and your representative. You get a bonus for a ""grand slam""—a network that reaches all 435 representatives and 100 senators.</p><p>There is a leaderboard and a network view so you can track how you are doing. You can also see how much of the country your chain is covering.</p>",,,2019-06-04 20:46:21.258,True,2017-02-13,FiftyNifty,PUBLIC,https://fiftynifty.org,True,Other,False
mit-knowledge-futures-group,trich,False,"<p>The MIT Knowledge Futures Group (KFG), a joint venture of the MIT Media Lab and the MIT Press, is an incubator for early-stage technologies that form part of a new open knowledge ecosystem. The partnership is the first of its kind between a leading publisher and a world-class research lab designing technologies of the future. The KFG seeks to incubate projects that enrich our open knowledge infrastructure, and leading by example, to spark a movement towards greater institutional ownership of that infrastructure. <br></p><p>The KFG currently incubates PubPub, an open authoring and publishing platform initially developed as a Media Lab project, by deploying it with dozens of MIT Press books and journals. PubPub socializes the process of knowledge creation by integrating conversation, annotation, and versioning into short and long-form digital publication. One of the flagship publications on PubPub is the Journal of Design and Science, which forges new connections between science and design and breaks down the barriers between academic disciplines. We envision JoDS as the node in a global online community rooted in the Media Lab’s research and design ethos. </p><p>The KFG also incubates The Underlay, an open, distributed knowledge store architected to capture, connect, and archive publicly available knowledge and its provenance. The Underlay provides mechanisms for distilling the knowledge graph from openly available publications, along with the archival and access technology to make the data and content hosted on PubPub available to other platforms. <br></p>",,,2018-08-23 19:38:55.666,False,2018-08-01,MIT Knowledge Futures Group,PUBLIC,,True,Other,False
journal-of-open-exploration,trich,False,"<p>In collaboration with&nbsp;<a href=""https://www.media.mit.edu/groups/viral-communications/overview/"">Viral Communications</a>&nbsp;and the&nbsp;<a href=""https://www.media.mit.edu/groups/space-exploration/overview/"">Space Exploration initiative</a>, Open Ocean is using the&nbsp;<a href=""https://www.media.mit.edu/projects/pubpub/overview/"">PubPub</a>&nbsp;platform&nbsp; to launch the Journal of Open Exploration. We want to make the process and results of exploration collaborative, open, playful, and–most importantly–shared with a wider audience than traditional academic journals.</p>",,,2018-09-25 20:43:42.274,True,2018-02-26,Journal of Open Exploration,LAB,https://explore.pubpub.org/,True,Other,False
curious-social-robot,cynthiab,False,"<p>Can robots learn to be social? Can they do that in a structured way? This project uses the DragonBot platform and state-of-the-art artificial curiosity algorithms to explore the possibility of robots learning to behave socially, similar to children. The robot reacts to people and receives internal rewards whenever the social interaction succeeds. Initially, the robot learns which behavior best initiates social interaction and later learns which behavior maintains that interaction for the longest period. The goal is to build a brain-inspired hierarchical curiosity-driven social behavior architecture, in which the robot autonomously learns a growing repertoire of social skills.</p>",2015-08-31,--Choose Location,2017-05-31 18:36:24.278,True,2014-01-01,Curious Social Robot,PUBLIC,,False,Personal Robots,False
social-robot-as-a-younger-curious-peer,cynthiab,False,"<p>One of the most fundamental and important characteristics of children is their curiosity. Can an interaction with a robot elicit, guide, and promote curiosity? In this project we use the DragonBot platform and a tablet app in a language educational setting with children. We test the hypothesis that a personalized, curiosity-driven behavior of a robot behaving as a younger peer, can affect a child's own curiosity. We use an in-house developed app in which the child's interaction with figures is automatically transformed into a spoken and written story. The social robot reacts to the story in an emotional way and then asks the child questions about it. The child and robot then switch roles, so that the robot tells the story and the child asks questions. The research question is whether the robot's curiosity-driven behavior affects the child's curiosity.</p>",2017-05-15,--Choose Location,2017-05-31 18:41:35.438,True,2014-01-01,Social Robot as a Younger Curious Peer,PUBLIC,,False,Personal Robots,False
panda-parental-affective-natural-driver-assistant,cynthiab,False,"<p>Drivers spend a significant amount of time multi-tasking while they are behind the wheel, especially parent drivers that attend to child passengers. These dangerous behaviors, particularly texting while driving, can lead to distractions and ultimately to accidents. Many in-car interfaces do not assist the driver with the task of entertaining the passengers. In a collaboration with Volkswagen/Audi and the SENSEable City Lab, we are developing PANDA (parental affective natural driver assistant), a robotic driver-vehicle interface that acts as a sociable partner and assists parent drivers. PANDA elicits facial expressions for engaging social interaction with the driver and passengers. PANDA uses car entertainment to entertain and engage the children in educational games while in the car and frees the parent to focus on the task of driving.</p>",2015-05-01,--Choose Location,2017-05-31 18:41:51.451,True,2014-01-01,PANDA: Parental Affective Natural Driver Assistant,PUBLIC,,False,Personal Robots,False
electric-parrot,cynthiab,False,"<p>Can you feel empathy for a robot? The electric parrot project is about designing a robot that can engender empathy. For this we are constructing a novel zoomorphic robot that can create its own life story: that is to say it can experience the world, be changed by the experience, and communicate the experience to us. We aim to show through psychometric tests that giving the robot an implicit life story will invoke empathy. Such a robot can subsequently be used for empathy intervention. </p>",2016-08-31,--Choose Location,2017-05-31 18:44:43.634,True,2015-01-01,Electric Parrot,PUBLIC,,False,Personal Robots,False
share-understanding-and-manipulating-attention-using-social-robots,cynthiab,False,"<p>SHARE is a robotic cognitive architecture focused on manipulating and understanding the phenomenon of shared attention during interaction. SHARE incorporates new findings and research in the understanding of nonverbal referential gesture, visual attention system research, and interaction science. SHARE's research incorporates new measurement devices, advanced artificial neural circuits, and a robot that makes its own decisions.</p>",2017-05-31,--Choose Location,2017-10-15 17:57:01.084,True,2014-09-01,SHARE: Understanding and Manipulating Attention Using Social Robots,PUBLIC,,False,Personal Robots,False
interactive-journaling,cynthiab,False,"<p>We developed a smartphone application that detects users’ affect and provides personalized positive psychology interventions in order to enhance users’ psychological wellbeing. Users’ emotional states were measured by analyzing facial expressions and the sentiment of SMS messages. A virtual character in the application prompted users to verbally journal about their day by providing three positive psychology interventions. The system used a Markov Decision Process (MDP) model and a State-Action-Reward-State-Action (SARSA) algorithm to learn users’ preferences about the positive psychology interventions. Nine participants were recruited for an experimental study to test the application. They used it daily for three weeks. The interactive journaling activity increased participants’ arousal and valence levels immediately following each interaction, and we saw a trend toward improved self-acceptance levels over the three week period. The interaction duration increased significantly throughout the study as well. The qualitative analysis on journal entries showed that the application users explored and reflected on various aspects of themselves by looking at daily events, and found novel appreciation for and meanings in their daily routine.</p>",2016-12-31,--Choose Location,2017-06-05 16:12:47.957,True,2015-01-01,Interactive Journaling,PUBLIC,,False,Personal Robots,False
soro,cynthiab,False,<p>The Social Robot Toolkit aims to provide a platform for children to learn through playful interaction. The social robot (Soro) toolkit allows preschool children to experiment with computational concepts while teaching a social robot new rules. The toolkit also provides a platform for learning interpersonal skills through the use of storytelling that integrates interpersonal and computational concepts. This harnesses preschoolers' natural interest in social interaction to familiarize them with new concepts.</p>,2017-04-30,,2017-06-05 16:13:22.057,True,2014-08-23,Social Robot Toolkit,PUBLIC,,False,Personal Robots,False
robot-vocal-expressiveness,cynthiab,False,"<p>Prior research with preschool children has established that book reading, especially when children are encouraged to actively process the story materials through dialogic reading, is an effective method for expanding young children’s vocabulary.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">A growing body of research also suggests that social robots have potential as learning companions and tutors for young children’s early language education. Social robots are new technologies that combine the adaptability, customizability, and scalability of technology with the embodied, situated world in which we operate.</span></p><p>In this project, we asked whether a social robot can effectively engage preschoolers in dialogic reading. Given that past work has shown that children can and do learn new words from social robots, we investigate what factors modulate their learning. In particular, we looked at whether the verbal expressiveness of the robot impacted children’s learning and engagement during a dialogic reading activity.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">This project was funded by an NSF Cyberlearning grant.</span></p>",2017-09-01,,2019-03-02 01:42:38.931,True,2015-09-01,Robot Expressiveness Affects Children's Learning,PUBLIC,http://robotic.media.mit.edu/portfolio/robot-vocal-expressivity,False,Personal Robots,False
children,cynthiab,False,"<p>When learning from human partners, infants and young children will pay attention to nonverbal signals, such as gaze and bodily orientation, to figure out what a person is looking at and why. They may follow gaze to determine what object or event triggered another's emotion, or to learn about the goal of another's ongoing action. They also follow gaze in language learning, using the speaker's gaze to figure out what new objects are being referred to or named.</p><p>In this project, we examine whether young children will attend to the same social cues from a robot as from a human partner during a word learning task, specifically gaze and bodily orientation.</p>",2015-12-31,,2017-06-07 16:38:44.917,True,2014-09-01,Children Use Nonverbal Cues to Learn from Robots,PUBLIC,http://robotic.media.mit.edu/portfolio/children-use-nonverbal-cues-learn-robots/,False,Personal Robots,False
lightswarm,cynthiab,False,"<p>LightSwarm is a platform for interaction between humans and swarm robots. Swarm robots are implemented as augmented-reality agents that communicate and interact through movements. While each robot is simple, the aggregate shows complex behavior. In this way, LightSwarm invites one to think of the mind as a loose consensus of a society of agents, which allows for more nuanced interactions.</p>",2015-09-01,--Choose Location,2016-12-05 00:17:16.350,True,2014-01-01,LightSwarm,PUBLIC,,False,Personal Robots,False
designing-social-robots-for-older-adults,cynthiab,False,"<p>Most countries are projected to see the number of people ages 65 and older surpass the population under the age of 15 by 2050. The limitations of current solutions to assisting older adults, the increased social and emotional toll on caregivers, and the inability of institutions to create structural solutions in a timely manner calls for a paradigm shift in the way we approach aging.</p><p>As these new meanings of age, aged, and aging are re-negotiated at a personal and collective level, the <b>main goal of this research initiative is to&nbsp;study aging adults’ daily living assistance, social and emotional needs, and intergenerational connection</b> while exploring the optimized modalities for embodied agents to successfully deliver these interactions.&nbsp;We see embodied agents as a method to enable older adults to age-in-place, supporting them in ways such as promoting social connectedness, tracking vitals, coaching in emotional wellness, and assisting with medical adherence.</p><p>Our work is rooted in partnering with the community through co-design and participatory design methods to inform robot design by empowering older adults to engage in our research. We prioritize developing robot interactions that can be tested long-term in older adults’ homes to better inform how social robots can shape aging-in-place.<br></p><p>Currently, we are running a long-term codesign study with older adults. Over the course of the year, older adults will engage in interviews, interactive artwork, living with a robot, prototyping on a robot, and design guideline generation.&nbsp;</p><p>If you are 70 years of age or older and interested in participating in future study opportunities, please contact Anastasia Ostrowski (akostrow@media.mit.edu).</p>",,,2019-05-10 19:51:29.310,True,2017-06-01,Designing social robots for older adults,PUBLIC,,True,Personal Robots,False
data-of-children-storytelling,cynthiab,False,<p>Collecting real world data to understand social interactions!</p>,,,2018-01-22 06:31:46.892,True,2017-05-01,Data of Children Storytelling,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,False
jibo-research-platform,cynthiab,False,"<p>The Jibo Research Platform is an in-the-field deployable Social Robotics experimentation and data collection infrastructure. Built upon the world's first commercial social robot for the home, it extends Jibo's design, hardware, and data security for research purposes.</p>",,,2018-10-15 01:36:16.436,True,2018-08-01,Jibo Social Robotic Research Platform,PUBLIC,,True,Personal Robots,False
talking-machines-democratizing-the-design-of-voice-based-agents-for-the-home,cynthiab,False,"<p>Embodied voice-based agents, such as Amazon’s Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most, these agents represent their first experience of&nbsp;<i>living</i> with artificial intelligence in such private and personal spaces.&nbsp;</p><p>However, little is known about people’s desires, preferences, and boundaries for these technologies. This projects seeks to answer questions surrounding this space:&nbsp;<b>How do we live with voice-based agents in the home? How do different generations interact with voice-based agents? How should these technologies be designed to incorporate people’s preferences, desires, and boundaries? What tools can be used to understand this space?</b></p><p>This work presents insights from a long-term exploration with over 70 children, adults, and older adults over a one-year period to interact with, discover, experience, reflect upon, and design voice-based agents. In addition, design tools and learnings from the experience have been developed into an open-source design kit to enable designers and researchers to explore these ideas with the broader population.</p><p>For more information, please contact <b>Nikhita Singh (nikhita@media.mit.edu) </b>and <b>Anastasia Ostrowski (akostrow@media.mit.edu)</b>.</p>",,,2018-07-15 03:34:47.660,True,2017-09-01,Talking Machines: Democratizing the design of voice-based agents for the home,PUBLIC,,True,Personal Robots,False
relational-ai,cynthiab,False,"<h2>Creating long-term interpersonal interaction and shared experiences with social robots&nbsp;<br></h2><p>Many of our current projects explore the use of social robots as a technology to support young children's early language development. In this project, instead of focusing on <i>how</i> to make social robots effective as an educational tools, we ask <i>why</i> they are effective. Based on our prior work, we hypothesize that a key aspect of why social robots can benefit children's learning is their nature as a<i> relational technology</i>—that is, a technology that can build long-term, social-emotional relationships with users. </p><p>Thus, in this project, our goals are twofold. First, we aim to understand how children conceptualize social robots as relational agents in learning contexts, and how children relate to these robots through time. Second, we explore the core nature of autonomous relational technologies, that is, relational AI. We will examine how adding features of relational AI to a social robot impacts longitudinal child-robot learning interactions, including children's learning, engagement, and relationships.</p><p>As part of this project, we are taking a second look at work we have done so far, this time through the lens of children's relationships. We are creating assessments for measuring young children's relationships. We are developing a computational relational AI model, which we will test during a longitudinal study with a social robot.</p><p><a href=""https://www.media.mit.edu/posts/making-new-robot-friends/"">Read more about children's relationships with robots here!</a><br></p>",,,2018-10-19 15:23:25.170,True,2016-09-01,Relational AI,PUBLIC,,True,Personal Robots,False
robot-mindset-and-curiosity,cynthiab,False,"<h1>Young Learner's Companion&nbsp;</h1><h2>Developing robots' growth mindset and pro-curious behavior and fostering the same in young learners via long-term interaction</h2><p>A growth mindset and curiosity have significant impact on children's academic and social achievements. We are developing and evaluating a novel expressive cognitive-affective architecture that synergistically integrates models of curiosity, understanding of mindsets, and expressive social behaviors to advance the state-of the-art of robot companions. In doing so, we aim to contribute major advancements in the design of AI algorithms for artificial curiosity, artificial mindset, and their verbal and non-verbal expressiveness in a social robot companion for children. In our longitudinal study, we aim to evaluate the robot companion's ability to sustain engagement and promote children's curiosity and growth mindset for improved learning outcomes in an educational play context.<br></p>",,--Choose Location,2018-05-09 04:35:35.760,True,2015-09-01,Robot Mindset and Curiosity,PUBLIC,,True,Personal Robots,False
tega-a-new-robot-platform-for-long-term-interaction,cynthiab,False,"<p>Tega is a new robot platform designed to support long-term, in-home interactions with children, with applications in early-literacy education from vocabulary to storytelling.<br></p>",,--Choose Location,2018-02-02 12:45:47.861,True,2015-01-01,Tega: A New Social Robot Platform,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,False
artificial-listener-with-social-intelligence,cynthiab,False,<p>A social robot modifies its behavior to change what you think about it!</p>,,,2019-04-17 18:38:39.381,True,2017-05-01,Artificial listener with social intelligence,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,False
text-to-motion-expressive-robot-motion-sequencing,cynthiab,False,"<p><b>Text-to-Motion</b> generates a sequence of contingent robot animations to accompany the sentiment analyzed from an input sentence and its spoken audio. We trained a linear classifier to transfer learn&nbsp;our&nbsp;corpus of animated robot speech from DeepMoji network,&nbsp; a long short-term memory (LSTM) network with an attention model trained on billion tweets.&nbsp;</p>",,,2019-04-17 18:48:02.883,True,2018-01-28,Text-to-Motion: Automatic sequencing of animative robot motions,PUBLIC,,True,Personal Robots,False
realtime-detection-of-social-cues,cynthiab,False,"<h2>Realtime detection of social cues in children’s voices</h2><p>In everyday conversation, people use what are known as backchannels to signal to someone that they are still listening, paying attention, and engaged. As listeners, we smile, nod, and say “uh-huh” to convey attentiveness, and we do this naturally with little thought. We give this feedback not randomly but at certain moments in the conversation because speakers give off social cues that signal upcoming backchanneling opportunities.</p>",,,2018-05-07 17:34:51.716,True,2017-03-01,Realtime Detection of Social Cues,PUBLIC,http://www.haewonpark.com/,True,Personal Robots,False
intentional-inference-of-emotions,cynthiab,False,<p>Emotion recognition modeled as a goal-directed process!</p>,,,2017-10-16 14:31:54.022,True,2016-10-18,Intentional Inference of Emotions,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,False
pop-kit,cynthiab,False,"<h2>How can we add the missing ""T"" and ""E"" in preschool STEAM education?</h2>",,,2019-05-02 20:11:55.104,True,2016-10-01,PopBots: An early childhood AI curriculum,PUBLIC,,True,Personal Robots,False
relationship-assessments,cynthiab,False,"<p>Social robots are increasingly being developed for long-term interactions with children in domains such as healthcare, education, therapy, and entertainment.&nbsp;In prior research, we have seen that children treat robots as more than mere artifacts, e.g., ascribing them mental states, psychological attributes, and moral standing. Thus, while children’s relationships with robots may not be like the relationships they have with their parents, pets, imaginary friends, or smart devices, they will form relationships of some kind.&nbsp;As such, we need to deeply understand how children’s relationships with robots develop through time, and find ways to characterize and measure these relationships.&nbsp;However, there are few validated assessments for measuring young children’s long-term relationships. Thus, we have adapted or created a variety of assessments for use in this context for children aged 5-6 years.&nbsp;</p><p><a href=""https://dam-prod.media.mit.edu/x/2018/04/25/KoryWestlund-IDC18.pdf"">Four of these assessments are presented in the associated paper.</a></p><p>This paper shows that children can appropriately respond to these assessments with reasonably high internal reliability, and that these assessments are able to capture child-robot relationship adjustments&nbsp; over a long-term interaction.</p>",,,2019-04-17 18:39:40.951,True,2017-04-01,Assessing Children's Relationships with Social Robots,PUBLIC,,True,Personal Robots,False
bot-blocks,cynthiab,False,,,,2019-03-27 13:40:23.668,False,2018-01-05,Block-Based Programming for Robotics Education,PUBLIC,http://robotic.media.mit.edu/peple/randi-williams,True,Personal Robots,False
design-inquiry,cynthiab,False,"<p>Social robots are seen as a potential device to promote and enable older adults to age-in-place. The work here is a component of our long-term, co-design process with older adults happening in 2019.&nbsp; Through co-creating artworks of&nbsp; interactions with a robot with older adults, abstract technology concepts such as security and privacy, accountability, and autonomy are translated into icons that older adults can leverage to express their thoughts around various interactions. The interactions focused upon for this study include medical adherence, exercise and physical therapy, body signal monitoring, cognitive health monitoring, emotional wellness, social connectedness, and financial literacy.&nbsp;</p><p>In this component, participants create a self-representation and a representation of the robot using physical models. These models are then scanned into a digital space where older adults can scale the representations and add icons to represent their thoughts and desires around the interactions in the artwork.</p>",,,2019-04-18 14:28:23.440,True,2019-04-01,Design Inquiry: Translating abstract technology concepts to artwork,LAB-INSIDERS,,True,Personal Robots,False
personalized-interaction-for-language-learning,cynthiab,False,"<p>The process by which children learn native languages is markedly different from the process of learning a second, or non-native, language. Children are typically immersed in their native languages. They receive input from the adults and other children surrounding them, based on immediate need and interaction, during every waking hour. &nbsp;</p><p>Second language learners are exposed to input from the new language in very different ways, most commonly in a classroom setting.&nbsp;The second language learner relies heavily on memory skills with sparse interaction, in contrast to the first language learner that can rely on environmental reinforcement and social interaction to learn words.&nbsp;</p><p>Social robots have the potential to drastically improve on this paradigm, making the second-language learning experience more like the experience of learning a native language by engaging the child in a rich, interactive exposure to the target language, especially aspects not typically covered by traditional technological solutions, such as prosody, fundamental phonetics, common linguistic structures, etc.</p><p>Our project explores how to design child-robot &nbsp;interactions that encourage child-driven language learning, that adapt and personalize each child’s learning experience. We incorporate game design and machine learning into the child-robot interaction design. The child and robot play through a suite of educational games together. Using real-time sensor data and gameplay features, the robot constructs a model of each child's learning and emotional trajectory, then uses these models to inform its own decision making during the game. Thus, the robot's behaviors become personalized to individual children based on their learning style, personality and knowledge/emotional states during gameplay.&nbsp;</p><p></p><p></p><p></p><p></p><p></p><p></p>",,,2018-05-03 20:56:50.720,True,2017-01-01,Personalized  Interaction for Language Learning,PUBLIC,,True,Personal Robots,False
shaping-engagement,cynthiab,False,"<p>Voice-user interfaces (VUIs), such as Amazon Echo and&nbsp;Google Home, are increasingly becoming present in domestic&nbsp;environments. Users attribute agency and personality traits to these AI agents. Due to the social attributes of these technologies,&nbsp; users try to understand the agents' characteristics based on social norms. These factors affect user experience quality and overall engagement, which, when considering first experiences, can impact continuous usage and engagement with VUI technology.</p><p>Our work examines users’ first impressions and interactions&nbsp;with&nbsp; VUI agents, such as Google Home, Amazon Echo, and Jibo, with varying brands and modalities. Using personality and experience questionnaires, we seek to understand how VUI modalities, form, and personality affect engagement with VUIs.</p>",,,2018-10-09 20:17:18.440,True,2018-08-13,Engagement with Voice-User Interface Agents,PUBLIC,,True,Personal Robots,False
cognimates,cynthiab,False,"<p>Cognimates&nbsp;is  a platform where parents and children (7-10 years old) participate in creative programming activities in which they learn how to build games, program robots, and train their own AI models. Some of the activities are mediated by embodied intelligent agents which help learners scaffold learning and better collaborate. <a href=""http://cognimates.me/"">Learn more about our research, projects, and learning guides</a>.</p><p>Conversational agents and connected toys are becoming common in homes. Increasing exposure to ""intelligent"" technology raises important questions about the ways that children understand it and how they could learn&nbsp;with and from it. Embodied intelligent agents, such as social robots, afford longer-term engagement in the home for children and their families .&nbsp;&nbsp;</p><p>Building on the prior experience in the <a href=""http://www.media.mit.edu/groups/personal-robots"">Personal Robots</a> group of designing social robots for nurturing children's curiosity and learning, we built a platform where children and parents can learn to program with embodied intelligent agents which in turn become learning companions (Cognimates). The goal is to enable&nbsp; learners&nbsp; to interact with a social robot but also program it, train it to remember and learn things over time, and have&nbsp; reflective conversations with their peers prompted by it.&nbsp;</p><p>Why, how, and when can embodied intelligent agents support children and parents to learn via reflective teaching? What are the new intergenerational learning pathways that Cognimates could facilitate? How can these future learning companions be integrated into various learning applications and what are the generalizable design considerations?&nbsp; In this research project we are addressing these questions by allowing children and parents to use a visual programming interface to control and customize an embodied intelligent agent.&nbsp;</p><p><a href=""https://drive.google.com/file/d/0B8pX8mypq8MsUnJNbHNFUjI0RmM/view?usp=sharing"">Demo video</a></p>",,,2019-03-27 16:43:04.722,True,2017-11-12,Cognimates: Collaborative creative learning with embodied intelligent agents,PUBLIC,http://www.drugastefania.com/,True,Personal Robots,False
curious-learning,cynthiab,False,"<p>Early literacy plays an important role in a child's future. However, the reality is that over 57 million children have no access to a school and another 100 million attend such inadequate schools that they will remain functionally non-literate.</p><p>Curious Learning is an open platform that addresses the deployment and learning challenges faced by under-resourced communities, particularly their limited access to literacy instruction.</p><p>We are developing a system of early literacy apps, games, toys, and robots that will triage how children are learning, diagnose literacy deficits, and deploy dosages of content to encourage app play using a mentoring algorithm that recommends an appropriate activity given a child's progress. Currently, over 200 Android-based tablets have been sent to children around the world; these devices are instrumented to provide a very detailed picture of how kids are using these technologies. We are using this big data to discover usage and learning models that will inform future educational development.&nbsp; The open-source software enables any Android device to be transformed into a literacy mentor. This platform is presently deployed in Ethiopia, Uganda, India, South Africa, and rural United States.</p><p>The open-source tablet software enables data collection across the deployment sites.  By employing a data-driven approach to understanding learning behaviors across cultures and contexts, this project seeks to design and develop a personalized, adaptive learning platform.&nbsp;</p>",,,2019-04-17 18:41:03.019,True,2016-09-01,Curious Learning: Understanding learning behaviors for early literacy,PUBLIC,http://www.curiouslearning.org/,True,Personal Robots,False
collaborative-robot-storyteller,cynthiab,False,"<p>Could a social robot collaboratively exchange stories with children as a peer and help improve their linguistic and storytelling skills? Tega uses machine learning algorithms to learn actions that improve children's storytelling and keep them engaged. &nbsp;We are also interested in how Tega can personalize its interaction with each child over multiple encounters, because every child learns and engages differently.&nbsp;</p><p>In Spring 2017, Tega went to twelve preschool classrooms in the Greater Boston area for&nbsp;three months, pioneering the field of long-term human-robot interaction.&nbsp;Using Q-learning, a policy was trained to tell stories optimized for each child’s engagement and linguistic skill progression. Tega monitored children's affect signals and asked dialogic questions during storytelling to gauge their engagement. Tega also invited children to&nbsp;tell&nbsp;it stories, which Tega used to assess each child's linguistic skill development. Our results show robot's interaction policy indeed personalized to each child. At the end of the sessions, the policy significantly differed from one child to the other. Children who interacted and built relationships with a personalized robot showed higher engagement, learned and retained more vocabularies, and used more complex syntax structure in their speech compared to where they had started.</p>",,--Choose Location,2019-01-22 17:42:46.144,True,2015-09-01,Personalized Robot Storytelling Companion,PUBLIC,,True,Personal Robots,False
machine-behavior,cynthiab,False,"<p>Machines powered by artificial intelligence (AI) increasingly                           mediate our social, cultural, economic, and                           political interactions. Understanding the                           behavior of AI systems is essential to our                           ability to control their actions, reap their                           benefits, and minimize their harms. We argue                           this necessitates a broad scientific research                           agenda to study machine behavior that                           incorporates but expands beyond the discipline                           of computer science and requires insights from                           across the sciences. Here we first outline a                           set of questions fundamental to this emerging                           field. We then explore the technical, legal,                           and institutional constraints facing the study                           of machine behavior.</p>",,,2019-04-30 19:56:54.922,True,2019-04-24,Machine Behavior,PUBLIC,,True,Personal Robots,False
moodyboost,cynthiab,False,"<p>Major depressive disorder is the leading cause of disability in the US for ages 15-44 and affects 14.8 million American adults, or about 6.7 percent of the US population age 18 and older. However, individual psychotherapy is still not yet widely accessible to the majority of people who need psychological interventions, and are not available to patients on a daily basis.&nbsp;</p><p>In order to increase accessibility to therapeutic interventions, we are developing a social robot that provides daily positive psychology interventions to&nbsp;enhance people's psychological wellbeing at their home. Our system uses multi-modal data&nbsp;and a reinforcement learning approach to personalize the interventions for each user based on his/her preferences, affect, and engagement.&nbsp; In our one-month deployment study, we expect to find higher engagement and better wellbeing from participants who are provided personalized interventions than from participants with non-personalized interventions.&nbsp;&nbsp;</p>",,,2019-05-20 18:58:40.920,True,2018-06-01,MoodyBoost,LAB-INSIDERS,,True,Personal Robots,False
tools-to-investigate-societal-impacts-of-robot-ai,cynthiab,False,"<p>Artificial intelligence (AI) agents in an embodied form, such as Jibo, Amazon Alexa, and Google Home, are increasingly becoming part of our daily lives and our homes. While there have been numerous studies in lab settings documenting short-term individual interactions with intelligent agents, we are at a point where we need to be exploring the larger impact of these technologies in the world, living with real people over longer periods of time.</p><p>From a design research perspective, understanding and developing robots and AI that intersect with society is a “wicked problem,” a problem with many components that cannot be solved without interdisciplinary approaches. Design research within interdisciplinary applications has sought to develop approaches, methods, tools, and techniques to investigate the impact of technologies and inform future development. This work focuses on developing tools for exploring robots’ and AI’s impact on daily lives to better inform the&nbsp;development of these technologies by elucidating academia’s and industry’s requirements of tools for this domain.</p><p>For more information, please contact Anastasia Ostrowski (<a href=""mailto:akostrow@media.mit.edu"">akostrow@media.mit.edu</a>).</p>",,,2019-04-17 18:48:44.577,True,2018-07-01,Tools to investigate societal impacts of robots and AI,PUBLIC,,True,Personal Robots,False
p2pstory,cynthiab,False,"<p>Understanding social-emotional behaviors in storytelling interactions plays a critical role in the development of interactive and educational technologies for children. A challenge when designing for such interactions using technologies like social robots, virtual agents, and tablets is understanding the social-emotional behaviors pertinent to the storytelling context—especially when emulating a natural peer-to-peer relationship between the child and the technology.&nbsp;&nbsp;We present P2PSTORY, a dataset of young children (5-6 years old) engaging in natural peer-to-peer storytelling interactions with fellow classmates.&nbsp;The dataset contains 58 recorded storytelling sessions along with a diverse set of behavioral annotations as well as developmental and demographic profiles of each child participant.&nbsp;</p><p>The CHI 2018 paper presenting this dataset can be found here:&nbsp;<br><b><a href=""https://www.media.mit.edu/publications/p2pstory-dataset-of-children-storytelling-and-listening-in-peer-to-peer-interactions/"">Nikhita Singh, Jin Joo Lee, Ishaan Grover, and Cynthia Breazeal (2018). P2PSTORY: Dataset of Children Storytelling and Listening in Peer-to-Peer Interactions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems.</a></b></p><br><p>See below for instructions on how to access the dataset.</p>",,,2019-06-10 09:19:51.212,True,2018-01-22,P2PSTORY: Dataset of children as storytellers and listeners in peer-to-peer interactions,PUBLIC,,True,Personal Robots,False
ai-ethics-for-middle-school,cynthiab,False,"<h2>How do we raise conscientious consumers and designers of AI?</h2><p>Children today live in the age of artificial intelligence. On average, US children tend to receive their first smartphone at age 10, and by age 12 over half of all children have their own social media account. Additionally, it's estimated that by 2022, there will be 58 million new jobs in the area of artificial intelligence. Thus, it's&nbsp;important that the youth of today are both conscientious consumers and designers of AI.&nbsp;</p><p>This project seeks to develop an open source curriculum for middle school students on the topic of artificial intelligence. Through a series of lessons and activities, students learn technical concepts—such as how to train a simple classifier—and the ethical implications those technical concepts entail, such as algorithmic bias.&nbsp;</p>",,,2019-04-17 18:36:01.110,True,2018-08-01,AI + Ethics Curriculum for Middle School,PUBLIC,http://blakeleyhoffman.me/,True,Personal Robots,False
children-s-perception-of-empathy-in-robot-robot-scenarios,cynthiab,False,"<p>Empathy is a core human skill. From early stages of our lives, being able to understand and behave with empathy is fundamental to our social experience. Research in the field of social robotics suggests that given a set of behaviors from a social robot, a child can perceive this agent as empathic. In this project, we explore a novel approach to modeling empathy in children using a social robot. Two social robots were programmed to have conversations containing interactions depicting empathic and non-empathic behaviors. Children were provided with opportunities to act on these interactions as well as to comment on the robot's behavior afterward.</p>",,,2019-04-17 18:49:27.482,True,2017-09-14,The role of social robots in fostering human empathy,PUBLIC,,True,Personal Robots,False
personalized-emotional-wellness-coach,cynthiab,False,"<p>The diagnosis and tracking of mood disorders still rely on clinical assessments, originating more than 50 years ago, of self-reported depressive symptoms via surveys and interviews. Such methods are known to provide limited accuracy and reliability in addition to being costly to track and scale. Once a problem is detected, providing personalized daily intervention and support is also too costly and does not scale. The goal of this pilot project is to develop a proof of concept of personalized emotional wellness coach focusing on key technology modules to create an emotionally intelligent social robot for this targeted domain. We shall also conduct a pilot evaluation with a five-week user study to evaluate the robot coach with respect to its ability to successfully sustain a user long-term adherence (i.e., daily self-report and efficacious advice)—with the expected result that it is more effective than state-of-the-art, gamified mobile apps currently used.&nbsp;</p>",2019-09-30,,2019-04-22 15:26:39.440,True,2018-09-01,Personalized Emotional Wellness Coach,PUBLIC,,True,Personal Robots,False
curious-social-robot,ggordon,False,"<p>Can robots learn to be social? Can they do that in a structured way? This project uses the DragonBot platform and state-of-the-art artificial curiosity algorithms to explore the possibility of robots learning to behave socially, similar to children. The robot reacts to people and receives internal rewards whenever the social interaction succeeds. Initially, the robot learns which behavior best initiates social interaction and later learns which behavior maintains that interaction for the longest period. The goal is to build a brain-inspired hierarchical curiosity-driven social behavior architecture, in which the robot autonomously learns a growing repertoire of social skills.</p>",2015-08-31,--Choose Location,2017-05-31 18:36:24.278,True,2014-01-01,Curious Social Robot,PUBLIC,,False,Personal Robots,False
social-robot-as-a-younger-curious-peer,ggordon,False,"<p>One of the most fundamental and important characteristics of children is their curiosity. Can an interaction with a robot elicit, guide, and promote curiosity? In this project we use the DragonBot platform and a tablet app in a language educational setting with children. We test the hypothesis that a personalized, curiosity-driven behavior of a robot behaving as a younger peer, can affect a child's own curiosity. We use an in-house developed app in which the child's interaction with figures is automatically transformed into a spoken and written story. The social robot reacts to the story in an emotional way and then asks the child questions about it. The child and robot then switch roles, so that the robot tells the story and the child asks questions. The research question is whether the robot's curiosity-driven behavior affects the child's curiosity.</p>",2017-05-15,--Choose Location,2017-05-31 18:41:35.438,True,2014-01-01,Social Robot as a Younger Curious Peer,PUBLIC,,False,Personal Robots,False
mapping-higher-education-network,flaviopp,False,"<p>The mismatch between the supply of graduates’ skills and the needs of the labor market has become increasingly obvious and problematic over the past years. This has prompt policymakers, educators, employers and applicants to reevaluate the role of higher education system. But how do each of these actors perceive higher education? How similar are, according to them, the different degree programs and institutions?</p><p>In this project, we use a data-driven approach to unveil the structure of similarities between degree programs as perceived from the candidates. To that end, we use applicants’ preferences to higher education in Chile and Portugal between the years of 2007 and 2014 as a proxy to measure the similarity between each pair of degree programs. We find that:</p><ol><li>The two structures share the same topological features, despite coming from two different political and social-economical contexts;</li><li>We quantify the mismatch between the current state of the art classification used by educators and policymakers and the structure identify</li><li>We find the existence of strong spatial patterns in the assortment of gender, application scores, demand and unemployment levels; and</li><li>We find that structure of similarities encapsulates non-trivial information about the nature of each degree program, allowing us to predict with high accuracy the level of unemployment by just taking into account the relative position of a degree program in the higher education option set.&nbsp;</li></ol><p>Currently, we are preparing a manuscript to present our findings.&nbsp;</p>",2017-12-31,,2017-10-17 18:44:40.535,True,2016-06-01,Mapping Higher Education Option Space,PUBLIC,,False,Collective Learning,False
a-new-project,flaviopp,False,"<p>It is well established that countries, regions and institutions tend to develop towards related activities. This implies that, for instance, countries are more likely to enter a new activity that is closer/related with the activities it has already developed. An empirical fact that results from the overlapping of the necessary knowledge of each activity. In this context, the product space—a network relating countries economic activities—has been instrumental in capturing the role of relatedness in the economic development of countries. But, although relatedness seems to be a major driver for the diversification of countries exports and research activities, there are many instances when countries deviate from this norm, but to what extent do they benefit from such actions? Is it possible to pinpoint a particular stage of development of a country in which these exceptions are more likely to occur or are they purely at random?</p><p>Using 50 years of trade date we have analyzed how countries diversify their products portfolios in the context of the Economic Complexity and Product Space10. We have shown that 1) there is an intermediate and non-trivial stage of economic development at which countries are more likely to develop towards unrelated activities; that 2) countries that do so achieve a faster economic growth; and 3) that low and high developed economies are the ones that are more likely to diversify towards related varieties.</p><p>These results have significant implications in the literature of regional development. For instance, recently the European Union presented a regional plan of development, coined as Smart Specialization, which advocates for a one rule that fits all: regions should develop the most related and highest reward activities. Our results suggest more caution. Indeed, our findings point out that the development stage of a country, or a region, plays a determinant role in devising a development strategy. For instance, while low and highly developed regions should look forward to developing related activities, regions at an intermediate level of development should be incentivized to pursue the development of unrelated activities and diversify. These results build up to the conclusions of the previous project (2.1), in the sense that economies should adopt dynamical diversification strategies in which the big challenge is to identify the narrow window for unrelated diversification.</p>",2017-12-31,,2017-10-11 00:23:45.415,True,2016-06-01,Do countries benefit from jumping into unrelated varieties?,PUBLIC,,False,Collective Learning,False
strategic-diffusion,flaviopp,False,"<p>One of the eternal challenges of economic development is how to identify the economic activities that a country, city, or region should target. During recent years, a large body of research has shown that countries, regions, and cities, are more likely to enter economic activities that are related to the ones they already have. For instance, a region specialized in the exports of frozen fish and crustaceans can more easily start exporting fresh fish than heavy machinery. This research has illuminated a new chapter in the economic development literature, but has left an important question unanswered: what is the right strategy for countries wanting to diversify their economies?&nbsp;</p>",,,2019-04-17 19:27:50.930,True,2018-04-02,What is the optimal way to diversify an economy?,PUBLIC,,True,Collective Learning,False
meet-me-in-the-middle-the-reunification-of-the-german-research-and-innovation-system,flaviopp,False,"<p>In 1990 Germany began the reunification of two separate research systems. Yet, the institutional unification of these system does not necessarily imply their actual unification. Here we study the evolution of the network of co-authorships between East and West German scholars between 1974 and 2014 to identify the fields that integrated more successfully, and also, the factors predicting re-unification success. We find that the unification of the German research network was fast during the 1990s, but then stagnated at an intermediate level of integration. Next, we study the integration of the twenty largest academic fields (by number of publications prior to reunification) and find an inverted U-shaped between a field's East or West ``dominance'' (a measure of the concentration of the scholarly output of a field in East or West Germany prior to 1990) and the field's subsequent level of integration. We check for the robustness of these results by running Monte Carlo simulations, and a differences-in-difference analysis. Both methods confirm that fields that were dominated by either West or East Germany prior to the reunification integrated less than those whose output was balanced among East and West. Finally, we explore the origins of this inverted U-shape relationship by comparing the mixing patterns, and show that this inverted U-shaped relationship can be explained as a consequence of a tendency of scholars from the most productive regions to collaborate preferentially with scholars from other top regions. These results shed light on the mechanisms governing the reintegration of networks in the content of scholarly communities that were separated by institutions.</p>",,,2017-10-11 15:39:59.915,True,2016-04-01,Meet me in the middle: The reunification of the German research and innovation system,PUBLIC,,True,Collective Learning,False
mapping-higher-education-network,ccandiav,False,"<p>The mismatch between the supply of graduates’ skills and the needs of the labor market has become increasingly obvious and problematic over the past years. This has prompt policymakers, educators, employers and applicants to reevaluate the role of higher education system. But how do each of these actors perceive higher education? How similar are, according to them, the different degree programs and institutions?</p><p>In this project, we use a data-driven approach to unveil the structure of similarities between degree programs as perceived from the candidates. To that end, we use applicants’ preferences to higher education in Chile and Portugal between the years of 2007 and 2014 as a proxy to measure the similarity between each pair of degree programs. We find that:</p><ol><li>The two structures share the same topological features, despite coming from two different political and social-economical contexts;</li><li>We quantify the mismatch between the current state of the art classification used by educators and policymakers and the structure identify</li><li>We find the existence of strong spatial patterns in the assortment of gender, application scores, demand and unemployment levels; and</li><li>We find that structure of similarities encapsulates non-trivial information about the nature of each degree program, allowing us to predict with high accuracy the level of unemployment by just taking into account the relative position of a degree program in the higher education option set.&nbsp;</li></ol><p>Currently, we are preparing a manuscript to present our findings.&nbsp;</p>",2017-12-31,,2017-10-17 18:44:40.535,True,2016-06-01,Mapping Higher Education Option Space,PUBLIC,,False,Collective Learning,False
temporal-scales-in-human-collective-forgetting,ccandiav,False,"<p>Collective memory and attention are sustained by two channels: oral communication (communicative memory) and the physical recording of information (cultural memory). Here, we use data on the citation of academic articles and patents, and on the online attention received by songs, movies, and biographies, to describe the temporal decay of the attention received by cultural products. We show that, once we isolate the temporal dimension of the decay, the attention received by cultural products decays following a universal biexponential function. We explain this universality by proposing a mathematical model based on communicative and cultural memory, which fits the data better than previously proposed log-normal and exponential models. Our results reveal that biographies remain in our communicative memory the longest (20–30 years) and music the shortest (about 5.6 years). These findings show that the average attention received by cultural products decays following a universal biexponential function.</p>",,,2019-04-17 19:27:00.707,True,2017-02-01,The universal decay of collective memory and attention,PUBLIC,,True,Collective Learning,False
when-bullying-meets-video-game-theory,ccandiav,False,"<p>Social learning has shown that people are more likely to learn from those who are seen as prestigious, talented, or who share demographic attributes with learners. In order to demonstrate that, many experiments and data-based studies have been conducted in many different systems; however, classroom environments have been understudied, because of different complications in both designing experiments and collecting data.</p><p>Combining both new technologies that are able to capture children's attention, e.g. video games, as well as experimental game theory, which provides us a formal framework to capture children's revealed preferences—a school classroom can provide an ideal environment for controlled social dilemma experiments, whose results can be contrasted against real-life indicators of school-life.</p><p>The connection between cooperation inside a classroom and social relationships is central in our framework. Here, we navigate the social network structure by running a non-anonymous dyadic cooperative (video) game (Fig. 1), in 50 different public primary school classrooms, between grades 3-5, allowing us to map cooperation networks for each classroom.</p><p>From the video game decisions, we build a weighted cooperation network for each classroom. The resulting network structure is able to capture different properties of the classroom, such as academic performance and social co-existence (Fig. 2). First,&nbsp;we find that positions in the social network have a significant power to identify, in an early stage, children who are susceptible to becoming&nbsp; the victims of bullying, and children who have a high probability to&nbsp;be bullies&nbsp;(Fig. 2A). Second,&nbsp;we find a positive and statistically significant relationship between network centrality—measured as the sum of the outcome on the video game—and student’s academic performance (measured as GPA, even controlling for others socio-behavioral characteristics that are correlated with GPA (Fig. 2 B)).</p><p>These results don't just help us to understand the elementary school environment, but also open new avenues for the role of networks in the education system, with a huge potential impact in education public policy. These results are useful inputs for decision makers and physiologists to prevent bullying and improve learning.</p>",,,2018-10-23 15:23:30.759,True,2017-10-17,When bullying meets (video) game theory: A novel framework to understand elementary school environments,PUBLIC,,True,Collective Learning,False
the-laws-of-forgetting-ii,ccandiav,False,"<p>In order to understand how exogenous shocks, like death, impact memorability by remembering, we use a data-set of biographies from Wikipedia for all individuals who have more than 15 different language editions. Here, we focus on different external shocks that are able to trigger remembering, such as Death, Nobel Prize, Academy Awards (Oscars), Ballon d'Or, Golden Globes, and Grammy's. All of these events show an exogenous-critical non-trivial herd behavior, as described by <a href=""http://www.pnas.org/content/105/41/15649"">Crane and Sornette 2008</a>.</p>",,,2018-10-20 16:47:24.281,True,2017-10-16,The laws of forgetting II: How death and exogenous events shape our collective memory,PUBLIC,,True,Collective Learning,False
micro-macro-fluidic-fabrication-of-a-mid-sole-running-shoe,neri,False,"<p>Micro-Macro Fluidic Fabrication (MMFF) enables the control of mechanical properties through the design of non-linear lattices embedded within multi-material matrices. At its core it is a hybrid technique that integrates molding, casting, and macro-fluidics. Its workflow allows for the fabrication of complex matrices with geometrical channels injected with polymers of different pre-set mechanical combinations. This novel fabrication technique is implemented in the design and fabrication of a midsole running shoe. The goal is to passively tune material stiffness across surface area in order to absorb the impact force of the user's body weight relative to the ground, and enhance the direction of the foot-strike impulse force relative to the center of body mass.</p>",2014-12-12,--Choose Location,2016-12-05 00:16:48.864,True,2014-01-01,Micro-Macro Fluidic Fabrication of a Mid-Sole Running Shoe,PUBLIC,,False,Mediated Matter,False
sample-project-for-neri,neri,False,<p>fja;sldjfa;boq</p>,,,2016-10-06 17:12:11.589,False,2016-10-06,Sample project for Neri,PUBLIC,,True,Mediated Matter,False
living-materials-library,neri,False,"<p>The control of living systems as part of design interfaces is of interest to both the scientific and design communities due to the ability of living organisms to sense and respond to their environments.  They may, for example, detect and break down harmful environmental agents, or create beneficial products when environmental levels dropped below a certain threshold.  However, it is also important for these systems to be reversible, so that the biological components are only active when their functionality is necessary, and the system can remain dormant otherwise.&nbsp;</p><p>The Living Material Library is an exploration of tunable hybrid systems. Our work in this area demonstrates the means through which intrinsic material properties may be functionally changed through environmental factors and, in turn, serve as dynamic substrates for living systems. Nearly all organisms have highly developed sensing capabilities, and have been shown to behaviorally respond to changes in substrate properties. By creating a tunable and reversible material system, we explore how cell behavior such as adhesion, patterning, and differentiation may be influenced via an active interface.
                    
                </p><p>In this iteration, we propose a reversible material system that allows for control of living interactions (much like a light switch). We are particularly interested in fluid material systems (such as electrorheological fluids) that transition from a liquid-like to a solid-like state when exposed to electric fields and currents.&nbsp;</p><p>This endeavor brings to light the complex relationship between dynamic materials and living systems. While other methods of cell intervention often rely on light, chemicals, or temperature, here we explore substrate material properties as inputs for organisms. &nbsp;Our library may allow for more directed inquiry into processes such as collective cell durotaxis, general mechanotaxis, and active sensing. This marks an initial foray into establishing candidate design methods for responsive applications.<br></p>",,,2017-08-04 19:50:13.221,True,2017-03-01,Living Materials Library,PUBLIC,,True,Mediated Matter,False
printing-multi-material-3d-microfluidics,neri,False,"<p>Computation and fabrication in biology occur in aqueous environments. Through on-chip mixing, analysis, and fabrication, microfluidic chips have introduced new possibilities in biology for over two decades. Existing construction processes for microfluidics use complex, cumbersome, and expensive lithography methods that produce single-material, multi-layered 2D chips. Multi-material 3D printing presents a promising alternative to existing methods that would allow microfluidics to be fabricated in a single step with functionally graded material properties. We aim to create multi-material microfluidic devices using additive manufacturing to replicate current devices, such as valves and ring mixers, and to explore new possibilities enabled by 3D geometries and functionally graded materials. Applications range from medicine to genetic engineering to product design.</p>",,--Choose Location,2016-12-14 01:49:52.124,True,2014-01-01,Printing Multi-Material 3D Microfluidics,PUBLIC,,True,Mediated Matter,False
vespers-iii,neri,False,"<p>Vespers is a collection of masks exploring what it means to design (with) life. From the relic of the death mask to a contemporary living device, the collection embarks on a journey that begins with an ancient typology and culminates with a novel technology for the design and digital fabrication of adaptive and responsive interfaces. We begin with a conceptual piece and end with a tangible set of tools, techniques and technologies combining programmable matter and programmable life.</p><p>The project points towards an imminent future where wearable interfaces and building skins are customized not only to fit a particular shape, but also a specific material, chemical and even genetic make-up, tailoring the wearable to both the body and the environment which it inhabits.</p><p>Imagine, for example, a wearable interface designed to guide ad-hoc antibiotic formation customized to fit the genetic makeup of its user; or, consider smart packaging or surface coatings devices that can detect contamination; finally, consider environmentally responsive architectural skins that can respond to, and adapt—in real time—to environmental cues. Research at the core of this project offers a new design space for biological augmentation across a wide breadth of application domains, leveraging resolution and scale.</p><p>The collection includes three series. The first series features the death mask as a cultural artefact. The final series features a living mask as an enabling technology. The second series mediates between the two, marking the process of ‘metamorphosis’ between the ancient relic and its contemporaneous interpretation.The living masks in the final series embody habitats that guide, inform and ‘template’ gene expression of living microorganisms. Such microorganisms have been synthetically engineered to produce pigments and/or otherwise useful chemical substances for human augmentation such as vitamins, antibodies or antimicrobial drugs.Combined, the three series of the Vespers collection represent the transition from death to life, or from life to death, depending on one’s reading of the collection.</p>",,,2018-10-23 15:19:03.760,True,2018-04-01,Vespers III,PUBLIC,,True,Mediated Matter,False
g3p-II,neri,False,"<h2><p><span style=""font-size: 18px; font-weight: normal;"">Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods—such as blowing, pressing, and forming—have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.</span><br></p></h2>",,--Choose Location,2018-10-23 15:19:17.532,True,2015-09-01,Glass II,PUBLIC,,True,Mediated Matter,False
gemini,neri,False,"<p>Gemini—an acoustical “twin chaise""—spans multiple scales of the  human existence extending from the warmth of the womb to the stretches  of the Gemini zodiac in deep space. It recapitulates a human cosmos: our  body—like the Gemini constellation—drifting in space. <br></p>",,--Choose Location,2018-10-19 20:01:10.378,True,2014-01-01,Gemini,PUBLIC,,True,Mediated Matter,False
3d-printed-hemi-ellipsoidal-dome,neri,False,"<p>The Digital Construction Environment is the first architectural-scale structure fabricated with the <a href=""https://www.media.mit.edu/projects/digital-construction-platform-v-2/overview/"">Digital Construction Platform (DCP)</a>.  Using the Mediated Matter group’s Print-In-Place construction technique, an open-domed structure with a diameter of 14.6 m and a height of 3.7 m was manufactured over a print time of 13.5 hours.&nbsp;</p>",,,2019-02-11 20:03:42.274,True,2016-07-17,DCP: Digital Construction Environment,PUBLIC,,True,Mediated Matter,False
digital-construction-platform-v-2,neri,False,"<p>The Digital Construction Platform (DCP) is an experimental enabling technology for large-scale digital manufacturing. In contrast to the typical gantry-based approach to digital construction, robotic arm systems offer the promise of greater task flexibility, dynamically expandable workspaces, rapid setup times, and easier implementation with existing construction techniques. Potential applications for this system include fabrication of non-standard architectural forms; incorporation of data gathered on-site in real time into fabrication processes; improvements in construction efficiency, quality, and safety; and exploration of autonomous construction systems for use in disaster relief, hazardous environments, and extraterrestrial exploration.</p>",,,2017-05-05 16:41:04.046,True,2015-08-01,Digital Construction Platform,PUBLIC,,True,Mediated Matter,False
aguahoja,neri,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
printing-living-materials,neri,False,"<p>How can biological organisms be incorporated into product, fashion, and architectural design to enable the generation of multi-functional, responsive, and highly adaptable objects? This research pursues the intersection of synthetic biology, digital fabrication, and design. Our goal is to incorporate engineered biological organisms into inorganic and organic materials to vary material properties in space and time. We aim to use synthetic biology to engineer organisms with varied output functionalities and digital fabrication tools to pattern these organisms and induce their specific capabilities with spatiotemporal precision.</p>",,--Choose Location,2017-04-03 20:39:02.693,True,2014-01-01,Printing Living Materials,PUBLIC,,True,Mediated Matter,False
wanderers,neri,False,"<p>The Wanderers&nbsp;were unveiled as part of the exhibition: ‘The  Sixth Element: Exploring the Natural Beauty of 3D Printing' on display  at EuroMold, 25-28 November, Frankfurt, Germany. This work was done in collaboration with <a href=""http://www.deskriptiv.de/"">Christoph Bader and Dominik Kolb</a>.  The wearables were 3D-printed with Stratasys multi-material 3D printing  technology. Members of the Mediated Matter group led by <a href=""http://matter.media.mit.edu/people/bio/william-patrick"">Will Patrick</a>&nbsp;and <a href=""http://matter.media.mit.edu/people/bio/sunanda-sharma"">Sunanda Sharma</a>  are currently working on embedding living matter in the form of  engineered bacteria within the 3D structures in order to augment the  environment.</p>",,,2018-04-27 17:06:00.697,True,2014-07-01,Wanderers,PUBLIC,,True,Mediated Matter,False
water-templating-and-biomaterial-skins-in-zero-gravity,neri,False,"<p>Water is life.&nbsp;</p><p>Extending the research that we are involved in with Mediated Matter’s water-based fabrication platform, we brought water-based biomaterial solutions into zero gravity observe their behavior. We created several small scaffolds of different geometries and surface textures that used the water-based solutions as their skins, taking advantage of&nbsp;the fact that van der Waal forces are the primary force affecting water in zero g, which allows water to maintain more structure. The solutions covered the body in a noticeable layering that would otherwise collapse at normal gravity.&nbsp;</p><p>The scaffolds were made of a hydrophilic polymer that is graded in its hydrophilicity, which combined with varying geometries, enabled us to vary the amount and type of liquid the scaffold can retain.&nbsp;Aqueous solution was rapidly applied to the scaffolds in zero gravity to see how thick of a skin we could create that is essentially all fluid.&nbsp;</p><p>This project has significance for research in dynamic cladding systems in space that could have biological functionality and carry mobile life-sustaining functions, as well as providing a skin that shields EM radiation.</p>",,,2018-05-10 18:43:35.423,True,2017-09-01,Water templating and biomaterial skins in zero gravity,LAB,,True,Mediated Matter,False
water-based-additive-manufacturing,neri,False,"<p>This research presents water-based robotic fabrication as a design approach and enabling technology for additive manufacturing (AM) of biodegradable hydrogel composites. We focus on expanding the dimensions of the fabrication envelope, developing structural materials for additive deposition, incorporating material-property gradients, and manufacturing architectural-scale biodegradable systems. The technology includes a robotically controlled AM system to produce biodegradable composite objects, combining natural hydrogels with other organic aggregates. It demonstrates the approach by designing, building, and evaluating the mechanics and controls of a multi-chamber extrusion system. Finally, it provides evidence of large-scale composite objects fabricated by our technology that display graded properties and feature sizes ranging from micro- to macro-scale. Fabricated objects may be chemically stabilized or dissolved in water and recycled within minutes. Applications include the fabrication of fully recyclable products or temporary architectural components, such as tent structures with graded mechanical and optical properties.</p>",,--Choose Location,2019-06-04 21:35:42.589,True,2014-01-01,Water-Based Additive Manufacturing,PUBLIC,,True,Mediated Matter,False
g3p,neri,False,"<p>Ancient yet modern, enclosing yet invisible, glass was first created in  Mesopotamia and Ancient Egypt 4,500 years ago. Precise recipes for its  production - the chemistry and techniques - often remain closely guarded  secrets. Glass can be molded, formed, blown, plated or sintered; its  formal qualities are closely tied to techniques used for its  formation.&nbsp;From the discovery of core-forming process for bead-making in  ancient Egypt, through the invention of the metal blow pipe during  Roman times, to the modern industrial Pilkington process for making  large-scale flat glass; each new breakthrough in glass technology  occurred as a result of prolonged experimentation and ingenuity, and has  given rise to a new universe of possibilities for uses of the material.  <br></p>",,--Choose Location,2017-10-13 19:43:29.330,True,2014-01-01,Glass I,PUBLIC,,True,Mediated Matter,False
living-mushtari,neri,False,<p>How can we design relationships between the most primitive and the most  sophisticated life forms? Can we design wearables embedded with  synthetic microorganisms that can enhance and augment biological  functionality? Can we design wearables that generate consumable energy  when exposed to the sun?</p>,,--Choose Location,2017-10-13 23:28:11.103,True,2015-01-01,Living Mushtari,PUBLIC,,True,Mediated Matter,False
synthetic-apiary,neri,False,"<p>The Synthetic Apiary proposes a new kind of environment, bridging urban and organismic scales by exploring one of the most important organisms for both the human species and our planet: bees. We explore the cohabitation of humans and other species through the creation of a controlled atmosphere and associated behavioral paradigms. The project facilitates Mediated Matter's ongoing research into biologically augmented digital fabrication with eusocial insect communities in architectural, and possibly urban, scales. Many animal communities in nature present collective behaviors known as ""swarming,"" prioritizing group survival over individuals, and constantly working to achieve a common goal. Often, swarms of organisms are skilled builders; for example, ants can create extremely complex networks by tunneling, and wasps can generate intricate paper nests with materials sourced from local areas.</p>",,--Choose Location,2017-10-13 23:33:55.831,True,2016-01-01,Synthetic Apiary,PUBLIC,,True,Mediated Matter,False
rottlace,neri,False,"<p>Rottlace is a family of masks designed for Icelandic singer-songwriter Björk. Inspired by Björk’s most recent album—Vulnicura—the Mediated Matter Group explored themes associated with self-healing and expressing ""the face without a skin."" The series originates with a mask that emulates Björk’s facial structure and concludes with a mask that reveals a new identity, independent of its origin. What originates as a form of portraiture culminates in reincarnation.</p>",,--Choose Location,2018-05-07 19:47:18.431,True,2016-01-01,Rottlace,PUBLIC,,True,Mediated Matter,False
vespers,neri,False,"<p>Novel technologies for additive manufacturing are enabling design and production at nature’s scale. We can seamlessly vary the physical properties of materials at the resolution of a sperm cell, a muscle cell, or a nerve cell. Stiffness, color, hygroscopy, transparency, conductivity, even scent, can be individually tuned for each three-dimensional pixel within a physical object. The generation of products is therefore no longer limited to assemblages of discrete parts with homogeneous properties. Rather like organs, objects can be computationally ""grown"" and 3D printed to form materially heterogeneous and multi-functional products.</p>",,,2018-05-07 19:48:07.209,True,2016-12-12,Vespers II,PUBLIC,,True,Mediated Matter,False
totems,neri,False,"<p>Biodiversity on planet Earth is under momentous threat, with extinction rates estimated between 100 and 1,000 times their pre-human level. The Mediated Matter group has been in search of materials and chemical substances that can sustain and enhance biodiversity across living systems, and that have so far endured the perils of climate change. Melanin is one such substance illustrating biodiversity at the genetic, species, and ecosystem levels.</p>",,,2019-05-07 13:44:31.523,True,2019-02-27,Totems,PUBLIC,,True,Mediated Matter,False
functionally-graded-filament-wound-carbon-fiber-prosthetic-sockets,neri,False,"<p>Prosthetic sockets belong to a family of orthoic devices designed for amputee rehabilitation and performance augmentation. Although such products are fabricated out of lightweight composite materials and designed for optimal shape and size, they are limited in their capacity to offer local control of material properties for optimizing load distribution and ergonomic fit over surface and volume areas. Our research offers a novel workflow to enable the digital design and fabrication of customized prosthetic sockets with variable impedance informed by MRI data. We implement parametric environments to enable the controlled distribution of functional gradients of a filament-wound carbon fiber socket.</p>",,--Choose Location,2019-04-18 19:50:23.822,True,2014-01-01,Functionally graded filament-wound carbon-fiber prosthetic sockets,PUBLIC,,True,Mediated Matter,False
making-data-matter,neri,False,"<p>We present a multimaterial voxel-printing method enabling the physical visualization of data sets commonly associated with scientific imaging. Leveraging voxel-based control of multimaterial 3D printing, our method enables additive manufacturing of discontinuous data types such as point cloud data, curve and graph data, image-based data, and volumetric data. By converting data sets into dithered material deposition descriptions, through modifications to rasterization processes, we demonstrate that data sets frequently visualized on screen can be converted into physical, materially heterogeneous objects.&nbsp;</p><p>Our approach alleviates the need to post-process data sets to boundary representations, preventing alteration of data and loss of information in the produced physicalizations. Therefore, it bridges the gap between digital information representation and physical material composition. We evaluate the visual characteristics and features of our method, assess its relevance and applicability in the production of physical visualizations, and detail the conversion of data sets for multimaterial 3D printing. We conclude with exemplary 3D printed datasets produced by our method pointing towards potential applications across scales, disciplines, and problem domains.</p>",,,2019-04-18 19:51:47.144,True,2018-05-30,Making Data Matter: Voxel-printing for the digital fabrication of data across scales and domains,PUBLIC,,True,Mediated Matter,False
maiden-flight,neri,False,"<p>Maiden Flight is an autonomous biological laboratory environment designed for studying the impact of space flight on the sole reproductive node of a bee colony: <b>the queen bee and her retinue.&nbsp;&nbsp;</b></p><p>It represents the first space module of its kind built specifically to cater to queen bees. The hybrid-ecology of the capsule was created to take into account the distributed and uniquely non-human nature of bee biology, in order to consider how to extend the bee reproductive system for environmental extremes. This aim is reflected in the structure of the capsule interior, which was assembled by humans and augmented by the bees’ natural fabrication.&nbsp;</p><p>In May 2019, the Mediated Matter group traveled to Texas to launch two laboratory capsules on Blue Origin’s sub-orbital rocket system, New Shepard. Each custom-designed&nbsp;<b>metabolic support capsule&nbsp;</b>comprised an experimental environment for one queen bee and an attending retinue of 10-20 nurse bees for a parabolic flight to a 100-kilometer micro-gravitational space apogee, and back.</p>",,,2019-05-17 13:11:46.844,True,2019-04-19,Maiden Flight,PUBLIC,,True,Mediated Matter,False
mind-in-the-machine,wonder,False,"<p>A technology that is thousands of years old, knitting is imbued with tradition, myth, and storytelling—to knit is to embody the work of our ancestors. In a hand knitted piece, each loop passes through needles and fingers, leaving small clues about its maker. Depending on our mood and the day, we might knit relaxed loops, tense knots, or daydream and slip a stitch. Unlike machines, we humans have unique quirks and tendencies, and this extends to our knitting. With the proliferation of factory mass production,&nbsp;knits have become standardized, with anonymous stitches that lack any mark of a sentient maker.</p><p>During this project, I spent a month working in a factory in China to learn about manufacturing and labor. Between rows of hundreds of identical machines, there are still many humans scurrying around to fix jammed needles, trim loose threads, stack and transport finished pieces. I was struck by how these workers live in service of machines, their labor and sweat as ghosts within the identical and perfect end products. This resulting project attempts to reintegrate emotional mark of the worker back into the process of mass production, to begin telling a story of each individual in a landscape of anonymous labor.</p><p>Automation has many functions and plays a critical role in our technological advancement, but is it possible to embed automation with gestures as intimate as the mark of a brushstroke in a painting? Taking the cognitive signatures of a human via their EEG signals, I translated these brain activities back into the knitting process. Depending on the EEG signal, the tensility of the weaving was programmed to vary, reflecting the mark of the worker's “cognitive” hand. Depending on the mental stress levels over a day of work, the tensility varies and the fabric ripples in empathy. &nbsp;</p><p>This project stitches a portrait of the factory worker, through their fluctuating mental states throughout the day—capturing moments of frustration, focus, and meditative work flow. The resulting fabrics tell a story, and each one is unique to the worker and particular moment in time.</p><p>There are many ways humans express themselves—what does this expression look like in the age of mechanical production? Is there a way to insert the mark of being human into the process? This project offers a moment of reflection—for both the consumer and the worker—to reflect on the labor,&nbsp;both mechanical and human, that is involved in our economy.</p><p>Learn more:&nbsp;<a href=""https://ani-liu.com/mind-in-machine"">https://ani-liu.com/mind-in-machine</a></p>",2017-08-02,,2019-03-26 19:13:50.509,True,2017-06-02,Mind in the Machine: Psyche in the age of mechanical production,PUBLIC,,False,Design Fiction,False
biome-botany-for-sensorial-memory-2,wonder,False,"<p>Nothing triggers memories like smell.  Momentary, fleeting, and at times unexpected, one scent can conjure up the warmth of a grandparent, or the heat of a first kiss.   </p><p>Certain botanicals are known for their olfactive properties.  Evolved to seduce pollinators and to proliferate the plant's own genes, the fragrance of flowers have also become entangled in our human dance of seduction.  </p><p>In the art of perfumery, we have long extracted the scents of flowers to apply to ourselves- what if we did the contrary and engineered a plant to emit the odor profile of a person instead?   Could we design new rituals for mourning, new biologies for remembering? </p><p>This project is the speculative design of a plant that smells like a person who is emotionally significant to me, but has passed away.  </p><p>Commercial agendas often drive the progress of certain trajectories of engineering.  This project explores the an alternative design of plants that is not driven nor thoroughly integrated in capitalist production.  Exploring emotions such as loneliness, isolation, and feelings of guilt and anxiety towards human impacts on the environment, the function of these inquiries is to reflect on past, current, and future trajectories of human influences on plant life.</p>",2017-08-31,--Choose Location,2017-10-11 20:31:21.574,True,2016-01-01,Forget Me Not: The Botany of Desire & Loss,PUBLIC,,False,Design Fiction,False
human-perfume,wonder,False,"<p>I am currently creating perfumes that capture the smell of individuals who have emotional significance to me.  An exploration in the use of science for emotional ends, I have successfully bottled the scent profiles of three people.  In the obsessively hygienic and reason-driven laboratory where I distill these smells, I often reflect on the constant negotiation between the animal and the cultured human within ourselves.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">This project is currently conducted in tandem with research in neuroscience, correlating olfactory stimuli with behavioral responses. &nbsp;</span></p>",2017-08-31,,2017-10-11 20:28:56.930,True,2016-09-01,Human Perfume,PUBLIC,,False,Design Fiction,False
woman-of-STEAM-grabs-back,wonder,False,"<p>The reproductive organs of the female body have long been a site of contention, where opposing ideologies in religion, politics, and cultural differences often play out. Of all the questions, that of reproductive rights strikes a particularly sensitive nerve.</p><p>US President Donald Trump recently signed an executive order which cut off all US funding to international NGOs whose work includes abortion services or advocacy. Images of this executive order being signed by Donald Trump flanked by a cabinet of men have circulated widely, begging the question: why do these men feel they have a right to determine women's reproductive choices?</p>",2017-08-31,,2017-10-11 20:28:30.946,True,2017-01-09,Brain-Controlled Interface for  the Motile Control of Spermatozoa,PUBLIC,,False,Design Fiction,False
telepresent-drawings-in-space,wonder,False,"<p>This project asks, how can we transmit experiences across space and time? The launching point of the project is to create a drawing that could only have been made in space, and to capture an emotional aspect of space travel, through an art piece based on notions of telepresence.&nbsp;</p><p>For my payload, a mark making object (graphite) is placed in the nano lab, which is lined with paper.&nbsp; As the payload makes its journey, the mark making object&nbsp;the MMO will float about making a&nbsp;unique&nbsp;drawing of its experience.&nbsp;A sensor inside the box will&nbsp;simultaneously&nbsp;record its movements and position.&nbsp;</p><p>This data and footage will be used to recreate the flight paths and movements of the graphite in an identical “sister” box with a CNC back on Earth, post flight.&nbsp;<br></p><p>This project occurs in two stages, and involves [1] the capture of an object’s experience in zero gravity through a sensor and a drawing made in zero g, and [2] the recreation of that object’s flight experience back on Earth through a robotic arm.</p><p>The resulting objects for exhibition would include the original box that went up for the launch, the drawings inside of it, and the ""sister"" box that shows the recreated movements of the graphite back on Earth.</p>",,,2019-05-06 14:20:29.475,True,2017-05-01,Telepresent Drawings in Space,PUBLIC,,True,Design Fiction,False
memories-of-earth,wonder,False,"<p>Each new technology forces us to confront who we are as humans.&nbsp; As technological innovations in deep space exploration keep advancing, and space travel becomes more accessible to the broader population, what might become of our memories of Earth?&nbsp;</p><p>Speculating on a future where some of us might embark on a one-way trip into space, this project investigates the sensory modalities of memory beyond the digital.&nbsp; In addition to the terabytes of data that we are sure to bring with us on this long journey, what other forms of communication and connection might we invent for an extraterrestrial future? Olfaction has been shown to have strong ties to emotion and memory.&nbsp;</p><p>This project consists of a sensory token for astronauts that contains the unique scent of three memories of Earth: that of a loved one, that of a home, and that of a natural resource. Chemically, the fragrances are embedded in a special polymer designed to contain and release the scents over a long period of time. Through a dial, the user can choose to program and re-live one of three Earth experiences through an immersive olfactive experience. As an emotional time capsule, this project is akin to the Voyager Golden Record, but for precious smells. An exploration in the use of science for emotional ends, this project investigates alternative biological and perceptual modalities of communication and memory through olfaction.</p>",,,2017-11-21 21:16:11.380,True,2017-11-06,Smells for Space: Olfactory Timecapsule for Earthly Memories,PUBLIC,,True,Design Fiction,False
screenspire,achituv,False,"<p>Screen interactions have been shown to contribute to increases in stress, anxiety, and deficiencies in breathing patterns. Since better respiration patterns can have a positive impact on wellbeing, ScreenSpire improves respiration patterns during information work using subliminal biofeedback. By using subtle graphical variations that are tuned to attempt to influence the user subconsciously, user distraction and cognitive load are minimized. To enable a truly seamless interaction, we have adapted an RF-based sensor (ResMed S+ sleep sensor) to serve as a screen-mounted contact-free and respiration sensor. Traditionally, respiration sensing is achieved with either invasive or on-skin sensors (such as a chest belt); having a contact-free sensor contributes to increased ease, comfort, and user compliance, since no special actions are required from the user.</p>",2016-12-31,--Choose Location,2017-08-25 11:59:51.368,True,2015-01-01,ScreenSpire,PUBLIC,,False,Fluid Interfaces,False
s-c-a-l-e,achituv,False,"<p>S.C.A.L.E. is a system for detecting localization of an external object or agent, utilizing weight and pressure as a controlling constant for the detection of place and pressure. &nbsp;The system is designed for simple prototyping of interactive products. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2016-12-05 00:16:27.706,True,2016-08-20,S.C.A.L.E.,LAB-INSIDERS,,True,Fluid Interfaces,False
scanner-grabber,achituv,False,"<p>Scanner Grabber is a digital police scanner that enables reporters to record, playback, and export audio, as well as archive public safety radio (scanner) conversations. Like a TiVo for scanners, it's an update on technology that has been stuck in the last century. It's a great tool for newsrooms. For instance, a problem for reporters is missing the beginning of an important police incident because they have stepped away from their desk at the wrong time. Scanner Grabber solves this because conversations can be played back. Also, snippets of exciting audio, for instance a police chase, can be exported and embedded online. Reporters can listen to files while writing stories, or listen to older conversations to get a more nuanced grasp of police practices or long-term trouble spots. Editors and reporters can use the tool for collaborating, or crowdsourcing/public collaboration.</p>",,--Choose Location,2016-12-05 00:16:48.984,True,2014-09-01,Scanner Grabber,PUBLIC,,True,Fluid Interfaces,False
g3p-II,achituv,False,"<h2><p><span style=""font-size: 18px; font-weight: normal;"">Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods—such as blowing, pressing, and forming—have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.</span><br></p></h2>",,--Choose Location,2018-10-23 15:19:17.532,True,2015-09-01,Glass II,PUBLIC,,True,Fluid Interfaces,False
scale2018,achituv,False,"<p>The SCALE platform adds <b>interactivity and trackability to everyday objects.&nbsp;</b>This load-sensitive surface is composed of only three loadcels beneath the surface, so that the system achieves <b>unobtrusive sensing</b>.</p><p>To capture and analyze human activities without any body-attached devices, a bunch of enabling technologies has been proposed. However, there have still been limitations in conventional approaches: RFID requires costly modification of the objects, and computer vision causes privacy issues. To address these problems, we propose ubiquitous load-sensing systems, networked platforms for giving interactivity and trackability to everyday objects by applying object-localization technology to every surface in our living environment and connecting all platforms with each other. Each platform is capable of localizing the position of external objects, utilizing pressure patterns acquired over time from multiple sensors as variables for the detection of location and weight. In addition, we treat the mass of an object as a unique identifier so that the system can trace the flow or detect consumption across platforms. As applications, we introduce a prototype that makes everyday objects tangible interfaces, privacy-preserving observation systems for family through medicine pill bottles and tooth brushes, and augmentation of existing interfaces.&nbsp;</p>",,,2018-10-22 20:46:39.996,True,2018-01-01,SCALE (2018),LAB-INSIDERS,,True,Fluid Interfaces,False
express,achituv,False,"<p>We are developing a new and exciting tool for expression in paint, combining technology and art to bring together the physical and the virtual through the use of robotics, artificial intelligence, signal processing, and wearable technology. Our technology promotes expression in paint not only by making it a lot more accessible, but also by making it flexible, adaptive, and fun, for everyone across the entire spectrum of abilities. With the development of the technology, new forms of art also emerge, such as hyper, hybrid, and collaborative painting. All of these can be extended to remote operation (or co-operation) thanks to the modular system design. For example, a parent and a child can be painting together even when far apart; a disabled person can experience an embodied painting experience; and medical professionals can reach larger populations with physical therapy, occupational therapy, and art therapy, including motor/neuromuscular impaired persons.</p>",,--Choose Location,2018-05-07 19:44:41.673,True,2015-01-01,Express,PUBLIC,,True,Fluid Interfaces,False
leakyphones,achituv,False,"<p><strong>LeakyPhones</strong>&nbsp;is a public/private headset that was designed to encourage face-to-face interactions, curiosity, and&nbsp;healthier&nbsp;social skills by letting users ""peek"" into each other's music just by looking at one another.&nbsp;</p><p>Gaze is an important social signal in human interaction.&nbsp;Though its interpretation may vary across cultures, it is generally agreed that eye contact indicates interest&nbsp;and the point of attention in a conversation. Despite this, many common personal computing technologies, such as our smartphones and headphones, require significant visual and auditory attention thereby inhibiting our ability&nbsp;to interact with others. LeakyPhones offers a new approach for addressing this challenge.&nbsp;</p>",,,2019-02-26 20:28:41.011,True,2017-03-16,LeakyPhones,PUBLIC,,True,Fluid Interfaces,False
roomscope,honghaod,False,<p>RoomScope is an immersive real-time architectural sensor visualization using Microsoft HoloLens.</p>,2017-10-31,,2017-04-05 18:40:23.388,True,2017-02-05,RoomScope,PUBLIC,,False,City Science,False
_amoeba,honghaod,False,"<p>Amoeba Wall: a context aware wall system. Amoetecture is a set of amoeba-like dynamic spatial elements, including transformable floors, ceilings, tables, chairs, and workstations. We focus on designing architecture robotics and platforms that enable a hyper-efficient and dynamically reconfigurable co-working space that accommodates a wide range of activities in a small area.</p><p><strong>&nbsp;Award</strong></p><p>-<a href=""https://www.designboom.com/design/2019-a-design-award-and-competition-call-for-entries-02-07-2019/""><strong>A' Design Award&nbsp;</strong><strong>2017 -&nbsp;</strong><strong>Gold Prize</strong></a></p><p>-Honorable Mention - Tomorrow Workplace Competition by METROPOLIS</p><p><b>Publication</b></p><p>-<a href=""http://www.academia.edu/35684876/Amoeba_Wall"">H Deng, H Ho, L Alonso, X Li, J Angulo, K Larson Amoeba Wall - PASAJES - archquitectura NO.143, pp8-9</a></p>",2017-11-30,,2019-02-11 15:09:42.775,True,2016-04-22,Amoetecture,PUBLIC,http://rnd.studio/project/amoeba-wall,False,City Science,False
amoeba-wall,honghaod,False,"<p>Amoeba Wall: A context aware wall system:&nbsp;Amoetecture is a set of amoeba-like dynamic spatial elements, including transformable floors, ceilings, tables, chairs, and workstations. We focus on designing architecture robotics and platforms that enable a hyper-efficient and dynamically reconfigurable coworking space that accommodates a wide range of activities in a small area.</p><p><strong>&nbsp;Award</strong></p><p><strong>-&nbsp;</strong><a href=""https://www.designboom.com/design/2019-a-design-award-and-competition-call-for-entries-02-07-2019/""><strong>A' Design Award&nbsp;</strong><strong>2017 -&nbsp;</strong><strong>Gold Prize</strong></a></p><p><strong>-&nbsp;</strong>Honorable Mention - Tomorrow Workplace Competition by METROPOLIS</p><p><b>Publication</b></p><p><b>-</b><a href=""http://www.academia.edu/35684876/Amoeba_Wall"">H Deng, H Ho, L Alonso, X Li, J Angulo, K Larson Amoeba Wall - PASAJES - archquitectura NO.143, pp8-9</a></p>",2016-12-31,,2019-02-11 03:26:16.506,True,2016-01-04,Amoeba Wall: A context-aware wall system,PUBLIC,http://rnd.studio/project/amoeba-wall,False,City Science,False
panda-parental-affective-natural-driver-assistant,michalg,False,"<p>Drivers spend a significant amount of time multi-tasking while they are behind the wheel, especially parent drivers that attend to child passengers. These dangerous behaviors, particularly texting while driving, can lead to distractions and ultimately to accidents. Many in-car interfaces do not assist the driver with the task of entertaining the passengers. In a collaboration with Volkswagen/Audi and the SENSEable City Lab, we are developing PANDA (parental affective natural driver assistant), a robotic driver-vehicle interface that acts as a sociable partner and assists parent drivers. PANDA elicits facial expressions for engaging social interaction with the driver and passengers. PANDA uses car entertainment to entertain and engage the children in educational games while in the car and frees the parent to focus on the task of driving.</p>",2015-05-01,--Choose Location,2017-05-31 18:41:51.451,True,2014-01-01,PANDA: Parental Affective Natural Driver Assistant,PUBLIC,,False,Personal Robots,False
soro,michalg,False,<p>The Social Robot Toolkit aims to provide a platform for children to learn through playful interaction. The social robot (Soro) toolkit allows preschool children to experiment with computational concepts while teaching a social robot new rules. The toolkit also provides a platform for learning interpersonal skills through the use of storytelling that integrates interpersonal and computational concepts. This harnesses preschoolers' natural interest in social interaction to familiarize them with new concepts.</p>,2017-04-30,,2017-06-05 16:13:22.057,True,2014-08-23,Social Robot Toolkit,PUBLIC,,False,Personal Robots,False
electric-parrot,palash,False,"<p>Can you feel empathy for a robot? The electric parrot project is about designing a robot that can engender empathy. For this we are constructing a novel zoomorphic robot that can create its own life story: that is to say it can experience the world, be changed by the experience, and communicate the experience to us. We aim to show through psychometric tests that giving the robot an implicit life story will invoke empathy. Such a robot can subsequently be used for empathy intervention. </p>",2016-08-31,--Choose Location,2017-05-31 18:44:43.634,True,2015-01-01,Electric Parrot,PUBLIC,,False,Personal Robots,False
lightswarm,palash,False,"<p>LightSwarm is a platform for interaction between humans and swarm robots. Swarm robots are implemented as augmented-reality agents that communicate and interact through movements. While each robot is simple, the aggregate shows complex behavior. In this way, LightSwarm invites one to think of the mind as a loose consensus of a society of agents, which allows for more nuanced interactions.</p>",2015-09-01,--Choose Location,2016-12-05 00:17:16.350,True,2014-01-01,LightSwarm,PUBLIC,,False,Personal Robots,False
newsclouds,thariq,False,"<p>NewsClouds presents a visual exploration of how the news reporting of an event evolves over time. Each ""cloud"" represents a publication and each competing news organization usually emphasizes different aspects of that same story. Using the time sliders, that evolution becomes evident. In addition, each word or phrase can be expanded to show its links and context. We are building an archive of events associated with ongoing US election developments.</p>",2018-06-06,--Choose Location,2018-10-09 01:51:14.404,True,2014-09-01,NewsClouds,PUBLIC,,False,Viral Communications,False
unlocking-sleep,thariq,False,"<p>Despite a vast body of knowledge about the importance of sleep, exercise, and healthy eating, our daily schedules are often planned around work and social events, not  healthy behaviors. We're prompted to plan&nbsp;throughout the day by devices and people, and we think about our schedules in terms of things to do; but sleep is rarely considered until we're tired and it's late. This project proposes a way that our everyday use of technology can help improve sleep habits and estimate mood.</p><p>A smartphone's unlock screen is an unobtrusive way of prompting user reflection throughout the day by posing ""microquestions"" as users unlock their phone. The questions are easily answered with a single swipe. Since we unlock our phones 50 to 200 times per day, microquestions can collect information with minimal intrusiveness to the user's daily life.&nbsp;</p><p>Can these swipe-questions help users mentally plan their day around wellbeing, and trigger healthier behaviors?</p>",2018-01-01,--Choose Location,2018-05-06 22:38:11.891,True,2015-01-01,Unlocking Wellbeing,LAB-INSIDERS,,False,Viral Communications,False
hydromorph,thariq,False,"<p>HydroMorph is an interactive display based on shapes formed by a stream of water. Inspired by the membrane formed when a water stream hits a smooth surface (e.g., a spoon), we developed a system that dynamically controls the shape of a water membrane. This project explores a design space of interactions around water shapes, and proposes a set of user scenarios in applications across scales, from the faucet to the fountain. Through this work, we look to enrich our interaction with water, an everyday material, with the added dimension of transformation.</p>",,--Choose Location,2016-12-05 00:17:14.252,True,2016-01-01,HydroMorph,PUBLIC,,True,Viral Communications,False
iot-recorder,thariq,False,"<p>The physical world is increasingly coming online. We have things that measure, sense, and broadcast to the rest of the world. We call this the Internet of Things (IoT). But our cameras are blind to this new layer of metadata on reality. The IoT recorder is a camera that understands what IoT devices it sees and what data they are streaming, thus creating a rich information ""caption-track"" for the videos it records. Using this meta-data, we intend to explore how this enables new video applications, starting with cooking.</p>",,--Choose Location,2016-12-05 00:16:32.661,True,2015-01-01,IoT Recorder,PUBLIC,,True,Viral Communications,False
pubpub,thariq,False,"<p>PubPub reinvents publication to align with the way the web was designed: collaborative, evolving, and open. PubPub uses a graphical format that is deliberately simple and allows illustrations and text that are programs as well as static PDFs. The intention is to create an author-driven,  distributed alternative to academic journals that is tuned to the dynamic nature of many of our modern experiments and discoveries. It is optimized for public discussion and academic journals, and is being used for both.  It is equally useful for a newsroom to develop a story that is intended for both print and online distribution.</p>",,--Choose Location,2018-05-31 18:16:31.248,True,2015-01-01,PubPub,PUBLIC,,True,Viral Communications,False
social-collective-memory,isal,False,"<p>Human success is largely attributed to our collective capacity to accumulate knowledge. This capacity, a.k.a. our Social Collective Memory, is constrained by a trade-off between knowledge creation and knowledge preservation. Through knowledge creation, populations bring about new ideas and through knowledge preservation—which usually entails communication. They keep ideas from being lost once individuals who posses them are no longer available. Here we study how communication shapes the size, volatility, longevity, and redundancy of our Social Collective Memory. We use a simple model to identify optimal levels of communication that maximize the Social Collective Memory and minimize the knowledge gap between individuals and the population as a whole. We find that when innovation is difficult, communication increases the Social Collective Memory and longevity of ideas, while reducing its volatility. Further, we find that the optimal level of communication needed to maximize the Social Collective Memory decreases with population size and are the lowest for the small-world communication network topology. Our findings contribute to a better understanding of the role of communication in creating human collective memory.</p>",2017-12-31,,2017-10-13 15:45:53.850,True,2017-02-01,Social Collective Memory,LAB-INSIDERS,,False,Human Dynamics,False
objective-asessment-of-depression-and-its-improvement,sfedor,False,"<p>Current methods to assess depression and then ultimately select appropriate treatment have many limitations. They are usually based on having a clinician rate scales, which were developed in the 1960s. Their main drawbacks are lack of objectivity, being symptom-based and not preventative, and requiring accurate communication. This work explores new technology to assess depression, including its increase or decrease, in an automatic, more objective, pre-symptomatic, and cost-effective way using wearable sensors and smart phones for 24/7 monitoring of different personal parameters such as physiological data, voice characteristics, sleep, and social interaction. We aim to enable early diagnosis of depression, prevention of depression, assessment of depression for people who cannot communicate, better assignment of a treatment, early detection of treatment remission and response, and anticipation of post-treatment relapse or recovery.</p>",2019-01-01,--Choose Location,2018-05-01 21:02:35.703,True,2014-01-01,Objective Asessment of Depression and its Improvement,PUBLIC,,False,Affective Computing,False
eda-explorer,sfedor,False,"<p>Electrodermal Activity (EDA) is a physiological indicator of stress and strong emotion. While an increasing number of wearable devices can collect EDA, analyzing the data to obtain reliable estimates of stress and emotion remains a difficult problem. We have built a graphical tool that allows anyone to upload their EDA data and analyze it. Using a highly accurate machine learning algorithm, we can automatically detect noise within the data. We can also detect skin conductance responses, which are spikes in the signal indicating a ""fight or flight"" response. Users can visualize these results and download files containing features calculated on the data to be used in their own analysis. Those interested in machine learning can also view and label their data to train a machine learning classifier. We are currently adding active learning, so the site can intelligently select the fewest possible samples for the user to label. </p>",,--Choose Location,2016-12-05 00:17:11.264,True,2015-01-01,EDA Explorer,PUBLIC,http://eda-explorer.media.mit.edu/,True,Affective Computing,False
real-time-assessment-of-suicidal-thoughts-and-behaviors,sfedor,False,"<p>Depression correlated with anxiety is one of the key factors leading to suicidal behavior, and is among the leading causes of death worldwide. Despite the scope and seriousness of suicidal thoughts and behaviors, we know surprisingly little about what suicidal thoughts look like in nature (e.g., How frequent, intense, and persistent are they among those who have them? What cognitive, affective/physiological, behavioral, and social factors trigger their occurrence?). The reason for this lack of information is that historically researchers have used retrospective self-report to measure suicidal thoughts, and have lacked the tools to measure them as they naturally occur. In this work we explore use of wearable devices and smartphones to identify behavioral, affective, and physiological predictors of suicidal thoughts and behaviors.</p>",,--Choose Location,2019-04-19 17:25:15.168,True,2015-01-01,Real-time assessment of suicidal thoughts and behaviors,PUBLIC,,True,Affective Computing,False
wavelet-based-motion-artifact-removal-for-electrodermal-activity,sfedor,False,"<p>Electrodermal activity (EDA) recording is a powerful, widely used tool for monitoring psychological or physiological arousal. However, analysis of EDA is hampered by its sensitivity to motion artifacts. We propose a method for removing motion artifacts from EDA, measured as skin conductance (SC), using a stationary wavelet transform (SWT). We modeled the wavelet coefficients as a Gaussian mixture distribution corresponding to the underlying skin conductance level (SCL) and skin conductance responses (SCRs). The goodness-of-fit of the model was validated on ambulatory SC data. We evaluated the proposed method in comparison with three previous approaches. Our method achieved a greater reduction of artifacts while retaining motion-artifact-free data.</p>",,--Choose Location,2019-04-19 17:39:31.969,True,2015-01-01,Wavelet-based motion artifact removal for electrodermal activity,PUBLIC,,True,Affective Computing,False
behavioral-indications-of-depression-severity,sfedor,False,"<p>In collaboration with Massachusetts General Hospital, we are conducting a clinical trial exploring objective methods for assessing depression and its severity.&nbsp;</p><p>We are challenging the assessment methods that were created decades ago and which rely mostly on self-reported measures. We are including information from wearable sensors and regular sensors in mobile phones to collect information about sleep, social interaction, and location changes to find behavioral patterns that are associated with depressive symptoms. 
                    
                </p>",,,2018-04-09 01:37:18.568,True,2015-01-01,Behavioral Indications of Depression Severity,PUBLIC,,True,Affective Computing,False
physio-freefall,sfedor,False,"<p>This project seeks to examine the effects of altered gravity on an individual’s physiology during parabolic flight. Specifically, we will collect flight participants’ heart rate, heart rate variability, breathing rate, skin temperature, and skin conductance measurements using wearable, wireless sensors in order to determine the response of these biosignals to zero/hyper/microgravity and feelings of nausea. </p><p>The results of this research will have both significant scientific and civilian value. To our knowledge, this experiment will be the first to investigate the new Multiple Arousal Theory in the context of motion sickness, as well as altered gravity. This theory was developed in the Affective Computing group at the MIT Media Lab and examines asymmetry in skin conductance signals from right and left wrists as differing metrics of emotional arousal and intensity. The parabolic flight configuration provides an inimitable circumstance to systematically analyze the evolution of these signals over the course of the repeated parabolic flight path. For example, we expect to see globally heightened stress and emotional arousal on the first pass, with maximal skin conductance peaks from both wrists just before the first moment of weightlessness. We expect these peaks to monotonically decrease over time with each pass, but to remain more elevated (relative to an individual’s baseline) for participants experiencing more self-reported nausea during flight. For individuals not experiencing extreme nausea, we expect to see a much higher skin conductance signal from their right wrists compared to their left (for right-handed participants) during the first few passes, with this difference decreasing steadily as the participant habituates to the flight pattern and sensations. </p><p>Note that NASA and other researchers—including the Boston-local scientists at the Ashton Graybiel Spatial Orientation Lab at Brandeis University—have investigated spatial orientation and motion sickness, but they are just beginning to add the use of physiological sensors to their work. Not only does this demonstrate that the proposed experiment is at the forefront of scientific inquiry, but it also facilitates potential collaboration with world-renowned experts in the Boston area!</p><p>In addition to sensor data, we intend to collect pre- and post-flight surveys recording participant reactions to different levels of gravity, including points at which they experienced nausea or discomfort. Pre-flight surveys will include nausea sensitivity metrics, designed to determine how likely a person is to feel nausea (i.e., separating those who feel carsick on a drive through town versus those who approach rollercoasters without hesitation). It will also ask about each participant’s feelings of anxiety, nausea, and excitement in anticipation of flying. Note that while these feelings may be experienced simultaneously, each one has a different effect on one’s physiology. </p><p>After the flight, we will ask participants to rank which sections of the flight (e.g., beginning, middle, end) prompted the greatest sensations of anxiety, nausea, and excitement and to what degree. We will also annotate the flight video recordings to denote periods of high anxiety, nausea, or excitement.</p><p>Then, we will use the survey, annotation, and sensor information to build a model that predicts when an individual might experience distress in altered gravity environments. This aspect of the study will leverage our research group’s unique expertise building machine learning algorithms for physiological data, but the results could have widespread impact. For example, such a system could be deployed to space travelers to help them monitor their physiology and anticipate or prevent feelings of discomfort during flight. As access to space travel becomes more pervasive, it is critical to understand the physiological effects of altered gravity on a population that does not solely include astronauts or specially trained individuals. Our models, along with the use of low-cost, commercially available sensors, would enable “space hacking” by tourists and other non-technical personnel, allowing them to measure and track their biosignals to achieve optimal wellness during space travel.&nbsp;</p>",,,2018-10-22 19:51:21.995,True,2017-05-11,Physio FreeFall,PUBLIC,,True,Affective Computing,False
auditable-logs,narula,False,,2017-05-26,,2017-04-05 05:23:05.553,False,2016-09-05,Auditable Logs,LAB-INSIDERS,,False,Initiatives,False
bitcoin-settled-dollar-futures,narula,False,"<p>Discreet Log Contracts (<a href=""https://adiabat.github.io/dlc.pdf"">DLC</a>) are a new type of smart contract which limit the information gained&nbsp;and influence of oracles, and can run on the very limited scripting system present in Bitcoin, without&nbsp;the need for more complex languages such as in Ethereum.&nbsp;</p><p>DLC works by precomputing a wide range of potential outcomes for a given contract, and when the oracle announces an event, the event-dependent correct outcome can be published. There are a number of applications where this model applies, and the one that we’ll be starting with first is <b>Bitcoin settled dollar futures</b>. This use case introduces a useful tool to mitigate the volatility of Bitcoin.</p>",2018-12-31,,2018-05-01 20:05:09.760,True,2018-01-01,Building smart contracts with Bitcoin,PUBLIC,,False,Initiatives,False
bitcoin-vending-machine,narula,False,"<p>Lit, the lightweight Lightning Network software developed at the MIT Media Lab, works with multiple Bitcoin-like blockchains. The DCI team has created this demo to experience how Lightning can be used at point-of-sale terminals as well as allow users to do device-to-device transactions using multiple coins. We show how an electronic mobile wallet works with three currencies (Bitcoin, Litecoin, and USD).</p>",,,2019-02-14 19:49:02.951,True,2018-07-02,Bitcoin vending machine,PUBLIC,https://dci.mit.edu/lightning-network/,True,Initiatives,False
zkledger-privacy-preserving-auditing,narula,False,"<h1>Privacy-preserving auditing on distributed ledgers</h1><p>zkLedger is a project that combines techniques from modern cryptography to analyze private data, while at the same time ensuring the integrity of that analysis by committing to the private data on a blockchain that is verified by all participants.</p><p>zkLedger uses permissioned blockchains, zero-knowledge proofs, and additively homomorphic commitment schemes to create a tamper-resistant, verifiable ledger of transactions which hides the amounts, senders, and recipients of transactions, and still allows for rich auditing.</p><h2>Auditing complex systems increases confidence that said systems work as intended</h2><p>Lack of auditability or inaccurate results from auditing can have devastating effects, as demonstrated by the 2008 financial crisis. Traditionally, auditability for companies has been solved by the use of trusted third party auditors, such as the “Big Four”: Deloitte, PriceWaterhouseCoopers, Ernst and Young, and KPMG. Auditability for financial institutions and exchanges has been insured by federal and state government agencies such as the OCC, the FDIC, and SEC, to name just a few. Unfortunately, this type of auditing is a laborious, time-consuming process, that is far from real-time. Blockchain technology proposes an alternative, yet for that alternative to work, direct competitors would need to share information that they consider proprietary.</p><h2>Permissioned blockchains</h2><p>Recently, financial institutions have formed consortia to investigate the use of a different architecture for securities settlement, inspired by blockchain technology. Bitcoin’s success has motivated institutions to consider upgrading their technical infrastructure by using permissioned blockchains, often maintained by participants with a consensus protocol. There are many strong players in this area that are making an impact, such as R3’s Corda system and IBM’s Hyperledger. With a large number of financial institutions already participating in these ledgers, what stands in the way of real-time auditing is a way to run computations on data while allowing participants to maintain the privacy of their data. This is where zkLedger can help.</p><h2>Zero-knowledge proofs</h2><p>Using zero-knowledge proofs, one party can prove that they know some secret information without revealing what that information is. One way to understand this is to look at an example: suppose that Alice has two billiard balls, one red and one green (they are otherwise identical). Bob, who is colorblind, cannot tell the difference between the balls, so he assumes that they are the same color. Alice wants to convince Bob that they are in fact different without revealing the colors of the balls to Bob, so Bob takes both balls, puts them behind his back, and either switches them or keeps them in the same hand. If Alice can correctly answer each time whether they have been switched or not, then she has some knowledge about the balls, but has never revealed what the color of either ball is. If she were to answer incorrectly once, then we know that she was guessing each time. We use zero-knowledge proofs in APL to ensure that transactions added to the ledger are consistent, and that auditing computations are performed correctly. Going further, zero-knowledge proofs can impact many aspects of the financial sector by providing both secrecy and accountability to financial institutions, and we’re exploring new ways to leverage this technology.</p><h2>zkLedger uses and current status</h2><p>We are exploring non-financial uses cases for zkLedger.&nbsp; Our paper ""zkLedger: Privacy-Preserving Auditing on Distributed Ledgers"" will appear at&nbsp;<a href=""https://www.usenix.org/conference/nsdi18/technical-sessions"">NSDI 2018</a>, and our prototype software will be released soon.</p><p><b>Read the paper below:</b></p>",,,2018-05-07 02:08:11.910,True,2018-01-01,zkLedger: Privacy-Preserving Auditing,PUBLIC,https://dci.mit.edu,True,Initiatives,False
context-aware-pipette,jacobson,False,"<p>Pipettes are the equivalent in biology of the keyboard for computer science: a key tool that enables interface with the subject matter. In the case of the pipette, it enables the scientist to move precise amounts of liquids. Pipette design hasn't changed in over 30 years. We've designed a new type of pipette that allows wireless, context-aware operation.</p>",2015-12-01,--Choose Location,2016-12-05 00:16:59.612,True,2014-01-01,Context-Aware Pipette,PUBLIC,,False,Molecular Machines,False
context-aware-biology,jacobson,False,"<p>Current biological research workflows make use of disparate, poorly integrated systems that impose a large mental burden on the scientist, leading to mistakes, often on long, complex, and costly experimental procedures. The lack of open tools to assist in the collection of distributed experimental conditions and data is largely responsible for making protocols difficult to debug, and laboratory practice hard to learn. In this work, we describe an open Protocol Descriptor Language (PDL) and system to enable a context-rich, quantitative approach to biological research. We detail the development of a closed-loop pipetting technology and a wireless sample-temperature sensor that integrate with our Protocol Description platform, enabling novel, real-time experimental feedback to the researcher, thereby reducing mistakes and increasing overall scientific reproducibility.</p>",2015-12-01,--Choose Location,2016-12-05 00:17:09.781,True,2014-01-01,Context-Aware Biology,PUBLIC,,False,Molecular Machines,False
evolutron,jacobson,False,"<p>Technological advances in the past decade have allowed us to take a close look at the proteomes of living organisms. As a result, more than 120,000 solved protein structures are readily available, and we are still on an exponential growth curve. By looking at the proteomes of current living organisms, we are essentially taking snapshots of the successful results in this evolutionary process of continuous adaptation to the environment. Could we process the information available to us from nature to design new proteins, without the need for millions of years of Darwinian evolution?</p><p>To answer this question, we are developing an integrated Deep Learning framework for the evolutionary analysis, search, and design of proteins, which we call Evolutron. Evolutron is based on a hierarchical decomposition of proteins into a set of functional motif embeddings. Two of our strongest motivations for this work are gene therapy and drug discovery. In both cases, protein analysis and design play a fundamental role in the implementation of safe and effective therapeutics.</p>",,--Choose Location,2019-04-17 19:37:56.781,True,2016-01-01,Evolutron: Deep Learning for Protein Design,PUBLIC,,True,Molecular Machines,False
synthetic-genome-engineering,jacobson,False,"<p>We are currently developing novel DNA editing technologies to broaden the scope of genome engineering. Our strategy is based on identifying and engineering endonucleases from diverse living systems, along with targeting with synthetic molecules. Together these components confer greater stability, minimize off-target DNA cleavage, and eliminate sequence restrictions for precision genetic manipulations within cells.</p>",,,2017-09-23 06:18:07.043,True,2016-06-06,Synthetic Genome Engineering,PUBLIC,,True,Molecular Machines,False
affinity-tensorflow,jacobson,False,"<p>Affinity is a high-level machine learning API (Application Programming Interface) dedicated exclusively to molecular geometry. Affinity is written in TensorFlow; a small proportion of high-performance code is in low-level C++.  Depending on the application it can be configured as multi-CPU, multi-CPU single GPU, or multi-GPU system. Affinity has  its own web page at <a href=""http://affinity.mit.edu"">affinity.mit.edu </a><br></p>",,,2019-04-17 19:38:38.863,True,2017-08-01,Affinity: Deep Learning API for Molecular Geometry,PUBLIC,https://affinity.mit.edu,True,Molecular Machines,False
opening-wider-genomic-access-with-a-flexible-crispr-enzyme,jacobson,False,"<p>The CRISPR-Cas9 system has proven to be a versatile tool for genome editing, with&nbsp;<span style=""font-size: 18px; font-weight: 400;"">numerous implications in medicine, agriculture, bioenergy, food security, and beyond. The&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">range of targetable DNA sequences is limited, however, by the need for a short sequence of DNA beside the target site, called the PAM. In total, there are only a handful of CRISPR enzymes with a short enough PAM sequence to be able to target a large portion of the total DNA in a genome. In this study, we identify a natural Cas9 enzyme from the bacterial genome of </span><i style=""font-size: 18px; font-weight: 400;"">Streptococcus canis</i><span style=""font-size: 18px; font-weight: 400;""> that has a PAM sequence with only a single G as its PAM sequence (5’-NNG-3’), allowing flexible targeting of up to 50% of all DNA sequences in living organisms. This new molecular tool potentially grants unprecedented access to correct disease-related mutations, enhance agricultural methods, and expand research efforts.&nbsp;</span></p>",,,2018-10-25 16:09:08.650,True,2017-01-04,Opening wider genomic access with a flexible CRISPR enzyme,PUBLIC,,True,Molecular Machines,False
context-aware-pipette,fracchia,False,"<p>Pipettes are the equivalent in biology of the keyboard for computer science: a key tool that enables interface with the subject matter. In the case of the pipette, it enables the scientist to move precise amounts of liquids. Pipette design hasn't changed in over 30 years. We've designed a new type of pipette that allows wireless, context-aware operation.</p>",2015-12-01,--Choose Location,2016-12-05 00:16:59.612,True,2014-01-01,Context-Aware Pipette,PUBLIC,,False,Molecular Machines,False
context-aware-biology,fracchia,False,"<p>Current biological research workflows make use of disparate, poorly integrated systems that impose a large mental burden on the scientist, leading to mistakes, often on long, complex, and costly experimental procedures. The lack of open tools to assist in the collection of distributed experimental conditions and data is largely responsible for making protocols difficult to debug, and laboratory practice hard to learn. In this work, we describe an open Protocol Descriptor Language (PDL) and system to enable a context-rich, quantitative approach to biological research. We detail the development of a closed-loop pipetting technology and a wireless sample-temperature sensor that integrate with our Protocol Description platform, enabling novel, real-time experimental feedback to the researcher, thereby reducing mistakes and increasing overall scientific reproducibility.</p>",2015-12-01,--Choose Location,2016-12-05 00:17:09.781,True,2014-01-01,Context-Aware Biology,PUBLIC,,False,Molecular Machines,False
closed-loop-optogenetics-for-peripheral-nerve-control,shriyas,False,"<p>Electrical stimulation (FES) is the current clinical stimulation modality used to restore function and provide therapy in a variety of clinical applications.&nbsp;However, its clinical implementation is riddled with challenges of fatigue, reverse order recruitment of motor units, and diffuse/non-specific excitation of surrounding tissues.&nbsp;</p><p>Optogenetics is a recently evolving field, which involves genetically altering cells so that they can be activated with light. Optogenetic techniques have largely been used to probe neural circuits and study the brain's function. Work in our lab has focused on implementing optogenetics as a stimulation modality for peripheral tissues. Under optogenetic stimulation, many of the challenges associated with electrical stimulation are overcome.&nbsp;</p><p>Most recently, we have utilized optogenetics in a closed-loop system to control a murine hind limb to follow desired movement patterns, mimicking climbing stairs and walking.&nbsp;&nbsp;In an ideal future, similar techniques may be used to restore functional movement in patients with paralysis or other motor impairments. We demonstrate that our methods outperform traditional electrical stimulation methods by having less fatigue and smoother movement.&nbsp; This system is the first proof-of-principle for peripheral limb control using closed-loop optogenetics and can perform with greater than 95% accuracy.</p>",2018-12-31,,2019-01-31 15:21:42.923,True,2017-09-01,Closed-Loop Optogenetics for Peripheral Nerve Control,PUBLIC,,False,Biomechatronics,False
electrical-interfaces,shriyas,False,"<h2>Nerve-Muscle Graft Chamber and micro-channel arrays tor interface to peripheral nerves for prosthesis control.&nbsp;</h2><p>This research effort consists of two sub-projects with the goal to develop a small implantable device for achieving bi-directional communication with the amputated nerves in a prosthesis user’s residuum. The nerve-muscle graft chamber (NMGC) is a small implanted device which contains one or more electrically isolated chambers (ca. 20mm&nbsp;<i>l&nbsp;</i>x 4mm&nbsp;<i>h&nbsp;</i>x 4mm&nbsp;<i>w )</i>&nbsp;that can be filled with muscle or cutaneous tissue. The electrical activities of the components of a compound peripheral nerve that in the intact limb sub-served different motor functions can be separated by mechanically dividing the nerve and placing each isolated nerve segment into apposition with a small piece of muscle tissue in each of the separate chambers of the NMGC.&nbsp; For example, the muscle filled chambers can be ganged together in a modular design so that a single implanted device containing three chambers would interface to motor nerve fascicles that provide prosthesis command signals for three different motor functions. For a mixed peripheral nerve that is known to contain cutaneous fascicles as well as motor fascicles, an additional compartment could be added that contains cutaneous tissue. This would be done to provide an appropriate target for regenerating cutaneous nerve fibers to prevent the cutaneous axons from competing with regenerating motor nerve fibers and errantly taking up residence in the muscle tissues. Also, by provide cutaneous&nbsp; target tissue, regenerating sensory afferent nerve fiber are less likely to result in the formation of potentially painful &nbsp;neuromas.</p><p>The second sub-project aims to develop a micro-channel array into which peripheral nerve fibers will grow into. Because the micro-channels are on the order of 100 to 200 um I.D., only a small number of nerve fibers will be present in an individual micro-channel. This can potentially provide greater separation of axons by their functionality. Such separation by function is important when seeking to provide cutaneous and proprioceptive feedback by means of direct electrical activation of the sensory components of the interfaced peripheral nerves.&nbsp;&nbsp;</p>",,,2019-04-26 19:04:39.951,True,2016-01-04,Neural Interfaces,PUBLIC,,True,Biomechatronics,False
transdermal-optogenetic-peripheral-nerve-stimulation,shriyas,False,"<p>
                    Optogenetic techniques have recently been applied to peripheral nerves as a scientific tool with the translatable goal of alleviating a variety of disorders, including chronic pain, muscle fatigue, glucose-related pathologies, and others.  When compared to the electrical stimulation of peripheral nerves, there are numerous advantages: the ability to target molecularly defined subtypes, access to opsins engendering neural inhibition, and optical recruitment of motor axons in a fashion that mimics natural recruitment, which eliminates the fatigue roadblock inherent to functional electrical stimulation. The ability to control peripheral nerves situated under deep tissue structures with transdermal, optical signals would be of enormous benefit, integrating all of the advantages conferred by optogenetics while averting the drawbacks associated with implantable devices, such as mechanical failure, device tissue heating, and a chronic foreign body response.&nbsp;</p><p>We work to develop novel molecular and optical methods in an effort to enable this transdermal optogenetic peripheral nerve control. A further example of a potential clinical application involves optogenetically targeting the vagus nerve, a peripheral cranial nerve implicated in numerous ailments, including epilepsy, migraines, obesity, hypertension, fibromyalgia, Crohn’s disease, asthma, depression, and obsessive-compulsive disorder.  An efficient method of stimulating the vagus nerve with minimal side-effects and high target specificity, such as described here, may have profound implications to the study of various illnesses and disabilities.</p>",,,2019-04-26 19:05:38.186,True,2015-01-01,Transdermal Optogenetic Peripheral Nerve Stimulation,PUBLIC,,True,Biomechatronics,False
amputation,shriyas,False,"<p>Lower-extremity amputation surgery has not seen significant change since the Civil War. This research is focused on the development of novel amputation paradigms that leverage native biological end organs to interpret efferent motor commands and to provide meaningful neural feedback from an artificial limb. Surgical replication of natural agonist-antagonist muscle pairings within the residuum allow us to use biomimetic constructs to communicate joint state and torque from the prosthesis directly to the peripheral nervous system. We hypothesize that these architectures will facilitate control of advanced prosthetic systems to improve gait and reduce metabolic cost of transport.
                    
                </p>",,,2019-04-26 19:05:58.968,True,2016-08-15,Revolutionizing amputation surgery for the restoration of natural neural sensation and mobility,PUBLIC,,True,Biomechatronics,False
agonist-antagonist-myoneural-interface-ami,shriyas,False,"<h2><b>Humans can accurately sense the position, speed, and torque of their limbs, even with their eyes shut. This sense, known as proprioception, allows humans to precisely control their body movements. </b></h2><p>Today’s conventional prosthetic limbs do not provide feedback to the nervous system. Because of this, people with amputated limbs cannot feel the position, speed, and torque of their prosthetic joints without looking at them, making it difficult to control their movement. In order to create a more complete prosthetic control experience, researchers at the Center for Extreme Bionics at the MIT Media Lab invented the&nbsp;<b>agonist-antagonist myoneural interface (AMI)</b>. The AMI is a method to restore proprioception to persons with amputation.</p>",,,2018-08-17 16:20:19.891,True,2014-06-01,Agonist-antagonist Myoneural Interface (AMI),PUBLIC,,True,Biomechatronics,False
closed-loop-optogenetics-for-peripheral-nerve-control,bmaimon,False,"<p>Electrical stimulation (FES) is the current clinical stimulation modality used to restore function and provide therapy in a variety of clinical applications.&nbsp;However, its clinical implementation is riddled with challenges of fatigue, reverse order recruitment of motor units, and diffuse/non-specific excitation of surrounding tissues.&nbsp;</p><p>Optogenetics is a recently evolving field, which involves genetically altering cells so that they can be activated with light. Optogenetic techniques have largely been used to probe neural circuits and study the brain's function. Work in our lab has focused on implementing optogenetics as a stimulation modality for peripheral tissues. Under optogenetic stimulation, many of the challenges associated with electrical stimulation are overcome.&nbsp;</p><p>Most recently, we have utilized optogenetics in a closed-loop system to control a murine hind limb to follow desired movement patterns, mimicking climbing stairs and walking.&nbsp;&nbsp;In an ideal future, similar techniques may be used to restore functional movement in patients with paralysis or other motor impairments. We demonstrate that our methods outperform traditional electrical stimulation methods by having less fatigue and smoother movement.&nbsp; This system is the first proof-of-principle for peripheral limb control using closed-loop optogenetics and can perform with greater than 95% accuracy.</p>",2018-12-31,,2019-01-31 15:21:42.923,True,2017-09-01,Closed-Loop Optogenetics for Peripheral Nerve Control,PUBLIC,,False,Biomechatronics,False
electrical-interfaces,bmaimon,False,"<h2>Nerve-Muscle Graft Chamber and micro-channel arrays tor interface to peripheral nerves for prosthesis control.&nbsp;</h2><p>This research effort consists of two sub-projects with the goal to develop a small implantable device for achieving bi-directional communication with the amputated nerves in a prosthesis user’s residuum. The nerve-muscle graft chamber (NMGC) is a small implanted device which contains one or more electrically isolated chambers (ca. 20mm&nbsp;<i>l&nbsp;</i>x 4mm&nbsp;<i>h&nbsp;</i>x 4mm&nbsp;<i>w )</i>&nbsp;that can be filled with muscle or cutaneous tissue. The electrical activities of the components of a compound peripheral nerve that in the intact limb sub-served different motor functions can be separated by mechanically dividing the nerve and placing each isolated nerve segment into apposition with a small piece of muscle tissue in each of the separate chambers of the NMGC.&nbsp; For example, the muscle filled chambers can be ganged together in a modular design so that a single implanted device containing three chambers would interface to motor nerve fascicles that provide prosthesis command signals for three different motor functions. For a mixed peripheral nerve that is known to contain cutaneous fascicles as well as motor fascicles, an additional compartment could be added that contains cutaneous tissue. This would be done to provide an appropriate target for regenerating cutaneous nerve fibers to prevent the cutaneous axons from competing with regenerating motor nerve fibers and errantly taking up residence in the muscle tissues. Also, by provide cutaneous&nbsp; target tissue, regenerating sensory afferent nerve fiber are less likely to result in the formation of potentially painful &nbsp;neuromas.</p><p>The second sub-project aims to develop a micro-channel array into which peripheral nerve fibers will grow into. Because the micro-channels are on the order of 100 to 200 um I.D., only a small number of nerve fibers will be present in an individual micro-channel. This can potentially provide greater separation of axons by their functionality. Such separation by function is important when seeking to provide cutaneous and proprioceptive feedback by means of direct electrical activation of the sensory components of the interfaced peripheral nerves.&nbsp;&nbsp;</p>",,,2019-04-26 19:04:39.951,True,2016-01-04,Neural Interfaces,PUBLIC,,True,Biomechatronics,False
transdermal-optogenetic-peripheral-nerve-stimulation,bmaimon,False,"<p>
                    Optogenetic techniques have recently been applied to peripheral nerves as a scientific tool with the translatable goal of alleviating a variety of disorders, including chronic pain, muscle fatigue, glucose-related pathologies, and others.  When compared to the electrical stimulation of peripheral nerves, there are numerous advantages: the ability to target molecularly defined subtypes, access to opsins engendering neural inhibition, and optical recruitment of motor axons in a fashion that mimics natural recruitment, which eliminates the fatigue roadblock inherent to functional electrical stimulation. The ability to control peripheral nerves situated under deep tissue structures with transdermal, optical signals would be of enormous benefit, integrating all of the advantages conferred by optogenetics while averting the drawbacks associated with implantable devices, such as mechanical failure, device tissue heating, and a chronic foreign body response.&nbsp;</p><p>We work to develop novel molecular and optical methods in an effort to enable this transdermal optogenetic peripheral nerve control. A further example of a potential clinical application involves optogenetically targeting the vagus nerve, a peripheral cranial nerve implicated in numerous ailments, including epilepsy, migraines, obesity, hypertension, fibromyalgia, Crohn’s disease, asthma, depression, and obsessive-compulsive disorder.  An efficient method of stimulating the vagus nerve with minimal side-effects and high target specificity, such as described here, may have profound implications to the study of various illnesses and disabilities.</p>",,,2019-04-26 19:05:38.186,True,2015-01-01,Transdermal Optogenetic Peripheral Nerve Stimulation,PUBLIC,,True,Biomechatronics,False
mmodm,joep,False,"<p>MMODM is an online drum machine based on the Twitter streaming API, using tweets from around the world to create and perform musical sequences together in real time. Users anywhere can express 16-beat note sequences across 26 different instruments, using plain-text tweets from any device. Meanwhile, users on the site itself can use the graphical interface to locally DJ the rhythm, filters, and sequence blending. By harnessing this duo of website and Twitter network, MMODM enables a whole new scale of synchronous musical collaboration between users locally, remotely, across a wide variety of computing devices, and across a variety of cultures.</p>",2017-06-01,--Choose Location,2017-05-17 22:49:11.048,True,2015-09-01,MMODM: Massively Multiplayer Online Drum Machine,PUBLIC,http://mmodm.co/,False,Responsive Environments,False
quadra-sense-confluence-of-uavs-enviornmental-sensor-networks-and-augmented-reality,joep,False,"<p>Aerial imaging and sensor nodes each present a unique view point into the world. Using Unmanned Aerial Vehicles (UAVs) to navigate the same space shared by sensor networks, this project aims explore interaction between two historically disparate systems for purposes of both immersive experiences and scientific research. Users will remotely navigate landscapes using latest generation of head-mounted dsplays and see real-time sensor data as an augmented overlay with a real-time video stream from the UAV.</p>",2016-01-01,--Choose Location,2017-05-17 23:11:48.250,True,2015-01-01,"Quadra-Sense: Confluence of UAVs, Enviornmental Sensor Networks, and Augmented Reality",PUBLIC,,False,Responsive Environments,False
fragile-instruments,joep,False,"<p>We introduce a family of fragile electronic musical instruments designed to be “played” through the act of destruction. Each Fragile Instrument consists of an analog synthesizing circuit with embedded sensors that detect the destruction of an outer shell, which is destroyed and replaced for each performance. Destruction plays an integral role in both the spectacle and the generated sounds.</p>",2017-12-01,,2017-05-17 23:43:56.207,True,2016-06-01,Fragile Instruments,PUBLIC,,False,Responsive Environments,False
kinephone,joep,False,"<p>This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.</p>",2016-08-01,,2017-05-18 01:07:33.691,True,2016-05-01,Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display,PUBLIC,,False,Responsive Environments,False
kicksoul-a-wearable-system-for-foot-interactions-with-digital-devices,joep,False,<p>KickSoul is a wearable device that maps natural foot movements into inputs for digital devices. It consists of an insole with embedded sensors that track movements and trigger actions in devices that surround us. We present a novel approach to use our feet as input devices in mobile situations when our hands are busy. We analyze the foot's natural movements and their meaning before activating an action.</p>,2017-08-01,--Choose Location,2017-08-07 14:36:03.410,True,2015-01-01,KickSoul: A Wearable System for Foot Interactions with Digital Devices,PUBLIC,,False,Responsive Environments,False
d-Abyss,joep,False,"<p><b>Can tattoos embrace technology in order to make the skin interactive?</b></p><p>The DermalAbyss project is the result of a collaboration between MIT researchers Katia Vega, Xin Liu, Viirj Kan and Nick Barry and Harvard Medical School researchers Ali Yetisen and Nan Jiang.&nbsp;<br></p><p>DermalAbyss is a proof-of-concept that presents a novel approach to bio-interfaces in which the body surface is rendered an interactive display. Traditional tattoo inks are replaced with biosensors whose colors change in response to variations in the interstitial fluid. It blends advances in biotechnology with traditional methods in tattoo artistry.&nbsp;</p><p>This is a research project, and there are currently no plans to develop Dermal Abyss as a product or to pursue clinical trials.<br></p>",2017-05-31,,2018-04-27 17:45:10.726,True,2016-06-01,DermalAbyss: Possibilities of Biosensors as a Tattooed Interface,PUBLIC,,False,Responsive Environments,False
fluxa,joep,False,"<p>Fluxa is a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a transient wearable display to foster richer self-expression and communication in daily life . It can be used to enhance existing social gestures such as handwaving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a decoration device that generates images around dancing bodies.</p>",2017-11-30,--Choose Location,2018-10-12 16:57:48.806,True,2016-01-01,Fluxa,PUBLIC,,False,Responsive Environments,False
skrin,joep,False,"<p>Skrin is an exploration project on digitalized body skin surface using embedded electronics and prosthetics. Human skin is a means for protection, a mediator of our senses, and a presentation of our selves. Through several projects, we expand the expression capacity of the body's surface and emphasize the dynamic aesthetics of body texture by technological means.&nbsp;<span style=""font-size: 18px;"">Working with conventional special effect makeup artists, we “hide” electronics into silicone which is applied onto skin and covered by cosmetics. The digitalized skin surface is connected with the affective experience, while the illuminated body is a representation of internal state.</span></p><p>Working with bionic pop artist Viktoria Modesta, we deployed the project in Music Tech Festival Berlin 2016 and transformed her body as a canvas along with the performance.</p>",2017-06-30,--Choose Location,2018-12-05 14:35:37.656,True,2016-01-01,Skrin,PUBLIC,,False,Responsive Environments,False
grouploop-a-collaborative-network-enabled-audio-feedback-instrument,joep,False,"<p>GroupLoop is a browser-based, collaborative audio feedback control system for musical performance. Upon logging in, GroupLoop users send their microphone stream to other participants while simultaneously controlling the mix of other users' streams played through their speakers. Complex feedback loops involving several participants are possible by working together—in some cases, multiple feedback paths may overlap and interact. Users are able to shape the feedback sounds in real time by adjusting delay, EQ, and gain, as well as by manipulating the acoustics of their portion of the audio feedback path.&nbsp;</p><p>GroupLoop is capable of diverse and unexpected sounds, immeasurable reconfigurability, and in some cases, unrepeatable complexity. It creates new topologies for collaboration in performance, and invites thoughtful reflection on future topologies for real-time music collaboration over distance.</p><p>Try it at https://feedback.davidbramsay.com<span style=""font-size: 18px; font-weight: normal;"">.</span></p>",2015-01-01,--Choose Location,2016-12-05 00:17:13.639,True,2015-09-01,"GroupLoop: A Collaborative, Network-Enabled Audio Feedback Instrument",PUBLIC,https://feedback.davidbramsay.com,False,Responsive Environments,False
mobile-wearable-sensor-data-visualization,joep,False,"<p>As part of the Living Observatory ecological sensing initiative, we've been developing new approaches to mobile, wearable sensor data visualization. The Tidmarsh app for Google Glass visualizes real-time sensor network data based on the wearer's location and gaze. A user can approach a sensor node to see 2D plots of its real-time data stream, and look across an expanse to see 3D plots encompassing multiple devices. On the back-end, the app showcases our Chain API, crawling linked data resources to build a dynamic picture of the sensor network. Besides development of new visualizations, we are building in support for voice queries, and exploring ways to encourage distributed data collection by users.</p>",2015-01-01,--Choose Location,2016-12-05 00:17:17.896,True,2014-01-01,"Mobile, Wearable Sensor Data Visualization",PUBLIC,,False,Responsive Environments,False
sensortape-modular-and-programmable-3d-aware-dense-sensor-network-on-a-tape,joep,False,"<p>SensorTape is a modular and dense sensor network in a form factor of a tape. SensorTape is composed of interconnected and programmable sensor nodes on a flexible electronics sub-strate. Each node can sense its orientation with an inertial measurement unit, allowing deformation self-sensing of the whole tape. Also, nodes sense proximity using time-of-flight infrared. We developed network architecture to automatically determine the location of each sensor node, as SensorTape is cut and rejoined. We also made an intuitive graphical interface to program the tape. Our user study suggested that SensorTape enables users with different skill sets to intuitively create and program large sensor network arrays. We developed diverse applications ranging from wearables to home sensing, to show low-deployment effort required by the user. We showed how SensorTape could be produced at scale and made a 2.3-meter long prototype.</p>",,--Choose Location,2019-04-19 14:34:20.486,True,2016-01-01,SensorTape: Modular and programmable 3D-aware dense sensor network on a tape,PUBLIC,,True,Responsive Environments,False
FabricKeyboard,joep,False,"<h2>Multimodal textile sensate media as an expressive and deformable musical interface</h2><p>In the area of intelligent textiles, we are exploring a multi-modal, fabric-based, stretchable sensate surface for physical interaction media, specifically as&nbsp;deformable musical interface.&nbsp;</p><p>The fabric keyboard consists of multi-layer textile sensors machine-sewn in a keyboard pattern, and it detects different stimuli such as touch, pressure, stretch, proximity, and electric field. This allows users to explore physical and non-contact gestures for expressive on-body and on-surface musical performance. We've also developed additional textile-based inputs such as ribbon controller, trackpad, and fur for more expressive control. This soft sensate surface contributes toward developing seamless, self-aware, and washable media.</p>",,,2018-01-04 23:22:07.671,True,2016-05-27,FabricKeyboard,PUBLIC,,True,Responsive Environments,False
experiential-lighting-new-user-interfaces-for-lighting-control,joep,False,"<p>The vision of pervasive computing is now mainstream. These connected devices permeate every aspect of our lives. Yet, we remain tethered to arcane user interfaces. Unlike consumer devices, building appliances and utilities perpetuate this outdated vision. Lighting control is a prime example. Here, we show how a data-driven methodology—using people and sensors—enables an entirely new method of lighting control.<br></p><p>We are evaluating new methods of interacting and controlling solid-state lighting based on our findings of how participants experience and perceive architectural lighting in our new lighting laboratory (E14-548S). This work, aptly named ""Experiential Lighting,"" reduces the complexity of modern lighting controls (intensity/color/space) into a simple mapping, aided by both human input and sensor measurement. We believe our approach extends beyond general lighting control and is applicable in situations where human-based rankings and preference are critical requirements for control and actuation. We expect our foundational studies to guide future camera-based systems that will inevitably incorporate context in their operation (e.g., Google Glass).</p>",,--Choose Location,2019-04-19 14:25:05.542,True,2014-01-01,Experiential Lighting: New user interfaces for lighting control,PUBLIC,,True,Responsive Environments,False
halo-wearable-lighting,joep,False,"<p>Imagine a future where lights are not fixed to the ceiling, but follow us wherever we are. In this colorful world we enjoy lighting that is designed to go along with the moment, the activity, our feelings, and our outfits. Halo is a wearable lighting device created to explore this scenario. Different from architectural lighting, this personal lighting device aims to illuminate and present its user. Halo changes the wearer's appearance with the ease of a button click, similar to adding a filter to a photograph. It can also change the user's view of the world, brightening up a rainy day or coloring a gray landscape. Halo can react to activities and adapt based on context. It is a responsive window between the wearer and his or her surroundings.</p>",,--Choose Location,2016-12-05 00:16:27.619,True,2014-01-01,Halo: Wearable Lighting,PUBLIC,,True,Responsive Environments,False
hearthere-ubiquitous-sonic-overlay,joep,False,"<p class="""">With our Ubiquitous Sonic Overlay, we are working to place virtual sounds in the user's environment, fixing them in space even as the user moves. We are working toward creating a seamless auditory display, indistinguishable from the user's actual surroundings. Between bone-conduction headphones, small and cheap orientation sensors, and ubiquitous GPS, a confluence of fundamental technologies is in place. However, existing head-tracking systems either limit the motion space to a small area (e.g., Oculus Rift), or sacrifice precision for scale using technologies like GPS. We are seeking to bridge the gap to create large outdoor spaces of sonic objects.</p>",,--Choose Location,2018-06-07 19:16:45.828,True,2014-09-01,HearThere: Ubiquitous Sonic Overlay,PUBLIC,,True,Responsive Environments,False
fingersynth-wearable-transducers-for-exploring-the-environment-through-sound,joep,False,"<p>The FingerSynth is a wearable musical instrument made up of a bracelet and set of rings that enables its players to produce sound by touching nearly any surface in their environments. Each ring contains a small, independently controlled audio exciter transducer. The rings sound loudly when they touch a hard object, and are silent otherwise. When a wearer touches their own (or someone else's) head, the contacted person hears sound through bone conduction, inaudible to others. A microcontroller generates a separate audio signal for each ring, and can take user input through an accelerometer in the form of taps, flicks, and other gestures. The player controls the envelope and timbre of the sound by varying the physical pressure and the angle of their finger on the surface, or by touching differently resonant surfaces. The FingerSynth encourages players to experiment with the materials around them and with one another.</p>",,--Choose Location,2019-04-19 14:26:25.030,True,2014-01-01,FingerSynth: Wearable transducers for exploring the environment through sound,PUBLIC,,True,Responsive Environments,False
augmented-airbrush,joep,False,"<p>We present an augmented handheld airbrush that allows unskilled painters to experience the art of spray painting. Inspired by similar smart tools for fabrication, our handheld device uses 6DOF tracking, mechanical augmentation of the airbrush trigger, and a specialized algorithm to let the painter apply color only where indicated by a reference image. It acts both as a physical spraying device and as an intelligent digital guiding tool that provides manual and computerized control. Using an inverse rendering approach allows for a new augmented painting experience with unique results. We present our novel hardware design, control software, and a discussion of the implications of human-computer collaborative painting.</p>",,--Choose Location,2016-12-05 00:17:07.027,True,2014-09-01,Augmented Airbrush,PUBLIC,,True,Responsive Environments,False
circuit-robots,joep,False,"<h2>Integrating sensors and actuators using flexible electronics</h2><p>Currently, the manufacturing of self-actuating and self-sensing robots requires non-standard manufacturing techniques and assembly steps to integrate electrical and mechanical systems. In this work, we developed a novel manufacturing technique, where such robots can be produced at a flexible electronics factory. We developed the technique using standard industrial machines, processes, and materials. Using a lamination process, we were able to integrate air pouches or shape memory alloy (SMA) inside a polyamide-based flexible circuit to produce bending actuators. The bend angle of the actuators is sensed with a chain of inertial measurement units integrated on the actuator. Air-pouch actuators can produce a force of a 2.24N, and a maximum bend angle of 74 degrees. To demonstrate, we manufactured a five-legged robot with the developed actuators and bend sensors, with all the supporting electronics (e.g., microcontrollers, radio) directly integrated into the flexible printed circuit. Such robots are flat and lightweight (15 grams) and thus conveniently compact for transportation and storage. We believe that our technique can allow inexpensive and fast prototyping and deployment of self-actuating and self-sensing robots.<br></p>",,,2019-04-17 19:28:42.580,True,2017-08-01,Circuit Robots: Mass manufacturing of self-actuating robots,PUBLIC,,True,Responsive Environments,False
circuit-storybook,joep,False,"<p>An interactive picture book that explores storytelling techniques through paper-based circuitry. Sensors, lights, and microcontrollers embedded into the covers, spine, and pages of the book add electronic interactivity to the traditional physical picture book, allowing us to tell new stories in new ways. The current book, ""Ellie,"" tells the adventures of an LED light named Ellie who dreams of becoming a star, and of her journey up to the sky.</p>",,--Choose Location,2016-12-05 00:17:08.516,True,2015-01-01,Circuit Storybook,PUBLIC,,True,Responsive Environments,False
neaq-2069,joep,False,"<p>The New England Aquarium was&nbsp;one of the world’s first modern aquariums when it opened its doors in Boston in 1969.&nbsp; Throughout its history, the aquarium has been a leader in innovative ways to share the ocean with the public, including the creation of the&nbsp;&nbsp;Giant Ocean Tank, the largest circular saltwater tank in the world when it opened in 1970.&nbsp;</p><p>Approaching its 50th anniversary, the New England Aquarium is working with the Open Ocean initiative and&nbsp;MIT Design Lab to develop future scenarios depicting what the experience of the aquarium will be in the next 50 years.</p>",,,2018-04-30 16:04:10.248,True,2018-01-22,NEAQ 2069: Envisioning the Future Aquarium Experience,PUBLIC,,True,Responsive Environments,False
low-power-gesture-input-with-wrist-worn-pressure-sensors,joep,False,"<p>We demonstrate an always-available, on-body gestural interface. Using an array of pressure sensors worn around the wrist, it can distinguish subtle finger pinch gestures with high accuracy (&gt;80?). We demonstrate that it is a complete system that works wirelessly in real time. The device is simple and light-weight in terms of power consumption and computational overhead. Prototype's sensor power consumption is 89uW, allowing the prototype to last more then a week on a small lithium polymer battery. Also, device is small and non-obtrusive, and can be integrated into a wristwatch or a bracelet. Custom pressure sensors can be printed with off-the-shelf conductive ink-jet technology. We demonstrate that number of gestures can be greatly extended by adding orientation data from an accelerometer. Also, we explore various usage scenarios with the device.</p>",,--Choose Location,2019-04-19 14:29:45.955,True,2014-01-01,Low-power gesture input with wrist-worn pressure sensors,PUBLIC,,True,Responsive Environments,False
low-power-wireless-environmental-sensor-node,joep,False,"<p>Tidmarsh is a 600-acre former cranberry farm near Plymouth, MA that has undergone a restoration to wetland. We have instrumented the site with an extensive network of custom low-power environmental sensor nodes, microphones, and cameras. The data from the network is made available in real time and has enabled a number of explorations into the ways that people can experience and learn from large-scale, long-term sensor installations.</p><p><a href=""https://tidmarsh.media.mit.edu"">See sensor data, listen to live audio, and watch live camera feeds on the Tidmarsh website.</a><br></p>",,,2019-04-19 14:30:37.248,True,2016-06-01,Low-power wireless environmental sensor network,PUBLIC,https://tidmarsh.media.mit.edu,True,Responsive Environments,False
chainform,joep,False,"<p>ChainFORM is a modular hardware system for designing linear shape-changing interfaces. Each module is developed based on a servo motor with added flexible circuit board, and is capable of touch detection, visual output, angular sensing, and motor actuation. Moreover, because each module can communicate with other modules linearly, it allows users and designers to adjust and customize the length of the interface. Using the functionality of the hardware system, we propose a wide range of applications, including line-based shape changing display, reconfigurable stylus, rapid prototyping tool for actuated crafts, and customizable haptic glove. We conducted a technical evaluation and a user study to explore capabilities and potential requirements for future improvement.</p>",,--Choose Location,2018-05-04 15:33:24.390,True,2015-09-01,ChainFORM,PUBLIC,,True,Responsive Environments,False
sensorknits,joep,False,"<p>Digital machine knitting is a highly programmable manufacturing process that has been utilized to produce apparel, accessories, and footwear.&nbsp;Our research presents three classes of textile sensors exploiting the resistive, piezoresistive, and capacitive&nbsp;properties of various textile structures enabled by machine knitting with conductive yarn.&nbsp;</p>",,,2019-04-09 13:53:02.936,True,2017-09-01,SensorKnits: Architecting textile sensors with machine knitting,PUBLIC,,True,Responsive Environments,False
rovables,joep,False,"<p>We introduce Rovables, a miniature robot that can move freely on unmodified clothing. The robots are held in place by magnetic wheels, and can climb vertically. The robots are untethered and have an onboard battery, microcontroller, and wireless communications. They also contain a low-power localization system that uses wheel encoders and IMU, allowing Rovables to perform limited autonomous navigation on the body. In the technical evaluations, we found that Rovables can operate continuously for 45 minutes and can carry up to 1.5N. We propose an interaction space for mobile on-body devices spanning sensing, actuation, and interfaces, and develop application scenarios in that space. Our applications include on-body sensing, modular displays, tactile feedback and interactive clothing and jewelry.
                    
                </p>",,,2017-03-03 18:38:10.500,True,2016-09-01,Rovables,PUBLIC,,True,Responsive Environments,False
tidzam,joep,False,"<p>Tid'Zam is an ambient sound analysis system for outdoor environments. It is a component of the Tidmarsh Farms project which monitors the environmental evolution of an industrial cranberry farm during its ecological restoration of wetland. Tid'Zam analyzes the audio streams generated by the deployed microphones in the wild in order to detect the sonic events happening on the site, such as bird calls, insects, frogs, rain, storms, car noise, human voices, and more.&nbsp;</p><p>This system is used to cross-validate other sensors for weather monitoring to identify, geolocalize, and track present wildlife and bird specimens over time. It also controls the audio mixers in order to mute or change the gain on noisy microphones.</p>",,--Choose Location,2017-10-02 15:14:53.312,True,2016-01-01,Tid'Zam,PUBLIC,http://deep-resenv.media.mit.edu:8080,True,Responsive Environments,False
sensorchimes-musical-mapping-for-sensor-networks,joep,False,"<p>SensorChimes aims to create a new canvas for artists leveraging ubiquitous sensing and data collection. Real-time data from environmental sensor networks are realized as musical composition. Physical processes are manifested as musical ideas, with the dual goal of making meaningful music and rendering an ambient display. The Tidmarsh Living Observatory initiative, which aims to document the transformation of a reclaimed cranberry bog, provides an opportunity to explore data-driven musical composition based on a large-scale environmental sensor network. The data collected from Tidmarsh are piped into a mapping framework, which a composer configures to produce music driven by the data.</p>",,--Choose Location,2019-04-19 14:33:18.956,True,2015-09-01,SensorChimes: Musical mapping for sensor networks,PUBLIC,,True,Responsive Environments,False
halo-flex,joep,False,"<p>Halo Flex is the latest version of a series of wearable lighting devices that illuminates the face. It explores how light manipulates our facial qualities and visual perception. It brings together flexible circuits with wearable sensing to define a new form of dynamically controlled E-makeup.
                    
                </p><p>A wire bend circlet functions as the base. A flexible circuit board curves around the frame. Translucent solder mask, copper, and translucent dielectric material assemble a decorative pattern. Ten RGB LEDs are positioned on the bottom layer to achieve a variety of lighting effects. Lighting compositions can be set manually using a Bluetooth-enabled device, such as a smart phone, or automatically using the onboard motion sensor. Halo explores opportunities for wearable lighting.<br></p>",,,2017-04-05 18:41:05.488,True,2015-09-01,Halo Flex,LAB-INSIDERS,,True,Responsive Environments,False
mediated-atmospheres,joep,False,"<h2>The Mediated Atmosphere project envisions a smart office that is capable of dynamically transforming itself to enhance occupants' work experience.</h2><p>In the knowledge economy, worker satisfaction is paramount to retention and productivity. Recent studies have identified a decline in workplace satisfaction. Our research demonstrates how Mediated Atmosphere address this growing need.&nbsp;We created a workspace prototype equipped with a modular real-time control infrastructure, integrating biosignal sensors, controllable lighting, projection, and sound.</p>",,,2018-07-16 12:52:08.873,True,2015-09-01,Mediated Atmosphere,PUBLIC,,True,Responsive Environments,False
spacehuman,joep,False,"<p>SpaceHuman is a soft robotics device designed to facilitate the exploration of environments with reduced gravity in a view of democratization and openness towards access to space and its exploration. &nbsp;It is based on the idea that one day, people who have not received a long preparation and training, as happens today with the astronauts, will be able to have access to the space having a type of conformation and physical configuration that is not adapted to this kind of setting.&nbsp;</p><p>The analysis of the unique seahorse's tail structure became the insight of the overall biomimetic design process. In fact, seahorse tail movement, gripping and protection to the seahorse while floating.&nbsp;Moreover, seahorses do not use their tails to swim; instead, they use them to grasp objects in their environment while they camouflage to hide from predators and hunts for prey. Flexibility and resiliency are key features that enable these behaviours.</p><p>SpaceHuman is an additive prosthesis or otherwise definable as a ""supernumerary robot."" SpaceHuman will facilitate the use of space in zero gravity or reduced gravity restoring the right motion and balance of our body and assigning a new function to a part of our body that until now has not been fully exploited except for the transport of loads, our back. Users will thus be able to build a new poetics of the body and its movements within this radically different space through SpaceHuman, creating new scenarios of its application. Through air chambers specifically designed to be able to change their shape and bend along a reinforcing rib of the material, the people who will use SpaceHuman will be able to cling to useful surfaces inside orbital housing or in Lunar or Martian villages.&nbsp;</p>",,,2019-05-13 21:47:19.873,True,2018-08-01,SpaceHuman,PUBLIC,,True,Responsive Environments,False
tidmarsh-living-observatory-portal,joep,False,"<p>The Tidmarsh Living Observatory Portal is a research project that focuses on the realization of a pavilion that will&nbsp;generate an immersive experience about the Tidmarsh Living Observatory. This&nbsp;site has been restored from a former cranberry farm to natural wetland.&nbsp;Through an extensive Responsive Environments research, this networked&nbsp; and&nbsp;outdoor instrumented site streams live&nbsp; data that will be part of the portal&nbsp;experience.</p>",,,2019-04-19 18:16:55.498,False,2019-04-19,Tidmarsh Living Observatory Portal,PUBLIC,,True,Responsive Environments,False
nailo,joep,False,"<p>NailO is a wearable input device in the form of a commercialized nail art sticker. It works as a miniaturized trackpad the size and thickness of a fingernail that can connect to your mobile devices; it also enables wearers to customize the device to fit the wearer’s personal style. NailO allows wearers to perform different functions on a phone or PC with different gestures, and the wearer can easily alter its appearance with a nail art design layer, creating a combination of functionality and aesthetics.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">From the fashion-conscious, to techies, and anyone in between, NailO can make a style, art, or a design statement; but in its more neutral, natural-looking example it can be worn and used only for its functionality. As a nail art sticker, NailO is small, discreet, and removable. Interactions through NailO can be private and subtle, for example attracting minimal attention when you are in a meeting but need to reply to an urgent text message. Mimicking the form of a cosmetic extension, NailO blends into and decorates one’s body when attached, yet remains removable at the wearer’s discretion, giving the wearer power and control over the level of intimacy of the device to one’s body.</span></p>",,--Choose Location,2017-03-27 21:12:03.637,True,2014-09-01,NailO,PUBLIC,http://nailo.media.mit.edu,True,Responsive Environments,False
visualsoundtrack,joep,False,"<p>We present VisualSoundtrack, a system designed  as a tool for soundtrack composers to experiment with original musical  content in differing musical “styles."" The system allows a user to  rapidly prototype musical ideas with respect to the target media (such  as a film or podcast) by having him/ her input original musical motifs,  capitalizing on a corpus of existing soundtrack samples to source  various styles, and allowing the user to identify the most appropriate  style sources for the target media by visually architecting a path  through a highly abstracted feature space.</p>",,,2018-10-19 20:45:43.275,True,2016-12-01,VisualSoundtrack,PUBLIC,,True,Responsive Environments,False
chromoskin,joep,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Makeup has long been used as a body decoration process for self-expression and for the transformation of one's appearance. While the material composition and processes for creating makeup products have evolved, they still remain static and non-interactive. But our social contexts demand different representations of ourselves; thus, we propose ChromoSkin, a dynamic color-changing makeup system that gives the wearer ability to alter seamlessly their appearance. We prototyped an interactive eye shadow tattoo composed of thermochromic pigments activated by electronics or ambient temperature conditions. We present the design and fabrication of these interactive cosmetics, and the challenges in creating skin interfaces that are seamless, dynamic, and fashionable.</span></p>",,--Choose Location,2018-02-08 20:50:38.533,True,2015-12-01,ChromoSkin,PUBLIC,https://vimeo.com/155460417,True,Responsive Environments,False
skinbot-a-wearable-skin-climbing-robot,joep,False,"<p>We introduce SkinBot: a lightweight robot that moves over the skin's surface with a two-legged suction-based locomotion mechanism and captures a wide range of body parameters with an exchangeable multipurpose sensing module. We believe that robots that live on our skin, such as SkinBot, will enable a more systematic study of the human body and offer great opportunities to advance our knowledge in many areas such as telemedicine, human-computer interfaces, body care, and fashion.</p>",,,2019-04-19 14:35:05.713,True,2016-11-01,"SkinBot: A wearable, skin-climbing robot",PUBLIC,http://www.artemdementyev.com,True,Responsive Environments,False
quantizer-sonification-platform-for-high-energy-physics-data,joep,False,"<p>Inspired by previous work in the field of data sonification, we built a data-driven composition platform that enables users to map collision event information from experiments in high-energy physics to audio properties, and thus make music from real-time data. The tool is used for outreach purposes, allowing physicists and composers to interact with collision data through novel interfaces. Three real-time compositions were streamed from May 2016–July 2016. Two additional compositions are streamed in fall 2018. This project can inspire the development of strategic mappings that facilitate the auditory perception of hidden regularities in high-dimensional datasets, and one day evolve into a useful analysis tool for physicists as well, possibly for the purpose of monitoring slow control data in experiment control rooms. The project is accessible at <a href=""http://quantizer.media.mit.edu"">Quantizer.media.mit.edu.</a></p>",,--Choose Location,2018-10-09 20:36:43.077,True,2015-01-01,Quantizer: Sonification Platform for High-Energy Physics Data,PUBLIC,http://www.quantizer.media.mit.edu,True,Responsive Environments,False
doppelmarsh-cross-reality-environmental-sensor-data-browser,joep,False,"<p>Doppelmarsh is a cross-reality sensor data browser built for experimenting with presence and multimodal sensory experiences. Built on evolving terrain data from a physical wetland landscape, the software integrates real-time data from an environmental sensor network with real-time audio streams and other media from the site. Sensor data is rendered in the scene in both visual representations and as 3D sonification. Users can explore this data by walking on the virtual terrain in a first person view, or flying high above it. This flexibility allows Doppelmarsh to serve as an interface to other research platforms on the site, such as Quadrasense, an augmented reality UAV system that blends a flying live camera view with a virtual camera from Doppelmarsh. We are currently investigating methods for representing subsurface data, such as soil and water temperatures at depth, as well as automation in scene and terrain painting.</p>",,--Choose Location,2019-04-19 14:24:11.949,True,2014-09-01,Doppelmarsh: Cross-reality environmental sensor data browser,PUBLIC,,True,Responsive Environments,False
mmodm,ddh,False,"<p>MMODM is an online drum machine based on the Twitter streaming API, using tweets from around the world to create and perform musical sequences together in real time. Users anywhere can express 16-beat note sequences across 26 different instruments, using plain-text tweets from any device. Meanwhile, users on the site itself can use the graphical interface to locally DJ the rhythm, filters, and sequence blending. By harnessing this duo of website and Twitter network, MMODM enables a whole new scale of synchronous musical collaboration between users locally, remotely, across a wide variety of computing devices, and across a variety of cultures.</p>",2017-06-01,--Choose Location,2017-05-17 22:49:11.048,True,2015-09-01,MMODM: Massively Multiplayer Online Drum Machine,PUBLIC,http://mmodm.co/,False,Responsive Environments,False
quadra-sense-confluence-of-uavs-enviornmental-sensor-networks-and-augmented-reality,ddh,False,"<p>Aerial imaging and sensor nodes each present a unique view point into the world. Using Unmanned Aerial Vehicles (UAVs) to navigate the same space shared by sensor networks, this project aims explore interaction between two historically disparate systems for purposes of both immersive experiences and scientific research. Users will remotely navigate landscapes using latest generation of head-mounted dsplays and see real-time sensor data as an augmented overlay with a real-time video stream from the UAV.</p>",2016-01-01,--Choose Location,2017-05-17 23:11:48.250,True,2015-01-01,"Quadra-Sense: Confluence of UAVs, Enviornmental Sensor Networks, and Augmented Reality",PUBLIC,,False,Responsive Environments,False
fragile-instruments,ddh,False,"<p>We introduce a family of fragile electronic musical instruments designed to be “played” through the act of destruction. Each Fragile Instrument consists of an analog synthesizing circuit with embedded sensors that detect the destruction of an outer shell, which is destroyed and replaced for each performance. Destruction plays an integral role in both the spectacle and the generated sounds.</p>",2017-12-01,,2017-05-17 23:43:56.207,True,2016-06-01,Fragile Instruments,PUBLIC,,False,Responsive Environments,False
soundform,ddh,False,"<p>SoundFORMS creates a&nbsp;<span style=""font-size: 18px; font-weight: 400;"">new method for composers of electronic music to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">interact with their compositions. Through the use of a&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">pin-based shape-shifting display, synthesized&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">waveforms are projected in three dimensions in real&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">time affording the ability to hear, visualize, and interact&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">with the timbre of the notes. Two types of music&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">composition are explored: generation of oscillator&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">tones, and triggering of pre-recorded audio samples.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The synthesized oscillating tones have three timbres:&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">sine, sawtooth and square wave. The pre-recorded</span></p><p>audio samples are drum tracks. Through the use of a&nbsp;<span style=""font-size: 18px; font-weight: 400;"">gestural vocabulary, the user can directly touch and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">modify synthesized waveforms.</span></p>",2016-06-01,,2017-05-26 15:59:57.300,True,2016-02-01,SoundFORMS: Manipulating Sound Through Touch,PUBLIC,,False,Responsive Environments,False
kinephone,ddh,False,"<p>This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.</p>",2016-08-01,,2017-05-18 01:07:33.691,True,2016-05-01,Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display,PUBLIC,,False,Responsive Environments,False
mobile-wearable-sensor-data-visualization,ddh,False,"<p>As part of the Living Observatory ecological sensing initiative, we've been developing new approaches to mobile, wearable sensor data visualization. The Tidmarsh app for Google Glass visualizes real-time sensor network data based on the wearer's location and gaze. A user can approach a sensor node to see 2D plots of its real-time data stream, and look across an expanse to see 3D plots encompassing multiple devices. On the back-end, the app showcases our Chain API, crawling linked data resources to build a dynamic picture of the sensor network. Besides development of new visualizations, we are building in support for voice queries, and exploring ways to encourage distributed data collection by users.</p>",2015-01-01,--Choose Location,2016-12-05 00:17:17.896,True,2014-01-01,"Mobile, Wearable Sensor Data Visualization",PUBLIC,,False,Responsive Environments,False
sensorknits,ddh,False,"<p>Digital machine knitting is a highly programmable manufacturing process that has been utilized to produce apparel, accessories, and footwear.&nbsp;Our research presents three classes of textile sensors exploiting the resistive, piezoresistive, and capacitive&nbsp;properties of various textile structures enabled by machine knitting with conductive yarn.&nbsp;</p>",,,2019-04-09 13:53:02.936,True,2017-09-01,SensorKnits: Architecting textile sensors with machine knitting,PUBLIC,,True,Responsive Environments,False
tidzam,ddh,False,"<p>Tid'Zam is an ambient sound analysis system for outdoor environments. It is a component of the Tidmarsh Farms project which monitors the environmental evolution of an industrial cranberry farm during its ecological restoration of wetland. Tid'Zam analyzes the audio streams generated by the deployed microphones in the wild in order to detect the sonic events happening on the site, such as bird calls, insects, frogs, rain, storms, car noise, human voices, and more.&nbsp;</p><p>This system is used to cross-validate other sensors for weather monitoring to identify, geolocalize, and track present wildlife and bird specimens over time. It also controls the audio mixers in order to mute or change the gain on noisy microphones.</p>",,--Choose Location,2017-10-02 15:14:53.312,True,2016-01-01,Tid'Zam,PUBLIC,http://deep-resenv.media.mit.edu:8080,True,Responsive Environments,False
doppelmarsh-cross-reality-environmental-sensor-data-browser,ddh,False,"<p>Doppelmarsh is a cross-reality sensor data browser built for experimenting with presence and multimodal sensory experiences. Built on evolving terrain data from a physical wetland landscape, the software integrates real-time data from an environmental sensor network with real-time audio streams and other media from the site. Sensor data is rendered in the scene in both visual representations and as 3D sonification. Users can explore this data by walking on the virtual terrain in a first person view, or flying high above it. This flexibility allows Doppelmarsh to serve as an interface to other research platforms on the site, such as Quadrasense, an augmented reality UAV system that blends a flying live camera view with a virtual camera from Doppelmarsh. We are currently investigating methods for representing subsurface data, such as soil and water temperatures at depth, as well as automation in scene and terrain painting.</p>",,--Choose Location,2019-04-19 14:24:11.949,True,2014-09-01,Doppelmarsh: Cross-reality environmental sensor data browser,PUBLIC,,True,Responsive Environments,False
cityscope-mark-iva-riyadh,jiw,False,"<p>We recently led a workshop in Saudi Arabia, with staff from the Riyadh Development Authority, to test a new version of our CityScope platform. With only an hour to work, four teams of five professionals competed to develop a redevelopment proposal for a neighborhood near the city center. The platform evaluated their designs according to energy, daylighting, and walkability.</p>",2016-09-01,--Choose Location,2018-06-26 14:28:55.162,True,2014-09-01,CityScope Riyadh,PUBLIC,http://irawinder.com/blog/riyadh,False,City Science,False
cityscope-barcelona,jiw,False,"<p>The ""Barcelona"" demo is an independent prototype designed to model and simulate human interactions within a Barcelona-like urban environment. Different types of land use (residential, office, and amenities) are configured into urban blocks and analyzed with agent-based techniques.</p>",2016-01-01,--Choose Location,2018-04-27 14:41:51.014,True,2015-09-01,CityScope Barcelona,PUBLIC,,False,City Science,False
gsk-manufacturing-initiative,jiw,False,"<p>This project is the first of two projects in collaboration with GSK. We are developing a computational simulation that allows a human user (or AI) to test drug manufacturing investment scenarios for an entire portfolio over multiple years. We aspire to help decision-makers understand the possible impact of new techniques such as CBM on selected key performance metrics. This game like simulation allows various stakeholders to come together and make collaborative decisions regarding the entire supply chain. The software works dynamically with a Tactile Matrix, which is an interactive decision support system that allows users to instantly and collaboratively explore the models in an approachable, tangible way. </p><p>Screenshots courtesy of Ira Winder. Photos by Nina Lutz.</p>",2018-06-01,,2018-08-20 20:42:11.340,True,2017-01-09,GSK Manufacturing Initiative,PUBLIC,,False,City Science,False
gsk-places-initiative,jiw,False,"<p>This project is part of a parallel research endeavor with GSK Manufacturing. By simulating how scientists at the Upper Providence site interact with one another and the space around them, we hope to help assist future renovations in a range of GSK locations. This was motivated by GSK’s drive to improve spatial configuration within their organization, including various open office environments and even smart labs.&nbsp;</p><p>We hope this project will serve as both a packaged decision support system and a framework for GSK scientists and stakeholders to reconfigure with their own spatial inquiries and case studies. Our goals can be enumerated as below.&nbsp;</p><ol>
<li>Design a decision support tool that allows R&amp;D to understand how changes to physical environments and adjacencies may have an impact on key workplace indicators. Encourage data-driven demonstration and discussion of decisions related to spatial changes.&nbsp;</li>
<li>Employ spatial mathematical models to calculate key workplace indicators, including</li>
<ol>

<li>Space utilization</li>
<li>Time accessibility between amenities</li>
<li>Synergy, defined as potential for interaction among researchers</li>

</ol>
<li>Deploy the decision support tool as an evolving platform that exists in two forms:&nbsp;</li>
<ol>

<li>Tangible user interface “Tactile Matrix”</li>
<li>Traditional executable application that can be used on personal machines with mouse and keyboard interface.&nbsp;</li>
<li>Provide transparent source code and open source, off the shelf technologies that allows for adaptation and customization for various case studies and applications.&nbsp;</li>

</ol>
</ol><br><p>Screenshots courtesy of Nina Lutz.&nbsp;<br></p>",2018-06-01,,2018-08-20 20:43:34.774,True,2017-01-09,GSK Places Initiative,PUBLIC,,False,City Science,False
geobits,jiw,False,"<p>This is an open source geospatial exploration tool. Using various public APIs including Open Street Map and the United States Census, we can make dynamic, flexible models of how people are moving through the city. These models include accessibility in cities, multimodal transportation networks, and diversity. Overall this allows anyone with or without an urban planning background to build strong models with geospatial and urban data. This system works dynamically with a Tactile Matrix, which is an interactive decision support system that allows users to instantly and collaboratively explore the models in a tangible way.</p><p>Photos by Nina Lutz.&nbsp;</p>",2018-06-01,,2018-09-04 19:47:57.544,True,2016-07-11,GeoBits,PUBLIC,,False,City Science,False
singapore-pedestrian-accessibility,jiw,False,<p>This project focused on pedestrian accessibility in collaboration with Singapore Centre for Liveable Cities. Researchers and planners came together to design an interface that would allow both citizens and planners to interact with a model regarding pedestrian accessibility. The tangible interface allows users to come together to have conversations and make interventions to make the case study area more accessible for pedestrians.&nbsp;</p>,2018-06-01,,2018-08-22 03:18:47.497,True,2016-06-06,Singapore Pedestrian Accessibility,PUBLIC,,False,City Science,False
OLD_cityscope-mark-ii-scout,jiw,False,"<p>The CityScope ""Scout"" prototype integrates augmented reality with real-time mathematical modeling of geospatial systems. In practice, the technology transforms any tabletop into a canvas for land-use planning and walkability optimization. Users perform rapid prototyping with LEGO bricks and receive real-time simulation and evaluation feedback.</p>",,--Choose Location,2017-10-16 03:13:05.718,True,2014-01-01,CityScope Mark II: Scout,PUBLIC,,True,City Science,False
OLD_cityscope-mark-iii-dynamic-3d,jiw,False,"<p>The Dynamic 3D prototype allows users to edit a digital model by moving physical 3D abstractions of building typologies. Movements are automatically detected, scanned, and digitized so as to generate inputs for computational analysis. 3D information is also projected back onto the model to give the user feedback while edits are made.</p>",,--Choose Location,2017-10-16 03:03:54.492,True,2014-01-01,CityScope Mark III: Dynamic 3D,PUBLIC,,True,City Science,False
OLD_cityscope-mark-ivb-land-usetransportation,jiw,False,"<p>CityScope MarkIVb is programmed to demonstrate and model the relationship between land use (live and work), population density, parking supply and demand, and traffic congestion. </p>",,--Choose Location,2017-10-16 03:05:09.935,True,2014-09-01,CityScope Mark IVb: Land Use/Transportation,PUBLIC,,True,City Science,False
last-mile-logistics,jiw,False,"<p>Developed by Ira Winder with the MIT Centre for Transportation and Logistics, the model seeks to use real population data and create a simulation to optimize delivery cost and coverage. This could be modified and applied to many disciplines, industries, and population types. The platform has the user place stores on a Tactile Matrix, a type of tangible interface, and displays the output of their potential delivery coverage and cost. This optimization game of sorts is a whole new approach to maximizing delivery potential. The interactive interface and layers of finely granulated and detailed data allow the user to make meaningful interventions and see the intertwining of many rich data sets. </p><p>Photos by James Li. Video by Nina Lutz.&nbsp;</p>",,,2017-10-16 15:43:32.494,True,2016-01-04,Last Mile Logistics,PUBLIC,,True,City Science,False
gui3d,jiw,False,"<p><a href=""https://github.com/irawinder/GUI3D/"">GUI3D</a> Template is a generic implementation of the GUI components that anyone might want for a 3D simulation in Processing 3.&nbsp;Components include navigation, zooming,&nbsp;rotation, control sliders, radio buttons, and the means to select objects in 3D space with a mouse.&nbsp;This template can speed up implementation of 3D simulations in Processing.</p><p>More information:&nbsp;<a href=""https://ira.mit.edu/blog/gui3d"">https://ira.mit.edu/blog/gui3d</a></p><p>Jump directly to the GitHub Repository:&nbsp;<a href=""https://github.com/irawinder/GUI3D"">https://github.com/irawinder/GUI3D</a></p>",,,2018-04-27 14:47:52.452,True,2017-12-01,GUI3D,PUBLIC,http://ira.mit.edu/blog/gui3d,True,City Science,False
andorra-dynamic-urban-planning,jiw,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>",,,2019-02-25 15:17:09.063,True,2016-09-01,Andorra | Dynamic Urban Planning,PUBLIC,,True,City Science,False
andorra-living-lab,jiw,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,City Science,False
CityscopeBostonbrt,jiw,False,"<p>The&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://mfc.mit.edu/"">Mobility Futures Collaborative</a>&nbsp;in the MIT Department of Urban Studies and Planning (DUSP) and the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://cp.media.mit.edu/"">Changing Places group</a>&nbsp;at the MIT Media Lab have developed new interactive tools aimed to better communicate the possible impacts of new transit systems. The Media Lab and DUSP have partnered with the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""https://www.barrfoundation.org/"">Barr Foundation</a>&nbsp;to test these tools in a series of community engagement workshops to examine the impacts of Bus Rapid Transit (BRT) systems in greater Boston. These tools include the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://cp.media.mit.edu/city-simulation/"">CityScope</a>&nbsp;— an interactive platform that utilizes physical models (built from LEGO bricks) and 3-D projection — to enable community members to engage in neighborhood and street-level decisions including alternative bus corridor designs and station-level variations (such as pre-pay boarding). The second tool,&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://mfc.mit.edu/innovations-participatory-design-brt-systems"">CoAXs</a>&nbsp;is a new interactive platform for collaborative transit planning that builds on open-source urban analytics tools such as&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://conveyal.com/projects/analyst/"">Conveyal Transport Analyst</a></p>",,--Choose Location,2017-10-16 18:32:03.412,True,2015-01-01,CityScope Boston BRT,PUBLIC,,True,City Science,False
cityscope_playground,jiw,False,"<p>This project depicts the design, deployment and operation of a Tangible Regulation Platform, a physical-technological apparatus made for the distilment of regulations. The platform is set to exemplify the effects of regulations on a designated territory, allowing planners, designers, stakeholders and community members a common ground for discussion and decision making. An accessible and self-explanatory tool, this platform illustrates the relationship between urban form and regulations, offering a seamless and transparent process of regulation-based urban design. Lastly, projecting on the foreseen future of law and urbanism, this project proposes an alternative data and performance-based approach for the making of new regulations. Beyond excelling the processes of design under regulations, this platform and other new tools are offered to help facilitate a discussion on the way future regulations will be devised, improving both the design processes and their final outcome.</p>",,--Choose Location,2019-04-09 14:40:07.692,True,2014-09-01,CityScope PlayGround: MIT East Campus,PUBLIC,http://ArielNoyman.com,True,City Science,False
andorra-tourism,jiw,False,"<p><span style=""font-size: 18px; font-weight: 400;""></span><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/""><span style=""font-size: 18px; font-weight: 400;"">View the main City Science Andorra project profile.</span></a><br></p><p><span style=""font-size: 18px; font-weight: 400;"">With more than eight million visitors a year, tourism represents almost 30% of the economy of Andorra. By gathering and analyzing data from social media, call detail records, and wifi, we can understand the country's dynamics of tourism and commerce as well as design interventions that can improve the experience for tourists, encouraging them to visit Andorra more frequently, stay longer, and increase spending.&nbsp;</span><br></p><h2><b>Current Projects</b></h2><ul><li>Event Analysis<br></li><li>Social Network<br></li><li>Location Recommendation system<br></li></ul><p> </p><h2><b>EVENT ANALYSIS</b></h2><p>Based on the analysis of call detail records and social media, the goal of this project is to understand the tourist behaviors in Andorra.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">After mining those anonymized data, we have been able to learn different patterns and behaviors of the tourism in Andorra thanks to an agent-based model developed in order to represent the flow of people. This simulation is also coupled with an interactive table called CityMatrix.</span></p>",,,2019-02-25 15:33:28.936,True,2015-08-01,Andorra | Tourism,PUBLIC,,True,City Science,False
cityscope-mark-iva-riyadh,mkh,False,"<p>We recently led a workshop in Saudi Arabia, with staff from the Riyadh Development Authority, to test a new version of our CityScope platform. With only an hour to work, four teams of five professionals competed to develop a redevelopment proposal for a neighborhood near the city center. The platform evaluated their designs according to energy, daylighting, and walkability.</p>",2016-09-01,--Choose Location,2018-06-26 14:28:55.162,True,2014-09-01,CityScope Riyadh,PUBLIC,http://irawinder.com/blog/riyadh,False,City Science,False
city-game,mkh,False,"<p>Planning a city is a complex task requiring collaboration between multiple stakeholders with different, and often conflicting, goals and objectives. Researchers have studied the role of technology in group collaboration for many years. It has been noted that when the task between collaborators increases in complexity, such as in a decision-making process, the use of computer technology could either enhance, or disturb, the collaboration process. City Game evaluates the impact of computer interfaces on a multi-objective negotiation problem. Using a tangible user interface (TUI) is more effective for multi-objective group decision-making than a graphical or multitouch user interface; this project will focus on designing and developing a TUI and a serious game for an urban planning scenario. We will test the game on different computer interfaces to evaluate the decision-making process between different collaborators with conflicting objectives.</p>",2016-12-01,--Choose Location,2017-10-11 13:06:14.048,True,2015-09-01,City Game,PUBLIC,,False,City Science,False
places,mkh,False,"<p>Changing Places researchers  are developing scalable strategies for creating hyper-efficient, technology-enabled spaces that can help make living more affordable, productive, enjoyable, and creative for urban dwellers.<br></p>",,,2019-05-24 21:06:01.300,True,2017-08-01,Theme | Changing Places,PUBLIC,,True,City Science,False
cityscope-barcelona,gowharji,False,"<p>The ""Barcelona"" demo is an independent prototype designed to model and simulate human interactions within a Barcelona-like urban environment. Different types of land use (residential, office, and amenities) are configured into urban blocks and analyzed with agent-based techniques.</p>",2016-01-01,--Choose Location,2018-04-27 14:41:51.014,True,2015-09-01,CityScope Barcelona,PUBLIC,,False,City Science,False
cityscope-barcelona,csmuts,False,"<p>The ""Barcelona"" demo is an independent prototype designed to model and simulate human interactions within a Barcelona-like urban environment. Different types of land use (residential, office, and amenities) are configured into urban blocks and analyzed with agent-based techniques.</p>",2016-01-01,--Choose Location,2018-04-27 14:41:51.014,True,2015-09-01,CityScope Barcelona,PUBLIC,,False,City Science,False
ant-based-modeling,csmuts,False,"<p>Ant-Based Modeling explores the possibility of&nbsp;<span style=""font-size: 18px; font-weight: 400;"">implementing agent-based modeling with living ants&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">and external stimuli such as electromagnetic radiations,&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">magnetic fields, and electric fields. In an experiment&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">with fire ants, we discovered that ultraviolet and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">infrared lights can affect their behavior in the form of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">attraction and dispersion towards the light,&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">respectively. The video highlights some of the LEGOmade&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">landscapes we use in our explorations and how&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">the behavior of ants can be influenced by ultraviolet&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">light to achieve certain purposes such as exploring a&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">new area or dragging a ping pong ball to a specific&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">location. These experiments have allowed us to learn&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">more about ants and have inspired us to explore novel&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">forms of human-ant interaction.</span></p>",2017-04-22,--Choose Location,2018-04-26 15:20:39.904,True,2016-04-22,Ant-Based Modeling,PUBLIC,http://oi7.me,False,City Science,False
OLD_replace,csmuts,False,"<p>RePlace is a 3D data visualization platform that takes the normally invisible activity of sensors and data loggers and overlays that information as 3D sprites in the environment. RePlace uses an augmented reality viewfinder as a window into the data environment, and enables interaction with real-time and historic data feeds through the viewfinder. </p>",2016-12-01,--Choose Location,2018-05-04 18:23:12.980,True,2015-09-01,RePlace,PUBLIC,,False,City Science,False
OLD_cityscope-mark-ivb-land-usetransportation,csmuts,False,"<p>CityScope MarkIVb is programmed to demonstrate and model the relationship between land use (live and work), population density, parking supply and demand, and traffic congestion. </p>",,--Choose Location,2017-10-16 03:05:09.935,True,2014-09-01,CityScope Mark IVb: Land Use/Transportation,PUBLIC,,True,City Science,False
andorra-innovation,csmuts,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>The MIT Media Lab's City Science research group, the University of Andorra, and national and international companies are collaborating in order to bring an innovative ecosystem into the capital of Andorra. This innovation district aims to engage local citizens, researchers, and R&amp;D from the companies in order to build together an Andorran living lab, an ""innovation district"" where national and international companies can test and deploy their products and ideas and cultivate human capital.</p><p><b>Current Projects</b></p><ul><li>Andorra Innovation Space</li><li>Andorra Cultural Heritage</li><li>Drones patterns and flows, collaboration living lab<br></li><li>Young Future</li></ul>",,,2018-07-09 18:49:41.844,True,2016-09-01,Andorra | Innovation,PUBLIC,,True,City Science,False
termites,csmuts,False,"<p>TerMITes are wireless environmental sensors that capture data to help us better understand our environments and human behavior. The sensor data is time-stamped and place-tagged, but otherwise hardware agnostic. TerMITes support multi-modal sensor attachments using common protocols and can be attached to objects in the home such as doors, windows, drawers, cabinets, tables, and chairs to register object usage. TerMITes directly log on to the Internet via low-power Wi-Fi for ease of connection and automatically upload&nbsp;to a centralized database. TerMITes bridge existing methods for qualitative inquiry about our experiences in various planes to quantitative recording based on sensor input. TerMITes are currently used to gather data on humidity, presence detection, ambient light, motion, carbon dioxide, and temperature.&nbsp;</p>",,,2019-04-08 16:56:32.037,True,2017-09-01,TerMITes,PUBLIC,http://termites.synthetic.space/,True,City Science,False
andorra-dynamic-urban-planning,csmuts,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>",,,2019-02-25 15:17:09.063,True,2016-09-01,Andorra | Dynamic Urban Planning,PUBLIC,,True,City Science,False
spatial-flux,csmuts,False,"<p>Structurally, zero gravity means that we do not have to contend with architecture's greatest arch-nemesis, gravity. This opens up a new world of possibilities where we can deploy structures that no longer have to counteract/resist gravitational force. We would like to explore new forms of rapid inflatable prototyping. Most importantly, this prototype explores surfaces utilizing materials that would normally fail on Earth, yet flourish in zero gravity.</p><p>This year the MIT Media Lab's City Science group had an opportunity to think of architecture at the scale of the body that was literally out of this world. These are the results.</p>",,,2019-04-17 19:45:01.556,True,2017-06-01,Spatial Flux: Body and architecture in space,PUBLIC,,True,City Science,False
andorra-living-lab,csmuts,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,City Science,False
andorra-energy-environment,csmuts,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p>",,,2018-10-22 21:46:23.783,True,2016-09-02,Andorra | Energy + Environment,PUBLIC,,True,City Science,False
escape-pod-1,csmuts,False,"<p>The esc-Pod&nbsp; (or Escape Pod) is an exploratory platform for researchers investigating moments of refuge within our bustling work lives.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">The core of the esc-Pod consists of actuated work and rest surfaces. This allows for moments of productivity and relaxation to occur within a single space.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The outer skin provides variable transparency, enabling a spectrum of visibility settings according to privacy requirements.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The inner skin provides an infrastructure for the modulation of spatial experiences. Each panel is a pixel, connecting itself to the skin network, and can embody an array of senses.</span></p>",,,2019-04-08 17:01:14.555,True,2016-08-01,Escape Pod,PUBLIC,,True,City Science,False
cityscope-cooper-hewitt,csmuts,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
city-science-andorra,csmuts,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
cityscope-barcelona,ryanz,False,"<p>The ""Barcelona"" demo is an independent prototype designed to model and simulate human interactions within a Barcelona-like urban environment. Different types of land use (residential, office, and amenities) are configured into urban blocks and analyzed with agent-based techniques.</p>",2016-01-01,--Choose Location,2018-04-27 14:41:51.014,True,2015-09-01,CityScope Barcelona,PUBLIC,,False,City Science,False
DUI,ryanz,False,<p>Analyze and visualize urban interaction with computer vision and deep neural net.&nbsp;</p>,,,2018-05-03 15:51:35.403,True,2018-02-14,Deep Urban Interaction,PUBLIC,,True,City Science,False
andorra-dynamic-urban-planning,ryanz,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>",,,2019-02-25 15:17:09.063,True,2016-09-01,Andorra | Dynamic Urban Planning,PUBLIC,,True,City Science,False
citymatrix,ryanz,False,"<h2><br>An Urban Decision-Support System Augmented by Artificial Intelligence<br></h2><p>The decision-making process in urban design and urban planning is outdated. Currently, urban decision-making is mostly a top-down process, with community participation only in its late stages. Furthermore, many design decisions are subjective, rather than based on quantifiable performance and data. Current tools for urban planning do not allow both expert and non-expert stakeholders to explore a range of complex scenarios rapidly with real-time feedback.&nbsp;</p><p>CityMatrix was an effort towards evidence-based, democratic decision-making. Its contributions lie in the application of Machine Learning as a versatile, quick, accurate, and low-cost approach to enable real-time feedback of complex urban simulations and the implementation of the optimization searching algorithms to provide open-ended decision-making suggestions.&nbsp;The goals of CityMatrix were:&nbsp;</p><br><ol><li><i>Designing an intuitive Tangible User Interface (TUI) to improve the accessibility of the decision-making process for non-experts.&nbsp;</i></li><li><i>Creating real-time feedback on multi-objective urban performances to help users evaluate their decisions, thus to enable rapid, collaborative decision-making.&nbsp;</i></li><li><i>Constructing a suggestion-making system that frees stakeholders from excessive, quantitative considerations and allows them to focus on the qualitative aspects of the city, thus helping them define and achieve their goals more efficiently.</i></li></ol><p>CityMatrix was augmented by Artificial Intelligence (AI) techniques including Machine Learning simulation predictions and optimization search algorithms. The hypothesis explored in this work was that the decision quality could be improved by the organic combination of both strengths of human intelligence and machine intelligence.</p><p>The system was pilot-tested and evaluated by comparing the problem-solving results of volunteers, with or without AI suggestions. Both quantitative and qualitative analytic results showed that CityMatrix is a promising tool that helps both professional and non-professional users understand the city better to make more collaborative and better-informed decisions.&nbsp;</p>",,,2018-10-17 18:10:46.064,True,2016-02-26,CityMatrix,PUBLIC,https://media.mit.edu/people/ryanz,True,City Science,False
cityscope-volpe,ryanz,False,"<p>CityScope Volpe is demonstrating most of the urban planning, analysis, and prediction features developed for the CityScope project. The site, a 14-acre parcel on the northern part of MIT/Kendall Square area of Cambridge, has been acquired and is&nbsp; being developed by MIT. City Science researchers designed and built a CityScope urban performance tool that is aiming to predict the outcomes of multiple planning and development scenarios.&nbsp;</p>",,,2019-02-25 15:14:11.403,True,2016-11-01,CityScope Volpe,PUBLIC,,True,City Science,False
andorra-mobility,ryanz,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile</a></p><p>With no airport or train service, most of the 8 million tourists who visit Andorra each year arrive by car, making traffic management and parking some of the country's most important challenges. We are currently developing different projects spanning from data science to the deployment of autonomous vehicles to help address these issues.<br></p>",,,2017-10-25 05:56:26.309,True,2016-09-01,Andorra | Mobility,PUBLIC,,True,City Science,False
city-science-lab-shanghai,ryanz,False,"<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>",,,2019-05-16 20:43:38.677,True,2017-02-14,City Science Lab Shanghai,PUBLIC,,True,City Science,False
cityscope-livingline-shanghai,ryanz,False,"<p>College of Design and Innovation of Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform to support the urban decision making that promotes urban vibrancy and innovation potential. </p><p>The “NICE2035 LivingLine” project in Shanghai, China, is a design-driven, community-based urban innovation initiated by Professor Yongqi Lou, Dean of College of Design of Innovation. LivingLine is a crowdsourcing and co-creation project aiming at building an ecosystem of innovation and entrepreneurship on the internal street of a typical gated residential neighborhood. By introducing radical programs such as living labs, co-working space, and startup-incubators into underutilized storefront space, LivingLine’s goal is to revitalize the urban space and to prototype diverse future lifestyles.<br></p>",,,2019-06-06 16:01:44.891,True,2018-03-01,CityScope LivingLine Shanghai,PUBLIC,,True,City Science,False
cityscope-cooper-hewitt,ryanz,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
city-science-andorra,ryanz,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
cityscope,ryanz,False,"<p>City Science researchers are developing a slew of tangible and digital platforms dedicated to solving spatial design and urban planning challenges. The tools range from simulations that quantify the impact of disruptive interventions in cities to communicable collaboration applications. We develop and deploy these tools around the world and maintain open source repositories for the majority of deployments. ""CityScope"" is a concept for shared, interactive computation for urban planning.</p><p>All current CityScope development, tools, and software are open source <a href=""https://cityscope.github.io/"">here</a>.&nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2019-05-16 20:42:57.474,True,2017-08-01,Theme | CityScope,PUBLIC,,True,City Science,False
desafio-aprendizagem-criativa-brasil-2018,leob,False,"<p><b>ATENÇÃO:</b>&nbsp;Saiu o <b>resultado do Desafio Aprendizagem Criativa Brasil 2018</b>! Clique <a href=""https://www.media.mit.edu/posts/resultado-do-desafio-aprendizagem-criativa-brasil-2018/"">aqui</a> para conhecer os fellows e os projetos selecionados!</p><p>----</p><p>O Desafio Aprendizagem Criativa Brasil é uma iniciativa da Fundação Lemann e do MIT Media Lab que visa fomentar a implementação de soluções inovadoras – novas tecnologias, produtos e serviços – que ajudem a tornar a educação brasileira mais mão na massa, significativa, colaborativa e lúdica.</p><p>O Desafio também tem como objetivo identificar, conectar e apoiar indivíduos brasileiros – artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decisão – que possam ter um papel-chave no avanço de práticas de Aprendizagem Criativa, especialmente no que se refere a projetos mão na massa envolvendo programação e construção no mundo físico, em escolas públicas (de Educação Infantil ao Ensino Médio) e ambientes de aprendizagem não formais de todo o Brasil.</p><p>Os representantes dos projetos selecionados ganharão uma&nbsp;<i>Creative Learning Fellowship&nbsp;</i>para ajudar a implementar seu trabalho.&nbsp;</p><p><b>As inscrições vão até o&nbsp;dia&nbsp; 9 de fevereiro e devem ser feitas única e exclusivamente através do formulário abaixo.</b></p><p>Clique <a href=""https://docs.google.com/document/d/1N6AgIZc7W6544cJKw5dsVmNSTgTzt1LAyMOiDDSKaqU/edit?usp=sharing"">aqui</a> para a&nbsp;<b>chamada de projetos</b>&nbsp;completa.<br></p><p>Clique <a href=""https://www.questionpro.com/t/AN4HdZbPeN"">aqui</a>&nbsp;para o&nbsp;<b>formulário de inscrição</b>.</p><p>Clique&nbsp;<a href=""https://docs.google.com/document/d/1LpYYcImuoZCeIm7XRcE1bBP62wdDgLQIfPmVN0_7AxM/edit?usp=sharing"">aqui</a>&nbsp;para respostas às&nbsp;<b>perguntas mais frequentes</b>.</p><p><b>Atenção</b>: &nbsp;esta página será atualizada periodicamente com mais informações sobre o Desafio. Discussões sobre o edital estão ocorrendo no&nbsp;&nbsp;<a href=""https://forum.aprendizagemcriativa.org/t/chamada-de-projetos-desafio-aprendizagem-criativa-brazil-2018/"">fórum da Rede Brasileira de Aprendizagem Criativa</a>.&nbsp;</p>",2018-12-31,,2018-12-03 22:52:00.488,True,2018-01-22,Desafio Aprendizagem Criativa Brasil 2018,PUBLIC,,False,Lifelong Kindergarten,False
desafio-aprendizagem-criativa,leob,False,"<p><b>ATENÇÃO:</b> Saiu o <b>resultado do Desafio Aprendizagem Criativa Brasil 2017</b>! Clique <a href=""https://www.media.mit.edu/posts/resultado-do-desafio-aprendizagem-criativa-brasil-2017/"">aqui</a> para conhecer os fellows e projetos selecionados!</p><p>----</p><p>&nbsp;Desafio Aprendizagem Criativa Brasil é uma iniciativa da Fundação Lemann e do MIT Media Lab que visa fomentar a implementação de soluções inovadoras – novas tecnologias, produtos e serviços – que ajudem a tornar a educação brasileira mais mão na massa, significativa, colaborativa e lúdica.</p><p>O Desafio também tem como objetivo identificar, conectar e apoiar indivíduos brasileiros – artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decisão – que possam ter um papel-chave no avanço de práticas de Aprendizagem Criativa, especialmente no que se refere a projetos mão na massa envolvendo programação e construção no mundo físico, em escolas públicas (de Educação Infantil ao Ensino Médio) e ambientes de aprendizagem não formais de todo o Brasil.</p><p>Os representantes dos projetos selecionados ganharão uma <i>Creative Learning Fellowship </i> para ajudar a implementar seu trabalho.&nbsp;</p><p><b>As inscrições vão até o dia 5 de fevereiro de 2017 e devem ser feitas única e exclusivamente através do formulário abaixo.</b><br></p><p>Clique <a href=""https://www.dropbox.com/s/taxr6e878q1e5h6/Desafio%20Aprendizagem%20Criativa%20Brasil%20-%2020170125a.pdf?dl=1"">aqui</a>&nbsp;para a <b>chamada de projetos</b> completa.<br></p><p>Clique <a href=""https://www.tfaforms.com/4597543"">aqui</a> para o <b>formulário de inscrição</b>.</p><p>Clique <a href=""https://docs.google.com/document/d/1N2GKlkc_t83Kp0V4dgR_PzeCp5FJvhov-93OnBXpY0Y/edit?usp=sharing"">aqui</a> para respostas às <b>perguntas mais frequentes</b>.</p><p><b>Atenção</b>: &nbsp;esta página será atualizada periodicamente com mais informações sobre o Desafio. Discussões sobre o edital estão ocorrendo no <a href=""http://forum.aprendizagemcriativa.org/t/chamada-de-projetos-desafio-aprendizagem-criativa-brazil/238"">fórum da Rede Brasileira de Aprendizagem Criativa</a>.</p>",2017-12-31,,2018-01-22 15:34:04.822,True,2017-01-13,Desafio Aprendizagem Criativa Brasil 2017,PUBLIC,,False,Lifelong Kindergarten,False
duct-tape-network,leob,False,"<p>The Duct Tape Network (DTN) is a series of fun, hands-on maker clubs that encourage young children (ages 7-10) to use cardboard, tape, wood, fabric, LED lights, motors, and more to bring their stories and inventions to life. We are designing an educational framework and toolkit to engage kids in the creation of things that they care about before they lose their curiosity or get pulled in by more consumer-oriented technology. Work on DTN started in 2014 as part of a collaboration with Autodesk and is now expanding to communities all around the world.</p>",,--Choose Location,2017-02-10 02:45:19.416,True,2015-01-01,Duct Tape Network,PUBLIC,,True,Lifelong Kindergarten,False
rede-brasileira-de-aprendizagem-criativa,leob,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Somos uma rede de educadores, artistas, pesquisadores, empreendedores, alunos e outros interessados na implementação de ambientes educacionais mais mão-na-massa, criativos e interessantes nas escolas, universidades, espaços não-formais de aprendizagem e residências de todo o Brasil.</span><br></p><p>A Rede Brasileira de Aprendizagem Criativa surgiu em 2015 a partir de uma parceria entre o Programaê (uma colaboração da Fundação Lemann com a Fundação Telefonica Vivo) e o Lifelong Kindergarten Group do MIT Media Lab. Atualmente, contamos com centenas de participantes de todo o Brasil.</p>",,,2017-04-19 17:55:54.327,True,2015-10-12,Rede Brasileira de Aprendizagem Criativa,PUBLIC,http://aprendizagemcriativa.org/,True,Lifelong Kindergarten,False
desafio-aprendizagem-criativa-brasil-2019,leob,False,"<p><b>ATENÇÃO:&nbsp;</b>Saiu o&nbsp;<b>resultado do Desafio Aprendizagem Criativa Brasil 2019!</b>&nbsp;Clique&nbsp;<a href=""https://www.media.mit.edu/posts/resultado-do-desafio-aprendizagem-criativa-brasil-2019/"">aqui</a>&nbsp;para conhecer os fellows e os projetos selecionados!</p><p>----</p><p>O Desafio Aprendizagem Criativa Brasil visa fomentar a implementação de soluções inovadoras que ajudem a tornar a educação brasileira mais criativa, prazerosa, relevante, colaborativa e inclusiva para crianças e jovens de todo o país.</p><p>Organizado pela <a href=""http://aprendizagemcriativa.org/"">Rede Brasileira de Aprendizagem Criativa</a>, e contando com o apoio da <a href=""http://fundacaolemann.org.br/"">Fundação Lemann</a> e do <a href=""http://media.mit.edu/"">MIT Media Lab</a>, o Desafio também tem como objetivo identificar, conectar e apoiar indivíduos brasileiros – artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decisão – que possam ter um papel-chave no avanço de práticas de aprendizagem criativa em escolas públicas (do Ensino Fundamental ao Ensino Médio) e ambientes de aprendizagem não formais de todo o Brasil.</p><p>Os representantes dos projetos selecionados ganharão uma&nbsp;<i>Creative Learning Fellowship&nbsp;</i>para ajudar a implementar seu trabalho.&nbsp;</p><p><b>As inscrições vão até o&nbsp;dia&nbsp; 13 de janeiro de 2019 e devem ser feitas única e exclusivamente através do formulário abaixo.</b></p><p>Clique&nbsp;<a href=""https://docs.google.com/document/d/1xmpCN_IDsOiRwd5KqDD3aeAAa46qXCKR5DpLWzwng7U/edit?usp=sharing"">aqui</a>&nbsp;para a&nbsp;<b>chamada de projetos</b>&nbsp;completa.<br></p><p>Clique&nbsp;<a href=""https://aprendizagemcriativa.fluidreview.com/"">aqui</a>&nbsp;para o&nbsp;<b>formulário de inscrição</b>.</p><p>Clique&nbsp;<a href=""https://docs.google.com/document/d/1LpYYcImuoZCeIm7XRcE1bBP62wdDgLQIfPmVN0_7AxM/edit?usp=sharing"">aqui</a>&nbsp;para respostas às&nbsp;<b>perguntas mais frequentes</b>.</p><p><b>Atenção</b>: &nbsp;esta página será atualizada periodicamente com mais informações sobre o Desafio. Discussões sobre o edital estão ocorrendo no&nbsp;<a href=""https://forum.aprendizagemcriativa.org/t/chamada-de-projetos-desafio-aprendizagem-criativa-brasil-2019/3547/10"">&nbsp;fórum da Rede Brasileira de Aprendizagem Criativa.</a>&nbsp;</p>",,,2019-02-23 00:21:05.368,True,2018-12-03,Desafio Aprendizagem Criativa Brasil 2019,PUBLIC,,True,Lifelong Kindergarten,False
creative-learning-in-brazil,leob,False,"<p>The Lemann Creative Learning Program is a collaboration between the MIT Media Lab and the Lemann Foundation to foster creative learning in Brazilian public education.&nbsp;</p><p>Established in February 2015, the program designs new technologies, support materials, and innovative initiatives to engage Brazilian public schools, afterschool centers, and families in learning practices that are more hands-on, creative, and centered on students' interests and ideas.&nbsp;</p><p>---</p><p>O Programa Lemann de Aprendizagem Criativa é uma colaboração entre o MIT Media Lab e a Fundação Lemann visando incentivar a aprendizagem criativa na educação pública do Brasil.</p><p>Criado em fevereiro de 2015, o programa cria novas tecnologias, materiais de apoio e iniciativas que ajudem escolas públicas, organizações de educação não formal, e famílias a implementar práticas de aprendizagem que sejam mais mão na massa, criativas e centradas nos interesses dos alunos.</p>",,--Choose Location,2018-12-11 21:36:57.377,True,2015-01-01,Creative Learning in Brazil / Aprendizagem Criativa no Brasil,PUBLIC,http://aprendizagemcriativa.org/,True,Lifelong Kindergarten,False
festival-de-invencao-e-criatividade,leob,False,"<p>The Festival of Invention and Creativity (FIC) is a great celebration of the inventive, collaborative, and hands-on&nbsp; spirit&nbsp; of Brazilian education. In it, children, young people, their families and educators have&nbsp; opportunity to explore high and low tech tools and materials, participate in interactive workshops and learn in a stimulating and relaxed way.</p><p>The Festival aims to disseminate, inspire and facilitate the implementation of creative learning activities&nbsp; in schools and non-formal educational environments.&nbsp;</p><p>In 2018, the Brazilian Creative Learning Network, with support from the MIT Media Lab and the Lemann Foundation, facilitated the organization of more than 15 regional Festivals throughout the country.</p><p>----</p><p>O Festival de Invenção e Criatividade (FIC) é uma grande celebração do espírito inventivo, colaborativo e mão na massa da educação brasileira. Nele, crianças, jovens, seus familiares e educadores tem a oportunidade de explorar materiais e tecnologias high e low tech, participar de atividades interativas, e aprender de forma estimulante e descontraída.</p><p>O Festival tem como objetivos divulgar, inspirar e facilitar a implementação de atividades de aprendizagem mão na massa em escolas e ambientes de educação não formal.</p><p>Só em 2018, a Rede Brasileira de Aprendizagem Criativa, contando com o apoio do MIT Media Lab e da Fundação Lemann, facilitou a organização de mais de 15 Festivais regionais por todo o país.</p>",,,2018-12-14 08:20:17.955,True,2017-01-10,Festival of Invention and Creativity,PUBLIC,http://www.ficmaker.org.br,True,Lifelong Kindergarten,False
havana-bread,xxxxxxin,False,"<p>Tourism is Cuba’s third largest source of foreign currency, behind the two dominant industries of sugar and tobacco. The number of visitors so far in 2016 jumped 13.5 percent over the year to 1.5 million tourists. However, the hunt for novelty in Cuba doesn’t conceal its disquieting poverty and struggles: lack of regulation on sex work, ungovernable black markets and creaky infrastructure. Everyday in Havana, we inevitably enjoyed the material comforts like all the other tourists while investigating the power relations of the nation as the artists.</p><p>This project calls attention to the actions of each individual, using the nation’s subsidized, soft, sweet, round daily buns as the vehicle. Made of imported flour, sugar, dry yeast and water, the daily bun tastes nothing but pale flour. Collaborating with IFF (International Flavor and Fragrance), we designed the bread with a complete flavor that balances the nuances of gasoline, sweat and white ginger flower (the national flower of Cuba).</p>",2017-05-31,,2017-11-30 15:56:33.024,True,2017-03-01,Havana Bread,PUBLIC,,False,Initiatives,False
masque,xxxxxxin,False,"<p>&nbsp;When the body senses itself internally and localizes its actions, it provides the basis for a material sense of self-existence. At the same time, the mind registers the sense of an agency with free will, the sense of being, the cause of voluntary action. Among all interoceptive experiences, respiration is the only one that we can regulate directly. There are many&nbsp;psychophysical breathing exercises&nbsp;to help self-regulation and reflection, that, combined with meditation and yoga, are designed to restore natural, smooth breathing appropriate to the physical needs of the body.&nbsp;</p>",2018-05-31,,2018-10-20 21:47:28.195,True,2017-01-01,Masque,PUBLIC,,False,Initiatives,False
d-Abyss,xxxxxxin,False,"<p><b>Can tattoos embrace technology in order to make the skin interactive?</b></p><p>The DermalAbyss project is the result of a collaboration between MIT researchers Katia Vega, Xin Liu, Viirj Kan and Nick Barry and Harvard Medical School researchers Ali Yetisen and Nan Jiang.&nbsp;<br></p><p>DermalAbyss is a proof-of-concept that presents a novel approach to bio-interfaces in which the body surface is rendered an interactive display. Traditional tattoo inks are replaced with biosensors whose colors change in response to variations in the interstitial fluid. It blends advances in biotechnology with traditional methods in tattoo artistry.&nbsp;</p><p>This is a research project, and there are currently no plans to develop Dermal Abyss as a product or to pursue clinical trials.<br></p>",2017-05-31,,2018-04-27 17:45:10.726,True,2016-06-01,DermalAbyss: Possibilities of Biosensors as a Tattooed Interface,PUBLIC,,False,Initiatives,False
fluxa,xxxxxxin,False,"<p>Fluxa is a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a transient wearable display to foster richer self-expression and communication in daily life . It can be used to enhance existing social gestures such as handwaving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a decoration device that generates images around dancing bodies.</p>",2017-11-30,--Choose Location,2018-10-12 16:57:48.806,True,2016-01-01,Fluxa,PUBLIC,,False,Initiatives,False
skrin,xxxxxxin,False,"<p>Skrin is an exploration project on digitalized body skin surface using embedded electronics and prosthetics. Human skin is a means for protection, a mediator of our senses, and a presentation of our selves. Through several projects, we expand the expression capacity of the body's surface and emphasize the dynamic aesthetics of body texture by technological means.&nbsp;<span style=""font-size: 18px;"">Working with conventional special effect makeup artists, we “hide” electronics into silicone which is applied onto skin and covered by cosmetics. The digitalized skin surface is connected with the affective experience, while the illuminated body is a representation of internal state.</span></p><p>Working with bionic pop artist Viktoria Modesta, we deployed the project in Music Tech Festival Berlin 2016 and transformed her body as a canvas along with the performance.</p>",2017-06-30,--Choose Location,2018-12-05 14:35:37.656,True,2016-01-01,Skrin,PUBLIC,,False,Initiatives,False
tree,xxxxxxin,False,"<p><i>Tree</i> is a virtual experience that transforms you into a rainforest tree. With your arms as branches and body as the trunk, you experience the tree’s growth from a seedling into its fullest form and witness its fate firsthand.&nbsp;Collaborating with director Milica Zec and Winslow Porter, we designed and constructed the entire tactile experience throughout the film. With precisely controlled physical elements including vibration, heat, fan and body haptics, the team created a fully immersive virtual reality storytelling to, where the audience no longer watches but is transformed into a new identity, a giant tree in the peruvian rainforest.</p><p><i>Tree</i> debuted at Sundance Film Festival 2017 New Frontier and also had its presentation in Tribeca Film Festival 2017. &nbsp;</p><p>The project is part of our research about body ownership illusion in virtual reality (early project: <a href=""https://www.media.mit.edu/projects/treesense/overview/"">TreeSense</a>).&nbsp;The tactile experience is crucial for establishing a body ownership illusion instead of restricting the experience to the visual world. We aim to have the audience not just see, but feel and believe ""being"" a tree.&nbsp;</p>",2017-06-30,,2018-10-20 22:01:11.764,True,2017-01-18,Tree,PUBLIC,,False,Initiatives,False
orbit-weaver-suit,xxxxxxin,False,"​​​​​​​​​<p>The Orbit Weaver Suit for&nbsp;zero gravity was&nbsp;designed by Media Lab Director's Fellow Andrea Lauer in collaboration with Xin Liu.&nbsp;</p><p>The design was inspired by a drawing Jordan Piantedosi made for Xin Liu, in which she&nbsp;is in a suit and casting&nbsp;strings out into space.&nbsp;The drawing is a reference to Orbit Weaver, a performance piece Xin created to test in a zero-gravity environment in November 2017.&nbsp;</p>",,,2017-11-30 15:54:37.271,True,2017-10-17,Orbit Weaver Suit,PUBLIC,,True,Initiatives,False
orbit-weaver,xxxxxxin,False,"<p>Gravity anchors all existence on Earth.<br></p><p>It pulls a chaotic world to one single point in every moment of life. Even though gravity is everywhere and unending, for most of the universe, vast empty space dominates, leaving us free from gravity’s tether.&nbsp;Is the weightless state a moment of true autonomy, or does the ungrounded body simply lose control?</p><p>Orbit Weaver is a series of imaginative interactions with moving bodies in zero gravity. The project aims to understand, create, and share the beautiful and sentimental moments of being weightless and lost in outer space.&nbsp;</p>",,,2019-03-13 14:18:01.925,True,2017-09-01,Orbit Weaver,PUBLIC,,True,Initiatives,False
living-distance,xxxxxxin,False,"<p>Living Distance is a mission and a fantasy realized, in which a wisdom tooth is sent to outer space and back down to Earth again. Carried by a crystalline robotic device called EBIFA, the tooth tells the inconsequential but unique story of a person in this universe.&nbsp;EBIFA's form and function follow an unusually personal approach to our technological space futures, one centered on visceral, active, empathic, and poetic engagement.<br></p>",,,2019-06-03 13:32:35.379,True,2018-09-03,Living Distance,PUBLIC,http://xxxxxxxxxinliu.com,True,Initiatives,False
slap-snap-tap,joyab,False,"<p>Slap Snap Tap combines wearable sensors with physical block programming to enable enhanced expression through movement. By slapping on a set of sensor straps, snapping in code that links movement triggers to sound actions, and tapping the sensors to activate a play experience, users can combine motion and sound in creative ways. A dancer can create music through movement; an athlete can add emphasis to her performance; demonstrators can synchronize and amplify a chant; and anyone can create sound effects for life moments. Slap Snap Tap is a method of the Slay Play endeavor which aims to broaden participation in computational creation by using movement as a pathway into computational thinking.</p>",2016-12-31,--Choose Location,2017-03-31 19:41:01.413,True,2016-01-01,Slap Snap Tap,PUBLIC,,False,Civic Media,False
gender-shades,joyab,False,"<p>The Gender Shades project pilots an intersectional approach to inclusive product testing for AI.</p><h1><b>Algorithmic Bias Persists</b></h1><p>Gender Shades is a preliminary excavation of the inadvertent negligence that will cripple the age of automation and further exacerbate inequality if left to fester. The deeper we dig, the more remnants of bias we will find in our technology. We cannot afford to look away this time, because the stakes are simply too high. &nbsp;We risk losing the gains made with the civil rights movement and women's movement under the false assumption of machine neutrality. Automated systems are not inherently neutral. They reflect the priorities, preferences, and prejudices—the coded gaze—of those who have the power to mold artificial intelligence.</p>",,,2018-04-30 15:22:20.722,True,2017-01-01,Gender Shades,PUBLIC,http://www.gendershades.org,True,Civic Media,False
code4rights,joyab,False,"<p>Code4Rights promotes human rights through technology education. By facilitating the development of rights-focused mobile applications in workshops and an online course, Code4Rights enables participants to create meaningful technology for their communities in partnership with local organizations. For example, Code4Rights, in collaboration with It Happens Here, a grassroots organization focused on addressing sexual violence, created the First Response Oxford App to address sexual violence at Oxford University. Over 30 young women contributed to the creation of the app, which provides survivors of sexual violence and friends of survivors with information about optional ways to respond, essential knowledge about support resources, critical contact details, and answers to frequently asked questions.  </p>",,--Choose Location,2016-12-05 00:16:54.952,True,2014-09-01,Code4Rights,PUBLIC,http://www.code4rights.org,True,Civic Media,False
actionable-auditing-coordinated-bias-disclosure-study,joyab,False,"<p>Algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, yet scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of <a href=""http://www.gendershades.org"">Gender Shades</a>, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the <a href=""http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf"">Gender Shades study</a>;&nbsp;2) presents new performance metrics from targeted companies IBM, Microsoft, and Megvii(Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018; 3) provides performance results on PPB by non-target companies Amazon and Kairos; and 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within seven months of the original audit, we find that all three targets released new API versions.&nbsp;</p><p>All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of&nbsp;<b>31.37%&nbsp;</b>and&nbsp;&nbsp;<b>22.50%&nbsp;</b>&nbsp;for the darker female subgroup, respectively.&nbsp;</p><p>While algorithmic fairness may be approximated through reductions in subgroup error rates or other performance metrics, algorithmic justice necessitates a transformation in the development, deployment, oversight,&nbsp;and regulation of facial analysis technology. Consequently, the potential for weaponization and abuse of facial analysis technologies cannot be ignored, nor the threats to privacy or breaches of civil liberties diminished even as accuracy disparities decrease. More extensive explorations of policy, corporate practice, and ethical guidelines are thus needed to ensure vulnerable and marginalized populations are protected and not harmed as this technology evolves.&nbsp;</p>",,,2019-02-11 18:35:25.732,True,2019-01-24,Actionable Auditing: Coordinated bias disclosure study,PUBLIC,,True,Civic Media,False
algorithmic-justice-league,joyab,False,"<b><a href=""http://www.ajlunited.org"">www.ajlunited.org</a></b><br><p>An unseen force is rising—helping to determine who is hired, granted a loan, or even how long someone spends in prison. This force is called the coded gaze.</p><p> </p><p>However, many people are unaware of the growing impact of the coded gaze and the rising need for fairness, accountability, and transparency in coded systems. Without knowing discriminatory practices are at play, citizens are unable to affirm their rights or identify violations.</p><p>The Algorithmic Justice League aims to:</p><ol><li>highlight algorithmic bias through provocative media and interactive exhibitions<br></li><li>provide space for people to voice concerns and experiences with coded discrimination&nbsp;</li><li>develop practices for accountability during the design, development, and deployment phases of coded systems.</li></ol>",,,2018-04-30 15:28:35.674,True,2016-10-14,Algorithmic Justice League,PUBLIC,http://www.poetofcode.com,True,Civic Media,False
Pol2,monsted,False,"<p>&nbsp;Research in Social Psychology has demonstrated that people's cognition is heavily influenced by an individual's sense of identity, which, in turn, is determined in part by their group relations.</p><p>Using large amounts of data from online social systems, this project aims to uncover the role played in online polarization by this mechanism, especially in conjunction with the currently emerging plethora of alternative news sources.</p>",2018-12-03,,2018-05-07 17:26:55.950,False,2018-05-01,Mechanisms of Political Polarization in Online Systems,PUBLIC,,False,Scalable Cooperation,False
cityoffice,hlarrea,False,"<p>Architectural robotics enable a hyper-efficient, dynamically reconfigurable co-working space that accommodates a wide range of activities in a small area.</p>",2016-12-01,--Choose Location,2017-10-18 00:44:53.325,True,2014-09-01,City Office,GROUP,,False,City Science,False
places,hlarrea,False,"<p>Changing Places researchers  are developing scalable strategies for creating hyper-efficient, technology-enabled spaces that can help make living more affordable, productive, enjoyable, and creative for urban dwellers.<br></p>",,,2019-05-24 21:06:01.300,True,2017-08-01,Theme | Changing Places,PUBLIC,,True,City Science,False
cityoffice,alonsolp,False,"<p>Architectural robotics enable a hyper-efficient, dynamically reconfigurable co-working space that accommodates a wide range of activities in a small area.</p>",2016-12-01,--Choose Location,2017-10-18 00:44:53.325,True,2014-09-01,City Office,GROUP,,False,City Science,False
_amoeba,alonsolp,False,"<p>Amoeba Wall: a context aware wall system. Amoetecture is a set of amoeba-like dynamic spatial elements, including transformable floors, ceilings, tables, chairs, and workstations. We focus on designing architecture robotics and platforms that enable a hyper-efficient and dynamically reconfigurable co-working space that accommodates a wide range of activities in a small area.</p><p><strong>&nbsp;Award</strong></p><p>-<a href=""https://www.designboom.com/design/2019-a-design-award-and-competition-call-for-entries-02-07-2019/""><strong>A' Design Award&nbsp;</strong><strong>2017 -&nbsp;</strong><strong>Gold Prize</strong></a></p><p>-Honorable Mention - Tomorrow Workplace Competition by METROPOLIS</p><p><b>Publication</b></p><p>-<a href=""http://www.academia.edu/35684876/Amoeba_Wall"">H Deng, H Ho, L Alonso, X Li, J Angulo, K Larson Amoeba Wall - PASAJES - archquitectura NO.143, pp8-9</a></p>",2017-11-30,,2019-02-11 15:09:42.775,True,2016-04-22,Amoetecture,PUBLIC,http://rnd.studio/project/amoeba-wall,False,City Science,False
amoeba-wall,alonsolp,False,"<p>Amoeba Wall: A context aware wall system:&nbsp;Amoetecture is a set of amoeba-like dynamic spatial elements, including transformable floors, ceilings, tables, chairs, and workstations. We focus on designing architecture robotics and platforms that enable a hyper-efficient and dynamically reconfigurable coworking space that accommodates a wide range of activities in a small area.</p><p><strong>&nbsp;Award</strong></p><p><strong>-&nbsp;</strong><a href=""https://www.designboom.com/design/2019-a-design-award-and-competition-call-for-entries-02-07-2019/""><strong>A' Design Award&nbsp;</strong><strong>2017 -&nbsp;</strong><strong>Gold Prize</strong></a></p><p><strong>-&nbsp;</strong>Honorable Mention - Tomorrow Workplace Competition by METROPOLIS</p><p><b>Publication</b></p><p><b>-</b><a href=""http://www.academia.edu/35684876/Amoeba_Wall"">H Deng, H Ho, L Alonso, X Li, J Angulo, K Larson Amoeba Wall - PASAJES - archquitectura NO.143, pp8-9</a></p>",2016-12-31,,2019-02-11 03:26:16.506,True,2016-01-04,Amoeba Wall: A context-aware wall system,PUBLIC,http://rnd.studio/project/amoeba-wall,False,City Science,False
cityscope-hamburg,alonsolp,False,"<p><a href=""https://medium.com/mit-media-lab/shifting-priorities-finding-places-9ad3bdbe38b8"">Read more about this project here</a></p><p>MIT City Science is working with HafenCity University to develop CityScope for the neighborhood of Rothenburgsort in Hamburg, Germany. The goal is to create an interactive stakeholder engagement tool that also serves as the platform for joint research of modules for city simulation. Researchers are developing modules for walkability, neighborhood connectivity, energy efficiency, and economic activity, among others.</p>",,--Choose Location,2019-05-10 19:07:36.178,True,2015-01-01,City Science Lab Hamburg,PUBLIC,,True,City Science,False
traffic-andorra,alonsolp,False,"<h2>Data Fusion&nbsp;for Dynamic Traffic Prediction</h2><p>Traffic congestion has huge negative impacts on the productivity, health and personal lives of city dwellers. To manage this problem&nbsp;effectively, transportation engineers need to predict traffic congestion throughout the road network at all hours of the day.&nbsp;Prediction of traffic typically involves travel surveys that are expensive, time consuming and do not capture temporal variation in travel demand.&nbsp;However,&nbsp;anonymised&nbsp;location data from mobile phones present an alternative source of data which is passively collected, widely available and naturally captures temporal trends.&nbsp;On the other hand, these data contain other biases and so if we use these data for transportation models, we must take care to correct for these biases using more reliable data. As part of the City Science collaboration with Andorra, we used&nbsp;a&nbsp;Bayesian network to build a calibrated transportation model for the country based on&nbsp;geolocated telecoms data and validated using a small sample of traffic counts.</p>",,,2019-04-17 19:43:06.032,True,2017-01-01,Dynamic Traffic Prediction in Andorra: a Bayesian network approach,PUBLIC,,True,City Science,False
andorra-innovation,alonsolp,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>The MIT Media Lab's City Science research group, the University of Andorra, and national and international companies are collaborating in order to bring an innovative ecosystem into the capital of Andorra. This innovation district aims to engage local citizens, researchers, and R&amp;D from the companies in order to build together an Andorran living lab, an ""innovation district"" where national and international companies can test and deploy their products and ideas and cultivate human capital.</p><p><b>Current Projects</b></p><ul><li>Andorra Innovation Space</li><li>Andorra Cultural Heritage</li><li>Drones patterns and flows, collaboration living lab<br></li><li>Young Future</li></ul>",,,2018-07-09 18:49:41.844,True,2016-09-01,Andorra | Innovation,PUBLIC,,True,City Science,False
places,alonsolp,False,"<p>Changing Places researchers  are developing scalable strategies for creating hyper-efficient, technology-enabled spaces that can help make living more affordable, productive, enjoyable, and creative for urban dwellers.<br></p>",,,2019-05-24 21:06:01.300,True,2017-08-01,Theme | Changing Places,PUBLIC,,True,City Science,False
andorra-dynamic-urban-planning,alonsolp,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>",,,2019-02-25 15:17:09.063,True,2016-09-01,Andorra | Dynamic Urban Planning,PUBLIC,,True,City Science,False
city-science-lab-aalto,alonsolp,False,"<p>Aalto University, Finland, and the MIT Media Lab’s City Science group are co-developing a version of the MIT CityScope platform for urban analysis, efficient resource utilization, and spatial programming for campus development, using Otaniemi as a testbed. Aalto joins a network of City Science collaborators which includes Tongji University (Shanghai), Taipei Tech (Taiwan), HafenCity University (Hamburg), and ActuaTech (Andorra).</p>",,,2019-05-07 19:59:14.315,True,2017-05-01,City Science Lab Aalto,PUBLIC,,True,City Science,False
citymatrix,alonsolp,False,"<h2><br>An Urban Decision-Support System Augmented by Artificial Intelligence<br></h2><p>The decision-making process in urban design and urban planning is outdated. Currently, urban decision-making is mostly a top-down process, with community participation only in its late stages. Furthermore, many design decisions are subjective, rather than based on quantifiable performance and data. Current tools for urban planning do not allow both expert and non-expert stakeholders to explore a range of complex scenarios rapidly with real-time feedback.&nbsp;</p><p>CityMatrix was an effort towards evidence-based, democratic decision-making. Its contributions lie in the application of Machine Learning as a versatile, quick, accurate, and low-cost approach to enable real-time feedback of complex urban simulations and the implementation of the optimization searching algorithms to provide open-ended decision-making suggestions.&nbsp;The goals of CityMatrix were:&nbsp;</p><br><ol><li><i>Designing an intuitive Tangible User Interface (TUI) to improve the accessibility of the decision-making process for non-experts.&nbsp;</i></li><li><i>Creating real-time feedback on multi-objective urban performances to help users evaluate their decisions, thus to enable rapid, collaborative decision-making.&nbsp;</i></li><li><i>Constructing a suggestion-making system that frees stakeholders from excessive, quantitative considerations and allows them to focus on the qualitative aspects of the city, thus helping them define and achieve their goals more efficiently.</i></li></ol><p>CityMatrix was augmented by Artificial Intelligence (AI) techniques including Machine Learning simulation predictions and optimization search algorithms. The hypothesis explored in this work was that the decision quality could be improved by the organic combination of both strengths of human intelligence and machine intelligence.</p><p>The system was pilot-tested and evaluated by comparing the problem-solving results of volunteers, with or without AI suggestions. Both quantitative and qualitative analytic results showed that CityMatrix is a promising tool that helps both professional and non-professional users understand the city better to make more collaborative and better-informed decisions.&nbsp;</p>",,,2018-10-17 18:10:46.064,True,2016-02-26,CityMatrix,PUBLIC,https://media.mit.edu/people/ryanz,True,City Science,False
andorra-living-lab,alonsolp,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,City Science,False
Reversed-Urbanism,alonsolp,False,"<h2>Predicting Urban Performance through Behavioral Patterns in Temporal Telecom Data</h2><p>This study explores a novel method to analyze diverse behavioral patterns in large urban populations and to associate them with discrete urban features. This work utilizes machine learning and anonymized telecom data to understand which fragments of the city has greater potential to attract dense and diverse populations over longer periods of time. Finally, this work suggests a road map for building spatial prediction tools in an effort to improve city-design and planning processes.&nbsp;&nbsp;</p><p><b><a href=""https://cityscope.github.io/CS_Andorra_RNC/"">Click here for an interactive visualization of this study</a>&nbsp;<br><br></b></p><p><b>Advisors:</b>&nbsp;Kent Larson&nbsp;and&nbsp;Esteban Moro<br><b>Thanks to</b> Andorra Telecom, ActuaTech,&nbsp;Núria Macià. <br>Data was&nbsp;obtained by Andorra Telecom as part of MIT Media Lab City Science and the State of Andorra collaboration.&nbsp;</p>",,,2019-02-24 23:21:12.068,True,2017-07-01,Reversed Urbanism,PUBLIC,http://ArielNoyman.com,True,City Science,False
cityscope-volpe,alonsolp,False,"<p>CityScope Volpe is demonstrating most of the urban planning, analysis, and prediction features developed for the CityScope project. The site, a 14-acre parcel on the northern part of MIT/Kendall Square area of Cambridge, has been acquired and is&nbsp; being developed by MIT. City Science researchers designed and built a CityScope urban performance tool that is aiming to predict the outcomes of multiple planning and development scenarios.&nbsp;</p>",,,2019-02-25 15:14:11.403,True,2016-11-01,CityScope Volpe,PUBLIC,,True,City Science,False
andorra-mobility,alonsolp,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile</a></p><p>With no airport or train service, most of the 8 million tourists who visit Andorra each year arrive by car, making traffic management and parking some of the country's most important challenges. We are currently developing different projects spanning from data science to the deployment of autonomous vehicles to help address these issues.<br></p>",,,2017-10-25 05:56:26.309,True,2016-09-01,Andorra | Mobility,PUBLIC,,True,City Science,False
city-science-anodrra2,alonsolp,False,,,,2017-10-10 18:39:43.857,False,2017-10-09,City Science Anodrra,PUBLIC,,True,City Science,False
urban-swarms,alonsolp,False,"<p>Modern cities have to respond to the growing demands of more efficient and sustainable urban development, as well as an increased quality of life. In this context, the cities of the future will need the ability to gain insight about current urban conditions and react dynamically to them. According to this view, ""smart cities"" can be seen as cybernetic urban environments in which different agents (e.g., citizens) and actuators (e.g., robots) exploit the city-wide infrastructure as a medium to operate synergistically.<b><i> Urban Swarms</i></b> explores the feasibility of swarm robotics systems in urban environments. By using bio-inspired methods, a swarm of robots is able to handle important urban systems and infrastructures, improving their efficiency and autonomy. A diverse set of simulation experiments were designed and conducted using real-world GIS data. Results show that the proposed combination is able to outperform current approaches. <i><b>Urban Swarms</b></i> not only aims to show the efficiency of our proposed solution, but also to give insights about how to design and customize these systems.&nbsp;<a href=""https://www.media.mit.edu/projects/cityscope-volpe/overview/"" style=""font-size: 18px; font-weight: 400;"">CityScope</a><span style=""font-size: 18px; font-weight: 400;"">&nbsp;Volpe ABM model has been customized to integrate Swarm behavior using the </span><a href=""https://gama-platform.github.io/"" style=""font-size: 18px; font-weight: 400;"">Gama Platform</a><span style=""font-size: 18px; font-weight: 400;""> as an </span><a href=""https://github.com/mitmedialab/UrbanSwarms"" style=""font-size: 18px; font-weight: 400;"">open source project</a><span style=""font-size: 18px; font-weight: 400;"">.&nbsp;</span></p>",,,2019-03-12 15:37:36.573,True,2018-10-01,Urban Swarms,PUBLIC,http://www.eduardocastello.com,True,City Science,False
city-science-lab-shanghai,alonsolp,False,"<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>",,,2019-05-16 20:43:38.677,True,2017-02-14,City Science Lab Shanghai,PUBLIC,,True,City Science,False
city-science-guadalajara,alonsolp,False,"<p>The University of Guadalajara, referred to as UdeG, is a university network composed of 15 campuses within the state of Jalisco and one online system. The University offers undergraduate and graduate studies to around 130,000 students. UdeG strives to understand urban performance metrics using evidence-based decision making tools, facilitated through a collaboration with the MIT Media Lab City Science group.</p>",,,2019-05-10 19:44:20.937,True,2018-11-01,City Science Collaboration Guadalajara,PUBLIC,,True,City Science,False
andorra-tourism,alonsolp,False,"<p><span style=""font-size: 18px; font-weight: 400;""></span><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/""><span style=""font-size: 18px; font-weight: 400;"">View the main City Science Andorra project profile.</span></a><br></p><p><span style=""font-size: 18px; font-weight: 400;"">With more than eight million visitors a year, tourism represents almost 30% of the economy of Andorra. By gathering and analyzing data from social media, call detail records, and wifi, we can understand the country's dynamics of tourism and commerce as well as design interventions that can improve the experience for tourists, encouraging them to visit Andorra more frequently, stay longer, and increase spending.&nbsp;</span><br></p><h2><b>Current Projects</b></h2><ul><li>Event Analysis<br></li><li>Social Network<br></li><li>Location Recommendation system<br></li></ul><p> </p><h2><b>EVENT ANALYSIS</b></h2><p>Based on the analysis of call detail records and social media, the goal of this project is to understand the tourist behaviors in Andorra.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">After mining those anonymized data, we have been able to learn different patterns and behaviors of the tourism in Andorra thanks to an agent-based model developed in order to represent the flow of people. This simulation is also coupled with an interactive table called CityMatrix.</span></p>",,,2019-02-25 15:33:28.936,True,2015-08-01,Andorra | Tourism,PUBLIC,,True,City Science,False
andorra-energy-environment,alonsolp,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p>",,,2018-10-22 21:46:23.783,True,2016-09-02,Andorra | Energy + Environment,PUBLIC,,True,City Science,False
aalto-saas,alonsolp,False,"<h2>How can we get more value from the same buildings?&nbsp;</h2><p>Cities contain many different resources and spaces and typically, these resources operate as products with&nbsp; a single function and a single owner and/or renter. However, the owner's demand&nbsp;for space often varies daily or seasonally, meaning that many buildings tend to be underutilized and are often vacant or partially vacant for large portions of each day.&nbsp;</p><p>Meanwhile, the ""sharing economy"" has been one of the most significant economic shifts in the last 10 years, with companies like Uber and Airbnb experiencing explosive growth. Along these lines, Aalto University—a member of the MIT Media Lab City&nbsp;Science network—has developed the concept&nbsp;of City-as-a-Service, where building space and other resources are shared among institutions, businesses, and citizens in a community. Aalto has already begun experimenting with School-as-a-Service, as a prototype of City-as-a-Service on their campus in Espoo.</p>",,,2019-05-07 20:00:40.076,True,2017-07-01,Aalto Campus-as-a-Service Simulations,PUBLIC,,True,City Science,False
basic,alonsolp,False,"<p>Autonomous vehicles (AVs), drones, and robots will revolutionize our way of traveling and understanding urban space. In order to operate, all of these devices are expected to collect and analyze a lot of sensitive data about our daily activities. However, current operational models for these devices have extensively relied on centralized models of managing these data. The security of these models unveiled significant issues.</p><p>This project&nbsp; proposes BASIC, the Blockchained Agent-based Simulator for Cities. This tool aims to verify the feasibility of the use of blockchain in simulated urban scenarios by considering the communication between agents through&nbsp;<i>smart contracts</i>. In order to test the proposed tool, we implemented a car-sharing model within the city of Cambridge (Massachusetts, USA). In this research, the relevant literature was explored, new methods were developed, and different solutions were designed and tested. Finally, conclusions about the feasibility of the combination between blockchain technology and agent-based simulations were drawn.</p><p>Developed using&nbsp;<a href=""https://gama-platform.github.io/"">Gama Platform</a>.&nbsp;&nbsp;</p><p>Click <a href=""https://github.com/mitmedialab/Basic"">here</a> for the Open Source Repository.</p>",,,2019-04-23 20:08:21.338,True,2018-09-03,BASIC: Blockchained Agent-based Simulator for Cities,PUBLIC,,True,City Science,False
cityscope-cooper-hewitt,alonsolp,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
city-science-andorra,alonsolp,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
cityscope,alonsolp,False,"<p>City Science researchers are developing a slew of tangible and digital platforms dedicated to solving spatial design and urban planning challenges. The tools range from simulations that quantify the impact of disruptive interventions in cities to communicable collaboration applications. We develop and deploy these tools around the world and maintain open source repositories for the majority of deployments. ""CityScope"" is a concept for shared, interactive computation for urban planning.</p><p>All current CityScope development, tools, and software are open source <a href=""https://cityscope.github.io/"">here</a>.&nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2019-05-16 20:42:57.474,True,2017-08-01,Theme | CityScope,PUBLIC,,True,City Science,False
industry-space-and-housing-price,sunlijun,False,"<p><span style=""font-size: 18px; font-weight: normal;"">The boom in Chinese housing prices in recent years has given rise to intensive concern about&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">&nbsp;the economic fundamentals of housing prices. This project mainly focuses on giving deep insight into&nbsp;</span>housing prices from the perspective of&nbsp;<span style=""font-size: 18px; font-weight: normal;"">industry composition, especially by checking the characteristics (agglomeration, innovation, diversity, and so on) of a city's position in the industry space.</span><span style=""font-weight: normal; font-size: 16px;"">&nbsp;</span></p>",2017-08-31,,2017-04-03 00:02:27.435,True,2017-02-10,Industry space and housing prices,PUBLIC,,False,Scalable Cooperation,False
global-cooperation,sunlijun,False,<h1>Measuring Cooperation at Scale</h1>,,,2018-01-10 16:18:38.348,True,2015-09-01,Global Cooperation,PUBLIC,,True,Scalable Cooperation,False
honest-crowds,sunlijun,False,"<p>The Honest Crowds project addresses shortcomings of traditional survey techniques in the modern information and big data age. Web survey platforms, such as Amazon's Mechanical Turk and CrowdFlower, bring together millions of surveys and millions of survey participants, which means paying a flat rate for each completed survey may lead to survey responses that lack desirable care and forethought. Rather than allowing survey takers to maximize their reward by completing as many surveys as possible, we demonstrate how strategic incentives can be used to actually reward information and honesty rather than just participation. The incentive structures that we propose provide scalable solutions for the new paradigm of survey and active data collection.</p>",,--Choose Location,2018-01-10 16:32:52.208,True,2015-09-01,Honest Crowds,PUBLIC,,True,Scalable Cooperation,False
industry-space-and-housing-price,lduan,False,"<p><span style=""font-size: 18px; font-weight: normal;"">The boom in Chinese housing prices in recent years has given rise to intensive concern about&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">&nbsp;the economic fundamentals of housing prices. This project mainly focuses on giving deep insight into&nbsp;</span>housing prices from the perspective of&nbsp;<span style=""font-size: 18px; font-weight: normal;"">industry composition, especially by checking the characteristics (agglomeration, innovation, diversity, and so on) of a city's position in the industry space.</span><span style=""font-weight: normal; font-size: 16px;"">&nbsp;</span></p>",2017-08-31,,2017-04-03 00:02:27.435,True,2017-02-10,Industry space and housing prices,PUBLIC,,False,Collective Learning,True
start-making,chrisg,False,"<p>The Lifelong Kindergarten group is collaborating with the Museum of Science in Boston to develop materials and workshops that engage young people in ""maker"" activities in Computer Clubhouses around the world, with support from Intel. The activities introduce youth to the basics of circuitry, coding, crafting, and engineering. In addition, graduate students are testing new maker technologies and workshops for Clubhouse staff and youth. The goal of the initiative is to help young people from under-served communities gain experience and confidence in their ability to design, create, and invent with new technologies.</p>",2016-08-31,--Choose Location,2016-12-05 00:17:01.293,True,2014-01-01,Start Making!,PUBLIC,,False,Lifelong Kindergarten,False
start-making,ria,False,"<p>The Lifelong Kindergarten group is collaborating with the Museum of Science in Boston to develop materials and workshops that engage young people in ""maker"" activities in Computer Clubhouses around the world, with support from Intel. The activities introduce youth to the basics of circuitry, coding, crafting, and engineering. In addition, graduate students are testing new maker technologies and workshops for Clubhouse staff and youth. The goal of the initiative is to help young people from under-served communities gain experience and confidence in their ability to design, create, and invent with new technologies.</p>",2016-08-31,--Choose Location,2016-12-05 00:17:01.293,True,2014-01-01,Start Making!,PUBLIC,,False,Lifelong Kindergarten,False
start-making,jieqi,False,"<p>The Lifelong Kindergarten group is collaborating with the Museum of Science in Boston to develop materials and workshops that engage young people in ""maker"" activities in Computer Clubhouses around the world, with support from Intel. The activities introduce youth to the basics of circuitry, coding, crafting, and engineering. In addition, graduate students are testing new maker technologies and workshops for Clubhouse staff and youth. The goal of the initiative is to help young people from under-served communities gain experience and confidence in their ability to design, create, and invent with new technologies.</p>",2016-08-31,--Choose Location,2016-12-05 00:17:01.293,True,2014-01-01,Start Making!,PUBLIC,,False,Other,False
circuit-robots,jieqi,False,"<h2>Integrating sensors and actuators using flexible electronics</h2><p>Currently, the manufacturing of self-actuating and self-sensing robots requires non-standard manufacturing techniques and assembly steps to integrate electrical and mechanical systems. In this work, we developed a novel manufacturing technique, where such robots can be produced at a flexible electronics factory. We developed the technique using standard industrial machines, processes, and materials. Using a lamination process, we were able to integrate air pouches or shape memory alloy (SMA) inside a polyamide-based flexible circuit to produce bending actuators. The bend angle of the actuators is sensed with a chain of inertial measurement units integrated on the actuator. Air-pouch actuators can produce a force of a 2.24N, and a maximum bend angle of 74 degrees. To demonstrate, we manufactured a five-legged robot with the developed actuators and bend sensors, with all the supporting electronics (e.g., microcontrollers, radio) directly integrated into the flexible printed circuit. Such robots are flat and lightweight (15 grams) and thus conveniently compact for transportation and storage. We believe that our technique can allow inexpensive and fast prototyping and deployment of self-actuating and self-sensing robots.<br></p>",,,2019-04-17 19:28:42.580,True,2017-08-01,Circuit Robots: Mass manufacturing of self-actuating robots,PUBLIC,,True,Other,False
circuit-storybook,jieqi,False,"<p>An interactive picture book that explores storytelling techniques through paper-based circuitry. Sensors, lights, and microcontrollers embedded into the covers, spine, and pages of the book add electronic interactivity to the traditional physical picture book, allowing us to tell new stories in new ways. The current book, ""Ellie,"" tells the adventures of an LED light named Ellie who dreams of becoming a star, and of her journey up to the sky.</p>",,--Choose Location,2016-12-05 00:17:08.516,True,2015-01-01,Circuit Storybook,PUBLIC,,True,Other,False
scratch-pad,jieqi,False,"<p>ScratchBit is an effort to enable children to create more seamlessly in both the physical and digital world by creating a dedicated physical interface for the&nbsp;<a href=""https://scratch.mit.edu"">Scratch</a>&nbsp;programming language and environment. Designed to be rugged, low cost, and highly composable, the ScratchBit allows children to take the materials around them—such as cardboard, clothes, skateboards, and trees—and &nbsp;transform them into inputs to their digital creations on Scratch. Unlike the <a href=""http://makeymakey.com/"">Makey Makey</a> which was designed to make these connections electronically, the ScratchBit is designed to create these connections through motion and mechanism.</p>",,,2018-11-03 16:11:24.635,True,2016-09-01,ScratchBit,PUBLIC,,True,Other,False
paper-circuits,jieqi,False,"<p>Paper circuitry blends conductive craft materials with electronics components to engage learners in circuit building and programming through making arts and crafts.  Learners can take advantage of the expressive richness of paper to create artifacts that are technically functional, aesthetically unique and personally meaningful. Chibitronics circuit stickers are a toolkit designed for paper circuits that transforms flexible circuit boards into interactive stickers for crafting circuits.</p>",,,2018-10-20 01:08:52.664,True,2014-01-01,Paper Circuits,PUBLIC,,True,Other,False
slantometer,kalli,False,"<p>This visual exploration exposes the slant of five online news sources regarding two mass shooting incidents: the 2017 Las Vegas shooting and the 2012 Sandy Hook incident.&nbsp; We show underlying facts, context, and the evolution of editorial slant derived from the event.</p><p>We designed and built a Chrome extension that lets the user highlight the text of an online article with three colors according to the amount of slant they believe the text contains. We used three categories: “facts” for clearly factual and unbiased text, “context” for topic that may be factual but has been conveniently chosen to surround a subject, and “slant” for clearly biased text/opinionated text. </p><p>The Chrome plugin is storing the highlighted text as well as the corresponding URL to a database, which we are using to identify the cumulative metrics per news source for the timepoints we are examining.</p>",2018-06-06,,2018-10-13 16:10:52.283,True,2017-09-04,Slantometer,LAB-INSIDERS,,False,Viral Communications,False
internet-as-an-object,kalli,False,"<p>In Cuba, the Internet has another name, and it fits in the palm of your hand. It is a hard drive called “El Paquete Semanal” (the Weekly Package)—a collection of one terabyte of information: shows, movies, music, PDFs, downloaded onto hard drives and distributed door-to-door and operated by its users. In the rest of the world we have Facebook.  For the past thirty years, social networks have demonstrated their power and usefulness to link us together and place communications in everyone’s hands.  At the same time, they have matured from being operated by the people who use them (e.g., Cuba, the WELL, BBSes), to large-scale commercial organizations operated on those users’ behalf (e.g., Facebook).  As Nicholas Negroponte said: in the early days of media, the users were the inventors—now they are two separate classes.&nbsp;</p><p>The central question addressed in this thesis is whether the design and underlying technology of entry points to a network change the way people interact with it and the experience they have. To explore this question, we designed and engineered a set of playful physical objects which function as nodes of a hyper-local network. Information bestowed upon this network remains within these nodes, cryptographically secure, and accessible only to local community members who are aware of the network's existence and mode of operation. This network was tested by deploying the node-objects in four real world locations, where participants could leave and retrieve audio messages from and to the nodes.</p>",2019-05-08,,2019-05-13 13:46:04.394,True,2018-10-15,Topophonia,LAB-INSIDERS,https://kalli-retzepi.com/,False,Viral Communications,False
radio-days,kalli,False,"<p>Following the 2016 election, the entirety of the nation became conscious of its polarization. According to a study by the National Bureau of Economic Research*, polarization has increased among Americans since 1990. The study observes, however, that in eight of the nine measures of polarization, older individuals (70+ age group) show higher rates of increase in polarization than other age groups. This age group also utilizes social media less than other age-groups. Could it be that social media is not the root cause of polarization?</p><p>In order to explore this further, we looked at polarization through talk radio, which is commonly thought to have political influence.&nbsp;</p>",2018-12-31,,2019-04-11 15:26:19.908,True,2018-10-12,Radio Days,PUBLIC,,False,Viral Communications,False
votomosaic,kalli,False,"<p>A tapestry where each pixel represents a pledge by an individual to vote. Anyone can participate and watch the growing and changing image that emerges as others agree to vote. The tapestry evolves and will encourage repeated attention. This tests local reinforcement to support actions.</p><p>Follow this <a href=""https://votomosaic.media.mit.edu/"">link</a> for more information.</p>",2018-12-31,,2019-04-18 15:05:48.332,True,2018-08-01,votoMosaic,PUBLIC,,False,Viral Communications,False
conjugate,kalli,False,"<p>A recent focus of our lab has been making use of Tangible Displays and Body Object Space to develop new assistive technologies. As a test case, we prototyped the Mario side-scrolling game for visually impaired users, using body movement analogies to control Mario in the game. Mario and 2D side scrollers present a particularly interesting case, as they keep the main character location in the center of the display and move the world around the character. The shape display itself provides spatial audio of enemy positions. We make use of the AUFLIP sensor platform to pick up body movements—walking and jumping, causing Mario to do the same in-game. This enables users to keep their hands engaged to understand the game landscape, while using their body to control Mario at the same time.&nbsp;</p>",,,2019-02-08 17:48:57.541,True,2017-03-01,CONJURE,PUBLIC,,True,Viral Communications,False
what-s-america-listening-to,kalli,False,,,,2018-04-21 00:41:22.039,True,2017-06-01,What's America Listening To?,PUBLIC,,True,Viral Communications,False
medrec,kalli,False,"<p>Increasingly in the US, people have to take responsibility for their health information.&nbsp; Simultaneously, medical providers must make patient data available. MedRec fully decentralizes access rights via an Ethereum blockchain, thereby giving patients control over record distribution. Our model is the World Wide Web:&nbsp; MedRec is a network. Patients and providers operate nodes that authorize others to retrieve data. It is a basis for a generally useful permissioning system.</p><p>There is no website or central repository of permissions. Instead, patients and medical records originators <i>establish a relationship </i>and based on that, the patient creates <i>smart contracts</i>&nbsp;that other members of the network can use to authorize access to a record database. The parameters of contracts are kept in a <i>blockchain </i>that is maintained by all member providers/originators who at the same time use those contracts to provide access to their database. The patient/user contracts themselves are held by the patients in a <i>wallet </i>that resides on their device[s] as an app. This app is secure and recoverable in case the physical device is lost or damaged.</p><p>For a full overview, technical documentation, and updates, visit the project's <a href=""https://medrec.media.mit.edu"">website</a>.</p>",,,2018-10-17 19:22:59.328,True,2018-01-01,MedRec,PUBLIC,https://medrec.media.mit.edu,True,Viral Communications,False
beneath-the-chip,tjlevy,False,"<p>Sculptural artifacts that model and reveal the embedded history of human thought and scientific principles hidden inside banal digital technologies. These artifacts provide alternative ways to engage and understand the deepest interior of our everyday devices, below the circuit, below the chip. They build a sense of the machines within the machine, the material, the grit of computation.</p>",2015-09-01,--Choose Location,2016-12-05 00:17:01.746,True,2014-01-01,beneath the chip,PUBLIC,,False,Playful Systems,False
radio_o,tjlevy,False,"<p>radiO_o is a battery-powered speaker worn by hundreds of party guests, turning each person into a local mobile sound system. The radiO_o broadcast system allows the DJ to transmit sounds over several pirate radio channels to mix sounds between hundreds of speakers roaming around the space and the venue's existing sound system.</p>",,--Choose Location,2016-12-05 00:17:21.437,True,2014-01-01,radiO_o,PUBLIC,,True,Playful Systems,False
quadra-sense-confluence-of-uavs-enviornmental-sensor-networks-and-augmented-reality,vram,False,"<p>Aerial imaging and sensor nodes each present a unique view point into the world. Using Unmanned Aerial Vehicles (UAVs) to navigate the same space shared by sensor networks, this project aims explore interaction between two historically disparate systems for purposes of both immersive experiences and scientific research. Users will remotely navigate landscapes using latest generation of head-mounted dsplays and see real-time sensor data as an augmented overlay with a real-time video stream from the UAV.</p>",2016-01-01,--Choose Location,2017-05-17 23:11:48.250,True,2015-01-01,"Quadra-Sense: Confluence of UAVs, Enviornmental Sensor Networks, and Augmented Reality",PUBLIC,,False,Responsive Environments,True
fragile-instruments,x_x,False,"<p>We introduce a family of fragile electronic musical instruments designed to be “played” through the act of destruction. Each Fragile Instrument consists of an analog synthesizing circuit with embedded sensors that detect the destruction of an outer shell, which is destroyed and replaced for each performance. Destruction plays an integral role in both the spectacle and the generated sounds.</p>",2017-12-01,,2017-05-17 23:43:56.207,True,2016-06-01,Fragile Instruments,PUBLIC,,False,Tangible Media,True
kinephone,x_x,False,"<p>This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.</p>",2016-08-01,,2017-05-18 01:07:33.691,True,2016-05-01,Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display,PUBLIC,,False,Tangible Media,True
moduland,x_x,False,"<h2>MODULAND is a playground kit for learning&nbsp;electronic music.</h2><p>MODULAND is an interactive project created by the&nbsp;<a href=""https://www.media.mit.edu/events/mlberlin-signalandnoise/"">MIT Media Lab Berlin – Signal &amp; Noise prototyping workshop</a>,&nbsp;within the track “Playful Machines that Make Music.""&nbsp;</p><p>With MODULAND, playgrounds become modular synthesizers to raise curiosity, exploration, and connection to electronic music making.</p><p>By creating playful machines that use LEGO bricks, sensors, and microcontrollers, it creates an embodied and interactive music lesson in an urban space.</p>",,,2018-08-29 14:44:22.539,True,2018-08-14,MODULAND,PUBLIC,https://medialabmoduland.wordpress.com/,True,Tangible Media,True
deep-empathy,cebrian,False,"<h2><i>What would&nbsp;<b>your city</b>&nbsp;look like after a disaster?</i>&nbsp;</h2><p>Deep Empathy&nbsp;is&nbsp;a collaboration between the&nbsp;<a href=""http://www.media.mit.edu/groups/scalable-cooperation"">Scalable Cooperation</a>&nbsp;group and the&nbsp;<a href=""https://www.unicef.org/innovation"">UNICEF Innovation Office</a>&nbsp;to pursue a scalable way to increase empathy.&nbsp;</p><p>The brutal, six-year-old Syrian war has affected more than 13.5 million people in Syria , including 80% of the country's children—8.4 million young lives shattered by violence and fear. Hundreds of thousands of people have been displaced and their homes destroyed.&nbsp;</p><p>But people generate a response that statistics can't. And technologists—through tools like AI—have opportunities to help people see things differently. We wondered: ""Can AI increase empathy for victims of far-away disasters?"" This question led us to create a provocation for the research community to examine how AI can create narratives to tell the stories of some of the world's most intractable problems.</p>",2018-05-01,,2017-12-04 19:54:02.578,True,2017-12-01,Deep Empathy,PUBLIC,http://deepempathy.mit.edu/,False,Scalable Cooperation,False
becoming-someone,cebrian,False,"<p>Becoming Someone is a multi-modal concept that lives across Medium, Instagram, and human minds. It is comprised of The Ever Contracting Void, &nbsp;an Instagram-based container by Micah Epstein commissioned by Manuel Cebrian and Iyad Rahwan as the visual metaphor for the &nbsp;essay Becoming Someone. The Ever Contracting Void is a media container that exists within &nbsp;<a href=""https://www.instagram.com/directory.of.worlds/"">The Directory of Worlds</a>, an installment in Micah’s ongoing Instagram installation. The Directory is a series of accounts, called ""worlds,"" which serve as spaces for exploration and reprieve from Instagram’s visual overload and hyper-targeted marketing. Where his previous worlds were an opportunity, the Void is a threat. The Void contains nothing but the source code of popular social media websites, which use machine learning to drive the internet ever inwards upon the individual. Trapped in its center is an individual, whose very identity hinges on the viewer double-tapping the screen to like the image. What remains, when the Void closes upon the individual, leaving nothing but pristine white grids and information superhighways?</p><p>Read the articulation <a href=""https://medium.com/mit-media-lab/becoming-someone-54ed1798a1b7"">here</a> and explore the visual container on Instagram <a href=""http://instagram.com/ever.contracting.void"">here</a>.&nbsp;</p>",2018-04-26,,2018-04-23 17:54:11.668,True,2018-04-05,Becoming Someone,PUBLIC,https://medium.com/mit-media-lab/becoming-someone-54ed1798a1b7,False,Scalable Cooperation,False
nostalgia-box,cebrian,False,"<h1>Nostalgia Box</h1><h2>A deep learning visualization of your own memories</h2><h2>By <a href=""http://cs.wellesley.edu/~asimonso/nostalgiabox/"">Aubrey Simonson</a>&nbsp;<br>Commissioned by Manuel Cebrian and Iyad Rahwan</h2><p>Nostalgia Box is a continually shifting soup of memories. Images, curated for their nostalgic emotional impact, are dreamed over one another using a neural style machine learning algorithm, based on the paper ""A Neural Algorithm of Artistic Style"" by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. The resulting images are then overlaid into a video which never fully solidifies into any one image, but instead is always several at once. To someone who is familiar with the content of the images, they are still recognizable. However, a stranger should see only a haze of vague shapes which occasionally contains the suggestion of a face. This is machine learning<i>, </i>stripped of the elements of spam. It is an amalgamation of its creator's digital history, but which is being used as a tool for reflection, rather than as a means to more effectively market products.&nbsp;&nbsp;</p>",2018-12-31,,2018-05-07 22:25:44.225,True,2018-02-01,Nostalgia Box,PUBLIC,http://cs.wellesley.edu/~asimonso/nostalgiabox,False,Scalable Cooperation,False
black-rock-atlas,cebrian,False,"<p>Burning Man is a magical place that gets the best of human creativity and collaboration to flourish. To further understand what makes this magic happen, we are creating the first ever Black Rock Atlas–a map of the social patterns and networks that exist on the playa. </p><p>To do this, we are tracking the decentralized journey of a multitude of vessels through the gift economy of Burning Man with GPS technology and generative photography. The Atlas will explore new ways of community interaction, storytelling, and data visualization.</p>",2018-12-11,,2018-10-15 20:44:54.352,True,2018-08-15,Black Rock Atlas,PUBLIC,https://blackrockatlas.mit.edu,False,Scalable Cooperation,False
nightmare-machine,cebrian,False,"<p>For centuries, across geographies, religions, and cultures, people try to innovate ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity. This challenge is especially important in a time when we wonder what the limits of Artificial Intelligence are: Can machines learn to scare us? Towards this goal, we present you Haunted Faces and Haunted Places: computer generated scary imagery powered by deep learning algorithms!
                    
                </p>",,,2017-01-31 04:41:35.401,True,2016-10-20,Nightmare Machine,PUBLIC,http://nightmare.mit.edu/,True,Scalable Cooperation,False
towards-understanding-the-impact-of-ai-on-labor,cebrian,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-03-22 16:27:19.712,False,2018-03-01,Towards Understanding the Impact of AI on Labor,PUBLIC,http://www.media.mit.edu/~mrfrank,True,Scalable Cooperation,False
shelley,cebrian,False,"<h2>Project website:&nbsp;<a href=""http://shelley.ai"">shelley.ai&nbsp;<br></a>Human-AI collaborated stories:&nbsp;<a href=""http://stories.shelley.ai"">stories.shelley.ai&nbsp;<br></a>Follow&nbsp;<a href=""http://twitter.com/shelley_ai"">@shelley_ai</a> to collaborate with Shelley!&nbsp;</h2><br><p>For centuries, across geographies, religions, and cultures, people have innovated ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity.&nbsp;This challenge is especially important at a time when we are exploring the limits of artificial intelligence: Can machines learn to scare us?&nbsp;</p><p>In Halloween 2016 we presented the&nbsp;<a href=""http://nightmare.mit.edu/"">Nightmare Machine</a>—computer-generated scary imagery powered by deep learning algorithms.&nbsp;</p><p>This Halloween, we present <b>Shelley:&nbsp;Human-AI Collaborated Horror Stories</b>!&nbsp;</p><p>Shelley is a deep-learning powered AI who was raised reading eerie stories coming from&nbsp;<a href=""http://reddit.com/r/nosleep"">r/nosleep</a>. Now, as an adult—and not unlike Mary Shelley, her Victorian idol—she takes a bit of inspiration in the form of a random seed, or a short snippet of text, and starts creating stories emanating from her creepy creative mind. But what Shelley truly enjoys is working collaboratively with humans, learning from their nightmarish ideas, creating the best scary tales ever. If you want to work with her, respond to the stories she'll start every hour on her Twitter <a href=""http://twitter.com/shelley_ai"">account</a>, and she will write with you the first AI-human horror anthology ever put together!</p>",,,2017-12-12 21:51:49.213,True,2017-10-15,Shelley: Human-AI Collaborated Horror Stories,PUBLIC,http://shelley.ai,True,Scalable Cooperation,False
turingbox,cebrian,False,"<p>TuringBox is a platform&nbsp;that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms.&nbsp;It is a two-sided marketplace. On one side, <i>AI contributors</i> upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">On the other side, </span><i style=""font-size: 18px; font-weight: 400;"">AI examiners</i><span style=""font-size: 18px; font-weight: 400;"">&nbsp;develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.</span></p>",,,2018-04-05 19:00:33.916,True,2018-03-21,TuringBox: Democratizing the study of AI,PUBLIC,http://turingbox.mit.edu,True,Scalable Cooperation,False
the-science-of-ai-research,cebrian,False,"<p>We must proactively tackle the economic, social, and societal implications that accompany the widespread deployment of AI technology. In service to this goal, examining the evolution of AI research itself could provide a valuable input into models of AI's impact (e.g., models of the future of work).&nbsp;</p>",,,2019-03-13 17:23:51.009,True,2018-05-01,The Science of AI Research,PUBLIC,,True,Scalable Cooperation,False
evolution-of-the-social-contract,cebrian,False,"<p>Political constitutions describe the fundamental principles by which nation-states are governed, the political and legal state institutions, the powers, procedures, and duties of those institutions, and the rights and responsibilities of individuals. How do these constitutions develop over long periods of time? What is the interplay between colonial history and global, time varying trends in determining the characteristics of a country's constitution? We explore these questions using new techniques of computational social science.</p>",,,2017-12-11 21:03:28.998,True,2016-07-01,Evolution of the Social Contract,PUBLIC,http://www.alexrutherford.org/constitutionology/,True,Scalable Cooperation,False
norman,cebrian,False,"<p>We present <a href=""http://norman-ai.mit.edu"">Norman</a>, world's first psychopath AI. Norman was inspired by the fact that the data used to teach a machine learning algorithm can significantly influence its behavior. So when people say that AI algorithms can be biased and unfair, the culprit is often not the algorithm itself, but the biased data that was fed to it. The same method can see very different things in an image, even ""sick"" things, if trained on the wrong (or, the right!) data set. Norman suffered from extended exposure to the darkest corners of Reddit, and represents a case study on the dangers of artificial intelligence gone wrong when biased data is used in machine learning algorithms.&nbsp;</p><p>Norman is an AI that is trained to perform image captioning; a popular deep learning method of generating a&nbsp;textual description of an image. We trained Norman on image captions from an infamous subreddit (its name is redacted due to its graphic content) that is dedicated to documenting and observing the disturbing reality of death. Then, we compared Norman's responses with a standard image-captioning neural network (trained on&nbsp;<a href=""http://mscoco.org/"">MSCOCO</a>&nbsp;dataset) on <a href=""https://en.wikipedia.org/wiki/Rorschach_test"">Rorschach inkblots</a>–a test that is used to detect underlying thought disorders.</p><p>Visit <a href=""http://norman-ai.mit.edu"">norman-ai.mit.edu</a> to explore what Norman sees!<br></p>",,,2019-02-14 19:39:41.066,True,2018-04-01,Norman,PUBLIC,,True,Scalable Cooperation,False
deep-angel-ai,cebrian,False,"<p><b>Deep Angel&nbsp;</b>is an artificial intelligence that erases objects from photographs. The algorithm is hosted on <a href=""http://deepangel.media.mit.edu"">http://deepangel.media.mit.edu</a>, which enables anyone&nbsp;to interact with the AI and explore what it can disappear.</p><p>Part philosophy, part technology, and part art, Deep Angel is designed to spark a series of conversations on technology in our daily lives and AI and media manipulation.&nbsp;&nbsp;</p><p>Deep Angel draws from&nbsp; Walter Benjamin's description of Paul Klee's Angelus Novus, the angel of history who has clairvoyance into the dark side of what appears to be progress. The angel sees the unravelling of all that matters in the world and would like to alert the world about his vision, but he's caught in the storm of progress and can't communicate any messages. The images that Deep Angel generates are intended to deliver the message that Angelus Novus would have sent if he could.&nbsp;</p><p>The algorithm applies computer vision techniques to automatically (1) detect and outline objects in images, (2) remove the outlined object from the image, and (3) imagine what the image would look like if that outlined object were removed from the image. Any image uploaded and transformed by Deep Angel can be published on the Deep Angel website by clicking the ""Publish to Deep Angel"" button.&nbsp;</p><p>The AI's performance varies across photographs. Sometimes, it's impossible to tell what has been disappeared. Other times, the images appear similar to the images from Adrian Piper's <i>Everything</i> series. The more people interact with the algorithm, the more attuned people will be to the potential and limitations of modern AI to manipulate the media. It's now possible to automate the vanishing commissar in Soviet photography, but the AI is not yet perfect. Below are two examples of the Deep Angel AI effect: (1) a gif generated by Deep Angel showing a father and daughter disappearing in the wilderness and (2) two images showing the before and after of Deep Angel peering into a photo of a professional surfer.&nbsp;</p>",,,2019-02-14 19:46:25.924,True,2018-08-06,Deep Angel: The AI behind the aesthetics of absence,PUBLIC,http://deepangel.media.mit.edu/,True,Scalable Cooperation,False
identifying-the-human-impacts-of-climate-change,cebrian,False,<p>Climate change is going to alter the environments that we depend on in myriad ways. We're using data to identify and quantify these potential human impacts.&nbsp;</p>,,,2019-04-19 17:44:02.613,True,2016-07-01,Identifying the human impacts of climate change,PUBLIC,,True,Scalable Cooperation,False
future-of-work-ai-automation-labor,cebrian,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-04-03 14:42:46.705,True,2017-06-06,"AI, Automation, Labor, and Cities: How to map the future of work",PUBLIC,,True,Scalable Cooperation,False
machine-behavior,cebrian,False,"<p>Machines powered by artificial intelligence (AI) increasingly                           mediate our social, cultural, economic, and                           political interactions. Understanding the                           behavior of AI systems is essential to our                           ability to control their actions, reap their                           benefits, and minimize their harms. We argue                           this necessitates a broad scientific research                           agenda to study machine behavior that                           incorporates but expands beyond the discipline                           of computer science and requires insights from                           across the sciences. Here we first outline a                           set of questions fundamental to this emerging                           field. We then explore the technical, legal,                           and institutional constraints facing the study                           of machine behavior.</p>",,,2019-04-30 19:56:54.922,True,2019-04-24,Machine Behavior,PUBLIC,,True,Scalable Cooperation,False
deep-empathy,dubeya,False,"<h2><i>What would&nbsp;<b>your city</b>&nbsp;look like after a disaster?</i>&nbsp;</h2><p>Deep Empathy&nbsp;is&nbsp;a collaboration between the&nbsp;<a href=""http://www.media.mit.edu/groups/scalable-cooperation"">Scalable Cooperation</a>&nbsp;group and the&nbsp;<a href=""https://www.unicef.org/innovation"">UNICEF Innovation Office</a>&nbsp;to pursue a scalable way to increase empathy.&nbsp;</p><p>The brutal, six-year-old Syrian war has affected more than 13.5 million people in Syria , including 80% of the country's children—8.4 million young lives shattered by violence and fear. Hundreds of thousands of people have been displaced and their homes destroyed.&nbsp;</p><p>But people generate a response that statistics can't. And technologists—through tools like AI—have opportunities to help people see things differently. We wondered: ""Can AI increase empathy for victims of far-away disasters?"" This question led us to create a provocation for the research community to examine how AI can create narratives to tell the stories of some of the world's most intractable problems.</p>",2018-05-01,,2017-12-04 19:54:02.578,True,2017-12-01,Deep Empathy,PUBLIC,http://deepempathy.mit.edu/,False,Human Dynamics,False
turingbox,dubeya,False,"<p>TuringBox is a platform&nbsp;that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms.&nbsp;It is a two-sided marketplace. On one side, <i>AI contributors</i> upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">On the other side, </span><i style=""font-size: 18px; font-weight: 400;"">AI examiners</i><span style=""font-size: 18px; font-weight: 400;"">&nbsp;develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.</span></p>",,,2018-04-05 19:00:33.916,True,2018-03-21,TuringBox: Democratizing the study of AI,PUBLIC,http://turingbox.mit.edu,True,Human Dynamics,False
deep-reinforcement-learning-inspired-by-human-collective-intelligence,dubeya,False,"<p>We know that it's groups, not individuals, that are capable of the most complex and daunting achievements. Why should AI be any different?&nbsp;</p><p>We show that deep reinforcement learning algorithms that use lessons from how humans learn and communicate with each other can provide large improvements over state of the art reinforcement learning methods.</p><p>Researchers have been studying how groups of problem-solvers organize themselves and communicate for years, under the field of ""collective intelligence."" It's been shown that there are some surprisingly simple relationships between a group's communication network structure and how well that group is able to perform on different kinds of tasks.&nbsp;</p><p>Using these simple lessons, we designed a deep reinforcement learning algorithm that, instead of using one massive neural network (NN), leverages a community of many smaller NNs. We enable these neural networks to communicate with one another, and to learn from each others' explorations and successes.&nbsp;</p><p>Using this strategy yields significant improvements over the state of the art. By placing these neural nets on a communication network that is similar in structure to how humans communicate,&nbsp;we see a 33% improvement in how fast the networks are able to learn and in how well they are able to perform at a benchmark reinforcement learning task.</p><p><br></p>",,,2018-05-09 14:23:16.868,True,2017-07-01,Deep Reinforcement Learning Inspired by Human Collective Intelligence,LAB-INSIDERS,,True,Human Dynamics,False
deep-angel-ai,dubeya,False,"<p><b>Deep Angel&nbsp;</b>is an artificial intelligence that erases objects from photographs. The algorithm is hosted on <a href=""http://deepangel.media.mit.edu"">http://deepangel.media.mit.edu</a>, which enables anyone&nbsp;to interact with the AI and explore what it can disappear.</p><p>Part philosophy, part technology, and part art, Deep Angel is designed to spark a series of conversations on technology in our daily lives and AI and media manipulation.&nbsp;&nbsp;</p><p>Deep Angel draws from&nbsp; Walter Benjamin's description of Paul Klee's Angelus Novus, the angel of history who has clairvoyance into the dark side of what appears to be progress. The angel sees the unravelling of all that matters in the world and would like to alert the world about his vision, but he's caught in the storm of progress and can't communicate any messages. The images that Deep Angel generates are intended to deliver the message that Angelus Novus would have sent if he could.&nbsp;</p><p>The algorithm applies computer vision techniques to automatically (1) detect and outline objects in images, (2) remove the outlined object from the image, and (3) imagine what the image would look like if that outlined object were removed from the image. Any image uploaded and transformed by Deep Angel can be published on the Deep Angel website by clicking the ""Publish to Deep Angel"" button.&nbsp;</p><p>The AI's performance varies across photographs. Sometimes, it's impossible to tell what has been disappeared. Other times, the images appear similar to the images from Adrian Piper's <i>Everything</i> series. The more people interact with the algorithm, the more attuned people will be to the potential and limitations of modern AI to manipulate the media. It's now possible to automate the vanishing commissar in Soviet photography, but the AI is not yet perfect. Below are two examples of the Deep Angel AI effect: (1) a gif generated by Deep Angel showing a father and daughter disappearing in the wilderness and (2) two images showing the before and after of Deep Angel peering into a photo of a professional surfer.&nbsp;</p>",,,2019-02-14 19:46:25.924,True,2018-08-06,Deep Angel: The AI behind the aesthetics of absence,PUBLIC,http://deepangel.media.mit.edu/,True,Human Dynamics,False
deep-empathy,pinary,False,"<h2><i>What would&nbsp;<b>your city</b>&nbsp;look like after a disaster?</i>&nbsp;</h2><p>Deep Empathy&nbsp;is&nbsp;a collaboration between the&nbsp;<a href=""http://www.media.mit.edu/groups/scalable-cooperation"">Scalable Cooperation</a>&nbsp;group and the&nbsp;<a href=""https://www.unicef.org/innovation"">UNICEF Innovation Office</a>&nbsp;to pursue a scalable way to increase empathy.&nbsp;</p><p>The brutal, six-year-old Syrian war has affected more than 13.5 million people in Syria , including 80% of the country's children—8.4 million young lives shattered by violence and fear. Hundreds of thousands of people have been displaced and their homes destroyed.&nbsp;</p><p>But people generate a response that statistics can't. And technologists—through tools like AI—have opportunities to help people see things differently. We wondered: ""Can AI increase empathy for victims of far-away disasters?"" This question led us to create a provocation for the research community to examine how AI can create narratives to tell the stories of some of the world's most intractable problems.</p>",2018-05-01,,2017-12-04 19:54:02.578,True,2017-12-01,Deep Empathy,PUBLIC,http://deepempathy.mit.edu/,False,Scalable Cooperation,False
how-to-generate-almost-anything,pinary,False,"<p>How to generate almost anything is a collaborative project between humans and AI, run by Pinar Yanardag Delul of the Scalable Cooperation group. The project combines expertise in making (inspired by Neil Gershenfeld’s How To Make Almost Anything class) from around MIT with generative adversarial neural networks (GANS). Each project chooses a focus for the human-machine collaboration — from music and fashion to pizza and perfume — to create outcomes that otherwise might never have been imagined!</p><p>To read more about the project, it's fully documented at:&nbsp;https://medium.com/@howtogeneratealmostanything, and is written about here:&nbsp;https://venturebeat.com/2018/09/10/mit-students-use-ai-to-cook-up-pizza-recipes/</p>",2018-09-30,,2018-09-17 15:49:17.289,True,2018-07-01,How To Generate Almost Anything,PUBLIC,,False,Scalable Cooperation,False
nightmare-machine,pinary,False,"<p>For centuries, across geographies, religions, and cultures, people try to innovate ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity. This challenge is especially important in a time when we wonder what the limits of Artificial Intelligence are: Can machines learn to scare us? Towards this goal, we present you Haunted Faces and Haunted Places: computer generated scary imagery powered by deep learning algorithms!
                    
                </p>",,,2017-01-31 04:41:35.401,True,2016-10-20,Nightmare Machine,PUBLIC,http://nightmare.mit.edu/,True,Scalable Cooperation,False
shelley,pinary,False,"<h2>Project website:&nbsp;<a href=""http://shelley.ai"">shelley.ai&nbsp;<br></a>Human-AI collaborated stories:&nbsp;<a href=""http://stories.shelley.ai"">stories.shelley.ai&nbsp;<br></a>Follow&nbsp;<a href=""http://twitter.com/shelley_ai"">@shelley_ai</a> to collaborate with Shelley!&nbsp;</h2><br><p>For centuries, across geographies, religions, and cultures, people have innovated ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity.&nbsp;This challenge is especially important at a time when we are exploring the limits of artificial intelligence: Can machines learn to scare us?&nbsp;</p><p>In Halloween 2016 we presented the&nbsp;<a href=""http://nightmare.mit.edu/"">Nightmare Machine</a>—computer-generated scary imagery powered by deep learning algorithms.&nbsp;</p><p>This Halloween, we present <b>Shelley:&nbsp;Human-AI Collaborated Horror Stories</b>!&nbsp;</p><p>Shelley is a deep-learning powered AI who was raised reading eerie stories coming from&nbsp;<a href=""http://reddit.com/r/nosleep"">r/nosleep</a>. Now, as an adult—and not unlike Mary Shelley, her Victorian idol—she takes a bit of inspiration in the form of a random seed, or a short snippet of text, and starts creating stories emanating from her creepy creative mind. But what Shelley truly enjoys is working collaboratively with humans, learning from their nightmarish ideas, creating the best scary tales ever. If you want to work with her, respond to the stories she'll start every hour on her Twitter <a href=""http://twitter.com/shelley_ai"">account</a>, and she will write with you the first AI-human horror anthology ever put together!</p>",,,2017-12-12 21:51:49.213,True,2017-10-15,Shelley: Human-AI Collaborated Horror Stories,PUBLIC,http://shelley.ai,True,Scalable Cooperation,False
norman,pinary,False,"<p>We present <a href=""http://norman-ai.mit.edu"">Norman</a>, world's first psychopath AI. Norman was inspired by the fact that the data used to teach a machine learning algorithm can significantly influence its behavior. So when people say that AI algorithms can be biased and unfair, the culprit is often not the algorithm itself, but the biased data that was fed to it. The same method can see very different things in an image, even ""sick"" things, if trained on the wrong (or, the right!) data set. Norman suffered from extended exposure to the darkest corners of Reddit, and represents a case study on the dangers of artificial intelligence gone wrong when biased data is used in machine learning algorithms.&nbsp;</p><p>Norman is an AI that is trained to perform image captioning; a popular deep learning method of generating a&nbsp;textual description of an image. We trained Norman on image captions from an infamous subreddit (its name is redacted due to its graphic content) that is dedicated to documenting and observing the disturbing reality of death. Then, we compared Norman's responses with a standard image-captioning neural network (trained on&nbsp;<a href=""http://mscoco.org/"">MSCOCO</a>&nbsp;dataset) on <a href=""https://en.wikipedia.org/wiki/Rorschach_test"">Rorschach inkblots</a>–a test that is used to detect underlying thought disorders.</p><p>Visit <a href=""http://norman-ai.mit.edu"">norman-ai.mit.edu</a> to explore what Norman sees!<br></p>",,,2019-02-14 19:39:41.066,True,2018-04-01,Norman,PUBLIC,,True,Scalable Cooperation,False
identifying-the-human-impacts-of-climate-change,pinary,False,<p>Climate change is going to alter the environments that we depend on in myriad ways. We're using data to identify and quantify these potential human impacts.&nbsp;</p>,,,2019-04-19 17:44:02.613,True,2016-07-01,Identifying the human impacts of climate change,PUBLIC,,True,Scalable Cooperation,False
deep-empathy,nobradov,False,"<h2><i>What would&nbsp;<b>your city</b>&nbsp;look like after a disaster?</i>&nbsp;</h2><p>Deep Empathy&nbsp;is&nbsp;a collaboration between the&nbsp;<a href=""http://www.media.mit.edu/groups/scalable-cooperation"">Scalable Cooperation</a>&nbsp;group and the&nbsp;<a href=""https://www.unicef.org/innovation"">UNICEF Innovation Office</a>&nbsp;to pursue a scalable way to increase empathy.&nbsp;</p><p>The brutal, six-year-old Syrian war has affected more than 13.5 million people in Syria , including 80% of the country's children—8.4 million young lives shattered by violence and fear. Hundreds of thousands of people have been displaced and their homes destroyed.&nbsp;</p><p>But people generate a response that statistics can't. And technologists—through tools like AI—have opportunities to help people see things differently. We wondered: ""Can AI increase empathy for victims of far-away disasters?"" This question led us to create a provocation for the research community to examine how AI can create narratives to tell the stories of some of the world's most intractable problems.</p>",2018-05-01,,2017-12-04 19:54:02.578,True,2017-12-01,Deep Empathy,PUBLIC,http://deepempathy.mit.edu/,False,Scalable Cooperation,False
black-rock-atlas,nobradov,False,"<p>Burning Man is a magical place that gets the best of human creativity and collaboration to flourish. To further understand what makes this magic happen, we are creating the first ever Black Rock Atlas–a map of the social patterns and networks that exist on the playa. </p><p>To do this, we are tracking the decentralized journey of a multitude of vessels through the gift economy of Burning Man with GPS technology and generative photography. The Atlas will explore new ways of community interaction, storytelling, and data visualization.</p>",2018-12-11,,2018-10-15 20:44:54.352,True,2018-08-15,Black Rock Atlas,PUBLIC,https://blackrockatlas.mit.edu,False,Scalable Cooperation,False
nightmare-machine,nobradov,False,"<p>For centuries, across geographies, religions, and cultures, people try to innovate ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity. This challenge is especially important in a time when we wonder what the limits of Artificial Intelligence are: Can machines learn to scare us? Towards this goal, we present you Haunted Faces and Haunted Places: computer generated scary imagery powered by deep learning algorithms!
                    
                </p>",,,2017-01-31 04:41:35.401,True,2016-10-20,Nightmare Machine,PUBLIC,http://nightmare.mit.edu/,True,Scalable Cooperation,False
global-cooperation,nobradov,False,<h1>Measuring Cooperation at Scale</h1>,,,2018-01-10 16:18:38.348,True,2015-09-01,Global Cooperation,PUBLIC,,True,Scalable Cooperation,False
turingbox,nobradov,False,"<p>TuringBox is a platform&nbsp;that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms.&nbsp;It is a two-sided marketplace. On one side, <i>AI contributors</i> upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">On the other side, </span><i style=""font-size: 18px; font-weight: 400;"">AI examiners</i><span style=""font-size: 18px; font-weight: 400;"">&nbsp;develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.</span></p>",,,2018-04-05 19:00:33.916,True,2018-03-21,TuringBox: Democratizing the study of AI,PUBLIC,http://turingbox.mit.edu,True,Scalable Cooperation,False
deepmoji,nobradov,False,"<p><i>Emotional content is an important part of language. There are many use cases now showing that natural language processing is becoming an increasingly important part of consumer products.&nbsp;We are attempting to learn more about human emotions.</i></p><p>In his 2006 book <i>The Emotion Machine</i>, legendary computer scientist Marvin Minsky (co-founder of the field of Artificial Intelligence and one of the founding faculty members of the MIT Media Lab) wrote about the central role of emotions in reasoning—reminding us that AI will only be capable of true commonsense reasoning once it has understood emotions. To Minsky, emotions are not the opposite of rational reason, something to be weeded out before we can think clearly; rather, emotions are just a different way of thinking.</p><p><b><a href=""http://deepmoji.mit.edu/"">TRY DEEPMOJI</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href=""https://deepmoji.mit.edu/contribute/"">HELP TEACH OUR AI ABOUT EMOTIONS</a></b><br></p><p>But this is hardly helpful to a computer scientist trying to construct an emotional machine by programming a concrete set of rules. If you ask two people to explain what makes a particular sentence happy, sad, serious, or sarcastic, you will likely get at least two different opinions. Much of what determines emotional content is context-specific, culturally constructed, and difficult to describe in an explicit set of rules.<br></p>",,,2019-05-29 13:10:23.998,True,2017-08-02,DeepMoji,PUBLIC,http://deepmoji.mit.edu,True,Scalable Cooperation,False
deep-angel-ai,nobradov,False,"<p><b>Deep Angel&nbsp;</b>is an artificial intelligence that erases objects from photographs. The algorithm is hosted on <a href=""http://deepangel.media.mit.edu"">http://deepangel.media.mit.edu</a>, which enables anyone&nbsp;to interact with the AI and explore what it can disappear.</p><p>Part philosophy, part technology, and part art, Deep Angel is designed to spark a series of conversations on technology in our daily lives and AI and media manipulation.&nbsp;&nbsp;</p><p>Deep Angel draws from&nbsp; Walter Benjamin's description of Paul Klee's Angelus Novus, the angel of history who has clairvoyance into the dark side of what appears to be progress. The angel sees the unravelling of all that matters in the world and would like to alert the world about his vision, but he's caught in the storm of progress and can't communicate any messages. The images that Deep Angel generates are intended to deliver the message that Angelus Novus would have sent if he could.&nbsp;</p><p>The algorithm applies computer vision techniques to automatically (1) detect and outline objects in images, (2) remove the outlined object from the image, and (3) imagine what the image would look like if that outlined object were removed from the image. Any image uploaded and transformed by Deep Angel can be published on the Deep Angel website by clicking the ""Publish to Deep Angel"" button.&nbsp;</p><p>The AI's performance varies across photographs. Sometimes, it's impossible to tell what has been disappeared. Other times, the images appear similar to the images from Adrian Piper's <i>Everything</i> series. The more people interact with the algorithm, the more attuned people will be to the potential and limitations of modern AI to manipulate the media. It's now possible to automate the vanishing commissar in Soviet photography, but the AI is not yet perfect. Below are two examples of the Deep Angel AI effect: (1) a gif generated by Deep Angel showing a father and daughter disappearing in the wilderness and (2) two images showing the before and after of Deep Angel peering into a photo of a professional surfer.&nbsp;</p>",,,2019-02-14 19:46:25.924,True,2018-08-06,Deep Angel: The AI behind the aesthetics of absence,PUBLIC,http://deepangel.media.mit.edu/,True,Scalable Cooperation,False
identifying-the-human-impacts-of-climate-change,nobradov,False,<p>Climate change is going to alter the environments that we depend on in myriad ways. We're using data to identify and quantify these potential human impacts.&nbsp;</p>,,,2019-04-19 17:44:02.613,True,2016-07-01,Identifying the human impacts of climate change,PUBLIC,,True,Scalable Cooperation,False
machine-behavior,nobradov,False,"<p>Machines powered by artificial intelligence (AI) increasingly                           mediate our social, cultural, economic, and                           political interactions. Understanding the                           behavior of AI systems is essential to our                           ability to control their actions, reap their                           benefits, and minimize their harms. We argue                           this necessitates a broad scientific research                           agenda to study machine behavior that                           incorporates but expands beyond the discipline                           of computer science and requires insights from                           across the sciences. Here we first outline a                           set of questions fundamental to this emerging                           field. We then explore the technical, legal,                           and institutional constraints facing the study                           of machine behavior.</p>",,,2019-04-30 19:56:54.922,True,2019-04-24,Machine Behavior,PUBLIC,,True,Scalable Cooperation,False
microperformativity-sex-hormones,maggic,False,"<p><span style=""font-weight: normal;""><i>I am a traveling saleswoman, a nomadic cuntress performing with sex hormones.</i>&nbsp;</span></p><p>Exhibited at Raumschiff Gallery during Ars Electronica in Linz, Hormone Microperformance is an installation of hormonal shrines situated next to the ""freak-science"" experimentation process from which it originates. Hormones, when isolated from the body, act as pheromones that can influence the mind and behavior through chemical signaling. Using urine samples given by the other artists of the show, sex hormones were extracted and connected to oxygen masks for the audience to inhale and experience. What they experience is a microcolonization of the mind that is both ancient and evolutionary, but imperceptible to the naked eye.<br></p>",2017-08-31,,2017-10-11 20:29:21.605,True,2016-09-07,Hormone Microperformance,PUBLIC,http://maggic.ooo,False,Design Fiction,False
open-source-estrogen,maggic,False,"<p>Biomolecules to biopolitics: hormones with institutional biopower! Open Source Estrogen combines do-it-yourself science, body and gender politics, and ethics of hormonal manipulation. The goal of the project is to create an open source protocol for estrogen biosynthesis. The kitchen is a politically charged space, prescribed to women as their proper dwelling, making it the appropriate place to prepare an estrogen synthesis recipe. With recent developments in the field of synthetic biology, the customized kitchen laboratory may be a ubiquitous possibility in the near future. Open-access estrogen would allow women and transgender females to exercise greater control over their bodies by circumventing governments and institutions. We want to ask: What are the biopolitics governing our bodies? More importantly, is it ethical to self-administer self-synthesized hormones?</p>",2017-10-31,--Choose Location,2017-10-11 20:29:44.233,True,2015-09-01,Open Source Estrogen,PUBLIC,http://maggic.ooo/Open-Source-Estrogen-2015,False,Design Fiction,False
estrofem-lab,maggic,False,"<p><span style=""font-weight: normal;"">Geeking, workshoplogy, and freak science on the microcolonization of estrogen biomolecules Estrofem! Lab is dedicated to the development of a mobile estrogen lab: a set of tools, protocols, and wetware for low-cost, participatory biohacking necessitated by its genesis project, Open Source Estrogen. Regarded sometimes as hobo science, freak science, and public amateurism, the Estrofem Lab and its workshopologies aim to detect and extract estrogen from bodies and environmental sources, providing the contextual framework for why we hack estrogen, and why we perform science as citizens and hacktivists. This ongoing artistic investigation has led to creation of yeast estrogen sensors (YES-HER yeast) containing human estrogen receptor for detection, vacuum pump solid phase extraction (SPE) using cigarette filters, and DIY column chromatography using broken glass bottles, smashed silica gel, and methanol.</span></p>",,,2016-12-05 00:17:04.078,True,2015-09-07,Estrofem! Lab,PUBLIC,http://maggic.ooo/Estrofem-Lab-2016,True,Design Fiction,False
egstrogen-farms,maggic,False,"<p></p><h2></h2><h2></h2><p><span style=""font-size: 18px;""><b>A transgenic chicken commercial for ovulating women</b></span></p><p>Esgtrogen Farms is a fictional company that raises genetically modified chickens that produce ovulation hormones in their egg whites. The eggs are marketed towards women who are either trying to get pregnant, or work as egg donors for the fertility industry. The slogan reads, ""One egg a day is the fertility way."" The project highlights a connection between women and chickens as raw commodities for the biotech industry, performing ways in which women are targeted for bio-consumerism. What is the rhetoric and imagery used in birth management products? Moreover, as avian transgenic technologies become further developed, is it possible to imagine a confluence of the poultry industry with the pharmaceutical health industry?</p>",,--Choose Location,2017-04-05 18:37:12.885,True,2015-09-07,Egstrogen Farms,PUBLIC,,True,Design Fiction,False
soundform,tice,False,"<p>SoundFORMS creates a&nbsp;<span style=""font-size: 18px; font-weight: 400;"">new method for composers of electronic music to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">interact with their compositions. Through the use of a&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">pin-based shape-shifting display, synthesized&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">waveforms are projected in three dimensions in real&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">time affording the ability to hear, visualize, and interact&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">with the timbre of the notes. Two types of music&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">composition are explored: generation of oscillator&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">tones, and triggering of pre-recorded audio samples.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The synthesized oscillating tones have three timbres:&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">sine, sawtooth and square wave. The pre-recorded</span></p><p>audio samples are drum tracks. Through the use of a&nbsp;<span style=""font-size: 18px; font-weight: 400;"">gestural vocabulary, the user can directly touch and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">modify synthesized waveforms.</span></p>",2016-06-01,,2017-05-26 15:59:57.300,True,2016-02-01,SoundFORMS: Manipulating Sound Through Touch,PUBLIC,,False,Viral Communications,False
kinephone,rebklein,False,"<p>This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.</p>",2016-08-01,,2017-05-18 01:07:33.691,True,2016-05-01,Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display,PUBLIC,,False,Opera of the Future,False
mumble-melody,rebklein,False,"<p>Mumble Melody uses musically altered sensory feedback as a potential treatment for stuttering.<br></p><p>Several studies have shown improvement in speech fluency with delayed and pitch-altered auditory feedback.&nbsp; In this project, we use sensory/auditory alterations that stimulate both the right (and left) hemispheres as a means of reducing the auditory feedback-mediated errors in basal ganglia-related motor selection&nbsp; &nbsp; &nbsp;<br></p><p>Stuttering is a condition characterized by involuntary, periodic disturbances in speech fluency, usually via speech sound repetitions, blockages, or prolongations. A host of other secondary features also accompany the condition (e.g. tongue thrusting, eye blinking, body movements), although these are not considered “core” features. Stuttering improves when an individual’s speech is played back to him or her in an altered manner, most famously when delayed by fractions of second, but also when the frequency is shifted, when masked with white noise, and when reading in choral speech. This rather interesting phenomenon of altered feedback-induced fluency is theorized to result from a reduced ability to detect small errors in articulation that occur in stuttering, which reduces its inhibition on speech initiation and output via the feedback mechanism.</p><p>In people who stutter, there is both structural and functional evidence of atypical hemispheric lateralization of speech and language. People who stutter, when speaking fluently, tend to activate the right hemisphere a during speech tasks. The white matter integrity is disrupted on the left. This rightward shift of speech function may be compensatory (as opposed to causal). Trials comparing fluent versus non-fluent trials in people who stutter reveal the former to associated with activity in the right hemisphere and latter with the left hemisphere. In addition, white matter integrity is negatively correlated with severity of dysfluency on the left, and positively correlated on the right. The overall notion is that stuttering is associated with atypical left-sided speech mechanisms, and that this can be overcome, at least partially, when the right hemisphere is able to effectively compensate. &nbsp;While most prominently explored in stuttering, the idea that left hemisphere lesions can be overcome by shifting the motor control of speech to the right is supported by other studies in post-stroke aphasia.</p><p>In this light, altered auditory feedback—a fluency-inducing intervention in stuttering—is associated with activity in right hemispheric sensory, motor, and language areas. In addition, singing—another fluency-evoking task—is known to activate right hemisphere motor areas compared with non-musical speech production.</p>",2019-01-01,,2018-10-22 16:47:03.821,True,2018-03-01,Mumble Melody,PUBLIC,,False,Opera of the Future,False
sound-cycles,rebklein,False,"<p>Sound Cycles is a new interface for exploring, re-mixing, and composing with large volumes of audio content. The project presents a simple and intuitive interface for scanning through long audio files or pre-recorded music. Sound Cycles integrates with the existing Digital Audio Workstation for on-the-fly editing, audio analysis, and feature extraction.</p>",,--Choose Location,2016-12-05 00:17:04.683,True,2015-01-01,Sound Cycles,PUBLIC,,True,Opera of the Future,False
joy-branch,rebklein,False,"<p>The Joy Branch project explores different user interfaces to allow parrots to shape their sonic environment.&nbsp;&nbsp;Animal agency—control of the environment—is an important and underutilized element of captive care.&nbsp;&nbsp;Parrot species are vocal learners, and as such are highly attuned to their sonic environment. Much of their brains are involved in the production and analysis of sound, and yet their sonic environment in managed care does not provide a rich experience. In this project, we assess the efficacy of new enrichment techniques that have the potential to improve the lives of these birds through music.&nbsp;&nbsp;The project involves the placement of sonic enrichment elements into the birds’ enclosures under controlled and supervised conditions.</p><p>The ""joystick branch"" element exposes only a standard wooden perch to the birds. The aim is to create naturalistic interactive methods for birds to generate sounds, and to assess their optional engagement with these new modes of control.&nbsp;</p>",2019-12-31,,2019-04-18 01:19:48.567,True,2019-01-01,Joy Branch,PUBLIC,,True,Opera of the Future,False
breathing-window,rebklein,False,"<p>Breathing Window is a tool for non-verbal dialogue that reflects on your own breathing while also offering a window on another person's respiration. This prototype is an example of shared human experiences (SHEs) crafted to improve the quality of human understanding and interactions. Our work on SHEs focuses on first encounters with strangers. We meet strangers every day, and without prior background knowledge of the individual we often form opinions based on prejudices and differences. In this work, we bring respiration to the foreground as one common experience of all living creatures.</p>",,--Choose Location,2016-12-05 00:16:16.294,True,2015-01-01,Breathing Window,PUBLIC,,True,Opera of the Future,False
fablur,rebklein,False,"<p>Fablur explores the limit of the self in its relationship to others through the medium of clothing. The augmented gown uses a rear dome projection system on the surface of the fabric. The system comprises laser projectors and mirror structures talking wirelessly with a computer, within which is contained both content and warp projection mapping software. This novel technological interface presents both a performative element and a seamless integration in a woman's life experience. This wearable project questions the boundary between the self and others, the boundary between the individual and society, and the boundary between the body and nature.</p>",,--Choose Location,2016-12-05 00:16:26.876,True,2016-01-01,Fablur,PUBLIC,,True,Opera of the Future,False
sidr-deep-learning-based-real-time-speaker-identification,rebklein,False,"<p>Consider each of our individual voices as a flashlight to illuminate how we project ourselves in society and how much sonic space we give ourselves or others. Thus, turn-taking computation through speaker recognition systems has been used as a tool to understand social situations or work meetings. We present SIDR, a deep learning-based, real-time speaker recognition system designed to be used in real-world settings. The system is resilient to noise, and adapts to room acoustics, different languages, and overlapping dialogues. While existing systems require the use of several microphones for each speaker or the need to couple video and sound recordings for accurate recognition of a speaker, SIDR only requires a medium-quality microphone or computer-embedded microphone.</p>",,--Choose Location,2016-12-05 00:17:02.465,True,2016-01-01,SIDR: Deep Learning-Based Real-Time Speaker Identification,PUBLIC,,True,Opera of the Future,False
empathy-and-the-future-of-experience,rebklein,False,"<p>Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of–as well as long-term commitment to–empathic communication.</p>",,--Choose Location,2019-04-17 19:59:42.795,True,2015-01-01,Empathy and the future of experience,PUBLIC,,True,Opera of the Future,False
immersound-vr,rebklein,False,"<p>ImmerSound is a virtual reality experience wherein one can compose music by drawing in 3D. The resulting composition is a sculpted soundscape to be experienced both visually and in 3D audio.&nbsp;</p><p>The user starts by choosing an instrument in the system and testing the sound that this instrument would produce at different locations. Then the user can ""paint"" a melody in space, where the elevation of the ""sound brush"" defines the pitch of the instrument, and the speed of the hand corresponds to the tempo of the melody created. &nbsp;<span style=""font-size: 18px; font-weight: normal;"">A wide range of instruments enables the creation of rich compositions with percussion, bass, classical instruments, and ambient sounds.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">This project associates sounds and space in a new way by offering an intuitive and natural way to interact with music. One can also imagine the same type of visual compositional space used as a neutral zone for collaboration between two or more people in different geographical locations and from different cultural backgrounds, using the universal language of music to connect in less-biased ways. This system is a first example of the potential of virtual reality for music and experiences of connection.</span></p>",,,2017-04-05 18:49:37.476,True,2016-09-15,ImmerSound VR,PUBLIC,,True,Opera of the Future,False
nebula,rebklein,False,"<p>Nebula is a voice-controlled interactive software app that allow users to conduct a choir of diverse vocal sounds by using only their voice as input. The system is based on the Constellation project by Akito van Troyer that takes sonic material and organizes it visually to let anyone compose creative soundscapes. Nebula uses hundreds of vocal samples that are represented as individual stars and organized by perceptual and spectral audio features. The samples get triggered and activated when the user sings or produces any sound with the voice. The voice is analyzed in real time, and this analysis is then used to to trigger and mix a cascade of sounds with similar features. The voice becomes a kind of conductor's baton that creates a dialogue without words between the individual and the community. And once a participant uses Nebula, their own voice, first used as a controller, is then transformed into a new sample adding an additional star to the experience for all subsequent participants. The result - a final cosmos of voices—provides material that might be used by composer Tod Machover for the final Philadelphia Voices City Symphony.&nbsp;</p>",,,2018-10-19 19:38:07.641,True,2017-08-01,Nebula,PUBLIC,,True,Opera of the Future,False
snasi,rebklein,False,"<p>SNaSI (Social Navigation through Subtle Interactions) is a wearable system to help blind people in face-to-face interactions. Assistive systems for the blind have been an important area of research in the wearable community for many decades. Most of those systems focus on spatial navigation issues. In the last few years, we start noticing a move toward technologies to assist the blind with social navigation. Most of those systems treat social navigation the same way as spatial navigation, focusing mainly on utilitarian aspects of human interaction (what is needed to obtain information, what information is exchanged, etc.).&nbsp;</p><p>In this project we argue that when moving into this space we need to think about social accessibility and respect of human connection first because, most of the time, face-to-face interaction aims primarily to create and reinforce human connection rather than exchanging information. With this in mind, we defend the importance of designing with subtlety in regards of framing, reasoning and design challenges. To this end, we present SNaSI—Social Navigation through Subtle Interactions—a wearable garment designed to help blind and visually impaired people in face-to-face interaction with sighted peers and we describe how our design criteria were guided by subtlety and social acceptability. Designed in collaboration with Microsoft Research UK, HXD group, Morrison Cecily, Huburn Josh, Grayson Martin.</p>",,,2017-10-13 20:15:18.696,True,2016-06-01,SNaSI: Wearable device to help the blind with social navigation,PUBLIC,,True,Opera of the Future,False
speech-co,rebklein,False,"<p>Speech Companion is an exploration in the domain of real-time extraction of musicality from speech. Speech is one of the richest and most ubiquitous modalities of communication used by human beings. Its richness lies in the combination of linguistic and nonlinguistic information. Musicality is one of the most crucial nonlinguistic components of speech and covers tempo and rhythms of the speaker as well as the pitch variation and unique texture of the vocal sounds. Abstracting musicality from a speech in real time presents several challenges from latency to subjective pitch identification or recognizing voiced/unvoiced sounds. In this paper, we describe a new system for real-time extraction of the music present in everyday speech based on time and pitch quantization. Our system offers several modes from a simple synchronized melody line to a more complex accompaniment much like a singer accompanying herself at the guitar.&nbsp;</p><p>&nbsp;With such a system, we offer a proof of concept and a working prototype to explore the real-life situations where the music of speech impacts speakers or listeners such as in the contexts of infant-directed speech, language acquisition, human-animal communication, speech pathology, aphasia reeducation, or even music learning and musical composition.<br></p>",,,2018-04-27 15:50:50.384,True,2018-01-01,Speech Companion,PUBLIC,,True,Opera of the Future,False
memory-music-box,rebklein,False,"<p>We are transforming a classic music jewelry box into a digital memory box and Skype portal that enable those not familiar with technology to stay in touch with their family and friends. The box has three different modes. To switch mode the user only has to turn the small crank in the back, like they would do with a regular music box. The crank is linked to a rotary encoder. The back of the box is covered with a two-way mirror covering a small LCD screen; when the screen is turned off, it looks like a regular mirror but when the screen is on, it looks like a display. In the first mode, the box plays the favorite music of the user with the screen off. In the second mode, the display shows photographs of family and friends. By turning the crank or by clicking on the characters in the photographs, the box goes into mode 3, which is a Skype portal enabling the user to instantly call a family member face-to-face. This device is mainly imagined for elderly parents with dementia or memory loss.&nbsp;</p>",,,2018-04-27 15:51:14.777,True,2017-10-01,Memory Music Box,PUBLIC,,True,Opera of the Future,False
egg-shell-carving,rebklein,False,"<p>Expressing the hidden things inside, working with one of the most fragile of media.</p><p>Hand carved illuminated Hen, Duck, Blue Duck and Goose egg shells. After emptying and cleaning the shells, I use a hand rotary tool to carve out the surface.</p>",,,2018-06-29 19:09:48.567,True,2016-07-01,Egg Shell Carving,PUBLIC,,True,Opera of the Future,False
sonic-enrichment-at-the-zoo,rebklein,False,<p>This project is a collaboration between the MIT Media Lab and the San Diego Zoo to design and build interactive sonic enrichment systems for animals in managed care. Our approach is based on the potential of animal-animal and human-animal relationship as an environmental enrichment for the welfare of zoo-housed animals specifically in terms of animal vocal communication.&nbsp; Enrichment is a way for caregivers to provide animals with the opportunity to express natural behaviors and reduce stereotypic behaviors.&nbsp;</p>,,,2019-04-17 20:03:16.443,True,2018-04-01,Sonic enrichment at the zoo,PUBLIC,,True,Opera of the Future,False
kinephone,thomassl,False,"<p>This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.</p>",2016-08-01,,2017-05-18 01:07:33.691,True,2016-05-01,Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display,PUBLIC,,False,City Science,False
a-pops,thomassl,False,"<p>A design project done in collaboration with the MIT Media Lab and the Laboratorio para la Ciudad (Laboratory for the City), Mexico City's experimental office for civic innovation and urban creativity, A-pops is a networked learning experience across Mexico City that supports young learners in engaging in emergent and playful opportunities in and beyond their local communities. In line with the ""Playful City"" goal, this project aims to embed playful learning experiences across Mexico City that are creative, collaborative, and public, by leveraging existing public spaces throughout neighborhoods and micro-communities across the city. By embedding a variety of playful learning experiences across a variety of locations, a wide range of learners have the ability to easily and socially engage in transformative experiences that support key skills in design, collaboration, creativity, programming, and learner agency.</p>",2017-06-01,--Choose Location,2018-10-20 18:17:15.133,True,2015-09-01,A-pops,PUBLIC,,False,City Science,False
bike-swarm,thomassl,False,"<p>As bikes navigate city streets after dark, they are often equipped with lights. The lights make the bikes visible to cars or other bikers, and the hazards of traffic less dangerous.</p><p>Imagine that as solitary bikes come together, their lights begin to pulsate at the same cadence. The bikers may not know each other, or may only be passing each other briefly, but for the moments they are together, their lights synchronize. The effect is a visually united presence, as groups of bikes illuminate themselves with a gently pulsing, collective light source.</p>",,,2019-06-11 16:37:33.942,True,2019-02-01,[bike] swarm,PUBLIC,http://aberke.com,True,City Science,False
music-visualization-via-musical-information-retrieval,thomassl,False,"<p>In a study of human perception of music in relation to different representations of video graphics, this project explores the automatic synchronization in real time between audio and image. This aims to make the relationship seem smaller and more consistent. The connection is made using techniques that rely on audio signal processing to automatically extract data from the music, which subsequently are mapped to the visual objects. The visual elements are influenced by data obtained from various Musical Information Retrieval (MIR) techniques. By visualizing music, one can stimulate the nervous system to recognize different musical patterns and extract new features.</p>",,--Choose Location,2016-12-05 00:17:18.424,True,2015-01-01,Music Visualization via Musical Information Retrieval,PUBLIC,,True,City Science,False
the-telemetron,thomassl,False,"<p>Today, the environments that humans occupy in space are designed for survival. Humans are carefully shuttled to and from space, and during their relatively short stays, they are provided with minimum supplies to remain alive and able to perform experiments. As we begin to plan less for short visits and more for life in space (such as a six to eight month trip to Mars and beyond) the question becomes: What does human culture look like in space?</p><p><a href=""https://www.instagram.com/nico_lh/"">Nicole L'Huillier</a> and <a href=""https://www.instagram.com/sandsfish"">Sands Fish</a> decided to explore how design and creativity might evolve as we begin to do more than merely survive in space. <b>The Telemetron</b> is a unique mode of musical performance that takes advantage of the poetics of zero gravity, and opens a new field of musical creativity. The project attempts to expand expression beyond the limits of earth-based instruments and performers. Leveraging sensors, data transmission and capture (for performance after flight), as well as their experience as composers and performers, Sands and Nicole explore a new body language for music. </p><p>The Telemetron was played for the first time during the inaugural Media Lab Space Exploration Initiative's Zero G flight. This instrument is a clear dodecahedron chamber that contains customized ""chimes"" containing gyroscopes. The chimes emit their telemetry as they spin and collide. Sensors record the position, direction, and spin of each chime. These elements create the composition. The performers play the instrument by moving it in space, shaking it, colliding it. The performance can be recorded to be experienced on earth or used as a live instrument during future space flights. The instrument can be played inside space craft or in the vacuum of space without the benefit of sound waves.</p><p>Recorded as a beautiful audio-visual experience, this experiment opens the doors for new forms of creative expression, and brings the magic of space to musicians. We hope to reach beyond the utilitarian, and toward the inspiring.</p>",,,2018-08-20 20:13:42.164,True,2017-08-01,The Telemetron,PUBLIC,,True,City Science,False
diastrophisms,thomassl,False,"<p><br>Diastrophisms is a sound installation with a modular system that sends images through rhythmic patterns. It is built on a set of debris from the Alto Río building that was destroyed by the 27F earthquake in 2010 in Chile. With&nbsp;&nbsp;Diastrophisms we were looking for a poetical, critical and political crossing between technology and matter, in order to raise questions about the relationship between human beings and nature, and to consider the construction of memory in a community by questioning the notion of monument, as well as to imagine new forms of communication in times of crisis.</p><p>Work by:&nbsp;Nicole L’Huillier, Thomas Sanchez Lengeling, and Yasushi Sakai</p><p>Exhibited at Siggraph Art Gallery 2018,&nbsp;curated by Andres Burbano. A&nbsp;paper about this work was published&nbsp;in Leonardo Journal for the special edition of Siggraph 2018 Art Papers and Art Gallery Exhibition. The paper was written by Nicole L’Huillier and Valentina Montero.</p><p>Diastrophisms&nbsp;was also exhibited as<a href=""http://www.bienaldeartesmediales.cl/13/obra/talking-rock/""> ""Diastrofismos""</a> at the <a href=""http://www.bienaldeartesmediales.cl/13/"">Media Arts Bienal,</a> Santiago de Chile, 2017, curated by Valentina Montero.</p>",,,2019-02-14 19:56:31.323,True,2017-10-01,Diastrophisms,PUBLIC,,True,City Science,False
cityscope-livingline-shanghai,thomassl,False,"<p>College of Design and Innovation of Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform to support the urban decision making that promotes urban vibrancy and innovation potential. </p><p>The “NICE2035 LivingLine” project in Shanghai, China, is a design-driven, community-based urban innovation initiated by Professor Yongqi Lou, Dean of College of Design of Innovation. LivingLine is a crowdsourcing and co-creation project aiming at building an ecosystem of innovation and entrepreneurship on the internal street of a typical gated residential neighborhood. By introducing radical programs such as living labs, co-working space, and startup-incubators into underutilized storefront space, LivingLine’s goal is to revitalize the urban space and to prototype diverse future lifestyles.<br></p>",,,2019-06-06 16:01:44.891,True,2018-03-01,CityScope LivingLine Shanghai,PUBLIC,,True,City Science,False
kinephone,pewebb,False,"<p>This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.</p>",2016-08-01,,2017-05-18 01:07:33.691,True,2016-05-01,Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display,PUBLIC,,False,Tangible Media,False
s-c-a-l-e,pewebb,False,"<p>S.C.A.L.E. is a system for detecting localization of an external object or agent, utilizing weight and pressure as a controlling constant for the detection of place and pressure. &nbsp;The system is designed for simple prototyping of interactive products. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2016-12-05 00:16:27.706,True,2016-08-20,S.C.A.L.E.,LAB-INSIDERS,,True,Tangible Media,False
fiftynifty,pewebb,False,"<p>This is a grassroots challenge to get friends to participate in democracy by making calls to congresspeople in all 50 states. Live phone calls are the best way to directly express your opinion on an issue to your elected officials. Your mission is to pass message this along to friends who will make calls and also pass the message/link along to others who will do the same. It's a social chain letter and a call to action for a better participatory democracy. &nbsp;<span style=""font-size: 18px; font-weight: normal;"">We help you make your call and you pass on an invitation for your friends to do the same. Your invite can stress your opinion on a given issue.&nbsp;</span></p><p>The winners are the first ten chains to reach 50 states and accumulate the most challenge points. You get 250 points for making a call, 125 points for a call that your friend makes, 65 points for the call their friend makes, on and on. Everyone on the chain earns points. Points count for your first call to each of your two senators and your representative. You get a bonus for a ""grand slam""—a network that reaches all 435 representatives and 100 senators.</p><p>There is a leaderboard and a network view so you can track how you are doing. You can also see how much of the country your chain is covering.</p>",,,2019-06-04 20:46:21.258,True,2017-02-13,FiftyNifty,PUBLIC,https://fiftynifty.org,True,Tangible Media,False
auto-inflatables,pewebb,False,,,,2017-11-25 17:16:39.275,True,2017-01-01,Auto-Inflatables,PUBLIC,,True,Tangible Media,False
kinephone,akito,False,"<p>This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.</p>",2016-08-01,,2017-05-18 01:07:33.691,True,2016-05-01,Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display,PUBLIC,,False,Opera of the Future,True
empathy-and-the-future-of-experience,akito,False,"<p>Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of–as well as long-term commitment to–empathic communication.</p>",,--Choose Location,2019-04-17 19:59:42.795,True,2015-01-01,Empathy and the future of experience,PUBLIC,,True,Opera of the Future,True
moduland,akito,False,"<h2>MODULAND is a playground kit for learning&nbsp;electronic music.</h2><p>MODULAND is an interactive project created by the&nbsp;<a href=""https://www.media.mit.edu/events/mlberlin-signalandnoise/"">MIT Media Lab Berlin – Signal &amp; Noise prototyping workshop</a>,&nbsp;within the track “Playful Machines that Make Music.""&nbsp;</p><p>With MODULAND, playgrounds become modular synthesizers to raise curiosity, exploration, and connection to electronic music making.</p><p>By creating playful machines that use LEGO bricks, sensors, and microcontrollers, it creates an embodied and interactive music lesson in an urban space.</p>",,,2018-08-29 14:44:22.539,True,2018-08-14,MODULAND,PUBLIC,https://medialabmoduland.wordpress.com/,True,Opera of the Future,True
_amoeba,oi7,False,"<p>Amoeba Wall: a context aware wall system. Amoetecture is a set of amoeba-like dynamic spatial elements, including transformable floors, ceilings, tables, chairs, and workstations. We focus on designing architecture robotics and platforms that enable a hyper-efficient and dynamically reconfigurable co-working space that accommodates a wide range of activities in a small area.</p><p><strong>&nbsp;Award</strong></p><p>-<a href=""https://www.designboom.com/design/2019-a-design-award-and-competition-call-for-entries-02-07-2019/""><strong>A' Design Award&nbsp;</strong><strong>2017 -&nbsp;</strong><strong>Gold Prize</strong></a></p><p>-Honorable Mention - Tomorrow Workplace Competition by METROPOLIS</p><p><b>Publication</b></p><p>-<a href=""http://www.academia.edu/35684876/Amoeba_Wall"">H Deng, H Ho, L Alonso, X Li, J Angulo, K Larson Amoeba Wall - PASAJES - archquitectura NO.143, pp8-9</a></p>",2017-11-30,,2019-02-11 15:09:42.775,True,2016-04-22,Amoetecture,PUBLIC,http://rnd.studio/project/amoeba-wall,False,Civic Media,False
ant-based-modeling,oi7,False,"<p>Ant-Based Modeling explores the possibility of&nbsp;<span style=""font-size: 18px; font-weight: 400;"">implementing agent-based modeling with living ants&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">and external stimuli such as electromagnetic radiations,&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">magnetic fields, and electric fields. In an experiment&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">with fire ants, we discovered that ultraviolet and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">infrared lights can affect their behavior in the form of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">attraction and dispersion towards the light,&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">respectively. The video highlights some of the LEGOmade&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">landscapes we use in our explorations and how&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">the behavior of ants can be influenced by ultraviolet&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">light to achieve certain purposes such as exploring a&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">new area or dragging a ping pong ball to a specific&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">location. These experiments have allowed us to learn&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">more about ants and have inspired us to explore novel&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">forms of human-ant interaction.</span></p>",2017-04-22,--Choose Location,2018-04-26 15:20:39.904,True,2016-04-22,Ant-Based Modeling,PUBLIC,http://oi7.me,False,Civic Media,False
amoeba-wall,oi7,False,"<p>Amoeba Wall: A context aware wall system:&nbsp;Amoetecture is a set of amoeba-like dynamic spatial elements, including transformable floors, ceilings, tables, chairs, and workstations. We focus on designing architecture robotics and platforms that enable a hyper-efficient and dynamically reconfigurable coworking space that accommodates a wide range of activities in a small area.</p><p><strong>&nbsp;Award</strong></p><p><strong>-&nbsp;</strong><a href=""https://www.designboom.com/design/2019-a-design-award-and-competition-call-for-entries-02-07-2019/""><strong>A' Design Award&nbsp;</strong><strong>2017 -&nbsp;</strong><strong>Gold Prize</strong></a></p><p><strong>-&nbsp;</strong>Honorable Mention - Tomorrow Workplace Competition by METROPOLIS</p><p><b>Publication</b></p><p><b>-</b><a href=""http://www.academia.edu/35684876/Amoeba_Wall"">H Deng, H Ho, L Alonso, X Li, J Angulo, K Larson Amoeba Wall - PASAJES - archquitectura NO.143, pp8-9</a></p>",2016-12-31,,2019-02-11 03:26:16.506,True,2016-01-04,Amoeba Wall: A context-aware wall system,PUBLIC,http://rnd.studio/project/amoeba-wall,False,Civic Media,False
openscope,oi7,False,"<p>OpenScope is an open source project that combines three components for anyone to explore the micro world anytime, anywhere. The 3D-printable open hardware turns your smartphone into a 200x microscope, the image processing application helps you recognize specific objects, and the online community allows you to share and contribute your findings from the microscope. OpenScope is expanding microscopy technologies beyond research laboratories and transforming the way we interact with the micro world.</p>",2017-01-01,--Choose Location,2018-05-04 10:52:12.271,True,2016-01-01,OpenScope,PUBLIC,http://oi7.me,False,Civic Media,False
smart-communal-spaces,oi7,False,"<p><b>Smart Communal Spaces </b>is a project from MIT Media Lab Dubai Workshop 2016, which was deployed and filmed at the Dubai Museum of the Future. It explores how Mixed Reality could enhance team communication in shared office spaces by looking into the interplay of Slack and HoloLens. While existing co-working spaces provide the benefits of openness and flexibility, they sacrifice our privacy and personal experience. However, Mixed Reality enables us to project holograms into our physical reality, visually and auditorily reorganizing how bits and atoms exist around us. In the demo video, we try to imagine and tell a story of how human-computer interactions might look like in an office when team communication tools are operated in a spatial context without any streaming device. We broke down the most important elements in the office space by researching the core functions of a team communication software and several holographic applications.
                    
                </p><p><b>Design Lead:</b> Chrisoula Kapelonis,&nbsp;</p><p><b>Technology Lead:</b> Poseidon Hai-Chi Ho&nbsp;</p><p><b>Students:&nbsp;</b>Rajeev Mylapalli, Yazan Fanous, Lamees Alhashimi, Moza Al Naimi, Esra'a Alsanie, Asalah Aranki</p><p><b>Special Thanks: </b>Joichi Ito, Noah Raford, Nick DuPey, Ashley Shaffer<br>MIT Media Lab, IDEO, Wamda, Dubai Museum of the Future</p>",2017-08-29,,2018-05-04 10:51:42.799,True,2016-08-29,Smart Communal Spaces,PUBLIC,,False,Civic Media,False
mission-wildlife,oi7,False,"<p>Mission Wildlife is a research collaboration between San&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Diego Zoo Global and the MIT Center for Civic Media to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">explore the potential for interactive technologies in&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">conservation education. In particular, we used&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">augmented reality (AR) to focus visitors’ attention&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">towards survival threats to endangered species. The&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">project was deployed during the 100th anniversary&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">year (2016) of the San Diego Zoo. Visitors competed&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">against each other to trigger 3D animations from&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">animal signage in the zoo and shared results on social&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">media to spread awareness about conservation issues.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">This case study demonstrates how AR can be tied to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">efforts to expand awareness of social issues.</span></p>",2017-06-20,,2018-05-04 10:49:30.350,True,2016-06-20,Mission Wildlife,PUBLIC,http://oi7.me,False,Civic Media,False
share-understanding-and-manipulating-attention-using-social-robots,ndepalma,False,"<p>SHARE is a robotic cognitive architecture focused on manipulating and understanding the phenomenon of shared attention during interaction. SHARE incorporates new findings and research in the understanding of nonverbal referential gesture, visual attention system research, and interaction science. SHARE's research incorporates new measurement devices, advanced artificial neural circuits, and a robot that makes its own decisions.</p>",2017-05-31,--Choose Location,2017-10-15 17:57:01.084,True,2014-09-01,SHARE: Understanding and Manipulating Attention Using Social Robots,PUBLIC,,False,Personal Robots,False
spoken-opinion-summarization,shayneob,False,"<p>Talk radio exerts significant influence on the political and social dynamics of the United States, but labor-intensive data collection and curation processes have prevented previous works from studying its content at scale. Over the past year, the Laboratory for Social Machines and Cortico have created a talk radio ingest system to record and automatically transcribe audio from more than 160 stations around the country. Using these transcripts, we propose novel compression-based methods for unsupervised summarization of spoken opinion in conversational dialogue. By relying on an unsupervised framework that obviates the need for labeled data, the summarization task becomes largely agnostic to human input beyond necessary decisions regarding model architecture, input data, and output length. As a result, trained models are able to produce a more accurate depiction of opinion. Using the outputs of my proposed methods, we conduct a case study to examine the variability of public opinion across America. In the interests of reproducibility and further research, we open-source all code and data used.&nbsp;</p>",2019-06-07,,2019-04-17 00:15:52.766,True,2018-08-01,Spoken Opinion Summarization,PUBLIC,https://github.com/shayneobrien,False,Social Machines,False
ant-based-modeling,m_kayser,False,"<p>Ant-Based Modeling explores the possibility of&nbsp;<span style=""font-size: 18px; font-weight: 400;"">implementing agent-based modeling with living ants&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">and external stimuli such as electromagnetic radiations,&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">magnetic fields, and electric fields. In an experiment&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">with fire ants, we discovered that ultraviolet and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">infrared lights can affect their behavior in the form of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">attraction and dispersion towards the light,&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">respectively. The video highlights some of the LEGOmade&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">landscapes we use in our explorations and how&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">the behavior of ants can be influenced by ultraviolet&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">light to achieve certain purposes such as exploring a&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">new area or dragging a ping pong ball to a specific&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">location. These experiments have allowed us to learn&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">more about ants and have inspired us to explore novel&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">forms of human-ant interaction.</span></p>",2017-04-22,--Choose Location,2018-04-26 15:20:39.904,True,2016-04-22,Ant-Based Modeling,PUBLIC,http://oi7.me,False,Mediated Matter,False
g3p,m_kayser,False,"<p>Ancient yet modern, enclosing yet invisible, glass was first created in  Mesopotamia and Ancient Egypt 4,500 years ago. Precise recipes for its  production - the chemistry and techniques - often remain closely guarded  secrets. Glass can be molded, formed, blown, plated or sintered; its  formal qualities are closely tied to techniques used for its  formation.&nbsp;From the discovery of core-forming process for bead-making in  ancient Egypt, through the invention of the metal blow pipe during  Roman times, to the modern industrial Pilkington process for making  large-scale flat glass; each new breakthrough in glass technology  occurred as a result of prolonged experimentation and ingenuity, and has  given rise to a new universe of possibilities for uses of the material.  <br></p>",,--Choose Location,2017-10-13 19:43:29.330,True,2014-01-01,Glass I,PUBLIC,,True,Mediated Matter,False
fiberbots,m_kayser,False,"<p>FIBERBOTS is a digital fabrication platform fusing cooperative robotic manufacturing with abilities to generate highly sophisticated material architectures. The platform can enable design and digital fabrication of large-scale structures with high spatial resolution leveraging mobile fabrication nodes, or robotic ""agents"" designed to <i>tune</i> the material make-up of the structure being constructed on the fly as informed by their environment.<br></p><p>Some of nature’s most successful organisms collaborate in a swarm fashion. Nature’s builders leverage hierarchical structures in order to control and optimize multiple material properties. Spiders, for instance, spin protein fibers to weave silk webs with tunable local and global material properties, adjusting their material composition and fiber placement to create strong yet flexible structures optimized to capture prey. Other organisms, such as bees, ants and termites cooperate to rapidly build structures much larger than themselves. </p>",,,2019-02-13 16:36:22.730,True,2016-01-01,"FIBERBOTS: Design of a multi-agent, fiber composite digital fabrication system",PUBLIC,,True,Mediated Matter,False
synthetic-apiary,m_kayser,False,"<p>The Synthetic Apiary proposes a new kind of environment, bridging urban and organismic scales by exploring one of the most important organisms for both the human species and our planet: bees. We explore the cohabitation of humans and other species through the creation of a controlled atmosphere and associated behavioral paradigms. The project facilitates Mediated Matter's ongoing research into biologically augmented digital fabrication with eusocial insect communities in architectural, and possibly urban, scales. Many animal communities in nature present collective behaviors known as ""swarming,"" prioritizing group survival over individuals, and constantly working to achieve a common goal. Often, swarms of organisms are skilled builders; for example, ants can create extremely complex networks by tunneling, and wasps can generate intricate paper nests with materials sourced from local areas.</p>",,--Choose Location,2017-10-13 23:33:55.831,True,2016-01-01,Synthetic Apiary,PUBLIC,,True,Mediated Matter,False
ant-based-modeling,javierhr,False,"<p>Ant-Based Modeling explores the possibility of&nbsp;<span style=""font-size: 18px; font-weight: 400;"">implementing agent-based modeling with living ants&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">and external stimuli such as electromagnetic radiations,&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">magnetic fields, and electric fields. In an experiment&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">with fire ants, we discovered that ultraviolet and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">infrared lights can affect their behavior in the form of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">attraction and dispersion towards the light,&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">respectively. The video highlights some of the LEGOmade&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">landscapes we use in our explorations and how&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">the behavior of ants can be influenced by ultraviolet&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">light to achieve certain purposes such as exploring a&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">new area or dragging a ping pong ball to a specific&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">location. These experiments have allowed us to learn&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">more about ants and have inspired us to explore novel&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">forms of human-ant interaction.</span></p>",2017-04-22,--Choose Location,2018-04-26 15:20:39.904,True,2016-04-22,Ant-Based Modeling,PUBLIC,http://oi7.me,False,Affective Computing,False
bioessence,javierhr,False,<h2><b>A wearable olfactory display that monitors cardio-respiratory information to support mental wellbeing.</b></h2><p>BioEssence is a novel wearable olfactory&nbsp;display that provides just-in-time release of scents based on&nbsp;the physiological state of the wearer. The device can release up&nbsp;to three scents and passively captures subtle chest vibrations&nbsp;associated with the beating of the heart and respiration through&nbsp;clothes.&nbsp;<br></p>,,,2019-05-11 00:16:22.469,True,2017-12-01,BioEssence,PUBLIC,http://www.judithamores.com/essence,True,Affective Computing,False
large-scale-pulse-analysis,javierhr,False,"<p>This study aims to bring objective measurement to the multiple ""pulse"" and ""pulse-like"" measures made by practitioners of traditional Chinese medicine (TCM). The measurements are traditionally made by manually palpitating the patient's inner wrist in multiple places, and relating the sensed responses to various medical conditions. Our project brings several new kinds of objective measurement to this practice, compares their efficacy, and examines the connection of the measured data to various other measures of health and stress. Our approach includes the possibility of building a smartwatch application that can analyze stress and health information from the point of view of TCM.</p>",,--Choose Location,2019-04-19 17:36:58.241,True,2015-09-01,Traditional Chinese medicine-inspired pulse analysis,PUBLIC,,True,Affective Computing,False
automated-tongue-analysis,javierhr,False,"<p>A common practice in Traditional Chinese Medicine (TCM) is visual examination of the patient's tongue. This study will examine ways to make this process more objective and to test its efficacy for understanding stress- and health-related changes in people over time. We start by developing an app that makes it comfortable and easy for people to collect tongue data in daily life together with other stress- and health-related information. We will obtain assessment from expert practitioners of TCM, and also use pattern analysis and machine learning to attempt to create state-of-the-art algorithms able to help provide better insights for health and prevention of sickness.</p>",,--Choose Location,2017-09-21 15:02:57.578,True,2015-09-01,Automated Tongue Analysis,PUBLIC,,True,Affective Computing,False
skinbot-a-wearable-skin-climbing-robot,javierhr,False,"<p>We introduce SkinBot: a lightweight robot that moves over the skin's surface with a two-legged suction-based locomotion mechanism and captures a wide range of body parameters with an exchangeable multipurpose sensing module. We believe that robots that live on our skin, such as SkinBot, will enable a more systematic study of the human body and offer great opportunities to advance our knowledge in many areas such as telemedicine, human-computer interfaces, body care, and fashion.</p>",,,2019-04-19 14:35:05.713,True,2016-11-01,"SkinBot: A wearable, skin-climbing robot",PUBLIC,http://www.artemdementyev.com,True,Affective Computing,False
emotional-navigation-system,javierhr,False,"<p>Before automobiles were invented and widely adopted, animals like horses were the most common mode of transportation. While this change brought significant improvements in terms of reliability and efficiency, it also removed a core component: the emotional relationship that existed between the person and the animal.</p><p>While largely ignored, the emotional states of drivers are quite important, as they influence not only driving behavior but also the safety of all road users. For instance, driving can be quite an emotionally stressful experience and, while certain amounts of stress help the driver to remain alert and attentive, too much or too little can negatively impact driving performance and safety. Furthermore, stress in large doses has been linked to a large array of adverse health conditions such as depression and various forms of cardiovascular disease.</p><p>The&nbsp;Emotion Navigation&nbsp;special interest group is led by Dr. Javier Hernandez with the goal of stimulating research efforts at the intersection of&nbsp;Automotive and&nbsp;Affective Computing.</p>",,,2019-04-19 19:34:00.344,True,2018-04-02,Emotion Navigation,PUBLIC,http://enavigation.media.mit.edu,True,Affective Computing,False
bitcoin-settled-dollar-futures,adragos,False,"<p>Discreet Log Contracts (<a href=""https://adiabat.github.io/dlc.pdf"">DLC</a>) are a new type of smart contract which limit the information gained&nbsp;and influence of oracles, and can run on the very limited scripting system present in Bitcoin, without&nbsp;the need for more complex languages such as in Ethereum.&nbsp;</p><p>DLC works by precomputing a wide range of potential outcomes for a given contract, and when the oracle announces an event, the event-dependent correct outcome can be published. There are a number of applications where this model applies, and the one that we’ll be starting with first is <b>Bitcoin settled dollar futures</b>. This use case introduces a useful tool to mitigate the volatility of Bitcoin.</p>",2018-12-31,,2018-05-01 20:05:09.760,True,2018-01-01,Building smart contracts with Bitcoin,PUBLIC,,False,Initiatives,False
bitcoin-vending-machine,adragos,False,"<p>Lit, the lightweight Lightning Network software developed at the MIT Media Lab, works with multiple Bitcoin-like blockchains. The DCI team has created this demo to experience how Lightning can be used at point-of-sale terminals as well as allow users to do device-to-device transactions using multiple coins. We show how an electronic mobile wallet works with three currencies (Bitcoin, Litecoin, and USD).</p>",,,2019-02-14 19:49:02.951,True,2018-07-02,Bitcoin vending machine,PUBLIC,https://dci.mit.edu/lightning-network/,True,Initiatives,False
bitcoin-settled-dollar-futures,tdryja,False,"<p>Discreet Log Contracts (<a href=""https://adiabat.github.io/dlc.pdf"">DLC</a>) are a new type of smart contract which limit the information gained&nbsp;and influence of oracles, and can run on the very limited scripting system present in Bitcoin, without&nbsp;the need for more complex languages such as in Ethereum.&nbsp;</p><p>DLC works by precomputing a wide range of potential outcomes for a given contract, and when the oracle announces an event, the event-dependent correct outcome can be published. There are a number of applications where this model applies, and the one that we’ll be starting with first is <b>Bitcoin settled dollar futures</b>. This use case introduces a useful tool to mitigate the volatility of Bitcoin.</p>",2018-12-31,,2018-05-01 20:05:09.760,True,2018-01-01,Building smart contracts with Bitcoin,PUBLIC,,False,Initiatives,False
bitcoin-vending-machine,tdryja,False,"<p>Lit, the lightweight Lightning Network software developed at the MIT Media Lab, works with multiple Bitcoin-like blockchains. The DCI team has created this demo to experience how Lightning can be used at point-of-sale terminals as well as allow users to do device-to-device transactions using multiple coins. We show how an electronic mobile wallet works with three currencies (Bitcoin, Litecoin, and USD).</p>",,,2019-02-14 19:49:02.951,True,2018-07-02,Bitcoin vending machine,PUBLIC,https://dci.mit.edu/lightning-network/,True,Initiatives,False
utreexo,tdryja,False,"<h1>Utreexo: a dynamic accumulator for bitcoin state&nbsp;</h1><p>A description of research by Thaddeus Dryja</p><p>One of the earliest-seen and most persistent problems with Bitcoin has been scalability. Bitcoin takes the idea of ""be your own bank"" quite literally, with every computer on the bitcoin network storing every account of every user who owns money in the system. In Bitcoin, this is stored as a collection of ""Unspent transaction outputs,"" or ""utxos"", which are somewhat unintuitive, but provide privacy and efficiency benefits over the alternative ""account"" based model used in traditional finance.</p><p>It's important to distinguish between the transaction history and the current state of the system. The transaction history in Bitcoin is currently 200GB, and contains every transaction since Bitcoin was launched in early 2009. The size of this history can of course only increase with time. The current system state, however, is much smaller, at under 4GB, and deals with only who owns what right now. This state size has generally increased over time, but has in fact decreased a bit this year.</p><p>The history, despite its much larger size, is not in fact the scalability concern, as it is not used in any time-critical fashion; one can discard the history after processing with no loss of security. The increasing state size, however, is a concern—one which utreexo solves.</p><p>Utreexo is a novel hash based dynamic accumulator, which allows the millions of unspent outputs to be represented in under a kilobyte—small enough to be written on a sheet of paper. There is no trusted setup or loss of security; instead, the burden of keeping track of funds is shifted to the owner of those funds.  </p><p>Current transactions specify inputs and outputs, and verifying an input requires you to know the whole state of the system. With Utreexo, the holder of funds maintains a proof that the funds exist, and provides that proof at spending time to the other nodes. These proofs are compact (under 1KB) but do represent the main downside in the utreexo model; they present an additional data transmission overhead, which allows much smaller state.</p><p>Utreexo pushes the costs of maintaining the network to the right place: an exchange creating millions of transactions may need to maintain millions of proofs, while a personal account with only a few unspent outputs will only need to maintain a few kilobytes of proof data. Utreexo also provides a long-term scalability solution, as the accumulator size grows very slowly with increasing size of the underlying set (the accumulator size is logarithmic with the set size).</p>",,,2019-04-09 18:22:46.627,True,2018-11-28,Utreexo,PUBLIC,,True,Initiatives,False
bitcoin-settled-dollar-futures,joe,False,"<p>Discreet Log Contracts (<a href=""https://adiabat.github.io/dlc.pdf"">DLC</a>) are a new type of smart contract which limit the information gained&nbsp;and influence of oracles, and can run on the very limited scripting system present in Bitcoin, without&nbsp;the need for more complex languages such as in Ethereum.&nbsp;</p><p>DLC works by precomputing a wide range of potential outcomes for a given contract, and when the oracle announces an event, the event-dependent correct outcome can be published. There are a number of applications where this model applies, and the one that we’ll be starting with first is <b>Bitcoin settled dollar futures</b>. This use case introduces a useful tool to mitigate the volatility of Bitcoin.</p>",2018-12-31,,2018-05-01 20:05:09.760,True,2018-01-01,Building smart contracts with Bitcoin,PUBLIC,,False,Opera of the Future,False
bitcoin-vending-machine,joe,False,"<p>Lit, the lightweight Lightning Network software developed at the MIT Media Lab, works with multiple Bitcoin-like blockchains. The DCI team has created this demo to experience how Lightning can be used at point-of-sale terminals as well as allow users to do device-to-device transactions using multiple coins. We show how an electronic mobile wallet works with three currencies (Bitcoin, Litecoin, and USD).</p>",,,2019-02-14 19:49:02.951,True,2018-07-02,Bitcoin vending machine,PUBLIC,https://dci.mit.edu/lightning-network/,True,Opera of the Future,False
smiletracker,jaquesn,False,"<p>SmileTracker is a system designed to capture naturally occurring instances of positive emotion during the course of normal interaction with a computer. A facial expression recognition algorithm is applied to images captured with the user's webcam. When the user smiles, both a photo and a screenshot are recorded and saved to the user's profile for later review. Based on positive psychology research, we hypothesize that the act of reviewing content that led to smiles will improve positive affect, and consequently, overall wellbeing.</p>",2015-09-01,--Choose Location,2016-12-05 00:17:02.371,True,2014-09-01,SmileTracker,PUBLIC,,False,Affective Computing,False
causal-influence-intrinsic-social-motivation-for-multi-agent-reinforcement-learning,jaquesn,False,"<p>Teaching multiple AI agents to coordinate their behavior represents a challenging task, that can be difficult to achieve without training all agents with a centralised controller, or allowing agents to view each others' reward functions. We present a new approach to multi-agent reinforcement learning (MARL), in which agents are given an incentive for being able to causally influence each others' actions. Causal influence is assessed using counterfactual reasoning. We show that this social influence reward gives rise to more coordinated behavior, better collective outcomes, and even emergent communication. In fact, the influence reward can be learn to train agents to use an explicit communication protocol in a meaningful way, when they cannot learn to do this under normal circumstances. Finally, we show that this reward can be computed by training each agent to model the actions of other agents. An agent can then ""imagine"" counterfactual actions it could have taken, and predict how this would have affected other agents behavior, thus computing its own influence reward. This mechanism allows each agent to be trained independently, representing a significant improvement over prior MARL work.&nbsp;</p>",2018-10-31,,2019-04-18 15:25:07.087,True,2018-05-14,Causal Influence Intrinsic Social Motivation for Multi-Agent Reinforcement Learning,PUBLIC,,False,Affective Computing,False
predicting-bonding,jaquesn,False,"<p>We show that using thin slices (&lt; 1 minute) of facial expression and body language data, we can train a deep neural network to predict whether two people in a conversation will bond with each other. Bonding is measured using the Bonding subscale of the Working Alliance Inventory. We show that participants who experience bonding perceive their conversational partner as interesting, charming, and friendly, and do not perceive them as distant or annoying. </p><p>The data are collected from a user study of naturalistic conversations, in which participants were asked to interact for 20 minutes, and were recorded using cameras, microphones, and Microsoft Kinects. To ensure participants did not become self-conscious of their non-verbal cues, they were told the purpose of the study was to train machine learning algorithms to read lips. </p><p>We show that not only can we accurately predict bonding from participants' personality, disposition, and traits, but that we can predict whether the participant will experience bonding up to 20 minutes later, using only one-minute thin slices of facial expression and body language data. This ability could be extremely useful to an intelligent virtual agent, because if it could detect at one-minute intervals whether it was bonding with its user, it could make course corrections to promote enjoyment and foster bonding. We provide an analysis of the facial expression and body language cues associated with higher bonding, and show how this information could be used by an agent to synthesize the appropriate non-verbal cues during conversation.</p>",,,2019-02-08 16:15:38.454,True,2015-10-01,Predicting Bonding in Conversations,PUBLIC,,True,Affective Computing,False
eda-explorer,jaquesn,False,"<p>Electrodermal Activity (EDA) is a physiological indicator of stress and strong emotion. While an increasing number of wearable devices can collect EDA, analyzing the data to obtain reliable estimates of stress and emotion remains a difficult problem. We have built a graphical tool that allows anyone to upload their EDA data and analyze it. Using a highly accurate machine learning algorithm, we can automatically detect noise within the data. We can also detect skin conductance responses, which are spikes in the signal indicating a ""fight or flight"" response. Users can visualize these results and download files containing features calculated on the data to be used in their own analysis. Those interested in machine learning can also view and label their data to train a machine learning classifier. We are currently adding active learning, so the site can intelligently select the fewest possible samples for the user to label. </p>",,--Choose Location,2016-12-05 00:17:11.264,True,2015-01-01,EDA Explorer,PUBLIC,http://eda-explorer.media.mit.edu/,True,Affective Computing,False
learning-via-social-awareness-improving-sketch-representations-with-facial-feedback,jaquesn,False,"<p>In the quest towards general artificial intelligence (AI), researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards. This paper argues that such research has overlooked an important and useful intrinsic motivator: social interaction. We posit that making an AI agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations, and could potentially impact AI safety. We collect social feedback in the form of facial expression reactions to samples from Sketch RNN, an LSTM-based variational autoencoder (VAE) designed to produce sketch drawings. We use a Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small group of viewers, and then show in an independent evaluation with 76 users that this model produced sketches that lead to significantly more positive facial expressions. Thus, we establish that implicit social feedback can improve the output of a deep learning model.</p>",,,2019-04-19 17:14:55.804,True,2018-03-20,Learning via Social Awareness: Improving sketch representations with facial feedback,PUBLIC,,True,Affective Computing,False
predicting-students-wellbeing-from-physiology-phone-mobility-and-behavioral-data,jaquesn,False,"<p>The goal of this project is to apply machine learning methods to model the wellbeing of MIT undergraduate students. Extensive data is obtained from the SNAPSHOT study, which monitors participating students on a 24/7 basis, collecting data on their location, sleep schedule, phone and SMS communications, academics, social networks, and even physiological markers like skin conductance, skin temperature, and acceleration.&nbsp;</p><p>We extract features from this data and apply a variety of machine learning algorithms, including Gaussian mixture models and Multi-task Multi-Kernel Learning; we are currently working to apply Bayesian hierarchical multi-task learning and Deep Learning as well.</p><p>Interesting findings include: when participants visit novel locations they tend to be happier; when they use their phones or stay indoors for long periods they tend to be unhappy; and when several dimensions of wellbeing (including stress, happiness, health, and energy) are learned together, classification accuracy improves. The biggest classification accuracy improvements come when we use multi-tasking algorithms to leverage group data while customizing a model for each participant.</p>",,--Choose Location,2019-04-19 17:23:56.302,True,2015-01-01,"Predicting students' wellbeing from physiology, phone, mobility, and behavioral data",PUBLIC,,True,Affective Computing,False
the-challenge,jaquesn,False,"<p>Mental wellbeing is intimately tied to both social support and physical activity. The Challenge is a tool aimed at promoting social connections and decreasing sedentary activity in a workplace environment. Our system asks participants to sign up for short physical challenges and pairs them with a partner to perform the activity. Social obligation and social consensus are leveraged to promote participation. Two experiments were conducted in which participants' overall activity levels were monitored with a fitness tracker. In the first study, we show that the system can improve users' physical activity, decrease sedentary time, and promote social connection. As part of the second study, we provide a detailed social network analysis of the participants, demonstrating that users' physical activity and participation depends strongly on their social community.</p>",,--Choose Location,2016-12-05 00:16:31.390,True,2015-01-01,The Challenge,PUBLIC,,True,Affective Computing,False
wavelet-based-motion-artifact-removal-for-electrodermal-activity,jaquesn,False,"<p>Electrodermal activity (EDA) recording is a powerful, widely used tool for monitoring psychological or physiological arousal. However, analysis of EDA is hampered by its sensitivity to motion artifacts. We propose a method for removing motion artifacts from EDA, measured as skin conductance (SC), using a stationary wavelet transform (SWT). We modeled the wavelet coefficients as a Gaussian mixture distribution corresponding to the underlying skin conductance level (SCL) and skin conductance responses (SCRs). The goodness-of-fit of the model was validated on ambulatory SC data. We evaluated the proposed method in comparison with three previous approaches. Our method achieved a greater reduction of artifacts while retaining motion-artifact-free data.</p>",,--Choose Location,2019-04-19 17:39:31.969,True,2015-01-01,Wavelet-based motion artifact removal for electrodermal activity,PUBLIC,,True,Affective Computing,False
physio-freefall,jaquesn,False,"<p>This project seeks to examine the effects of altered gravity on an individual’s physiology during parabolic flight. Specifically, we will collect flight participants’ heart rate, heart rate variability, breathing rate, skin temperature, and skin conductance measurements using wearable, wireless sensors in order to determine the response of these biosignals to zero/hyper/microgravity and feelings of nausea. </p><p>The results of this research will have both significant scientific and civilian value. To our knowledge, this experiment will be the first to investigate the new Multiple Arousal Theory in the context of motion sickness, as well as altered gravity. This theory was developed in the Affective Computing group at the MIT Media Lab and examines asymmetry in skin conductance signals from right and left wrists as differing metrics of emotional arousal and intensity. The parabolic flight configuration provides an inimitable circumstance to systematically analyze the evolution of these signals over the course of the repeated parabolic flight path. For example, we expect to see globally heightened stress and emotional arousal on the first pass, with maximal skin conductance peaks from both wrists just before the first moment of weightlessness. We expect these peaks to monotonically decrease over time with each pass, but to remain more elevated (relative to an individual’s baseline) for participants experiencing more self-reported nausea during flight. For individuals not experiencing extreme nausea, we expect to see a much higher skin conductance signal from their right wrists compared to their left (for right-handed participants) during the first few passes, with this difference decreasing steadily as the participant habituates to the flight pattern and sensations. </p><p>Note that NASA and other researchers—including the Boston-local scientists at the Ashton Graybiel Spatial Orientation Lab at Brandeis University—have investigated spatial orientation and motion sickness, but they are just beginning to add the use of physiological sensors to their work. Not only does this demonstrate that the proposed experiment is at the forefront of scientific inquiry, but it also facilitates potential collaboration with world-renowned experts in the Boston area!</p><p>In addition to sensor data, we intend to collect pre- and post-flight surveys recording participant reactions to different levels of gravity, including points at which they experienced nausea or discomfort. Pre-flight surveys will include nausea sensitivity metrics, designed to determine how likely a person is to feel nausea (i.e., separating those who feel carsick on a drive through town versus those who approach rollercoasters without hesitation). It will also ask about each participant’s feelings of anxiety, nausea, and excitement in anticipation of flying. Note that while these feelings may be experienced simultaneously, each one has a different effect on one’s physiology. </p><p>After the flight, we will ask participants to rank which sections of the flight (e.g., beginning, middle, end) prompted the greatest sensations of anxiety, nausea, and excitement and to what degree. We will also annotate the flight video recordings to denote periods of high anxiety, nausea, or excitement.</p><p>Then, we will use the survey, annotation, and sensor information to build a model that predicts when an individual might experience distress in altered gravity environments. This aspect of the study will leverage our research group’s unique expertise building machine learning algorithms for physiological data, but the results could have widespread impact. For example, such a system could be deployed to space travelers to help them monitor their physiology and anticipate or prevent feelings of discomfort during flight. As access to space travel becomes more pervasive, it is critical to understand the physiological effects of altered gravity on a population that does not solely include astronauts or specially trained individuals. Our models, along with the use of low-cost, commercially available sensors, would enable “space hacking” by tourists and other non-technical personnel, allowing them to measure and track their biosignals to achieve optimal wellness during space travel.&nbsp;</p>",,,2018-10-22 19:51:21.995,True,2017-05-11,Physio FreeFall,PUBLIC,,True,Affective Computing,False
sequence-tutor,jaquesn,False,,,,2017-03-31 16:27:47.445,False,2016-09-01,Sequence Tutor,PUBLIC,,True,Affective Computing,False
sequence-tutor,jaquesn,False,"<p>We propose a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications: 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.</p>",,,2017-07-11 15:22:59.660,True,2016-09-01,Sequence Tutor,PUBLIC,,True,Affective Computing,False
improving-rnn-sequence-generation-with-rl,jaquesn,False,"<p>This project investigates a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN) using deep reinforcement learning (RL). Our method, which we call Sequence Tutor, allows models to improve sequence quality with RL, while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation for drug discovery. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.</p>",,,2017-10-16 19:27:53.053,True,2016-06-01,Improving RNN Sequence Generation with RL,PUBLIC,,True,Affective Computing,False
elsa,jaquesn,False,"<h2><b><i>What is ELSA?</i></b></h2><p>ELSA is an AI-powered chatbot that acts as an empathetic companion, encouraging users to talk about their day through a form of interactive journaling.</p><p>You can try some of the current ELSA bots in this online&nbsp;<a href=""http://elsaneural.net"">demo</a>.&nbsp;</p><h2><b><i>How does ELSA work?</i></b></h2><p>Our project goal is to build a more empathetic neural network conversational AI by incorporating a deeper understanding of both the affective content of the conversation and the topic.&nbsp; More specifically, we build hierarchical recurrent neural network models that can converse like people &nbsp;and use transfer learning of topic and emotional tone recognition models to improve our final model.</p><h2><b><i>What are the applications of ELSA?</i></b></h2><p>Beyond the development of chatbots that act as an empathetic companion, we have a more ambitious and longer term goal: deploy the empathetic companion bots to support mental health.&nbsp; In particular, &nbsp;we aim to make ELSA useful for:</p><ul><li>Eliciting journaling</li><li>Suggesting behavioura interventions</li><li>Using Cognition Behavioral Therapy</li><li>Detecting individuals at risk of depression or suicide</li></ul><h2><b><i>Work in progress</i></b></h2><p>ELSA is a recently started project in the Affective Computing group. You can see an example of ELSA bot conversations below. You can also try our online <a href=""http://elsaneural.net"">demo</a>. &nbsp;&nbsp;</p>",,,2019-04-22 19:06:31.702,True,2019-03-03,"ELSA: Empathy learning, socially-aware agents",PUBLIC,,True,Affective Computing,False
smiletracker,cvx,False,"<p>SmileTracker is a system designed to capture naturally occurring instances of positive emotion during the course of normal interaction with a computer. A facial expression recognition algorithm is applied to images captured with the user's webcam. When the user smiles, both a photo and a screenshot are recorded and saved to the user's profile for later review. Based on positive psychology research, we hypothesize that the act of reviewing content that led to smiles will improve positive affect, and consequently, overall wellbeing.</p>",2015-09-01,--Choose Location,2016-12-05 00:17:02.371,True,2014-09-01,SmileTracker,PUBLIC,,False,Affective Computing,False
improving-sleep-wake-schedule-using-sleep-behavior-visualization-and-a-bedtime-alarm,cvx,False,"<p><a href=""http://dl.acm.org/citation.cfm?id=2897442.2897469"">Humans need sleep</a>, along with food, water, and oxygen, to survive. With about one-third of our lives spent sleeping, there has been increased attention and interest in understanding sleep and the overall state of our ""sleep health."" The rapid adoption of smartphones, along with a growing number of sleep tracking applications for these devices, presents an opportunity to use phones to encourage better sleep hygiene. Procrastinating going to bed and being unable to stick to a consistent bedtime can lead to inadequate sleep time, which in turn affects quality of life and overall wellbeing. To help address this problem, we developed two applications, Lights Out and Sleep Wallpaper, which provide a sensor-based bedtime alarm and a connected peripheral display on the wallpaper of the user's mobile phone to promote awareness with sleep data visualization.</p>",,--Choose Location,2016-12-05 17:35:08.047,True,2015-09-01,Improving Sleep-Wake Schedule Using Sleep Behavior Visualization and a Bedtime Alarm,PUBLIC,,True,Affective Computing,False
eda-explorer,cvx,False,"<p>Electrodermal Activity (EDA) is a physiological indicator of stress and strong emotion. While an increasing number of wearable devices can collect EDA, analyzing the data to obtain reliable estimates of stress and emotion remains a difficult problem. We have built a graphical tool that allows anyone to upload their EDA data and analyze it. Using a highly accurate machine learning algorithm, we can automatically detect noise within the data. We can also detect skin conductance responses, which are spikes in the signal indicating a ""fight or flight"" response. Users can visualize these results and download files containing features calculated on the data to be used in their own analysis. Those interested in machine learning can also view and label their data to train a machine learning classifier. We are currently adding active learning, so the site can intelligently select the fewest possible samples for the user to label. </p>",,--Choose Location,2016-12-05 00:17:11.264,True,2015-01-01,EDA Explorer,PUBLIC,http://eda-explorer.media.mit.edu/,True,Affective Computing,False
deep-gif,cvx,False,"<p>
                    Animated GIFs are widely used on the Internet to express emotions, but automatic analysis of their content is largely unexplored. To help with the search and recommendation of GIFs, we aim to predict &nbsp;how their emotions will be perceived by humans based on their content. Since previous solutions to this problem only utilize image-based features and lose all the motion information, we propose to use 3D convolutional neural networks (CNNs) to extract spatiotemporal features from GIFs. We evaluate our methodology on a crowdsourcing platform called GIFGIF with more than 6,000 animated GIFs, and achieve better accuracy than any previous approach in predicting crowdsourced intensity scores of 17 emotions. We have also found that our trained model can be used to distinguish and cluster emotions in terms of valence and risk perception.</p>",,,2019-04-19 17:22:36.655,True,2016-10-18,Predicting perceived emotions in animated GIFs with 3D convolutional neural networks,PUBLIC,,True,Affective Computing,False
large-scale-pulse-analysis,cvx,False,"<p>This study aims to bring objective measurement to the multiple ""pulse"" and ""pulse-like"" measures made by practitioners of traditional Chinese medicine (TCM). The measurements are traditionally made by manually palpitating the patient's inner wrist in multiple places, and relating the sensed responses to various medical conditions. Our project brings several new kinds of objective measurement to this practice, compares their efficacy, and examines the connection of the measured data to various other measures of health and stress. Our approach includes the possibility of building a smartwatch application that can analyze stress and health information from the point of view of TCM.</p>",,--Choose Location,2019-04-19 17:36:58.241,True,2015-09-01,Traditional Chinese medicine-inspired pulse analysis,PUBLIC,,True,Affective Computing,False
wavelet-based-motion-artifact-removal-for-electrodermal-activity,cvx,False,"<p>Electrodermal activity (EDA) recording is a powerful, widely used tool for monitoring psychological or physiological arousal. However, analysis of EDA is hampered by its sensitivity to motion artifacts. We propose a method for removing motion artifacts from EDA, measured as skin conductance (SC), using a stationary wavelet transform (SWT). We modeled the wavelet coefficients as a Gaussian mixture distribution corresponding to the underlying skin conductance level (SCL) and skin conductance responses (SCRs). The goodness-of-fit of the model was validated on ambulatory SC data. We evaluated the proposed method in comparison with three previous approaches. Our method achieved a greater reduction of artifacts while retaining motion-artifact-free data.</p>",,--Choose Location,2019-04-19 17:39:31.969,True,2015-01-01,Wavelet-based motion artifact removal for electrodermal activity,PUBLIC,,True,Affective Computing,False
automated-tongue-analysis,cvx,False,"<p>A common practice in Traditional Chinese Medicine (TCM) is visual examination of the patient's tongue. This study will examine ways to make this process more objective and to test its efficacy for understanding stress- and health-related changes in people over time. We start by developing an app that makes it comfortable and easy for people to collect tongue data in daily life together with other stress- and health-related information. We will obtain assessment from expert practitioners of TCM, and also use pattern analysis and machine learning to attempt to create state-of-the-art algorithms able to help provide better insights for health and prevention of sickness.</p>",,--Choose Location,2017-09-21 15:02:57.578,True,2015-09-01,Automated Tongue Analysis,PUBLIC,,True,Affective Computing,False
how-to-generate-almost-anything,agnescam,False,"<p>How to generate almost anything is a collaborative project between humans and AI, run by Pinar Yanardag Delul of the Scalable Cooperation group. The project combines expertise in making (inspired by Neil Gershenfeld’s How To Make Almost Anything class) from around MIT with generative adversarial neural networks (GANS). Each project chooses a focus for the human-machine collaboration — from music and fashion to pizza and perfume — to create outcomes that otherwise might never have been imagined!</p><p>To read more about the project, it's fully documented at:&nbsp;https://medium.com/@howtogeneratealmostanything, and is written about here:&nbsp;https://venturebeat.com/2018/09/10/mit-students-use-ai-to-cook-up-pizza-recipes/</p>",2018-09-30,,2018-09-17 15:49:17.289,True,2018-07-01,How To Generate Almost Anything,PUBLIC,,False,Viral Communications,False
waste-at-mit,agnescam,False,"<p>Let's play: Waste at MIT is a game about trash; it’s also a game about understanding civic infrastructure. This project explores the use of games to engage citizens, impacting real-world actions in complex civic systems. Working with waste management and sustainability efforts on campus, this project uses MIT as a living lab to study the link between interaction and action in civic media. More generally, this project looks at ways in which participatory, critical and exploratory games can give people agency over the complex systems that surround them.</p><p>Participate in the project by playing! Visit <a href=""http://wasteatmit.media.mit.edu"">wasteatmit.media.mit.edu</a> to learn more. The code for this project is open-sourced at:&nbsp;<a href=""http://github.com/agnescameron/trashgame"">http://github.com/agnescameron/trashgame</a></p>",2019-06-07,,2019-06-04 19:37:53.019,True,2019-01-01,Let's play: Waste at MIT,PUBLIC,,False,Viral Communications,False
election-arg,agnescam,False,"<p>Only 40% of the eligible population votes in the typical US midterm election, and among young people turnout is even lower. In this experiment, we develop a game that encourages people to influence their friends to physically go to the polls. The system is reminiscent of <a href=""https://www.media.mit.edu/projects/fiftynifty/overview/"">Fifty Nifty</a>, where people competed to amass points by both calling representatives and spreading the message to others. In addition to awarding points, vote.lol aims to motivate players by allowing them influence over the outcome of a shared narrative that develops in real-time before (and during) the election. Interactive stories with real-world game mechanics are characteristic of alternate reality games (ARGs), which have received scholarly attention for their potential to instigate viral communications among players who self-organize to solve complex problems. The purpose of this study is to test whether ARG techniques can motivate gamers to solve the intractable problem of getting their peers to vote.</p>",2018-12-31,,2019-04-18 00:52:49.802,True,2018-10-01,Election ARG,PUBLIC,,False,Viral Communications,False
medrec,agnescam,False,"<p>Increasingly in the US, people have to take responsibility for their health information.&nbsp; Simultaneously, medical providers must make patient data available. MedRec fully decentralizes access rights via an Ethereum blockchain, thereby giving patients control over record distribution. Our model is the World Wide Web:&nbsp; MedRec is a network. Patients and providers operate nodes that authorize others to retrieve data. It is a basis for a generally useful permissioning system.</p><p>There is no website or central repository of permissions. Instead, patients and medical records originators <i>establish a relationship </i>and based on that, the patient creates <i>smart contracts</i>&nbsp;that other members of the network can use to authorize access to a record database. The parameters of contracts are kept in a <i>blockchain </i>that is maintained by all member providers/originators who at the same time use those contracts to provide access to their database. The patient/user contracts themselves are held by the patients in a <i>wallet </i>that resides on their device[s] as an app. This app is secure and recoverable in case the physical device is lost or damaged.</p><p>For a full overview, technical documentation, and updates, visit the project's <a href=""https://medrec.media.mit.edu"">website</a>.</p>",,,2018-10-17 19:22:59.328,True,2018-01-01,MedRec,PUBLIC,https://medrec.media.mit.edu,True,Viral Communications,False
droplet-io,ken_n,False,"<p>DropletIO uses aqueous droplets as a form of programmable material for human-material interaction. We demonstrate how fluids in our environment can be &nbsp;programmed droplet-wise to move, merge, and split. Through these operations we can then digitally regulate fluid properties—smell, color, chemical, and biological characteristics. When seamlessly integrated into a range of everyday objects and spaces, droplets become ubiquitous displays and other interactive elements aiding creative activity such as painting, storytelling, and art. Beyond this, programmable droplets have applications in digital biology and chemistry.</p>",2017-04-06,,2017-10-05 21:52:41.593,True,2016-10-01,Droplet IO: Programmable Droplets for Human-material Interaction,LAB-INSIDERS,http://udayan-u.com,False,Tangible Media,False
lineform,ken_n,False,"<p>We propose a novel shape-changing interface that consists of a single line. Lines have several interesting characteristics from the perspective of interaction design: abstractness of data representation; a variety of inherent interactions/affordances; and constraints such as boundaries or borderlines. By using such aspects of lines together with added transformation capability, we present various applications in different scenarios: shape-changing cords, mobiles, body constraints, and data manipulation to investigate the design space of line-based shape-changing interfaces.</p>",,--Choose Location,2018-05-04 15:31:24.514,True,2015-01-01,LineFORM,PUBLIC,,True,Tangible Media,False
hydromorph,ken_n,False,"<p>HydroMorph is an interactive display based on shapes formed by a stream of water. Inspired by the membrane formed when a water stream hits a smooth surface (e.g., a spoon), we developed a system that dynamically controls the shape of a water membrane. This project explores a design space of interactions around water shapes, and proposes a set of user scenarios in applications across scales, from the faucet to the fountain. Through this work, we look to enrich our interaction with water, an everyday material, with the added dimension of transformation.</p>",,--Choose Location,2016-12-05 00:17:14.252,True,2016-01-01,HydroMorph,PUBLIC,,True,Tangible Media,False
animastage,ken_n,False,"<p>We present AnimaStage: a hands-on animated craft platform based on an actuated stage. Utilizing a pin-based shape display, users can animate their crafts made from various materials. Through this system, we intend to lower the barrier for artists and designers to create actuated objects and to contribute to interaction design using shape-changing interfaces for inter-material interactions.</p><p>We introduce a three-phase design process for AnimaStage with examples of animated crafts. We implemented the system with several control modalities that allow users to manipulate the motion of the crafts so that they could easily explore their desired motion through an iterative process. Dynamic landscapes can also be rendered to complement the animated crafts. We conducted a user study to observe the subject and process by which people make crafts using AnimaStage. We invited participants with different backgrounds to design and create crafts using multiple materials and craft techniques. A variety of outcomes and application spaces were found in this study.</p><p><a href=""http://tangible.media.mit.edu/project/animastage/"">Project Page</a></p>",,,2017-06-21 18:35:35.551,True,2017-06-12,AnimaStage,PUBLIC,http://tangible.media.mit.edu/project/animastage,True,Tangible Media,False
transform-adaptive-and-dynamic-furniture,ken_n,False,"<p>Introducing TRANSFORM, a shape-changing desk. TRANSFORM is an exploration of how shape display technology can be integrated into our everyday lives as interactive, transforming furniture. These interfaces not only serve as traditional computing devices, but also support a variety of physical activities. By creating shapes on demand or by moving objects around, TRANSFORM changes the ergonomics and aesthetic dimensions of furniture, supporting a variety of use cases at home and work: it holds and moves objects like fruit, game tokens, office supplies, and tablets, creates dividers on demand, and generates interactive sculptures to convey messages and audio.</p>",,--Choose Location,2019-04-17 19:36:00.162,True,2014-09-01,TRANSFORM: Adaptive and dynamic furniture,PUBLIC,,True,Tangible Media,False
trans-dock,ken_n,False,"<p>We introduce TRANS-DOCK, a passive docking system for pin-based shape displays that enhances the interaction capability for both the output and input. By simply switching the ""transducer"" module to be docked on a single shape display, users can selectively switch between different display sizes and resolutions, movement modalities, as well as pin alignments enabled by the transducers. We introduce a design space consisting of mechanical elements and enabled interaction capabilities. We utilized several mechanical elements to develop the docking hardware for customizable interactions. With this idea, we present potential application spaces, which include digital 3D model explorations, active tangible interface prototyping, gaming, and dynamic house models. TRANS-DOCK intends to expand what a single shape display can do for dynamic physical interactions, by converting arrays of linear motion to several types of dynamic motion in an adaptable and flexible manner.</p>",,,2018-10-05 18:57:23.901,True,2018-08-01,TRANS-DOCK,LAB-INSIDERS,,True,Tangible Media,False
chainform,ken_n,False,"<p>ChainFORM is a modular hardware system for designing linear shape-changing interfaces. Each module is developed based on a servo motor with added flexible circuit board, and is capable of touch detection, visual output, angular sensing, and motor actuation. Moreover, because each module can communicate with other modules linearly, it allows users and designers to adjust and customize the length of the interface. Using the functionality of the hardware system, we propose a wide range of applications, including line-based shape changing display, reconfigurable stylus, rapid prototyping tool for actuated crafts, and customizable haptic glove. We conducted a technical evaluation and a user study to explore capabilities and potential requirements for future improvement.</p>",,--Choose Location,2018-05-04 15:33:24.390,True,2015-09-01,ChainFORM,PUBLIC,,True,Tangible Media,False
materiable-rendering-dynamic-material-properties-with-shape-changing-interfaces,ken_n,False,"<p>Shape-changing interfaces give physical shape to digital data so that users can feel and manipulate data with their hands and body. Combining techniques from haptics with the field of shape-changing interfaces, we propose a technique to build a perceptive model of material properties by taking advantage of the shape display's ability to dynamically render flexibility, elasticity, and viscosity in response to the direct manipulation of any computationally rendered physical shape. Using a computer-generated relationship between the manipulated pins and nearby pins in the shape display, we can create human proprioception of various material properties. Our results show that users can identify varying material properties in our simulations through direct manipulation, and that this perception is gathered mainly from their physical relationship (touch) with the shape display and its dynamic movements.</p>",,--Choose Location,2016-12-16 20:08:16.037,True,2015-09-01,Materiable,PUBLIC,,True,Tangible Media,False
in-force,ken_n,False,"<p>We propose a novel tangible interaction with pin-based shape display that can reproduce haptic perception of shape, material stiffness, and heterogeneous internal structures of volumetric shape. This is enabled by newly developed pin display, inFORCE, that can detect the force that is applied to each pin, and exert arbitrary force to contact body and objects at the same time. Our proposed interaction methods enabled people to ""press through"" computationally rendered shapes to understand the internal structure of 3D volumetric information. Our design space explores a range of interaction capability enabled by the Force Shape Display system including capturing physical material properties.</p>",,,2019-03-25 13:24:48.264,True,2017-10-01,inFORCE,LAB-INSIDERS,http://tangible.media.mit.edu/project/inforce/,True,Tangible Media,False
auto-inflatables,ken_n,False,,,,2017-11-25 17:16:39.275,True,2017-01-01,Auto-Inflatables,PUBLIC,,True,Tangible Media,False
particles,ken_n,False,"<p>PARTICLES is a modular hardware system that can dynamically control attraction forces in-between, aiming to provide malleable tangible affordances (e.g. squishing, splitting, and scooping). Each module contains Spherical Gears + Switchable Magnet to variably control the attraction force on the surface. Having a number of these modules, we aim to dynamically tune perceived material property through the attraction force control. We imagine scalable applications such as a novel tangible display for data exploration in medical and geo-science. Another application can be an educational tool that people can learn nano-scale molecule structures and attraction forces through tangible interaction with particles.&nbsp;</p>",,,2018-05-23 19:46:16.346,True,2017-10-01,PARTICLES,LAB-INSIDERS,,True,Tangible Media,False
computational-food,inamura,False,"<p>Computational Food is a series of experiments around the shape-changing nature of food and its associated unique sensory experiences. We used food&nbsp;as a <i>medium</i>&nbsp;and a <i>platform</i> to develop dynamic, shape-changing prototypes that are edible or that enhance the culinary experience.</p>",2014-12-31,,2019-04-29 22:10:01.940,True,2014-09-16,Computational Food,PUBLIC,,False,Mediated Matter,False
g3p-II,inamura,False,"<h2><p><span style=""font-size: 18px; font-weight: normal;"">Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods—such as blowing, pressing, and forming—have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.</span><br></p></h2>",,--Choose Location,2018-10-23 15:19:17.532,True,2015-09-01,Glass II,PUBLIC,,True,Mediated Matter,False
g3p,inamura,False,"<p>Ancient yet modern, enclosing yet invisible, glass was first created in  Mesopotamia and Ancient Egypt 4,500 years ago. Precise recipes for its  production - the chemistry and techniques - often remain closely guarded  secrets. Glass can be molded, formed, blown, plated or sintered; its  formal qualities are closely tied to techniques used for its  formation.&nbsp;From the discovery of core-forming process for bead-making in  ancient Egypt, through the invention of the metal blow pipe during  Roman times, to the modern industrial Pilkington process for making  large-scale flat glass; each new breakthrough in glass technology  occurred as a result of prolonged experimentation and ingenuity, and has  given rise to a new universe of possibilities for uses of the material.  <br></p>",,--Choose Location,2017-10-13 19:43:29.330,True,2014-01-01,Glass I,PUBLIC,,True,Mediated Matter,False
computational-food,djain,False,"<p>Computational Food is a series of experiments around the shape-changing nature of food and its associated unique sensory experiences. We used food&nbsp;as a <i>medium</i>&nbsp;and a <i>platform</i> to develop dynamic, shape-changing prototypes that are edible or that enhance the culinary experience.</p>",2014-12-31,,2019-04-29 22:10:01.940,True,2014-09-16,Computational Food,PUBLIC,,False,Speech + Mobility,False
amphibian-terrestrial-scuba-diving-simulator-using-virtual-reality,djain,False,"<p>SCUBA diving as a sport has enabled people to explore the magnificent ocean diversity of beautiful corals, striking fish, and mysterious wrecks. However, only a small number of people are able to experience these wonders, as diving is expensive, mentally and physically challenging, needs a large time investment, and requires access to large bodies of water. Most existing SCUBA diving simulations in VR are limited to visual and aural displays. We propose a virtual reality system, Amphibian, that provides an immersive SCUBA diving experience through a convenient terrestrial simulator. Users lie on their torso on a motion platform with their outstretched arms and legs placed in a suspended harness. Users receive visual and aural feedback through the Oculus Rift head-mounted display and a pair of headphones. Additionally, we simulate buoyancy, drag, and temperature changes through various sensors.</p>",,--Choose Location,2018-08-03 18:51:54.353,True,2015-01-01,Amphibian: Terrestrial SCUBA Diving Simulator Using Virtual Reality,PUBLIC,,True,Speech + Mobility,False
computational-food,viirj,False,"<p>Computational Food is a series of experiments around the shape-changing nature of food and its associated unique sensory experiences. We used food&nbsp;as a <i>medium</i>&nbsp;and a <i>platform</i> to develop dynamic, shape-changing prototypes that are edible or that enhance the culinary experience.</p>",2014-12-31,,2019-04-29 22:10:01.940,True,2014-09-16,Computational Food,PUBLIC,,False,Mediated Matter,False
d-Abyss,viirj,False,"<p><b>Can tattoos embrace technology in order to make the skin interactive?</b></p><p>The DermalAbyss project is the result of a collaboration between MIT researchers Katia Vega, Xin Liu, Viirj Kan and Nick Barry and Harvard Medical School researchers Ali Yetisen and Nan Jiang.&nbsp;<br></p><p>DermalAbyss is a proof-of-concept that presents a novel approach to bio-interfaces in which the body surface is rendered an interactive display. Traditional tattoo inks are replaced with biosensors whose colors change in response to variations in the interstitial fluid. It blends advances in biotechnology with traditional methods in tattoo artistry.&nbsp;</p><p>This is a research project, and there are currently no plans to develop Dermal Abyss as a product or to pursue clinical trials.<br></p>",2017-05-31,,2018-04-27 17:45:10.726,True,2016-06-01,DermalAbyss: Possibilities of Biosensors as a Tattooed Interface,PUBLIC,,False,Mediated Matter,False
social-textiles,viirj,False,"<p>The way we represent ourselves in social media is intangible. What we choose to wear is public to the world and we are aware of it. In contrast, what we post online about ourselves reaches thousands of people and generates social consequences, but it doesn’t feel that way. Is the current form of social media really making our relationships better? Current technologies are good at connecting people at a distance, but less so at connecting them within the same environment.</p><p>Social textiles embodies who you are and dynamically reflects your shared interests with people nearby. It enables you to gain access to communities of people in the physical world and enhances social affordances and icebreaking interactions through wearable social messaging.</p><p>Social Textiles embody who you are and dynamically reflect your shared interests with people nearby. They enable you to gain access to communities of people in the physical world and enhance social affordances and icebreaking interactions through wearable social messaging. Social Textiles can connect community members with niche interests, philosophical beliefs, personalities, emotional statuses, and ethical views. They have the potential to enable members to bypass superficial or generic interests through ""filtering"" individuals, in order to tune social experiences toward people who are more compatible.</p>",2015-01-01,--Choose Location,2018-10-12 16:50:34.930,True,2014-09-01,Social Textiles,PUBLIC,,False,Mediated Matter,False
transform-adaptive-and-dynamic-furniture,viirj,False,"<p>Introducing TRANSFORM, a shape-changing desk. TRANSFORM is an exploration of how shape display technology can be integrated into our everyday lives as interactive, transforming furniture. These interfaces not only serve as traditional computing devices, but also support a variety of physical activities. By creating shapes on demand or by moving objects around, TRANSFORM changes the ergonomics and aesthetic dimensions of furniture, supporting a variety of use cases at home and work: it holds and moves objects like fruit, game tokens, office supplies, and tablets, creates dividers on demand, and generates interactive sculptures to convey messages and audio.</p>",,--Choose Location,2019-04-17 19:36:00.162,True,2014-09-01,TRANSFORM: Adaptive and dynamic furniture,PUBLIC,,True,Mediated Matter,False
organic-primitives,viirj,False,"<p>A large portion of the chemical and biological processes underlying our everyday experience remains imperceptible to us. Be it the contents of rain, the ocean, or human tears, chemical codes mediate interactions between organic systems from the environment to our bodies and food. <br></p><p>As humans, we understand information mediated by our senses—through textures, symbols, odors, and tastes. In order to design for a wider array of sensory modalities in representing fluid-based information and enable user interaction with these systems, we have developed Organic Primitives. It is a new medium for transforming objects into information displays. Chemical input is converted into human senses through a set of color-, odor-, and form-changing materials. <br></p>",,,2017-10-26 00:15:41.352,True,2014-11-23,Organic Primitives,PUBLIC,http://www.OrganicPrimitives.com,True,Mediated Matter,False
a-pops,jgroff,False,"<p>A design project done in collaboration with the MIT Media Lab and the Laboratorio para la Ciudad (Laboratory for the City), Mexico City's experimental office for civic innovation and urban creativity, A-pops is a networked learning experience across Mexico City that supports young learners in engaging in emergent and playful opportunities in and beyond their local communities. In line with the ""Playful City"" goal, this project aims to embed playful learning experiences across Mexico City that are creative, collaborative, and public, by leveraging existing public spaces throughout neighborhoods and micro-communities across the city. By embedding a variety of playful learning experiences across a variety of locations, a wide range of learners have the ability to easily and socially engage in transformative experiences that support key skills in design, collaboration, creativity, programming, and learner agency.</p>",2017-06-01,--Choose Location,2018-10-20 18:17:15.133,True,2015-09-01,A-pops,PUBLIC,,False,Lifelong Kindergarten,False
learning-genome,jgroff,False,"<p>Content and curriculum lie at the heart of every educational system, learning environment, and learning technology. Yet the infrastructure to support this main pillar is severely lacking, and largely consists of a scattered set of elements: research on learning constructs, various curricula and syllabi, loosely tied together by jurisdiction-based standards documents (i.e. the Common Core State Standards, Finland’s education standards, etc.). These elements evolved before the digital era, based on different constituent's needs. However, the disjointedness of these structures leave the management of knowledge on learning constructs—and the design/access of learning tools and technologies for these learning constructs—in a messy, challenging state, unable to successfully carry us into the 21st century and the future of learning technologies and educational systems.</p><p>The LearningGraph&nbsp;is a research initiative that uses systems engineering practices to reengineer this infrastructure, using learning sciences frameworks and modern data modeling and ontological design. The goal of the project is to create a unified data model that supports the multiple stakeholders who work with learning constructs (teachers, learners, learning and assessment designers, learning scientists, educational technologist, informal learning facilitators, etc.), and provide an ongoing ""living sandbox"" where this knowledge is modeled and used to support teaching and learning across learning environments, technologies, and experiences.<br></p>",2018-05-01,,2018-06-20 20:00:43.152,True,2016-10-20,The LearningGraph,PUBLIC,,False,Lifelong Kindergarten,False
a-pops,gbernal,False,"<p>A design project done in collaboration with the MIT Media Lab and the Laboratorio para la Ciudad (Laboratory for the City), Mexico City's experimental office for civic innovation and urban creativity, A-pops is a networked learning experience across Mexico City that supports young learners in engaging in emergent and playful opportunities in and beyond their local communities. In line with the ""Playful City"" goal, this project aims to embed playful learning experiences across Mexico City that are creative, collaborative, and public, by leveraging existing public spaces throughout neighborhoods and micro-communities across the city. By embedding a variety of playful learning experiences across a variety of locations, a wide range of learners have the ability to easily and socially engage in transformative experiences that support key skills in design, collaboration, creativity, programming, and learner agency.</p>",2017-06-01,--Choose Location,2018-10-20 18:17:15.133,True,2015-09-01,A-pops,PUBLIC,,False,Fluid Interfaces,False
physiohmd,gbernal,False,"<p>Virtual and augmented reality headsets are unique as they have access to our facial area, an area that presents an excellent opportunity for always-available input and insight into the user's state. Their position on the face makes it possible to capture bio-signals as well as facial expressions.&nbsp; The&nbsp; PhysioHMD platform&nbsp;introduces a software and hardware modular interface built for collecting affect and physiological data from users wearing a head-mounted display. The platform enables researchers and developers to aggregate and interpret signals in real-time and use them to develop novel, personalized interactions, as well as evaluate virtual experiences. Our design offers seamless integration with standard HMDs, requiring minimal setup effort for developers and those with less experience using game engines. The PhysioHMD platform is a flexible architecture that offers an interface that is not only easy to extend but also complemented by a suite of tools for testing and analysis. We hope that PhysioHMD can become a universal, publicly available testbed for VR and AR researchers.</p><p>To create a seamless experience, we have integrated several bio-signal sensors into the faceplate of an HTC VIVE VR headset and utilized the Shimmer3 sensor for emotion-sensing. For the collection of Galvanic Skin Response, dry electrodes were positioned on the forehead area due to the fact that it is one of the areas most dense with sweat glands. GSR data reflects emotional arousal, but in order to identify how arousal, valence, motivation, and cognition interact in response to physical or psychological stimuli, it becomes necessary to complement GSR with other biosensors. For the heart rate, a PPG (photoplethysmogram) sensor, which senses the rate of blood flow by utilizing light to monitor the heart’s pumping action, was placed in the temple region of the user. This is done to get insights into the respondent's physical state, anxiety and stress levels (arousal), and to determine how changes in their physiological state relate to their actions and decisions.<br></p>",,,2018-11-14 19:05:00.306,True,2017-02-01,PhysioHMD,PUBLIC,,True,Fluid Interfaces,False
neuroknit,gbernal,False,"<p>NeuroKnit is the interplay between the physical and virtual that is explored in response to the current lack of culture, expression, and emotions in VR  experiences; we propose a two-fold solution. First, the integration of bio-signal sensors into the HMD and techniques to detect aspects of the emotional state of the user. Second, the use of this data to generate expressive avatars.</p>",,,2019-06-07 12:59:19.489,True,2017-07-01,Emotional Beasts Parte Dos: NeuroKnit,PUBLIC,,True,Fluid Interfaces,False
engineering-dreams,gbernal,False,"<p>Our dream is a sci-fi future, where dreams are controllable.&nbsp;</p><p>We are working to build technology that interfaces with the sleeping mind.&nbsp;As the dreamer descends into sleep, we track different sleep-stages using brain activity, muscle tension, heart rate, and movement data.&nbsp;External stimuli in the form of scent, audio, and muscle stimulation affect the content of the dreams. We are working on integrating multiple projects&nbsp; developed at the Fluid Interfaces group towards a vision where sleep is controllable.</p><p><a href=""http://dreams.media.mit.edu"">dreams.media.mit.edu</a><br></p>",,,2019-04-30 15:12:45.024,True,2018-03-21,Engineering Dreams,PUBLIC,,True,Fluid Interfaces,False
paper-dreams,gbernal,False,"<p><b>Paper Dreams explores the various modes of creativity that can be enabled by artificial intelligence.</b></p><p>A fair amount of current research has been focused on using machine intelligence to learn “creativity,” from transfer learning of artistic styles to translation of languages. However, the application of current machine learning algorithms and multi-modal inputs to the augmentation of existing human creativity is a relatively unexplored area. By focusing on the dynamics of this human-machine interaction and working with the representations inside machine learning models, we can give people new tools for reasoning.</p><p>Our system at its current form provides a user with a canvas that it is more like a mirror, where we draw strokes and re-evaluate what our drawing looks like at each step in order to continue. We do this by building a neural network which takes a small number of input variables, called <i>latent variables</i>, and produces the entire sketch as output.</p><p>The machine can use analog modes of thinking to recognize what we are drawing and help us move forward. At the moment, Paper Dreams augments the drawing experience in three ways: by adding textures/colors, suggesting other elements/drawings for the scene, and introducing serendipity. To adjust the level of serendipity, the user has control of a dial that determines how ""predictable"" vs ""unpredictable/nutty"" you want these machine additions to be.</p><p><b>Some of the driving questions for this project are:</b></p><ul><li>To what extent are these new tools enabling creativity? </li><li>Can they be used to generate ideas which are truly surprising and new, or are the ideas clichés, based on trivial recombinations of existing ideas?</li><li>Can such systems be used to develop fundamental new interface primitives? </li><li>How will those new primitives change and expand the way humans think?&nbsp;</li></ul>",,,2019-04-30 16:09:38.788,True,2018-03-01,Paper Dreams,PUBLIC,,True,Fluid Interfaces,False
emotionalbeasts-1,gbernal,False,"<p>With advances in virtual reality (VR) and physiological sensing technology, even more immersive computer-mediated communication through life-like characteristics is possible. As a solution for the current lack of culture, expression, and emotions in VR avatars, we propose a two-fold solution. First, integrate bio-signal sensors into the head-mounted display (HMD) and implement techniques to detect aspects of the emotional state of the user. Second, connect the data collected to an expressive avatar: Emotional Beasts. The creation of Emotional Beasts allowed us to experiment with the manipulation of a user's self-expression in VR space and as well as the perception of others in it, with the goal of pulling the avatar design away from the uncanny valley and making it more expressive, more relatable to our own mannerisms. Based on this we have implemented a prototype system in which VR, human motion, and physiological signals are integrated to allow avatars to become more expressive in virtual environments in real time.</p>",,,2018-10-29 02:39:13.728,True,2016-08-01,EmotionalBeasts,PUBLIC,,True,Fluid Interfaces,False
a-pops,minasg,False,"<p>A design project done in collaboration with the MIT Media Lab and the Laboratorio para la Ciudad (Laboratory for the City), Mexico City's experimental office for civic innovation and urban creativity, A-pops is a networked learning experience across Mexico City that supports young learners in engaging in emergent and playful opportunities in and beyond their local communities. In line with the ""Playful City"" goal, this project aims to embed playful learning experiences across Mexico City that are creative, collaborative, and public, by leveraging existing public spaces throughout neighborhoods and micro-communities across the city. By embedding a variety of playful learning experiences across a variety of locations, a wide range of learners have the ability to easily and socially engage in transformative experiences that support key skills in design, collaboration, creativity, programming, and learner agency.</p>",2017-06-01,--Choose Location,2018-10-20 18:17:15.133,True,2015-09-01,A-pops,PUBLIC,,False,Social Machines,False
playful-words,minasg,False,"<p>While there are a number of literacy technology solutions developed for individuals, the role of social—or networked—literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.</p><p>By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.</p><p>To learn more about this project, please check out:&nbsp;<a href=""http://playfulwords.org/"">http://playfulwords.org/</a></p>",,--Choose Location,2018-04-30 20:28:15.298,True,2014-09-01,Playful Words,PUBLIC,,True,Social Machines,False
play-analytics,minasg,False,"<p>Analyzing detailed data from SpeechBlocks&nbsp;to understand how kids engage with constructionist literacy learning technologies, with the goal of empowering caregivers (e.g. parents, older siblings, tutors) with these insights.</p>",,,2018-04-30 20:56:57.445,True,2016-02-01,Play Analytics,PUBLIC,,True,Social Machines,False
media-meter-focus,a_hashmi,False,<p>Media Meter Focus shows focus mapping of global media attention. What was covered in the news this week? Did the issues you care about get the attention you think they deserved? Did the media talk about these topics in the way you want them to? The tool-set also shows news topics mapped against country locations.</p>,2015-01-01,--Choose Location,2016-12-05 00:17:02.737,True,2014-01-01,Media Meter Focus,PUBLIC,,False,Civic Media,True
mapping-the-globe,a_hashmi,False,<p>Mapping the Globe is an interactive tool and map that helps us understand where the <i>Boston Globe</i> directs its attention. Media attention matters—in quantity and quality. It helps determine what we talk about as a public and how we talk about it. Mapping the Globe tracks where the paper's attention goes and what that attention looks like across different regional geographies in combination with diverse data sets like population and income. Produced in collaboration with the <i>Boston Glob</i>e.</p>,2015-09-01,--Choose Location,2016-12-05 00:17:16.960,True,2014-09-01,Mapping the Globe,PUBLIC,,False,Civic Media,True
media-attention-and-health,a_hashmi,False,"<p>A tool that would allow journalists to compare the proportion of coverage�based on Media Cloud�on a given health subject to 1) the proportion of related research spending in a given year, and 2) the relative health impact of that subject on the population.</p>",,--Choose Location,2016-12-05 00:16:35.669,True,2014-01-01,Media Attention and Health,LAB,,True,Civic Media,True
biobricks,sdruga,False,"<p>The main purpose of this project is to develop a hands-on learning activity around tinkering with microbiological bricks to be play-tested with kids and educators. </p><p class=""""><span style=""font-size: 18px; font-weight: normal;"" class="""">I am exploring how microorganisms can create generative art and music and &nbsp;how we could transform any affordable webcam into a &nbsp;microscope that can connect directly to the computer. I am also looking at how this live stream from microcosmos world could &nbsp;be visualized in many different ways by applying multiple filters and image transformations such that it provides us with many new perspective about the world around us. This live connection to living organisms enables us also to imagine new ways of interacting and tinkering with them. &nbsp;Some initial explorations lead to the creation of a bio-kaleidoscope that is alive (literally).&nbsp;</span></p><p class=""""><span style=""font-size: 18px; font-weight: normal;"" class="""">I hope this project will &nbsp;explore new ways in which kids could  play with the micro-cosmos around us, explore the magic at the intersection of digital, physical and bio tinkering and discover how using familiar things, such as webcams, to do unfamiliar actions can boost our creative confidence.&nbsp;</span></p>",2017-09-30,,2017-06-05 21:44:02.634,False,2016-11-01,BioBricks,PUBLIC,http://drugastefania.com,False,Personal Robots,False
cognimates,sdruga,False,"<p>Cognimates&nbsp;is  a platform where parents and children (7-10 years old) participate in creative programming activities in which they learn how to build games, program robots, and train their own AI models. Some of the activities are mediated by embodied intelligent agents which help learners scaffold learning and better collaborate. <a href=""http://cognimates.me/"">Learn more about our research, projects, and learning guides</a>.</p><p>Conversational agents and connected toys are becoming common in homes. Increasing exposure to ""intelligent"" technology raises important questions about the ways that children understand it and how they could learn&nbsp;with and from it. Embodied intelligent agents, such as social robots, afford longer-term engagement in the home for children and their families .&nbsp;&nbsp;</p><p>Building on the prior experience in the <a href=""http://www.media.mit.edu/groups/personal-robots"">Personal Robots</a> group of designing social robots for nurturing children's curiosity and learning, we built a platform where children and parents can learn to program with embodied intelligent agents which in turn become learning companions (Cognimates). The goal is to enable&nbsp; learners&nbsp; to interact with a social robot but also program it, train it to remember and learn things over time, and have&nbsp; reflective conversations with their peers prompted by it.&nbsp;</p><p>Why, how, and when can embodied intelligent agents support children and parents to learn via reflective teaching? What are the new intergenerational learning pathways that Cognimates could facilitate? How can these future learning companions be integrated into various learning applications and what are the generalizable design considerations?&nbsp; In this research project we are addressing these questions by allowing children and parents to use a visual programming interface to control and customize an embodied intelligent agent.&nbsp;</p><p><a href=""https://drive.google.com/file/d/0B8pX8mypq8MsUnJNbHNFUjI0RmM/view?usp=sharing"">Demo video</a></p>",,,2019-03-27 16:43:04.722,True,2017-11-12,Cognimates: Collaborative creative learning with embodied intelligent agents,PUBLIC,http://www.drugastefania.com/,True,Personal Robots,False
multi-spring,ktj,False,"<h2>Overview</h2><p>We have created a new customizable, multi-user research-through-play platform designed to facilitate social skill development for children with ASD. Through the highly motivating, individualized play environment, children can progress at their own pace while practicing social skills. Early results suggest that SPRING’s novel multi-player environment elicits social interaction in a way that can engage learners with very different interests.</p><h2>Background</h2><p>Practitioners have long explored using motivating, personalized reinforcement to achieve developmental goals for children with Autism&nbsp;Spectrum Disorder (ASD) (Koegel &amp; Mentis, 1985; Vismara &amp; Lyons, 2007). SPRING–Smart Platform for Research, Intervention, and Neurodevelopmental Growth– is a customizable, interactive, research-through-play platform, built to systematically probe the effects of these reinforcement modalities on learning and physiological regulation (Johnson &amp; Picard, 2017). SPRING has shown promise in facilitating increased engagement and skill development for children with autism and other neuro-differences, but it has lacked multi-user functionality and built-in means to prompt social skills, such as joint attention, turn taking, and cooperative play–until&nbsp;now. Adding this functionality allows practitioners to customize the reinforcement and developmental challenge of each individual SPRING unit while simultaneously encouraging social engagement by linking the units over a virtual network.</p><h2>Objectives</h2><p>Here, we present a Multi-SPRING system designed to</p><ol><li>Stimulate early social experiences, such as joint attention, turn taking, and cooperative play;</li><li>Facilitate simultaneous play between individuals with different skill levels, such as typical and autistic peers or siblings, while providing personalized reinforcement tuned to each individual’s motivating interests;</li><li>Reduce anxiety associated with unaided social interactions and extend engagement in a multi-person activity; and</li><li>Passively capture time-synchronized, quantitative measures of users’ affective states via wearable physiological sensors and data of users’ play progressions via SPRING.</li></ol><h2>Methods</h2><p>Each SPRING unit has a removable, modular center that can be adapted to the needs of an individual child by inserting different physical modules. An integrated smartphone and embedded LEDs allow user-selected customization of motivating reinforcement, such as favorite video clips, images, music, or colorful light displays. The smartphone also enables scaffolded difficulty levels within a single module so each child can be met with the “just right” challenge. Embedded digital sensors capture and store time-stamped data of a child’s interaction with SPRING. Paired with wearable physiological sensors, these data allow multimodal analysis of a child’s affective state and learning progression.</p><p>The new Multi-SPRING system links multiple SPRING units in realtime through a virtual private room, much like a chat room. This method enables social multiplayer interactions, such as turn taking and cooperative play, while continuously capturing activity data logs from every child for future study.</p>",2018-08-01,,2018-06-11 14:03:21.507,True,2017-08-01,Multi-SPRING,PUBLIC,,False,Affective Computing,False
open-source-spring,ktj,False,<h1><b>Open-Source Instructions for Building SPRING System</b></h1>,,,2017-12-24 14:02:54.051,True,2017-12-20,Open-Source SPRING,PUBLIC,,True,Affective Computing,False
the-entrain-study,ktj,False,"<p>Individuals with autism are known to have difficulties connecting with other people, reciprocating social interactions, and being emotionally regulated by others. Yet, until recently, very little attention has been given to the way people interact together, in a system, rather than by themselves. We propose a new way to collect data on how caregivers and their children, with and without autism, affect and are affected by each other (i.e., how they ""sync up"" with one another), both in their behavior and in their physiology. We also introduce a customizable digital-physical smart toy platform that will allow us to test hypotheses and collect data about patterns of caregiver-child synchrony in a naturalistic and engaging environment. MIT and Northeastern are forging a new collaboration between smart toy technology and autism research that will help uncover how the social brain develops.</p>",,--Choose Location,2019-04-19 17:28:26.918,True,2016-09-01,The enTRAIN Study: Physiological synchrony in children with autism,PUBLIC,,True,Affective Computing,False
comm-phys-gest,ktj,False,"<p>We are exploring physiology and nonverbal gestures in real-life contexts, particularly for children with Autism Spectrum Disorders (ASD). Our long-term goal is to enhance understanding and communication by pairing knowledge from wearable devices with context and individualized information. Our focus is to understand longitudinal patterns of these signals in an individual's day-to-day life, and then develop personalized algorithms to interpret signals.</p>",,,2019-04-17 14:57:48.343,True,2019-03-16,ECHOS: Enhancing Communication using Holistic Observations and Sensing,PUBLIC,,True,Affective Computing,False
physio-freefall,ktj,False,"<p>This project seeks to examine the effects of altered gravity on an individual’s physiology during parabolic flight. Specifically, we will collect flight participants’ heart rate, heart rate variability, breathing rate, skin temperature, and skin conductance measurements using wearable, wireless sensors in order to determine the response of these biosignals to zero/hyper/microgravity and feelings of nausea. </p><p>The results of this research will have both significant scientific and civilian value. To our knowledge, this experiment will be the first to investigate the new Multiple Arousal Theory in the context of motion sickness, as well as altered gravity. This theory was developed in the Affective Computing group at the MIT Media Lab and examines asymmetry in skin conductance signals from right and left wrists as differing metrics of emotional arousal and intensity. The parabolic flight configuration provides an inimitable circumstance to systematically analyze the evolution of these signals over the course of the repeated parabolic flight path. For example, we expect to see globally heightened stress and emotional arousal on the first pass, with maximal skin conductance peaks from both wrists just before the first moment of weightlessness. We expect these peaks to monotonically decrease over time with each pass, but to remain more elevated (relative to an individual’s baseline) for participants experiencing more self-reported nausea during flight. For individuals not experiencing extreme nausea, we expect to see a much higher skin conductance signal from their right wrists compared to their left (for right-handed participants) during the first few passes, with this difference decreasing steadily as the participant habituates to the flight pattern and sensations. </p><p>Note that NASA and other researchers—including the Boston-local scientists at the Ashton Graybiel Spatial Orientation Lab at Brandeis University—have investigated spatial orientation and motion sickness, but they are just beginning to add the use of physiological sensors to their work. Not only does this demonstrate that the proposed experiment is at the forefront of scientific inquiry, but it also facilitates potential collaboration with world-renowned experts in the Boston area!</p><p>In addition to sensor data, we intend to collect pre- and post-flight surveys recording participant reactions to different levels of gravity, including points at which they experienced nausea or discomfort. Pre-flight surveys will include nausea sensitivity metrics, designed to determine how likely a person is to feel nausea (i.e., separating those who feel carsick on a drive through town versus those who approach rollercoasters without hesitation). It will also ask about each participant’s feelings of anxiety, nausea, and excitement in anticipation of flying. Note that while these feelings may be experienced simultaneously, each one has a different effect on one’s physiology. </p><p>After the flight, we will ask participants to rank which sections of the flight (e.g., beginning, middle, end) prompted the greatest sensations of anxiety, nausea, and excitement and to what degree. We will also annotate the flight video recordings to denote periods of high anxiety, nausea, or excitement.</p><p>Then, we will use the survey, annotation, and sensor information to build a model that predicts when an individual might experience distress in altered gravity environments. This aspect of the study will leverage our research group’s unique expertise building machine learning algorithms for physiological data, but the results could have widespread impact. For example, such a system could be deployed to space travelers to help them monitor their physiology and anticipate or prevent feelings of discomfort during flight. As access to space travel becomes more pervasive, it is critical to understand the physiological effects of altered gravity on a population that does not solely include astronauts or specially trained individuals. Our models, along with the use of low-cost, commercially available sensors, would enable “space hacking” by tourists and other non-technical personnel, allowing them to measure and track their biosignals to achieve optimal wellness during space travel.&nbsp;</p>",,,2018-10-22 19:51:21.995,True,2017-05-11,Physio FreeFall,PUBLIC,,True,Affective Computing,False
spring,ktj,False,"<p>SPRING is a custom-built hardware and software platform for children with neuro-differences. The system automates data acquisition, optimizes learning progressions, and encourages social, cognitive, and motor development in a positive, personalized, child-led play environment. The quantitative data and developmental trajectories captured by this platform enable systematic, mutli-modal, long-term studies of different therapeutic and educational approaches to autism and other developmental disorders, as well as a better understanding of motivation, engagement, and learning for the general population.</p>",,--Choose Location,2019-04-19 17:26:39.652,True,2016-01-01,"SPRING: A Smart Platform for Research, Intervention, and Neurodevelopmental Growth",PUBLIC,,True,Affective Computing,False
cube-puzzles,ktj,False,"​<p><b>Cube Puzzles: A tangible platform for dynamic assessment of cognitive and psychomotor skills</b></p><p>Cognitive and psychomotor assessments are a vital scientific tool; however, they are resource intensive, can be biased by examiner presence and interaction, and can disadvantage individuals with neurodevelopmental differences such as autism or ADHD. We present a new tangible user interface, Cube Puzzles, that dynamically tracks placement and orientation of colored wooden blocks. The system not only automates data acquisition and captures real-time learning progressions, but it also enables customizable, motivating feedback and synchronization with biosensors.&nbsp;</p><p>We highlight the potential of the system with two user studies involving 22 participants. The first study (n=10) validates and visualizes the dynamic data capture of real-time user activity. The second study (n=12) explores Cube Puzzles as a gamified cognitive assessment platform by examining the effects of reinforcement on user performance, engagement, and physiological arousal. Our work has broad applications within educational and assessment domains, particularly for neurodiverse and elderly individuals.</p><p><b>Part of the SPRING system</b></p><p>Cube Puzzles is a new game-play module for SPRING. In this module, children move and orient colored blocks in order to match a displayed design. This module is inspired by the Block Design activity in the Wechsler Intelligence Scales (e.g., WAIS), quantifying visual-spatial orientation and motor skills. In free-play mode, Cube Puzzles also encourages open-ended design and play with geometric shapes and colors.&nbsp;&nbsp;</p>",,,2019-04-22 18:36:42.408,True,2017-01-09,Cube Puzzles,PUBLIC,,True,Affective Computing,False
pal-project-on-affinities-language,ktj,False,"<p>The Project on Affinities and Language (PAL) is designed to help us understand what happens in a child’s brain when they engage with their interests, passions, or hobbies – also known as “affinities.”</p>",,,2019-04-22 20:25:25.031,False,2018-03-01,PAL: Project on Affinities & Language,PUBLIC,,True,Affective Computing,False
reality-editor-20,benolds,False,"<p>The Reality Editor is a web browser for the physical world: Point your phone or tablet at a physical object and an interface pops up with information about that object as well as services related to that object. The Reality Editor platform is open and entirely based on web standards making it easy for anyone to create Reality Editor enabled objects as well as Reality Editor applications that integrate the physical and digital world in one experience.<br></p><p>Reality Editor version 2.0<br></p><p>&nbsp;<b><a href=""http://realityeditor.org"">Reality Editor &nbsp;version 2.0</a>&nbsp;</b>is now available for download and adds the following features:</p><ul><li><b>World Wide Web</b> conform content creation.<br></li><li><span style=""font-size: 18px;""><b>Spatial Search</b> -&nbsp;</span>Instantly browse through relevant information in the physical world around you. you to browse reality.<br></li><li><span style=""font-size: 18px;""><b>Bi-Directional AR</b> - A real-time interactions system.</span></li><li><b style=""font-size: 18px;"">Private and Decentralized</b><span style=""font-size: 18px;""> infrastructure for connecting the IoT objects.</span><br></li><li><span style=""font-size: 18px;""><b>Logic Crafting</b> - A visual programming language designed for Augmented Reality.<br></span></li></ul><p>The &nbsp;Reality Editor works on iOS and you can get it <a href=""https://itunes.apple.com/us/app/reality-editor/id997820179""><b>here</b></a>.&nbsp;<span style=""font-size: 18px;"">Try</span><span style=""font-size: 18px;"">&nbsp;it out with our <a href=""https://www.dropbox.com/s/3vd1d2v9bmm7e7s/Reality%20Editor.dmg?dl=1""><b>Starter App</b></a> and some Philips Hue Lights or the Lego WeDo 2.0. Learn more about Logic Crafting in our <a href=""http://realityeditor.org/getting-started/""><b>User Interface 101</b></a>.</span></p><p></p>",2017-12-31,,2018-10-11 18:46:45.692,True,2017-05-22,Reality Editor 2.0,PUBLIC,http://www.valentinheun.com,False,Fluid Interfaces,False
reality-editor,benolds,False,"<p>The Reality Editor is a new kind of tool for empowering you to connect and manipulate the functionality of physical objects. Just point the camera of your smartphone at an object and its invisible capabilities will become visible for you to edit. Drag a virtual line from one object to another and create a new relationship between these objects. With this simplicity, you are able to master the entire scope of connected objects.</p>",2017-06-30,--Choose Location,2018-04-30 14:19:37.322,True,2014-01-01,Reality Editor,PUBLIC,http://www.realityeditor.org,False,Fluid Interfaces,False
ambienbeat,yun_choi,False,"<p>&nbsp;We present<b> a mobile heart rate regulator</b>—ambienBeat—which provides <b>closed-loop ambient biofeedback via subliminal tactile stimulus</b> based on a user's heartbeat rate variability (HRV). We applied the principle of interpersonal physiological synchronization to achieve our goal of effortless regulation of HRV, which is tightly coupled with mental stress levels. ambienBeat provides various patterns of subliminal tactile stimuli, which mimics the feeling of a heartbeat pulse, to guide a user's HRV to resonate with its rhythmic, tactile patterns. The strength and rhythmic patterns of tactile stimulation are controlled to a level below the cognitive threshold of an individual's tactile sensitivity on their wrist so as to minimize task disturbance. Here we present an acoustically noiseless, soft voice-coil actuator to render the ambient tactile stimulus and present the system and implementation process. We evaluated our system by comparing it to ambient auditory and visual guidance. Results from the user study shows that the subliminal tactile stimulation was effective in guiding a user's HRV to resonate with ambienBeat to either calm or boost the heart rate using minimal cognitive load.&nbsp;</p>",2019-05-31,,2019-04-22 16:40:11.719,True,2019-01-01,ambienBeat: Mobile tactile biofeedback for subliminal heart rate rhythmic regulation,LAB-INSIDERS,,False,Conformable Decoders,False
remi,yun_choi,False,"<h2>Translating ambient sounds of moment into tangible and shareable memories through animated paper</h2><p>We present a tangible memory notebook—reMi—that records ambient sounds and translates them into a tangible and shareable memory using animated paper. The paper replays the recorded sounds and deforms its shape to generate synchronized motions with the sounds. Computer-mediated communication interfaces have allowed us to share, record, and recall memories easily through visual records. However, those digital visual-cues that are trapped behind the device’s 2D screen are not the only means to recall a memory we experienced with more than the sense of vision. To develop a new way to store, recall, and share a memory, we investigate how tangible motion of a paper that represents sound can enhance ""reminiscence.""</p>",2019-05-31,,2019-04-10 14:41:04.282,True,2017-10-30,reMi,PUBLIC,http://www.mallcong.com,False,Conformable Decoders,False
dancing-membrane,yun_choi,False,"<p>We present an interactive shape-changing display—Dancing Membrane—using the deformation of fabric and airflow control. To explore the new way of rendering an organic and natural experience with the shape-changing display, we have been experimenting with different types of fabric that can give soft textures. </p><p>In order to create and control the local deformation of fabric, we developed a 6-DOF variable diameter nozzle platform which enables us to control the direction and pressure of airflow.&nbsp;By controlling those, we were able to create variable sizes of fabric deformation and vibration of fabric with the computed simulation results. The computational model we created allowed us to predict the responsive dynamic motion of fabric to the airflow. For the next step, we hope to exhibit it as an interactive art installation—as well as a shape-changing display in general for games, projection mapping with organic textures, and create a library of different types of fabric interacting with airflow for a computational simulation model.</p>",,,2019-04-23 21:15:19.625,True,2018-07-01,Dancing Membrane: Controlling the local deformation of fabric for an interactive shape-changing display and its computational simulation,LAB-INSIDERS,,True,Conformable Decoders,False
respire-tangible-vital-sign-feedback-interface-for-self-regulation-of-respiration,yun_choi,False,"<p>We present a shape-changing interface—reSpire—that provides tangible feedback based on users’ vital signals through its volume change and pulse generation with the goal of helping them develop better <b>attention-to-breath (ATB)</b>, as well as to&nbsp;calm them and reduce stress. <b>Mindful breathing</b> has been identified as an effective technique&nbsp;<b>to reduce people’s stress level </b>by controlling their heart rate indirectly, known as <b>respiratory sinus arrhythmia (RSA)</b>. To explore this effective way to guide people to calmness, we developed a soft, cushion-like, shape-changing device that renders the chest motion by breathing and small pulses by a heartbeat, which provides tactile feedback with a feeling of comfort and intimacy. To monitor users' heart and respiration rates and guide them to reach the calm state by tangible interactions with reSpire, we also present the sensing and feedback mechanism.</p>",,,2019-04-11 18:19:16.225,True,2018-04-01,reSpire-M: Tangible vital sign feedback interface for self-regulation of respiration,LAB-INSIDERS,http://www.mallcong.com,True,Conformable Decoders,False
respire,yun_choi,False,"<p><b>""What happens if a static object starts to move and react to your gestures like a living creature?""</b></p><p><span style=""font-size: 18px; font-weight: 400;"">The pillow we know and use every night is not passive anymore: a</span>s an interaction between active and active materials, we have developed a soft pillow that can breathe with you.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">reSpire is a soft actuated pillow that breathes in synchronization with the user's respiration.&nbsp; With it, users can touch and feel their breath.</span></p><p>By projecting the breath pattern of the user onto the inflation motion of the neck pillow, reSpire not only enables the user to realize how they have been breathing, like an augmented prosthetic lung, but can also let a loved one feel your presence even when you are apart.</p>",,,2018-05-07 16:45:39.369,True,2018-02-01,reSpire: A soft actuated pillow synchronized with breath patterns for tactile telecommunication,LAB-INSIDERS,http://www.mallcong.com,True,Conformable Decoders,False
radio-days,anderton,False,"<p>Following the 2016 election, the entirety of the nation became conscious of its polarization. According to a study by the National Bureau of Economic Research*, polarization has increased among Americans since 1990. The study observes, however, that in eight of the nine measures of polarization, older individuals (70+ age group) show higher rates of increase in polarization than other age groups. This age group also utilizes social media less than other age-groups. Could it be that social media is not the root cause of polarization?</p><p>In order to explore this further, we looked at polarization through talk radio, which is commonly thought to have political influence.&nbsp;</p>",2018-12-31,,2019-04-11 15:26:19.908,True,2018-10-12,Radio Days,PUBLIC,,False,Viral Communications,False
nuestra-vista,anderton,False,,,,2019-04-17 19:53:30.162,False,2018-01-01,Nuestra Vista,PUBLIC,http://nv.mit.edu,True,Viral Communications,False
boycott,anderton,False,"<p>A web browser extension that reveals less well-known aspects of corporate public behavior such as environmental respect and political bias. When one engages in a search, we place an image next to the link to a corporate site that graphically reveals relevant information. It might be a donkey versus an elephant, or a measure of ""greenness.""&nbsp; We seed the system with public information and allow users to contribute to the database.&nbsp;</p><p>Data collected can be further explored and mapped out using data visualizations, allowing&nbsp; perception of network distributions and polarizations.</p>",2020-01-01,,2019-04-18 01:20:40.082,True,2018-10-01,Boycott!,PUBLIC,,True,Viral Communications,False
plusnudge,anderton,False,"<p>+nudge helps people to become their imagined future self one&nbsp;nudge&nbsp;at a time.</p><p>Distracted by all of the demands on our time from urgent notifications, reminders, and advertising on our phones and laptops, it is increasingly challenging to align our day to day actions with what we believe matters most inside. +nudge&nbsp;creates specifically tuned, subtle reminders throughout your day to reflect and be mindful of the things in life that really matter to you, and as a consequence assist you in making better, more holistic decisions.</p>",,,2019-04-19 16:09:59.016,True,2019-01-01,+nudge,PUBLIC,,True,Viral Communications,False
layer,anderton,False,"<p><b>layer</b> decentralizes recommendation systems and intersects third-party recommendations with your locally stored, personal information to result in privacy-respecting, relevant recommendations.</p><p>We envision growing the repertoire of personal information beyond purchase choices.</p>",,,2019-04-18 17:13:41.858,True,2019-01-01,layer,PUBLIC,,True,Viral Communications,False
what-s-america-listening-to,anderton,False,,,,2018-04-21 00:41:22.039,True,2017-06-01,What's America Listening To?,PUBLIC,,True,Viral Communications,False
superglue,anderton,False,"<p>SuperGlue is a core news research initiative that is a ""digestion system"" and metadata generator for mass media. An evolving set of analysis modules annotate 14 DirecTV live news broadcast channels as well as web pages and tweets. The video is archived and synchronized with the analysis. Currently, the system provides named-entity extraction, audio expression markers, face detectors, scene/edit point locators, excitement trackers, and thumbnail summarization. We use this to organize material for presentation, analysis, and summarization. SuperGlue supports other news-related experiments.</p><p>SuperGlue is a framework for media digestion and metadata generation. The digestion work flow also has applications for media more broadly including conversational ecommerce.</p>",,--Choose Location,2019-04-08 16:48:12.894,True,2014-09-01,SuperGlue,PUBLIC,,True,Viral Communications,False
newssense,anderton,False,"<p>How something is presented can be as important as the message itself. We use various artificial intelligence techniques to model the ""subcarriers of information"" present in a TV newscast, to automatically detect and understand visual and auditory cues beyond the spoken word including the layout of the set, the affect of the participants, the nature of the motion, and other cues. This would enable a broad-range, comprehensive analysis of <b>how news presentation is trying to shape the public debate</b>. Insights in this area are of vital importance in the age of political polarization and lead directly to applications that can break our bubbles.</p><p>See:&nbsp; viral.media.mit.edu/pub/tbd</p>",,,2019-04-18 16:50:14.749,False,2018-02-05,Unspoken News,PUBLIC,,True,Viral Communications,False
self-advertising,anderton,False,"<p>Self-Advertising&nbsp;reclaims and repurposes attention from&nbsp;advertising&nbsp;media online. As we use the Internet, we often feel advertising following us around, distracting our focus, and leading us down misaligned paths, or we block it out entirely. Here we explore repurposing the advertising space online to be aligned with our goals and desires.</p>",,,2019-06-04 19:35:02.989,True,2019-01-01,Self-Advertising,PUBLIC,,True,Viral Communications,False
search-intent-optimization,anderton,False,"<p>Search Intent Optimization unwinds SEO by externalizing intent. Users select their intent during a search’s initiation. We offer a set of example controls to distinguish between shopping and information access. Adding your intent suppresses irrelevant noise results—no more ""I'm feeling lucky.""</p>",,,2019-04-18 17:12:22.057,True,2019-01-01,Search Intent Optimization,PUBLIC,,True,Viral Communications,False
wall-of-now,anderton,False,"<p>Wall of Now is a multi-dimensional media browser of recent news items. It attempts to address our need to know everything by presenting a deliberately overwhelming amount of media, while simplifying the categorization of the content into single entities. Every column in the wall represents a different type of entity: people, countries, states, companies, and organizations. Each column contains the top-trending stories of that type in the last 24 hours. Pressing on an entity will reveal a stream of video that relates to that specific entity. The Wall of Now is a single-view experience that challenges previous perceptions of screen space utilization towards a future of extremely large, high-resolution displays.</p>",,--Choose Location,2019-04-05 20:29:14.403,True,2015-01-01,Wall of Now,PUBLIC,,True,Viral Communications,False
radio-days,hbedri,False,"<p>Following the 2016 election, the entirety of the nation became conscious of its polarization. According to a study by the National Bureau of Economic Research*, polarization has increased among Americans since 1990. The study observes, however, that in eight of the nine measures of polarization, older individuals (70+ age group) show higher rates of increase in polarization than other age groups. This age group also utilizes social media less than other age-groups. Could it be that social media is not the root cause of polarization?</p><p>In order to explore this further, we looked at polarization through talk radio, which is commonly thought to have political influence.&nbsp;</p>",2018-12-31,,2019-04-11 15:26:19.908,True,2018-10-12,Radio Days,PUBLIC,,False,Viral Communications,False
cocoverse,hbedri,False,<h2>Real-time collaborative self-expression in virtual reality&nbsp;</h2>,,,2019-04-17 20:08:49.312,True,2016-10-03,CocoVerse: A playground for co-creation and communication in virtual reality,PUBLIC,,True,Viral Communications,False
vr-physics-lab,hbedri,False,"<p>Room-scale virtual reality opens up exciting new possibilities for exploratory learning. Phenomena that otherwise cannot be experienced directly (e.g. subjects that are microscopic, remote, or dangerous) can be transformed into environments that are immersive, interactive and social. Electrostatic Playground is a VR physics lab where multiple users can explore and discover principles of electrostatics through experimentation.&nbsp;<span style=""font-size: 18px;"">It also concretizes abstract notions of electrostatics in the form of tangible, interactive objects. Users can learn by directly manipulating physics objects while receiving real-time feedback from the environment. We've incorporated the ability to record these interactions in order to provide a means of authoring content, reviewing one's notes, and teaching others. Electrostatic Playground is a multi-user lab where users can explore and discover principles in electrostatics.</span><br></p>",,,2019-04-17 20:09:56.460,True,2016-06-01,Electrostatic Playground: A multi-user virtual reality physics learning experience,PUBLIC,,True,Viral Communications,False
what-s-america-listening-to,hbedri,False,,,,2018-04-21 00:41:22.039,True,2017-06-01,What's America Listening To?,PUBLIC,,True,Viral Communications,False
viralcasting,hbedri,False,"<p>Ditch the truck. Live, collaborative broadcasting through mixed reality.</p>",,,2019-04-18 01:30:20.552,True,2016-09-06,Broadercasting,PUBLIC,,True,Viral Communications,False
let-s-see-a-game,hbedri,False,"<p><b>In ""Let's see a game!"" we&nbsp;expose the different perspectives in&nbsp;TV&nbsp;sports and news in order to build broadcasting systems that unify rather than divide. We use the galvanizing impact of sports and live events as a forum, and then we add production and viewing opportunities to distinguish fact from opinion and to challenge the basis of those opinions.</b></p><p>In 1951, when the Dartmouth football team played against Princeton, there was deep disagreement between the two schools as to what had happened during the game. In ""They Saw a Game: A Case Study,"" the psychologists Albert Hastorf and Hadley Cantril found that when the <i>same </i>motion picture of the game was shown to a sample of undergraduates at each school,&nbsp; each individual <i>perceived a different game</i>, and their versions of the game was just as ""real"" as other versions were to other people.&nbsp;</p><p><b>However,&nbsp; little is known about whether and how broadcasting media are adding fuel to the fire. In order to study the relationship between storytelling/perspectives and opinion formation, we built the following two applications: ""Let's see a game!"" and ""Let's watch news!""</b></p><p>In the first step, we built an interactive application&nbsp;that exposes different perspectives in sports broadcasting.&nbsp;The application plays two broadcasts of the same game, created for each team's home audience.&nbsp;The user can tune into an audio channel by moving the slider. Additional buttons allow the user to take other actions.</p>",,,2019-04-18 14:04:57.014,True,2017-09-11,Let's see a game!,PUBLIC,,True,Viral Communications,False
holonews,hbedri,False,"<p>&nbsp;News reporting today suffers from sensationalism. News agencies are constantly fighting for attention and clicks, leading to headlines and photos that exaggerate a single perspective.&nbsp;</p><p>What if you could get a full perspective on certain news topics by exploring the news in AR? The spatial nature of AR allows a user to gain a more complete perspective on a story.&nbsp; Having a constant holographic widget on your desk also allows you to follow developing stories, such as a foreign conflict. In addition, the interactive nature of AR means that users can explore the news in a delightful way.</p>",,,2018-10-22 16:52:05.905,True,2017-09-28,Holonews,PUBLIC,,True,Viral Communications,False
mixed-mode-systems-in-disaster-response,bl00,False,"<p>Pure networks and pure hierarchies both have distinct strengths and weaknesses. These become glaringly apparent during disaster response. By combining these modes, their strengths (predictability, accountability, appropriateness, adaptability) can be optimized, and their weaknesses (fragility, inadequate resources) can be compensated for. Bridging these two worlds is not merely a technical challenge, but also a social issue.</p>",2016-01-01,--Choose Location,2016-12-05 00:17:03.349,True,2015-01-01,Mixed-Mode Systems in Disaster Response,PUBLIC,,False,Civic Media,False
framework-for-consent-policies,bl00,False,"<p>This checklist is designed to help projects that include an element of data collection to develop appropriate consent policies and practices. The checklist can be especially useful for projects that use digital or mobile tools to collect, store, or publish data, yet understand the importance of seeking the informed consent of individuals involved (the data subjects). This checklist does not address the additional considerations necessary when obtaining the consent of groups or communities, nor how to approach consent in situations where there is no connection to the data subject. This checklist is intended for use by project coordinators, and can ground conversations with management and project staff in order to identify risks and mitigation strategies during project design or implementation. It should ideally be used with the input of data subjects.</p>",2015-01-01,--Choose Location,2016-12-05 00:17:12.642,True,2014-01-01,Framework for Consent Policies,PUBLIC,,False,Civic Media,False
micronauts,carolx,False,"<p>Microbes are the foundation upon which life on Earth depends: they set the boundaries of habitability for all plants and animals and create half of the oxygen we breathe. Ocean-dwelling microbes regulate the global climate and could hold the secrets to the origin of life. Put simply, we wouldn’t be here without microbes, yet most people don't realize how ubiquitous and important they are.<br>&nbsp;<br>The Micronauts project sought to build an emotional bridge over this gap. Microscopic creatures are, by definition, typically hidden from view, and the challenge of seeing them and perceiving their importance prevents emotional involvement and investment.&nbsp;We built an interactive installation that invites people from a general audience to step into the&nbsp;microscopic world. Visitors who step into the interactive projection represent sources of food or light, which allow for the microorganisms to live and reproduce. The reproduction and death rates for the visualization are based on mathematical models generated from the analysis of samples from Cape Cod. This kind of interactive interface has a special appeal to introduce children to Science concepts that are more abstract or far from their everyday experience. The exhibit happened during the&nbsp;<a href=""https://www.media.mit.edu/events/allhandsondeck/"">2018 National Ocean Exploration Forum</a>&nbsp;on November 8 and 9, at the MIT Media Lab.</p><h1>Team Members</h1><ul><li>Jeffrey Marlow (Harvard)</li><li>Benjamin Bray (MIT Sea Grant)</li><li>Keith Ellenbogen (MIT Sea Grant)</li><li>Craig McLean (MIT/WHOI Joint Program)</li><li>Raquel Fornasaro (Contemporary Artist)</li><li>Mark Adams (National Park Service/Painter)</li><li><a href=""https://www.media.mit.edu/people/carolx/overview/"">Caroline Rozendo</a>&nbsp;(MIT Media Lab, Object-Based Media)</li></ul>",2018-12-31,,2019-04-22 16:59:56.008,True,2018-07-01,Micronauts,PUBLIC,,False,Object Based Media,False
perform-1,carolx,False,"<p>PerForm explores the intuitive meanings associated with the shape of objects, and how a shape-changing tool can allow for different forms of tangible interaction.&nbsp;Is it possible to map how ideas feel, and use the connections between senses to create more intuitive interfaces?&nbsp;PerForm addresses that question by allowing users to transform a physical tool&nbsp;to fit their intentions. This way, a user can play different musical instruments or take different actions in games, simply by varying the shape of the tool. Since the meanings associated with the shapes would be dependent on context, we are giving special focus to studying possible mappings of between the perception of sound and shape.</p><p><b>SOUND-SHAPE CORRESPONDENCES</b></p><p><b>“<i>Music is not limited to the world of sound. There exists a music of the visual world.</i>”</b></p><p><b>—Oskar Fischinger, 1951.</b></p><p>When the German-American animator and filmmaker Oskar Fischinger created musically inspired animations and works of art, he touched on the intuitive associations our minds make between all the different sensory stimuli received from the environment.&nbsp;There is strong evidence that our brains forge relationships between shapes and seemingly corresponding sounds.</p><p>PerForm explores how the associations between visual and auditory perception can be used in interaction design. We developed a physical interface that users can transform by bending to create geometric shapes or symbols.&nbsp;By investigating possible correlations, natural or forged, between perceptual components of shape and its correlates in sound, we enable the&nbsp;tool to become a new instrument, with different sound timbre depending on the geometry of the object.</p><p><b>A SHAPE-SHIFTING GAMING CONTROLLER</b></p><p>One of the applications of this shape-shifting device would be to enable different modes of interaction through changes in shape. Instead of having to buy multiple controller devices for each genre of gaming or kind of interaction, or simply using a single, fixed-form controller that limits the embodied experience, a device capable of transformation would enable users to have a more imaginative and creative gaming experience, even enabling new kinds of games in which the user can invent tools by varying shapes.</p>",2019-09-01,,2019-04-18 14:46:14.807,True,2018-05-01,PerForm,PUBLIC,,True,Object Based Media,False
hybrid-radios,nicolelh,False,"<p>Hybrid Radio: A parasitic molecular infrastructure</p><p>This work opens a dialogue around the possibilities of re-thinking radio communication as an open tool for transmitting and receiving in order to create streams for civic communication, engagement, and expression. This text explores the history of radio, as well as&nbsp;free radio&nbsp;theories around the world, and proposes to re-appropriate the space of the airwaves that has been drastically regulated, privatized and institutionalized. Radio acts as an invisible and mobile architecture, having the characteristic of breaking down boundaries, territories, and walls. Understanding radio as a parasitic system can provide a setting to grow in an organic and molecular way. The objective is to explore the potentials in radio infrastructure, its invisibility and the possible ways of using it to foster expression, and trigger discussions about decentralized communication networks and open streams of coexistence.</p><p>Hybrid Radio was curated by Nomeda and Gediminas Urbonas for the&nbsp;<a href=""http://www.swamp.lt/"">Swamp Pavilion</a>&nbsp;(Lithuanian Pavilion) at the&nbsp;<a href=""http://www.labiennale.org/en"">Venice Architecture Biennale 2018</a></p><p>Tardigrade Radio, The Radio for Almost Invisible Beings</p><p>This project is in the form of an installation and a series of community engagement and hybrid-radio building workshops. This project has the objective of deploying an open radio infrastructure and a layered invisible architecture of sound in the city. By combining sonic compositions and narratives from an interspecies perspective, the Tardigrade Radio works as a platform to give voice to different organisms (micro/macro/meta), humans, non-human agents, and matter.</p><p>This piece proposes to re-appropriate the airwaves space, which has been drastically regulated, privatized and institutionalized. Radio pieces are caught by a “parasitic radio receiver module” built with biolab tools an installed in public spaces. Sonic compositions and narratives are taken from the inside of research laboratories to common spaces, blurring the boundaries between inside and outside, lab and nature, fiction and reality. The first installation consists of 1 “parasitic radio receiver module” and three sonic pieces inspired by the Lab Tardigrade’s story and environment. These compositions are meant to be absorbed in a non-linear narrative dispersed on space. Future iterations consider the installation of many “parasitic radio receiver modules,"" creating a large-scale choreography of radio transmitters and receivers. The piece uses radio as a spatial medium for mobile spaces, transversal structures, and build layered invisible architectures.</p>",2018-11-30,,2018-10-19 19:28:33.774,True,2017-11-01,Hybrid Radios,PUBLIC,,False,Opera of the Future,False
spaces-that-perform-themselves-1,nicolelh,False,"<p>As we generally experience on earth, there is no space without sound and there is no sound without space. Building on the understanding of music and architecture as creators of spatial experience, this project presents a novel way of unfolding music’s spatial qualities in the physical world.&nbsp;<i>Spaces that Perform Themselves&nbsp;</i>exposes an innovative response to the current relationship between sound and space: where we build static spaces to contain dynamic sounds. What if we change the static parameter of the spaces and start building dynamic spaces to contain dynamic sounds?&nbsp;</p><p>A multi-sensory kinetic architectural system is built in order to augment our sonic perception through a cross-modal spatial choreography that combines sound, movement, light, color, and vibration. By breaking down boundaries between music and architecture, possibilities of a new typology that morphs responsively with a musical piece can be explored. As a result,&nbsp;spatial&nbsp;and musical composition can exist as one synchronous entity. These spatial choreographies build up the scenario to study the possible relationships between a human body and a robotic architectural body, throughout a dance of perception and matter.&nbsp;</p><p>This project seeks to contribute a novel perspective on leveraging technology, art, science, and design to provide a setting to enrich and augment the way we relate to the built environment. The objective is to enhance our perception and challenge models of thinking by presenting a post-humanistic phenomenological encounter of the world.</p>",,,2019-04-17 20:06:12.678,True,2016-09-01,Spaces that Perform Themselves,PUBLIC,,True,Opera of the Future,False
the-telemetron,nicolelh,False,"<p>Today, the environments that humans occupy in space are designed for survival. Humans are carefully shuttled to and from space, and during their relatively short stays, they are provided with minimum supplies to remain alive and able to perform experiments. As we begin to plan less for short visits and more for life in space (such as a six to eight month trip to Mars and beyond) the question becomes: What does human culture look like in space?</p><p><a href=""https://www.instagram.com/nico_lh/"">Nicole L'Huillier</a> and <a href=""https://www.instagram.com/sandsfish"">Sands Fish</a> decided to explore how design and creativity might evolve as we begin to do more than merely survive in space. <b>The Telemetron</b> is a unique mode of musical performance that takes advantage of the poetics of zero gravity, and opens a new field of musical creativity. The project attempts to expand expression beyond the limits of earth-based instruments and performers. Leveraging sensors, data transmission and capture (for performance after flight), as well as their experience as composers and performers, Sands and Nicole explore a new body language for music. </p><p>The Telemetron was played for the first time during the inaugural Media Lab Space Exploration Initiative's Zero G flight. This instrument is a clear dodecahedron chamber that contains customized ""chimes"" containing gyroscopes. The chimes emit their telemetry as they spin and collide. Sensors record the position, direction, and spin of each chime. These elements create the composition. The performers play the instrument by moving it in space, shaking it, colliding it. The performance can be recorded to be experienced on earth or used as a live instrument during future space flights. The instrument can be played inside space craft or in the vacuum of space without the benefit of sound waves.</p><p>Recorded as a beautiful audio-visual experience, this experiment opens the doors for new forms of creative expression, and brings the magic of space to musicians. We hope to reach beyond the utilitarian, and toward the inspiring.</p>",,,2018-08-20 20:13:42.164,True,2017-08-01,The Telemetron,PUBLIC,,True,Opera of the Future,False
ritual-i-the-thing-itself,nicolelh,False,"<p>Ritual&nbsp;I: The Thing Itself consists of a choreographed robotic body that is in constant flux. It performs a dance of repetitive patterns that&nbsp;become&nbsp;a trance ritual of vibrations and movement. The thing or dancing body stands on a metal sheet that vibrates with every move it makes; this way the body affects its territory with every movement. In return, the vibrations of the metal&nbsp;add&nbsp;to the vibration of the thing itself while it moves, and in this way the body is affected by its territory.&nbsp;</p><p>This is a feedback system, a cyclic loop, a transduction network, a ritual dance between a body and its territory. This ritual explores how agency becomes increasingly distributed among bodies and territories, which opens interactions of hybrid selves, blurring the limits of bodies and its environment, understanding them all as an assemblage of vibrant matter.&nbsp;The architecture&nbsp;comprises complex assemblages—nothing is something by itself, but things are themselves by being in&nbsp;a relationship&nbsp;with others. This is an entangled architecture of bodies. This is a way to explore and diversify the imaginative projections and potentials of a kinetic non-human body and how sound and vibration are key to trigger agency and vibrant presence.</p><blockquote>It is the thing itself that has been allowed to be deployed as multiple, and thus allowed to be grasped through different viewpoints, before being possibly unified in some later stage depending on the abilities of the collective to unify them.<br></blockquote><p>—Bruno Latour,&nbsp;<i>Assembling the Social: An Introduction to Actor-Network-Theory</i>&nbsp;(Oxford: Oxford University Press, 2005), 116.&nbsp;</p>",,,2018-10-19 19:41:24.966,True,2018-05-01,Ritual I: The Thing Itself,PUBLIC,,True,Opera of the Future,False
the-telemetron-adventures,nicolelh,False,"<p>The&nbsp;Telemetron&nbsp;is a musical instrument specially designed to be performed in microgravity environments. It is created to explore the poetics of movement in outer space and the relational aspects of an antigravitational performance between human and non-human bodies. Through this line of work, we explore how the creation of culture might evolve as we leave Earth. The Telemetron project proposes a space in space for everybody - a space to share, to create, to listen.</p><p>During the summer of 2018,&nbsp; the Telemetron was presented on different occasions:</p><p>The Telemetron was exhibited&nbsp;at <a href=""https://ars.electronica.art/news/en/"">Ars Electronica</a>&nbsp;as part of the exhibition <a href=""https://ars.electronica.art/error/en/a-glitch-in-the-stars/"">""A Glitch in the Stars""</a> curated by the MIT Media Lab Space Exploration Initiative. Nicole L'Huillier designed the exhibition along with Sands Fish and Xin Liu.</p><p>The Telemetron was featured at <a href=""https://sonarplusd.com/"">Sónar+D</a>, a festival that explores how creativity is changing our present and imagining new futures. Nicole was invited to speak in the <a href=""https://sonarplusd.com/en/programs/barcelona-2018/areas/talks/making-music-in-space"">""Making Music in Space""</a> panel. She also gave a workshop called <a href=""https://sonarplusd.com/en/programs/barcelona-2018/areas/workshops/antigravitational-luthiers"">""Antigravitational Luthiers""</a>. Also, the Telemetron was part of <a href=""https://sonarplusd.com/en/programs/barcelona-2018/areas/the-zero-gravity-band/the-zero-gravity-band"">The Zero Gravity Band</a> Exhibition.</p><p>We published the paper <a href=""http://www.nime.org/proceedings/2018/nime2018_paper0066.pdf"">“Telemetron: a musical instrument for performance in zero gravity”</a> at <a href=""http://nime2018.icat.vt.edu/"">NIME</a>, and Sands Fish presented it at the international conference on New Interfaces for Musical Expression, describing the technical design of our first Telemetron.&nbsp;</p><p>Nicole also gave a talk about the Telemetron at the<a href=""https://www.eventrid.cl/eventos/atenea/en-orbita-2018""> En Orbita Festival</a> in NYC.</p><p>The Telemetron was created by Nicole L'Huillier and Sands Fish. With the assistance of Thomas Sanchez Lengeling, Sarah Hua, and Matt Carney. It was created on the context of the Space Exploration Initiative first zero gravity research flight. We are currently working on more space instruments, stay tuned.</p>",,,2019-02-14 19:38:59.554,True,2018-06-13,The Telemetron Adventures,PUBLIC,,True,Opera of the Future,False
spaces-perform-themselves,nicolelh,False,"<p>As we generally experience on earth, there is no space without sound and there is no sound without space. Building on the understanding of music and architecture as creators of spatial experience, this project presents a novel way of unfolding music’s spatial qualities in the physical world.&nbsp;<i>Spaces That Perform Themselves&nbsp;</i>exposes an innovative response to the current relationship between sound and space: where we build static spaces to contain dynamic sounds. What if we change the static parameter of the spaces and start building dynamic spaces to contain dynamic sounds?&nbsp;</p><p>A multi-sensory kinetic architectural system is built in order to augment our sonic perception through a cross-modal spatial choreography that combines sound, movement, light, color, and vibration. By breaking down boundaries between music and architecture, possibilities of a new typology that morphs responsively with a musical piece can be explored. As a result,&nbsp;spatial&nbsp;and musical composition can exist as one synchronous entity. These spatial choreographies build up the scenario to study the possible relationships between a human body and a robotic architectural body, throughout a dance of perception and matter.&nbsp;</p><p>This project seeks to contribute a novel perspective on leveraging technology, art, science, and design to provide a setting to enrich and augment the way we relate to the built environment. The objective is to enhance our perception and challenge models of thinking by presenting a post-humanistic phenomenological encounter of the world.</p><p><i>Update</i>: A trigger system has been implemented to enable users to play the cube in real time and explore interactive ways to compose spaces with a palette of movements, sounds, light, color, and vibration.</p>",,,2019-01-30 16:50:51.633,True,2017-04-02,Spaces that Perform Themselves,PUBLIC,,True,Opera of the Future,False
diastrophisms,nicolelh,False,"<p><br>Diastrophisms is a sound installation with a modular system that sends images through rhythmic patterns. It is built on a set of debris from the Alto Río building that was destroyed by the 27F earthquake in 2010 in Chile. With&nbsp;&nbsp;Diastrophisms we were looking for a poetical, critical and political crossing between technology and matter, in order to raise questions about the relationship between human beings and nature, and to consider the construction of memory in a community by questioning the notion of monument, as well as to imagine new forms of communication in times of crisis.</p><p>Work by:&nbsp;Nicole L’Huillier, Thomas Sanchez Lengeling, and Yasushi Sakai</p><p>Exhibited at Siggraph Art Gallery 2018,&nbsp;curated by Andres Burbano. A&nbsp;paper about this work was published&nbsp;in Leonardo Journal for the special edition of Siggraph 2018 Art Papers and Art Gallery Exhibition. The paper was written by Nicole L’Huillier and Valentina Montero.</p><p>Diastrophisms&nbsp;was also exhibited as<a href=""http://www.bienaldeartesmediales.cl/13/obra/talking-rock/""> ""Diastrofismos""</a> at the <a href=""http://www.bienaldeartesmediales.cl/13/"">Media Arts Bienal,</a> Santiago de Chile, 2017, curated by Valentina Montero.</p>",,,2019-02-14 19:56:31.323,True,2017-10-01,Diastrophisms,PUBLIC,,True,Opera of the Future,False
myths-of-the-cosmos-indigenous-cosmologies,nicolelh,False,,,,2019-05-30 15:05:50.758,True,2018-01-01,Myths of the Cosmos: Indigenous Cosmologies,LAB,,True,Opera of the Future,False
holobits,afuste,False,"<p>The onset of Mixed Reality as a platform offers the opportunity to create new, playful paradigms for building and fostering creativity. The Holobits application leverages the tried and tested features of physical block building platforms like LEGO and introduces the benefits of building in mixed environments to support making and storytelling. The proposed system combines the hand tracking capabilities of the Leap Motion with the spatial mapping of Hololens to enable hands-on building experiences with virtual blocks, denoted as “bits.” These blocks have different attributes and characteristics that determine how they look and behave within the mixed reality building space. The platform also allows users to share their creative building process in a frame-by-frame fashion that enables remixing and reflection on every play session. Holobits allows users to record their interactions with their creations to make animated environments with ease and support storytelling. Another way to enable collaboration is to let kids share models (or download someone else's creation in your space), allowing multiple users to build in a shared physical space or over distances, where a player can “hop” into the physical space of the Hololens user using virtual reality. Last but not least, we intend to integrate the Scratch visual programming toolkit into the Holobits platform to allow users to orchestrate their virtual creations and create the ultimate interactive stories.</p>",2018-05-31,,2018-11-30 17:21:40.954,True,2017-02-13,Holobits: Creativity and fun in Mixed Reality,PUBLIC,,False,Speech + Mobility,False
hypercubes,afuste,False,"<p>HyperCubes is an augmented reality platform to help children understand computational concepts drawn from their physical surroundings, from their most immediate and tangible reality. Children become creators and learn while tinkering with commands such as transformations in space that control little characters or geometry shapes.&nbsp;</p>",2018-12-31,,2018-10-18 01:45:06.375,True,2018-05-07,HyperCubes: Learning computational concepts in Augmented Reality,PUBLIC,,False,Speech + Mobility,False
deep-reality,afuste,False,"<p>We present an interactive virtual reality (VR) experience&nbsp;that uses biometric information for reflection and relaxation.&nbsp;We monitor in real-time brain activity using a modified version&nbsp;of the Muse EEG and track heart rate (HR) and electro&nbsp;dermal activity (EDA) using an Empatica E4 wristband. We&nbsp;use this data to procedurally generate 3D creatures and&nbsp;change the lighting of the environment to reflect the internal&nbsp;state of the viewer in a set of visuals depicting an underwater&nbsp;audiovisual composition. These 3D creatures are&nbsp;created to unconsciously influence the body signals of the&nbsp;observer via subtle pulses of light, movement and sound.&nbsp;We aim to decrease heart rate and respiration by subtle,&nbsp;almost imperceptible light flickering, sound pulsations and&nbsp;slow movements of these creatures to increase relaxation.</p>",,,2019-05-09 19:18:17.333,True,2017-04-01,"Deep Reality: An underwater VR experience to promote relaxation by unconscious HR, EDA and brain activity biofeedback",PUBLIC,,True,Speech + Mobility,False
artextiles,afuste,False,"<p>Abstract data visualizations for enhancing social&nbsp;<span style=""font-size: 18px; font-weight: normal;"">interactions through clothing and augmented reality.</span></p>",,,2018-04-18 18:33:06.161,True,2016-09-01,ARTextiles: Promoting Social Interactions Around Personal Interests,PUBLIC,http://annafuste.com,True,Speech + Mobility,False
inertia,afuste,False,"<p>Inertia is a platform for exploring physical interactions between real active agents and virtual elements in an augmented reality environment. The user is able to interact with a tangible active element that reacts to collisions and forces from virtual objects displayed in augmented reality. Physical forces and collisions in real-time can be better understood when applied to a tangible object, raising possibilities for learning and gaming.</p>",,,2018-04-23 18:30:20.455,True,2017-01-01,Inertia,PUBLIC,http://annafuste.com,True,Speech + Mobility,False
holobits,pe25171,False,"<p>The onset of Mixed Reality as a platform offers the opportunity to create new, playful paradigms for building and fostering creativity. The Holobits application leverages the tried and tested features of physical block building platforms like LEGO and introduces the benefits of building in mixed environments to support making and storytelling. The proposed system combines the hand tracking capabilities of the Leap Motion with the spatial mapping of Hololens to enable hands-on building experiences with virtual blocks, denoted as “bits.” These blocks have different attributes and characteristics that determine how they look and behave within the mixed reality building space. The platform also allows users to share their creative building process in a frame-by-frame fashion that enables remixing and reflection on every play session. Holobits allows users to record their interactions with their creations to make animated environments with ease and support storytelling. Another way to enable collaboration is to let kids share models (or download someone else's creation in your space), allowing multiple users to build in a shared physical space or over distances, where a player can “hop” into the physical space of the Hololens user using virtual reality. Last but not least, we intend to integrate the Scratch visual programming toolkit into the Holobits platform to allow users to orchestrate their virtual creations and create the ultimate interactive stories.</p>",2018-05-31,,2018-11-30 17:21:40.954,True,2017-02-13,Holobits: Creativity and fun in Mixed Reality,PUBLIC,,False,Object Based Media,False
hoverband,pe25171,False,"<p>We designed, implemented, and tested a proof of concept, wrist-based wearable object identification system which allows users to “hover” their hands over objects of interest and gain access to contextual information that is tied to them, through an intelligent personal assistant. The system uses a fusion of sensors to be able to perform the identification of an object under a variety of conditions. Among these sensors, there is a camera (operating in the visible and infrared spectrum), a small solid-state radar, and multi-spectral light spectroscopy sensors. Users can interact with contextual information tied to an object through conversations with an intelligent assistant to permit a hands-free, non-obtrusive, and discreet experience. The system explores audio interfacing with augmented reality content without the hassle of phones or head-mounted devices.</p>",,,2019-03-05 18:50:14.679,True,2017-09-01,Hover,PUBLIC,,True,Object Based Media,False
the-8k-data-manipulator,pe25171,False,"<p>The 8K Data Manipulator is a Unity game engine application which harnesses the gesture recognition and limb tracking available through Microsoft Kinect and combines it with the multitouch capability of our 8K display. Using either gesture at a distance or close-up touch, a user can rotate, zoom, and slice a very large graphical dataset. The 8K Data Manipulator is currently being used to visualize seismic data; however, it is capable of loading an arbitrary point cloud or volume, supporting medical, terrestrial, biochemical, social network, design, and other applications.</p>",,,2019-03-05 18:52:23.376,True,2016-09-15,The 8K Data Manipulator,PUBLIC,,True,Object Based Media,False
where-s-pedro,pe25171,False,"<p>An 85-inch television screen combined with 8K resolution provides a tool through which multiple users can perform quick, highly detailed, visual analysis of a dataset to identify anomalies.&nbsp; In order to test this, we developed a fully 3D application similar to the popular picture game Where’s Waldo.&nbsp; In the Where’s Waldo game, users must find a person (Waldo) within a densely-populated picture.&nbsp; In the developed application, users utilize touch and gestures to navigate a large 3D city in which a person of a varying size is placed randomly.&nbsp; The application serves as a proof of concept that multiple users can interact with one large, high resolution dataset to visually find anomalies.&nbsp; &nbsp;&nbsp;</p>",,,2019-04-16 16:41:36.312,True,2017-01-07,Where's Pedro?,PUBLIC,,True,Object Based Media,False
holobits,minakhan,False,"<p>The onset of Mixed Reality as a platform offers the opportunity to create new, playful paradigms for building and fostering creativity. The Holobits application leverages the tried and tested features of physical block building platforms like LEGO and introduces the benefits of building in mixed environments to support making and storytelling. The proposed system combines the hand tracking capabilities of the Leap Motion with the spatial mapping of Hololens to enable hands-on building experiences with virtual blocks, denoted as “bits.” These blocks have different attributes and characteristics that determine how they look and behave within the mixed reality building space. The platform also allows users to share their creative building process in a frame-by-frame fashion that enables remixing and reflection on every play session. Holobits allows users to record their interactions with their creations to make animated environments with ease and support storytelling. Another way to enable collaboration is to let kids share models (or download someone else's creation in your space), allowing multiple users to build in a shared physical space or over distances, where a player can “hop” into the physical space of the Hololens user using virtual reality. Last but not least, we intend to integrate the Scratch visual programming toolkit into the Holobits platform to allow users to orchestrate their virtual creations and create the ultimate interactive stories.</p>",2018-05-31,,2018-11-30 17:21:40.954,True,2017-02-13,Holobits: Creativity and fun in Mixed Reality,PUBLIC,,False,Fluid Interfaces,False
pal,minakhan,False,"<p>PAL (Personalized Active Learner) is a <i>wearable</i> system&nbsp;with on-device machine learning&nbsp;to help users with&nbsp;<i>real-time, personalized,</i> and <i>context-aware</i> <b>memory augmentation</b>,<b> language learning, </b>and<b> self-awareness</b>.</p>",,,2019-01-31 17:17:43.024,True,2018-06-01,PAL,PUBLIC,,True,Fluid Interfaces,False
mathland,minakhan,False,"<p>Mathematical experiences are intrinsic to our everyday lives, yet mathematics education is mostly confined to textbooks. Seymour Papert used the term ""Mathland"" to propose a world where one would learn mathematics as naturally as one learns French while growing up in France. We built a mixed reality application that augments the physical world with interactive mathematical concepts and annotations to create a real-life Mathland. Using Mathland, people can collaboratively explore, experience, and experiment with mathematical phenomena in their real, physical environments using tangible objects. Mathland opens up new opportunities for mathematical learning using Papert's constructionist principles in an immersive environment that affords situated learning, embodied interaction and playful constructionism.</p>",,,2019-04-17 20:11:43.769,True,2017-03-08,Mathland: Play with math in mixed reality,PUBLIC,,True,Fluid Interfaces,False
wearable-wisdom,minakhan,False,"<p>Having good mentors and role models is important for personal growth. Knowledge, advice, and inspiration from people a person admires can help motivate people when making life choices. Empirical research has shown that having a role model and insights from a mentor can positively affect the performance and progression of a person's career. However, such advice is not always available from the right people at the right time. Some of our personal heroes have passed away leaving only their writings and other artifacts. We present Wearable Wisdom, a context-aware, audio-based system in a glasses form factor for mediating wisdom from personal mentors to users. Our novel system offers just-in-time knowledge, advice, and inspiration from these mentors, based on the user's inquiry and current context. It does so by performing automated semantic analysis of the mentors' written text and selecting the most relevant quote to the user's inquiry.&nbsp;</p>",,,2019-04-28 03:26:25.734,True,2018-10-18,Wearable Wisdom,PUBLIC,,True,Fluid Interfaces,False
solar-micro-mining,aekblaw,False,"<p>Bitcoin generates net-new value from ""mining"" in a distributed network. In this work, we explore solar micro-mining rigs that transform excess energy capacity from renewable energy (hard to trade) into money (fungible). Each rig runs a small Bitcoin miner and produces Bitcoin dust for micropayments. We envision these micro-miners populating a highly distributed network, across rooftops, billboards, and other outdoor spaces. Where systematic or environmental restrictions limit the ability to freely trade the underlying commodity, micro-mining produces new economic viability. Renewable energy-based, micropayment mining systems can broaden financial inclusion in the Bitcoin network, particularly among populations that need a currency for temporary store of value and must rely on flexible electricity off the grid (e.g., unbanked populations in the developing world). This exploration seeds a longer-term goal to enable open access to digital currency via account-free infrastructure for the public good.</p>",2017-06-06,--Choose Location,2018-10-08 01:32:51.238,True,2015-09-01,Solar Micro-Mining,PUBLIC,,False,Responsive Environments,False
spacehuman,aekblaw,False,"<p>SpaceHuman is a soft robotics device designed to facilitate the exploration of environments with reduced gravity in a view of democratization and openness towards access to space and its exploration. &nbsp;It is based on the idea that one day, people who have not received a long preparation and training, as happens today with the astronauts, will be able to have access to the space having a type of conformation and physical configuration that is not adapted to this kind of setting.&nbsp;</p><p>The analysis of the unique seahorse's tail structure became the insight of the overall biomimetic design process. In fact, seahorse tail movement, gripping and protection to the seahorse while floating.&nbsp;Moreover, seahorses do not use their tails to swim; instead, they use them to grasp objects in their environment while they camouflage to hide from predators and hunts for prey. Flexibility and resiliency are key features that enable these behaviours.</p><p>SpaceHuman is an additive prosthesis or otherwise definable as a ""supernumerary robot."" SpaceHuman will facilitate the use of space in zero gravity or reduced gravity restoring the right motion and balance of our body and assigning a new function to a part of our body that until now has not been fully exploited except for the transport of loads, our back. Users will thus be able to build a new poetics of the body and its movements within this radically different space through SpaceHuman, creating new scenarios of its application. Through air chambers specifically designed to be able to change their shape and bend along a reinforcing rib of the material, the people who will use SpaceHuman will be able to cling to useful surfaces inside orbital housing or in Lunar or Martian villages.&nbsp;</p>",,,2019-05-13 21:47:19.873,True,2018-08-01,SpaceHuman,PUBLIC,,True,Responsive Environments,False
space-health,aekblaw,False,"<p>The MIT Media Lab Space Exploration Initiative is pushing the boundaries of space exploration innovation across the Media Lab’s many healthcare-focused and healthcare-adjacent research thrusts. The program focuses on the protection, long-duration preservation, and predictive adaptation of life beyond the earthbound, integrated throughout the Space Exploration Initiative’s portfolio.&nbsp;This research platform builds towards a near future where space-faring humans can both survive and thrive wherever the future takes us—back and forth to the surface of Earth, to low Earth orbit, to the moon, Mars, and beyond.&nbsp;</p><p>In addition to technology for the sake of deep space exploration, we note a dual opportunity for the technologies developed as part of this program to enhance and improve healthcare treatment regimens on Earth, in the long tradition of NASA spinoffs yielding benefit to wider populations. We aim to jumpstart a creative, interdisciplinary approach to space healthcare innovation, in the spirit of the Space Exploration Initiative’s cross-cutting technological development across multiple fields (from synthetic neurobiology to human-robotic interaction and AI).</p><h2>Research Areas<br></h2><br><p><b>Neural Oscillations.&nbsp;</b>The Alzheimer's research aims to use non-invasive sensory-induced gamma entrainment to attenuate space-induced physiological impairments. On an upcoming parabolic flight, we&nbsp;will test the effect of microgravity on gamma entrainment in humans. Mouse models will be used to evaluate the microglial response to sensory-mediated gamma entrainment. Microglia are macrophage-like immune cells in the central nervous system that scavenge extracellular debris, prune synapses, and support neural function.&nbsp;</p><p><b>Tardigrade Cryptobiosis.&nbsp;</b>Explores the feasibility of reverse engineering Tardigrade species cryptobiosis capabilities (i.e., desiccation, radiation and temperature-swing resistance) into other organisms, to “flip the paradigm” of organism survival in space—make the organism inherently space-tolerant, rather than relying strictly on life support systems.</p><p><b><a href=""https://www.media.mit.edu/projects/space-food/overview/"">Space Food</a>.&nbsp;</b>How can we best meet the nutritional, performance, and emotional needs of astronauts through food? Our space food research area aims to address the unique challenges associated with eating in space—from the microbiome scale to the “envirome” scale—including fermentation and probiotics,&nbsp; improving waning or shifting appetite, and&nbsp;preservation of freshness and nutrient quality.&nbsp;This research area explicitly addresses earth-based markets as well, as the foods&nbsp; and eating experiences developed for space can be re-used in many Earth contexts.&nbsp;</p><p><b><a href=""https://www.media.mit.edu/projects/social-robots-in-zero-gravity-scenarios/overview/"">Personal Robots in Space</a>.&nbsp;</b>Can we enable social connectivity between astronauts and people on Earth through an embodied agent?&nbsp;While in zero gravity, the embodied social agent interacts with people on cognitive, creative, and social tasks with varying degrees of proactive behavior. We collect physiological, audio, and video data of the experience as individuals complete a series of tasks with the agent with the goal of designing agents that can enable us to be more socially connected.</p><p><b><a href=""https://www.media.mit.edu/projects/SensorySynchrony/overview/"">Sensory Synchrony</a>.&nbsp;</b>The primary goal of this research project is to investigate vestibular system stimulation techniques to combat motion sickness and create more intuitive experiences when being in a non-natural gravity environments.&nbsp;A prototype built for multipole vestibular stimulation for simulating acceleration in roll and pitch axis will be tested on the upcoming zero gravity flight for minimizing the effects of alterations between micro and hyper gravity phases.</p><p><b><a href=""https://www.media.mit.edu/projects/physio-freefall/overview/"">Physio Freefall</a>.&nbsp;</b>This project seeks to examine the effects of altered gravity on an individual’s physiology during parabolic flight. Specifically, we will collect flight participants’ heart rate, heart rate variability, breathing rate, skin temperature, and skin conductance measurements using wearable, wireless sensors in order to determine the response of these biosignals to zero/hyper/microgravity and feelings of nausea.</p><p><b><a href=""https://www.media.mit.edu/projects/mediated-atmospheres-in-space/overview"">Mediated Atmospheres in Space</a>.&nbsp;</b>Designing the atmosphere and sensorial qualities of physical space can have a remarkable influence on human experience and behavior. This project envisions a workspace or space station that is capable of dynamically transforming to enhance occupants’ work experience and cognitive ability, via both subtle and overt customizations tailored to bio-signal inputs.</p><h2><b>Testing and Product Development</b></h2><p><b>Zero Gravity Flight: </b>We charter an annual parabolic flight (20 zero-g parabolas, 25 researchers, 15 experiments), with a focus on prototypes uniquely designed for the affordances of microgravity.&nbsp;</p><p><b>Research Collaboration with NASA Translational Research Institute for Space Health (TRISH):</b>&nbsp;We are actively developing a suite of health-focused prototypes that will mediate human interaction with interior space habitats to improve cognitive performance and overall wellness.&nbsp;</p><p><b>&nbsp;ISS Interior and External Deployment Tests:</b> Coming in the next 12 months.&nbsp;</p>",,,2019-05-30 13:00:55.855,True,2019-05-01,Space Health,PUBLIC,,True,Responsive Environments,False
tesserae,aekblaw,False,"<p>The future of human habitation in space, from Low Earth Orbit (LEO) to planetary systems far beyond, lies in self-assembling, adaptive, and reconfigurable structures. Rather than transporting the weight of gantries and risking astronaut Extravehicular Activities (EVAs), we can lower payload weight, reduce assembly complexity and revolutionize space-structure modularity by relying on these new paradigms of construction and structure deployment. This project proposes a multi-year research effort to study, characterize and prototype TESSERAE (Tessellated Electromagnetic Space Structures for the Exploration of Reconfigurable, Adaptive Environments). TESSERAE will function as multi-use, low-cost orbiting modules, thus supplying a critical space infrastructure for the next generation of zero gravity habitats, science labs, staging areas for on-surface exploration and more. Unlike large scale habitats proposed for entire space colonies, the TESSERAE should be thought of as flexible and reconfigurable modules to aid in agile mission operations. Our mission concept focuses on supporting Mars surface operations, with multiple, interlocking TESSERAE acting as an orbiting base, in addition to supporting the coming waves of space tourists in Low Earth Orbit.&nbsp;</p>",,,2017-11-20 16:01:50.612,True,2017-06-01,TESSERAE: Self-assembling Space Architecture,LAB,,True,Responsive Environments,False
journal-of-open-exploration,aekblaw,False,"<p>In collaboration with&nbsp;<a href=""https://www.media.mit.edu/groups/viral-communications/overview/"">Viral Communications</a>&nbsp;and the&nbsp;<a href=""https://www.media.mit.edu/groups/space-exploration/overview/"">Space Exploration initiative</a>, Open Ocean is using the&nbsp;<a href=""https://www.media.mit.edu/projects/pubpub/overview/"">PubPub</a>&nbsp;platform&nbsp; to launch the Journal of Open Exploration. We want to make the process and results of exploration collaborative, open, playful, and–most importantly–shared with a wider audience than traditional academic journals.</p>",,,2018-09-25 20:43:42.274,True,2018-02-26,Journal of Open Exploration,LAB,https://explore.pubpub.org/,True,Responsive Environments,False
myths-of-the-cosmos-indigenous-cosmologies,aekblaw,False,,,,2019-05-30 15:05:50.758,True,2018-01-01,Myths of the Cosmos: Indigenous Cosmologies,LAB,,True,Responsive Environments,False
tesserae-self-assembling-space-architecture,aekblaw,False,"<h1>Overview:&nbsp;</h1><p>How will we build the coming generations of Space Architecture--the modules, space ships, and space stations that will ensconce our space-faring species? Can we move beyond the 20th century paradigm of cylindrical tubes in orbit, to&nbsp;<b>geodesic dome habitats,&nbsp; to microgravity concert halls, to space cathedrals</b>?The next generation of space architecture should delight, inspire, and protect humanity for our future in the near, and far, reaches of space.&nbsp;</p><p>The future of human habitation in space lies in self-assembling, adaptive, and reconfigurable structures. Rather than transporting fixed, rigid habitation modules and risking astronaut Extravehicular Activities (EVAs) during construction, we can lower payload weight, reduce assembly complexity, and revolutionize space-structure modularity by relying on reconfigurable, self-assembly.&nbsp;</p><p>This project proposes a multi-year research effort to study, characterize, prototype and test ""TESSERAE"":&nbsp; Tessellated Electromagnetic Space Structures for the Exploration of Reconfigurable, Adaptive Environments.&nbsp; Each TESSERAE structure is made from a set of tiles. These tiles are tuned to self-assemble into a particular geometry--in our initial prototypes, we have focused on the buckminsterfullerene (20 hexagonal tiles, 12 pentagonal tiles).&nbsp; Each tile at minimum includes&nbsp; a rigid outer shell, responsive sensing for bonding diagnosis, electro-permanent magnets for dynamically controllable bonding actuation, and an on-board power harvesting and power management system. Habitat-scale TESSERAE tiles will also including clamping and sealing for pressurization.&nbsp; Tiles are released in microgravity testing environments to quasi-stochastically self assemble.&nbsp;</p><p>The “TESSERAE” name and multi-tile structure hearken to the small, colored tiles used in Roman mosaics, where many standard pieces, or “tesserae,” interlock to form a larger creation. We make this reference to ancient history, when designing an artifact of our space exploration future, to tie architectural elements together across scales and across millennia.</p><p>&nbsp;TESSERAE will function as multi-use, low-cost orbiting modules that supply a critical space infrastructure for the next generation of zero gravity habitats, science labs, staging areas for on-surface exploration, and more. Unlike large-scale habitats proposed for entire space colonies, the TESSERAE should be thought of as flexible and reconfigurable modules to aid in agile mission operations.&nbsp;Our mission concept focuses on supporting LEO, Lunar and Mars operations, with dual-use orbit &amp; surface capability:&nbsp;</p><ul><li>Tiles are packed flat and condensed for launch</li><li>Tiles are released after orbit insertion to quasi-stochastically self-assemble into the target geometry, while floating in microgravity</li><li>Once assembled, the structure can be reconfigured on demand (e.g., where a berthing port tile was needed yesterday, a cupola tile can be replaced tomorrow)</li><li>Tiles can be disassembled entirely, packed flat again in an EDL (Entry, Descent and Landing) vehicle, and then deployed and ""snap-assembled"" with astronaut assists on the lunar or martian surface</li></ul><p>Multiple, interlocking TESSERAE can serve as a larger volume orbiting base (e.g.&nbsp; ""MOSAIC"": Mars Orbiting Self-Assembling Interlocking Chambers), in addition to supporting the coming waves of space tourists and space hotels in Low Earth Orbit.&nbsp;</p>",,,2019-05-01 20:55:49.503,True,2017-06-01,TESSERAE: Self-Assembling Space Architecture,PUBLIC,,True,Responsive Environments,False
mediated-atmospheres-in-space,aekblaw,False,"<p>Designing the atmosphere and sensorial qualities of physical space can have a remarkable influence on human experience and behavior. The<i>&nbsp;<a href=""https://www.media.mit.edu/posts/mediated-atmosphere/"">Mediated Atmospheres</a>&nbsp;</i>project envisions a smart office that is capable of dynamically transforming to enhance occupants’ work experience and cognitive ability, via both subtle and overt customizations tailored to bio-signal inputs.&nbsp;The workspace prototype is equipped with a modular real-time control infrastructure, integrating biosignal sensors, controllable lighting, image projection, heat, smell, and sound. This technology (from customizable individual elements to entire room or building scale) can be applied to a space exploration habitat or cabin context to address&nbsp;behavioral health risk reduction and cognitive performance improvement&nbsp;for human life in deep space. Manipulating light, sound, smell, and visual-field objects can have a powerful effect on cognitive performance, mood, and physiology—in combination with robotic agents, we envision a holistic, data-driven approach to the “envirome,” an optimization of built-environment space experiences meant to supplement and interact with the human microbiome. This research area will combine and integrate projects from within the Space Exploration Initiative’s existing portfolio, including Spatial Flux, Social Robots in Space, VR Maze, and Sensory Synchrony.</p><p><b>Selected, Recent Mediated Atmospheres Publications:</b></p><p>Zhao, Nan, Asaph Azaria, and Joseph A. Paradiso. ""Mediated Atmospheres: A Multimodal Mediated Work Environment.""&nbsp;<i>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</i>&nbsp;1, no. 2 (2017): 31. Available:&nbsp;<a href=""https://dl.acm.org/citation.cfm?id=3090096"">https://dl.acm.org/citation.cfm?id=3090096</a></p><p>Richer, Robert, Nan Zhao, Judith Amores, Bjoern M. Eskofier, and Joseph A. Paradiso. ""Real-time Mental State Recognition using a Wearable EEG."" In&nbsp;<i>2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</i>, pp. 5495-5498. IEEE, 2018. Available:<a href=""https://ieeexplore.ieee.org/abstract/document/8513653"">https://ieeexplore.ieee.org/abstract/document/8513653</a></p><p>""Image-based perceptual analysis of lit environments."" Nan Zhao, Christoph F. Reinhart, and Joseph A. Paradiso in Lighting Research and Technology Journal 0: 1–21, 2018.&nbsp;<a href=""https://resenv.media.mit.edu/pubs/papers/2018_Zhao_LRTJ.pdf"">https://resenv.media.mit.edu/pubs/papers/2018_Zhao_LRTJ.pdf</a></p><p>""Real-time Mental State Recognition using a Wearable EEG."" Robert Richer, Nan Zhao, Judith Amores, Bjoern M. Eskofier, and Joseph A. Paradiso in the International Conference of the IEEE Engineering in Medicine and Biology Society, July 17-21, 2018, Honolulu, HI, USA.&nbsp;<a href=""https://resenv.media.mit.edu/pubs/papers/richer18_eeg_scores.pdf"">https://resenv.media.mit.edu/pubs/papers/richer18_eeg_scores.pdf</a></p>",,,2019-05-24 19:08:37.923,True,2019-05-01,Mediated Atmospheres in Space,LAB,,True,Responsive Environments,False
hand-development-kit,sangwon,False,"<p>Recent developments in wearable robots and human augmentation open up new possibilities of designing computational interfaces integrated to the body. Particularly, supernumerary robot is a recently established field of research that investigates a radical idea of adding robotic limbs to users. Such augmentations, however, pose a limit in how much we can add to the body due to weight or interference with other body parts. To address that, we explore the use of soft robots as supernumerary robotic fingers. We present a pair of soft robotic fingers driven by cables and servomotors, and applications using the robotic fingers in various contexts.&nbsp;</p>",2018-08-01,,2018-08-20 16:45:50.186,True,2016-08-01,Hand Development Kit,PUBLIC,,False,Fluid Interfaces,False
cord-uis-controlling-devices-with-augmented-cables,sangwon,False,"<p>Cord UIs are sensorial augmented cords that allow for simple, metaphor-rich interactions to interface with their connected devices. Cords offer a large, under-explored space for interactions, as well as unique properties and a diverse set of metaphors that make them potentially interesting tangible interfaces. We use cords as input devices and explore different interactions like tying knots, stretching, pinching, and kinking to control the flow of data and/or power. We also look at ways to use objects in combination with augmented cords to manipulate data or properties of a device. For instance, placing a clamp on a cable can obstruct the audio signal to the headphones. Using special materials such as piezo copolymer cables and stretchable cords, we built five working prototypes to showcase these interactions.</p>",2014-09-01,--Choose Location,2016-12-05 00:17:09.974,True,2014-01-01,Cord UIs: Controlling Devices with Augmented Cables,PUBLIC,,False,Fluid Interfaces,False
l-evolved,sangwon,False,"<p>Ubiquitous computing has been focusing on creating smart agents that are submerged into everyday environments, however, recent development on physical computing is demanding a shift from calm computing to a physically engaging form. Computing is no more limited to increasing our comfort through passive and pervasive deployment, they can now be created as being more actively and physically intermeshed into our tasks. We present L’evolved, autonomous ubiquitous utilities that assist in user tasks through active physical participation. They not only dynamically adapt to individual user needs and actions, but also work in close tandem with the users. Among explorations on potential applications, we harness drone technology to realize the design and implementation of example utilities that afford free motions and computational controls. Through various use scenarios of those exemplary utilities, we show how this new form of smart agents promises new ways of interacting with our physical environments. We also discuss design implications and technical details of our implementations.</p>",,,2018-10-12 19:55:34.922,False,2015-09-11,L'evolved,PUBLIC,,True,Fluid Interfaces,False
guitar-machine-ii,sangwon,False,"<p>Exploring robotic sound generation mixed with human movements on the guitar. Guitar Machine II is a robotic guitar that responds to human gestures, as well as other input means such as midi controllers or algorithmic composition.&nbsp;</p>",,,2018-10-23 15:19:45.702,True,2017-09-01,Guitar Machine II,PUBLIC,,True,Fluid Interfaces,False
guitar-machine,sangwon,False,"<p>Symbiotic guitar playing between human and machine fingers. The system can be used as a learning tool or a real-time augmentation to the human guitar player, offering previously impossible combinations of notes.&nbsp;</p>",,,2018-05-07 01:50:19.235,True,2017-04-01,Guitar Machine,PUBLIC,http://www.sangww.net/2018/04/guitar-machine-i.html,True,Fluid Interfaces,False
hand-development-kit,yuhanhu,False,"<p>Recent developments in wearable robots and human augmentation open up new possibilities of designing computational interfaces integrated to the body. Particularly, supernumerary robot is a recently established field of research that investigates a radical idea of adding robotic limbs to users. Such augmentations, however, pose a limit in how much we can add to the body due to weight or interference with other body parts. To address that, we explore the use of soft robots as supernumerary robotic fingers. We present a pair of soft robotic fingers driven by cables and servomotors, and applications using the robotic fingers in various contexts.&nbsp;</p>",2018-08-01,,2018-08-20 16:45:50.186,True,2016-08-01,Hand Development Kit,PUBLIC,,False,Fluid Interfaces,False
opus-exploring-scientific-data-through-visualizations,arista,False,"<p>Scientific managers and administrators need to understand the impact of the research they support. Yet, most of the online tools available to explore scholarly publication data (e.g. Google Scholar and Citeseerx) present atomized views focused on single scholars and papers, failing to put scholars in a social, institutional, and national context; and failing to provide aggregate views for universities and countries. Here, we introduce<span style=""font-size: 18px; font-weight: 400;"">&nbsp;Opus,</span><span style=""font-size: 18px; font-weight: 400;"">&nbsp;an interactive online platform that integrates, aggregates, and visualizes scholarly data from Google Scholar and Microsoft Academic graph. Opus present users with scientific data at five different scales: scholars, countries, organizations, journals, and papers; and at each scale, it provides properly benchmarked visualizations that facilitate the comparison among scholars (e.g. from the same age), organizations, and nations. We build Opus using React as a front-end framework and Replot a new React visualization library developed in the course of this project as its central visualization library. Finally, we will have users compare Opus with similar tools such as Google Scholar, Microsoft Academic, and Citeseerx.</span></p>",2018-07-04,,2017-10-16 23:04:37.291,True,2017-01-10,Opus: Exploring Scientific Data through Visualizations,LAB-INSIDERS,,False,Other,False
economic-complexity-and-income-inequality,arista,False,"<p>Decades ago development scholars argued that the productive structure of a country (i. e. the mix of industries operating in the country) constrains its ability to generate and distribute income. They were correct! It was recently shown that the mix of products that a country exports is predictive of its future pattern of diversification and economic growth. But what is the link between a country's productive structure and its ability to distribute income?&nbsp;Here, we combine methods from econometrics, network science, and economic complexity, together with data on income inequality and world trade, to show that countries exporting complex products have lower levels of income inequality than countries exporting simpler products. Using multivariate regression analysis, we show that economic complexity is a significant and negative predictor of income inequality and that this relationship is robust to controlling for aggregate measures of income, institutions, export concentration, and human capital. Moreover, we introduce a measure that associates a product to a level of income inequality equal to the average GINI of the countries exporting that product (weighted by the share the product represents in that country’s export basket). The Product-GINI index, or PGI,&nbsp;can provide important insights on the constraints to inequality imposed by a country's productive structure. Finally, we integrate our results to the Observatory of Economic Complexity, an online resource that allows its users to visualize the structural transformation of over 150 countries&nbsp;and their associated changes in income inequality during 1963–2008.</p>",,--Choose Location,2017-10-10 16:03:06.641,True,2014-09-01,Inequality and the impact of industrial structures,PUBLIC,,True,Other,False
opus-exploring-scientific-data-through-visualizations,almaha,False,"<p>Scientific managers and administrators need to understand the impact of the research they support. Yet, most of the online tools available to explore scholarly publication data (e.g. Google Scholar and Citeseerx) present atomized views focused on single scholars and papers, failing to put scholars in a social, institutional, and national context; and failing to provide aggregate views for universities and countries. Here, we introduce<span style=""font-size: 18px; font-weight: 400;"">&nbsp;Opus,</span><span style=""font-size: 18px; font-weight: 400;"">&nbsp;an interactive online platform that integrates, aggregates, and visualizes scholarly data from Google Scholar and Microsoft Academic graph. Opus present users with scientific data at five different scales: scholars, countries, organizations, journals, and papers; and at each scale, it provides properly benchmarked visualizations that facilitate the comparison among scholars (e.g. from the same age), organizations, and nations. We build Opus using React as a front-end framework and Replot a new React visualization library developed in the course of this project as its central visualization library. Finally, we will have users compare Opus with similar tools such as Google Scholar, Microsoft Academic, and Citeseerx.</span></p>",2018-07-04,,2017-10-16 23:04:37.291,True,2017-01-10,Opus: Exploring Scientific Data through Visualizations,LAB-INSIDERS,,False,Collective Learning,False
replot,almaha,False,"<p>Replot is a new and soon-to-be-open sourced visualization library for the web, built natively on the ReactJS, the extremely popular open-source web framework that powers most modern websites today.</p><p>Replot is written from the ground up in React, rendering native SVG visualizations and fully leveraging the idea of user interfaces being pure functions of an underlying data ""state."" This enables developers to construct rich and engaging visualizations with very few lines of code that bind automatically to their datasets, and animate automatically when their data change.&nbsp;</p><p>Replot also introduces a paradigm shift in customizability of your visualizations, enabling you to trigger transformations in your visualizations by simply manipulating state variables.</p>",,,2017-10-11 19:58:05.810,True,2017-06-01,Replot,LAB-INSIDERS,,True,Collective Learning,False
d-Abyss,nbarry,False,"<p><b>Can tattoos embrace technology in order to make the skin interactive?</b></p><p>The DermalAbyss project is the result of a collaboration between MIT researchers Katia Vega, Xin Liu, Viirj Kan and Nick Barry and Harvard Medical School researchers Ali Yetisen and Nan Jiang.&nbsp;<br></p><p>DermalAbyss is a proof-of-concept that presents a novel approach to bio-interfaces in which the body surface is rendered an interactive display. Traditional tattoo inks are replaced with biosensors whose colors change in response to variations in the interstitial fluid. It blends advances in biotechnology with traditional methods in tattoo artistry.&nbsp;</p><p>This is a research project, and there are currently no plans to develop Dermal Abyss as a product or to pursue clinical trials.<br></p>",2017-05-31,,2018-04-27 17:45:10.726,True,2016-06-01,DermalAbyss: Possibilities of Biosensors as a Tattooed Interface,PUBLIC,,False,Synthetic Neurobiology,False
d-Abyss,katiav,False,"<p><b>Can tattoos embrace technology in order to make the skin interactive?</b></p><p>The DermalAbyss project is the result of a collaboration between MIT researchers Katia Vega, Xin Liu, Viirj Kan and Nick Barry and Harvard Medical School researchers Ali Yetisen and Nan Jiang.&nbsp;<br></p><p>DermalAbyss is a proof-of-concept that presents a novel approach to bio-interfaces in which the body surface is rendered an interactive display. Traditional tattoo inks are replaced with biosensors whose colors change in response to variations in the interstitial fluid. It blends advances in biotechnology with traditional methods in tattoo artistry.&nbsp;</p><p>This is a research project, and there are currently no plans to develop Dermal Abyss as a product or to pursue clinical trials.<br></p>",2017-05-31,,2018-04-27 17:45:10.726,True,2016-06-01,DermalAbyss: Possibilities of Biosensors as a Tattooed Interface,PUBLIC,,False,Responsive Environments,False
fluxa,katiav,False,"<p>Fluxa is a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a transient wearable display to foster richer self-expression and communication in daily life . It can be used to enhance existing social gestures such as handwaving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a decoration device that generates images around dancing bodies.</p>",2017-11-30,--Choose Location,2018-10-12 16:57:48.806,True,2016-01-01,Fluxa,PUBLIC,,False,Responsive Environments,False
skrin,katiav,False,"<p>Skrin is an exploration project on digitalized body skin surface using embedded electronics and prosthetics. Human skin is a means for protection, a mediator of our senses, and a presentation of our selves. Through several projects, we expand the expression capacity of the body's surface and emphasize the dynamic aesthetics of body texture by technological means.&nbsp;<span style=""font-size: 18px;"">Working with conventional special effect makeup artists, we “hide” electronics into silicone which is applied onto skin and covered by cosmetics. The digitalized skin surface is connected with the affective experience, while the illuminated body is a representation of internal state.</span></p><p>Working with bionic pop artist Viktoria Modesta, we deployed the project in Music Tech Festival Berlin 2016 and transformed her body as a canvas along with the performance.</p>",2017-06-30,--Choose Location,2018-12-05 14:35:37.656,True,2016-01-01,Skrin,PUBLIC,,False,Responsive Environments,False
chromoskin,katiav,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Makeup has long been used as a body decoration process for self-expression and for the transformation of one's appearance. While the material composition and processes for creating makeup products have evolved, they still remain static and non-interactive. But our social contexts demand different representations of ourselves; thus, we propose ChromoSkin, a dynamic color-changing makeup system that gives the wearer ability to alter seamlessly their appearance. We prototyped an interactive eye shadow tattoo composed of thermochromic pigments activated by electronics or ambient temperature conditions. We present the design and fabrication of these interactive cosmetics, and the challenges in creating skin interfaces that are seamless, dynamic, and fashionable.</span></p>",,--Choose Location,2018-02-08 20:50:38.533,True,2015-12-01,ChromoSkin,PUBLIC,https://vimeo.com/155460417,True,Responsive Environments,False
proof-of-patience,oceane,False,"<p>To help prevent the mindless sharing of content and promote the sharing of thoughtful content, a socially acceptable “stamp of approval” was created, backed by distributed ledger technology (DLT) that would integrate itself into a post. These stamps of approval&nbsp;are social indicators to one's network that the person sharing the content (and receiving the stamp of approval)&nbsp;has reviewed the content before sharing.</p><p>DLTs, specifically blockchains, have the architectural benefit of providing a public ledger platform where all recordings/transactions are immutable and verifiable, thus being an excellent platform for audits and verifying provenance—even though this isn’t what we care about. In traditional blockchain architecture, miners perform a computational intensive process called PoW (proof-of-work) in order to prove to the complete blocks that maintain the network. Using “patience” as a variable to allow a user-defined block completion algorithm for every assertion made, we created proof-of-patience. A user defines the level of patience—time and computational resources they are willing to give up, from a scale of one to three and then they mine their block.</p><p>When the user is done mining for their block, we created PoPBot (""Proof of Patience""-bot)to tweet at the user the content they want to share with a badge that is representative of the work/patience they put in and the user can then retweet it to their network.&nbsp;</p>",2019-01-01,,2019-04-17 13:55:21.434,True,2018-11-11,Proof of Patience,PUBLIC,,False,Responsive Environments,False
civic-link,oceane,False,"<p>CivicLink is an online organization in a box. A leader or moderator plugs it in, invites members, and the tools needed to build community action are in place. It is grassroots mobilization recursed to the lower level: we envision extremely large networks of extremely local, single-issue orgs. Core elements are an events calendar and forum for each event. The link is a private server that retains all communications and personal information within it; there is no contribution to an online cloud. The architecture is extensible to add features such as mapping, canvassing, etc. It is designed to be used where groups physically meet and for access via a smartphone app.</p><p>Related work tests whether privacy is important to users, whether games can be used to promote actions, how web sites can be distributed offline via QR codes, and how this link can merge culturally unique resonances.</p>",,,2019-04-18 01:23:05.324,True,2018-09-04,CivicLink,PUBLIC,,True,Responsive Environments,False
provenance-in-pacific-island-fisheries,oceane,False,"<p>New work:&nbsp; Fisheries in the Pacific Islands operate in an opaque market where an auditing tool built on a distributed ledger would be beneficial to the local economy by providing an immutable ""stamp""&nbsp; that could leverage semi-trusted, third party auditors and vet fisheries for:</p><ul>

<li>labor conditions on-board</li>
<li>types of fish caught (Skipjack, Yellowfin, Bigeye)</li>
<li>sustainability of fishing method (fish aggregating device (FAD), seine net fishing, longline)</li>
</ul>",,,2019-04-16 19:18:03.919,True,2019-02-01,Distributed ledgers for ocean conservation and fisheries,PUBLIC,,True,Responsive Environments,False
depicting-token-quality,oceane,False,"<p>This project delivers a series of Github-signal based metrics that would be useful to both the layman and investor, helping look beyond the rocky trends of the cryptocurrency market to the actual viability of the token as a utility or security from the perspective of developer dynamic. By doing a quantitative analysis of source code and developer activity within a community, we can provide a rich set of insights into the health of a token’s foundation.<br></p><p><b>Assessment Methodology</b></p><p>Building upon&nbsp;<a href=""https://github.com/manganese/alteramentum-repo-data/blob/master/repos.csv"">previous research</a>, we gathered a ranked list of 13,695 repositories that correlate with 1,011 tokens—where we assess a token (i.e., Bitcoin) which has multiple repositories (i.e., bips, bitcoin, libbase58).&nbsp;The repository metadata information includes <i>url data, forks_count, subscribers_count, network_count, open_issues_count, watchers_count, stargazers_count, size, created_at, updated_at, pushed_at, has_wiki, has_downloads, has_issues, is_fork.</i> We have built a classifier that accepts this metadata as input features &nbsp;and returns a label to help us assess ranking qualities of the specified token.</p><p>Co-authored with <a href=""https://www.linkedin.com/in/kokje/"">Yashashree Kokje</a></p>",,,2019-04-19 03:41:54.685,True,2019-01-01,Humanized Cryptoassets,PUBLIC,,True,Responsive Environments,False
social-textiles,katsuyaf,False,"<p>The way we represent ourselves in social media is intangible. What we choose to wear is public to the world and we are aware of it. In contrast, what we post online about ourselves reaches thousands of people and generates social consequences, but it doesn’t feel that way. Is the current form of social media really making our relationships better? Current technologies are good at connecting people at a distance, but less so at connecting them within the same environment.</p><p>Social textiles embodies who you are and dynamically reflects your shared interests with people nearby. It enables you to gain access to communities of people in the physical world and enhances social affordances and icebreaking interactions through wearable social messaging.</p><p>Social Textiles embody who you are and dynamically reflect your shared interests with people nearby. They enable you to gain access to communities of people in the physical world and enhance social affordances and icebreaking interactions through wearable social messaging. Social Textiles can connect community members with niche interests, philosophical beliefs, personalities, emotional statuses, and ethical views. They have the potential to enable members to bypass superficial or generic interests through ""filtering"" individuals, in order to tune social experiences toward people who are more compatible.</p>",2015-01-01,--Choose Location,2018-10-12 16:50:34.930,True,2014-09-01,Social Textiles,PUBLIC,,False,Fluid Interfaces,False
fluxa,mingrui,False,"<p>Fluxa is a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a transient wearable display to foster richer self-expression and communication in daily life . It can be used to enhance existing social gestures such as handwaving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a decoration device that generates images around dancing bodies.</p>",2017-11-30,--Choose Location,2018-10-12 16:57:48.806,True,2016-01-01,Fluxa,PUBLIC,,False,Fluid Interfaces,False
oasis,sra,False,"<p>Oasis won <a href=""http://www.edisonawards.com/winners2017.php"">Silver at the Edison Awards 2017</a>.</p><p>Oasis received the <a href=""https://www.vrst2016.lrz.de/keynotes-award/"">Best Paper Award</a> at VRST 2016.<br></p><p>Oasis is a novel system for automatically generating immersive and interactive virtual reality environments using the real world as a template. The system captures indoor scenes in 3D, detects obstacles like furniture and walls, and maps walkable areas to enable real-walking in the generated virtual environment. Depth data is additionally used for recognizing and tracking objects during the VR experience. The detected objects are paired with virtual counterparts to leverage the physicality of the real world for a tactile experience. Our system allows a casual user to easily create and experience VR in any indoor space of arbitrary size and shape without requiring specialized equipment or training.</p><p>Oasis can be used, for example, to create storyspaces where friends and family can remotely participate in a session of storytelling around the campfire. The freedom to move around and interact with the virtual world allows for a new form of storytelling when combined with &nbsp;traditional &nbsp;narration techniques like vocalization, movement, and gestures. We call this <b>human-in-the-loop storytelling</b>,&nbsp;distinguishing it from current VR storytelling experiences where the software system is the storyteller.</p>",2017-08-31,,2018-08-20 16:24:31.868,True,2016-02-01,Oasis,PUBLIC,,False,Fluid Interfaces,False
full-body-tracking,sra,False,<p>The setup involves adding one controller/tracker per foot and one at the base of the back along with two hand-held controllers and the HMD. I'm using the FinalIK asset from the Unity Asset Store.</p>,2018-07-31,,2018-08-20 16:21:46.064,True,2017-01-09,VR Full Body Tracking,PUBLIC,,False,Fluid Interfaces,False
auris-creating-affective-virtual-spaces-from-music,sra,False,"<p>Light, color, texture, geometry and other architectural design elements have been shown to produce predictable and measurable effects on our minds, brains, and bodies. This suggests spaces that can mirror or transform feelings or serve specific purposes like improving learning or enhancing wellbeing can be designed. With Auris, we take a first step towards the design of such spaces in virtual reality by attempting to automatically generate affective virtual environments that can affect our emotions. The input to Auris is a song (audio and lyrics) and the output is a VR world that encapsulates the mood and content of the song.</p>",2018-07-31,,2018-08-20 16:22:53.032,True,2017-02-01,Auris: Creating Affective Virtual Spaces from Music,PUBLIC,,False,Fluid Interfaces,False
breathvr,sra,False,"<p>Breathing actions are used to augment controller-based input by giving superpowers to players in two VR games. Blowing out long and strong turns you into a fire-breathing dragon, while holding your breath sends you into stealth mode.&nbsp; By using&nbsp; breathing as a directly controlled physiological signal, BreathVR can facilitate unique and engaging play experiences through natural interaction in single and multiplayer virtual reality games. Paper available here:&nbsp;<a href=""http://web.media.mit.edu/~sra/breathvr.html"">http://web.media.mit.edu/~sra/breathvr.html</a></p>",2018-07-31,,2018-08-20 16:27:18.375,True,2017-08-16,BreathVR,PUBLIC,,False,Fluid Interfaces,False
galvr-a-novel-collaboration-interface-using-gvs,sra,False,"<p>GVS or galvanic vestibular stimulation is a technology that directly affects a user's vestibular system by altering their sense of balance and direction. It works through electrical stimulation via electrodes placed on the mastoid bones behind each ear. In standing users, GVS evokes a prolonged ""galvanic body sway."" In walking users, it affects balance and causes users to stagger in the anodal direction. However, in walking users, with their head pitched forward, it causes them to turn smoothly from their planned trajectory in the anodal direction. Dark Room is a cooperative asymmetrical ""escape the room"" style game played by a VR and a PC user, inspired by the single-player mobile game Dark Echo. The PC user controls the walking direction of the VR user to guide them around virtual or physical obstacles. The VR player uses echolocation to detect obstacles. Video and paper available here:&nbsp;<a href=""http://web.media.mit.edu/~sra/gvs.html"">http://web.media.mit.edu/~sra/gvs.html</a></p>",2018-07-31,,2019-04-19 17:08:16.864,True,2017-08-15,A VR Collaboration Interface Using Galvanic Vestibular Stimulation,PUBLIC,,False,Fluid Interfaces,False
improving-sleep-wake-schedule-using-sleep-behavior-visualization-and-a-bedtime-alarm,sra,False,"<p><a href=""http://dl.acm.org/citation.cfm?id=2897442.2897469"">Humans need sleep</a>, along with food, water, and oxygen, to survive. With about one-third of our lives spent sleeping, there has been increased attention and interest in understanding sleep and the overall state of our ""sleep health."" The rapid adoption of smartphones, along with a growing number of sleep tracking applications for these devices, presents an opportunity to use phones to encourage better sleep hygiene. Procrastinating going to bed and being unable to stick to a consistent bedtime can lead to inadequate sleep time, which in turn affects quality of life and overall wellbeing. To help address this problem, we developed two applications, Lights Out and Sleep Wallpaper, which provide a sensor-based bedtime alarm and a connected peripheral display on the wallpaper of the user's mobile phone to promote awareness with sleep data visualization.</p>",,--Choose Location,2016-12-05 17:35:08.047,True,2015-09-01,Improving Sleep-Wake Schedule Using Sleep Behavior Visualization and a Bedtime Alarm,PUBLIC,,True,Fluid Interfaces,False
move-u,sra,False,"<p>MoveU is a wearable vestibular stimulation device for providing proprioceptive haptic feedback in virtual reality (VR).&nbsp; The device induces sensations of motion corresponding to virtual motion, thereby increasing immersion in VR and reducing cybersickness.&nbsp;</p><p>MoveU non-invasively stimulates the vestibular system using a technique called galvanic vestibular stimulation (GVS).&nbsp;GVS is a specific way to elicit vestibular reflexes&nbsp;<span style=""font-size: 18px; font-weight: 400;"">using electrical current&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">that has been used for over a century to study the function of the vestibular system. In addition to GVS, the device supports physiological sensing by connecting heart rate, electrodermal activity, and other sensors&nbsp; using a plug and play mechanism.&nbsp;MoveU supports multiple categories of virtual reality applications with different types of virtual motions such as driving, navigating by flying, teleporting, or riding.&nbsp;</span></p>",,,2019-05-06 20:24:10.520,True,2018-07-15,MoveU,PUBLIC,,True,Fluid Interfaces,False
snowballvr,sra,False,"<p><a href=""http://dl.acm.org/citation.cfm?id=2984779&amp;CFID=856493995&amp;CFTOKEN=37757229"">Multiplayer virtual reality</a> games introduce the problem of variations in the physical size and shape of each user’s space for mapping into a shared virtual space. We designed an asymmetric approach to solve the spatial variation problem, by allowing people to choose roles based on the size of their space. We demonstrate this design through the implementation of a virtual snowball fight where players can choose from multiple roles, namely the shooter, the target, or an onlooker depending on whether the game is played remotely or together in one large space. In the co-located version, the target stands behind an actuated cardboard fort that responds to events in VR, providing non-VR spectators a way to participate in the experience.
                    
                </p>",,,2017-04-20 08:59:00.349,True,2016-01-01,SnowballVR,PUBLIC,,True,Fluid Interfaces,False
onthego,sra,False,"<p>As mobile device screens continue to get smaller (smartwatches, head-mounted devices like Google Glass), touch-based interactions with them become harder. With OnTheGo, our goal is to complement touch- and voice-based input on these devices by adding interactions through in-air gestures around the devices. Gestural interactions are not only intuitive for certain situations where touch may be cumbersome like running, skiing, or cooking, but are also convenient for things like quick application and task management, certain types of navigation and interaction, and simple inputs to applications.</p>",,--Choose Location,2018-04-20 16:50:52.900,True,2014-09-01,OnTheGo,PUBLIC,,True,Fluid Interfaces,False
learning-to-dance-in-vr,sra,False,"<p>Virtual reality can help realize mediated social experiences where we interact as richly with those around the world as we do with those in the same room. The design of social virtual experiences presents a challenge for remotely located users with differently sized, room-scale setups like those afforded by recent commodity virtual reality devices like the HTC Vive. This work explores how we can allow remote users to learn to dance together in VR by mapping their individual physical spaces to a shared virtual space.&nbsp; Video and paper available here:&nbsp;<a href=""http://web.media.mit.edu/~sra/dancing.html"">http://web.media.mit.edu/~sra/dancing.html</a></p>",,,2018-12-17 15:39:47.482,True,2017-02-01,Learning to dance in VR,PUBLIC,,True,Fluid Interfaces,False
meta-physical-space-vr,sra,False,"<p>Most current virtual reality interactions are mediated by hand-held input devices or hand gestures, and usually display only a partial representation of the user in the synthetic environment. We believe that representing the user as a full avatar controlled by the natural movements of the person in the real world can lead to a greater sense of presence in VR. <a href=""http://dl.acm.org/citation.cfm?id=2817802&amp;CFID=661831763&amp;CFTOKEN=43505533"">MetaSpace I</a> is a virtual reality system that allows co-located users to explore a VR world together by walking around in physical space. Each user’s body is represented by an avatar that is dynamically controlled by their body movements. Users can see their own avatar and the other person’s avatar allowing them to perceive and act intuitively in the virtual environment.</p>",,--Choose Location,2018-05-07 20:12:11.319,True,2014-11-01,MetaSpace I,PUBLIC,,True,Fluid Interfaces,False
magicrobot,tempest,False,"<h1>
                    Can a magician and a robot collaborate on stage to create a believable, evocative performance?</h1><p><span style=""font-size: 18px; font-weight: normal;"">We are studying how the quality of robot movement, perceived robot agency, and blended static/dynamic interactivity between a robot and human performer might influence an audience’s emotional state and belief in the validity of a robot character during a performance.&nbsp;</span></p><h2>Agency and Believability</h2><p>Robotic animation techniques for live performance typically rely on backstage human puppeteering or playback of pre-rendered animation sequences. However, these methods are insufficient for high-speed, close human-robot proximity and coordination, especially when the human performer’s position and timing are unpredictable (ex. rapid passing of objects between human-hands and robot-grippers). Furthermore, simple playback of animation can detract from the believability of the performance if an audience is not convinced that the robot has agency (i.e. its ability to act on its own).</p><h2>Static / Dynamic Interaction</h2><p>We are developing tools that allow us to compose a human-robot performance that blends pre-rendered choreography with key moments of dynamic interactivity to enhance the realism of the character. If the performance successfully modulates the degree to which the robot responds to the human in a pre-defined manner versus behavior that is completely reactive to the dynamic performer, then the audience might still perceive the robot as having complete agency. For example, as the robot is playing back a choreographed series of poses, it might also track the face of the performer to maintain eye contact. By blurring lines in this interaction, the audience might be more willing to believe the robot is animate.</p><h2><b>Additional Research Questions</b></h2><ul><li>•What is the affect space for a human-magician performance?</li><li class="""">Can we improve on current robot animation techniques by including computational choreography and aesthetics-influenced motion planning in ways that lead to desired emotional reactions in observed human-robot collaboration?</li><li class="""">What are challenges and opportunities when designing human-robot performances? • Can we generate a new class of tools and approaches that facilitate artistic and functional robot programming by non-experts?</li></ul><p class=""""><br></p>",2015-12-31,,2017-11-25 16:09:32.395,True,2015-03-01,Magic Robot Interaction,PUBLIC,http://magiclab.nyc,False,Initiatives,False
magicrobot,dnunez,False,"<h1>
                    Can a magician and a robot collaborate on stage to create a believable, evocative performance?</h1><p><span style=""font-size: 18px; font-weight: normal;"">We are studying how the quality of robot movement, perceived robot agency, and blended static/dynamic interactivity between a robot and human performer might influence an audience’s emotional state and belief in the validity of a robot character during a performance.&nbsp;</span></p><h2>Agency and Believability</h2><p>Robotic animation techniques for live performance typically rely on backstage human puppeteering or playback of pre-rendered animation sequences. However, these methods are insufficient for high-speed, close human-robot proximity and coordination, especially when the human performer’s position and timing are unpredictable (ex. rapid passing of objects between human-hands and robot-grippers). Furthermore, simple playback of animation can detract from the believability of the performance if an audience is not convinced that the robot has agency (i.e. its ability to act on its own).</p><h2>Static / Dynamic Interaction</h2><p>We are developing tools that allow us to compose a human-robot performance that blends pre-rendered choreography with key moments of dynamic interactivity to enhance the realism of the character. If the performance successfully modulates the degree to which the robot responds to the human in a pre-defined manner versus behavior that is completely reactive to the dynamic performer, then the audience might still perceive the robot as having complete agency. For example, as the robot is playing back a choreographed series of poses, it might also track the face of the performer to maintain eye contact. By blurring lines in this interaction, the audience might be more willing to believe the robot is animate.</p><h2><b>Additional Research Questions</b></h2><ul><li>•What is the affect space for a human-magician performance?</li><li class="""">Can we improve on current robot animation techniques by including computational choreography and aesthetics-influenced motion planning in ways that lead to desired emotional reactions in observed human-robot collaboration?</li><li class="""">What are challenges and opportunities when designing human-robot performances? • Can we generate a new class of tools and approaches that facilitate artistic and functional robot programming by non-experts?</li></ul><p class=""""><br></p>",2015-12-31,,2017-11-25 16:09:32.395,True,2015-03-01,Magic Robot Interaction,PUBLIC,http://magiclab.nyc,False,Opera of the Future,True
empathy-and-the-future-of-experience,dnunez,False,"<p>Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of–as well as long-term commitment to–empathic communication.</p>",,--Choose Location,2019-04-17 19:59:42.795,True,2015-01-01,Empathy and the future of experience,PUBLIC,,True,Opera of the Future,True
tree,yedan,False,"<p><i>Tree</i> is a virtual experience that transforms you into a rainforest tree. With your arms as branches and body as the trunk, you experience the tree’s growth from a seedling into its fullest form and witness its fate firsthand.&nbsp;Collaborating with director Milica Zec and Winslow Porter, we designed and constructed the entire tactile experience throughout the film. With precisely controlled physical elements including vibration, heat, fan and body haptics, the team created a fully immersive virtual reality storytelling to, where the audience no longer watches but is transformed into a new identity, a giant tree in the peruvian rainforest.</p><p><i>Tree</i> debuted at Sundance Film Festival 2017 New Frontier and also had its presentation in Tribeca Film Festival 2017. &nbsp;</p><p>The project is part of our research about body ownership illusion in virtual reality (early project: <a href=""https://www.media.mit.edu/projects/treesense/overview/"">TreeSense</a>).&nbsp;The tactile experience is crucial for establishing a body ownership illusion instead of restricting the experience to the visual world. We aim to have the audience not just see, but feel and believe ""being"" a tree.&nbsp;</p>",2017-06-30,,2018-10-20 22:01:11.764,True,2017-01-18,Tree,PUBLIC,,False,Fluid Interfaces,False
cue,tomasero,False,"<h2>Exploring contextual multimodal cues as memory aids&nbsp;</h2><p><b>Objective&nbsp;<br></b></p><p>We are exploring the potential of proximity-triggered contextual audio and visual cues to help early-stage Alzheimer’s patients&nbsp;recall familiar people and places. In particular, we are using proximity beacons to determine when the user is physically close to another person, such as a loved one. The beacons will then trigger cues in the form of:</p><ol><li>audio conveying contextual information such as name, relationship, time/place/details of last interaction;</li><li>images and video (using AR) showing previous interactions along with text displaying contextual information; and</li><li>music in the form of specific songs associated with specific individuals.</li></ol><p><b>Research Questions</b></p><p>We’re interested in tackling the following questions:</p><ul><li>Which cue modalities are the most effective in improving recognition in early-stage Alzheimer’s patients?</li><li>What advantages and challenges are afforded by each of the different modalities?</li></ul>",2017-12-20,,2018-12-14 17:56:32.014,True,2017-09-22,Cue,LAB-INSIDERS,,False,Fluid Interfaces,False
masca,tomasero,False,"<p>Masca is a flexible mask for sleep stage detection. Our device adapts to the human body using conformable piezoresistive fabric and silicone, enabling eyelid motion detection in a comfortable, affordable form factor.&nbsp; <a href=""https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.1995.tb03410.x"">Eyes and eyelids change movement frequency predictably</a>&nbsp;as sleep stage transitions occur, allowing for a simpler, more portable system than the typical high-density EEG required for sleep tracking. Tracking and influencing of sleep cognition opens up doors to targeted reactivation of daytime experience during sleep: a future in which the consolidation of emotion, memory, and learning in sleep is rendered controllable by wearable electronics.&nbsp;</p><p>This device is modeled after Prof. Robert Stickgold's <a href=""https://www.sciencedirect.com/science/article/pii/S1053810084710026"">Nightcap</a>, and we are grateful for his ongoing assistance with this project, aiming at further extending the benefits of sleep neuroscience.<br></p>",,,2019-03-26 13:32:12.436,True,2018-04-01,Masca,PUBLIC,,True,Fluid Interfaces,False
frisson,tomasero,False,"<p>This project unites embodied cognition and on-body device design to ask questions about the origin of emotions and the potential to hack our brains and behavior by hacking the body.&nbsp;</p><p>Frisson, also known as aesthetic chills, are the wave of chills you get while experience peak emotional moments in a song or peak meaning-making moments in a speech.&nbsp;&nbsp;At once transcendent and physiological, the subtle signals of beauty and semantics meet mechanism as the sublime literally cascades across skin. Conveniently, frisson seems to be an<a href=""https://www.researchgate.net/publication/227131588_Aesthetic_Chills_as_a_Universal_Marker_of_Openness_to_Experience"">&nbsp;almost universal marker</a>&nbsp;of peak emotional experiences across a wide range of cultures and continents.&nbsp;Embodied cognition has done much to illuminate links between our physical experience and our psychological experience, and studies on<a href=""https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2420110403"">&nbsp;misattribution of arousal</a>&nbsp;show us we can drive cognition by driving physical sensation. This points to opportunities: if we can drive frisson, perhaps we can also drive the openness to experience, relief in stress, increase in empathy, and experience of meaning that frisson has been tied to in past research.</p><p>So we built a device to trigger frisson.&nbsp;Alongside&nbsp;<a href=""https://www.researchgate.net/publication/305892666_Aesthetic_Chills_Knowledge-Acquisition_Meaning-Making_and_Aesthetic_Emotions"">Félix Schoeller</a>, first we're testing whether inducing chills can increase&nbsp;<a href=""http://journals.sagepub.com/doi/abs/10.1177/0305735615572358"">deep attention and openness to experiences.&nbsp;</a>&nbsp;Psychophysiology driving thought from the spine upwards!</p>",,,2019-03-26 13:36:01.178,True,2019-01-01,Frisson,PUBLIC,,True,Fluid Interfaces,False
sleep-creativity,tomasero,False,"<p>Sleep is a forgotten country of the mind. A vast majority of our technologies are built for our waking state, even though a third of our lives are spent asleep. Current technological interfaces miss an opportunity to access the unique, imaginative, elastic cognition ongoing during dreams and semi-lucid states. In turn, each of us misses an opportunity to use interfaces to influence our own processes of memory consolidation, creative insight generation, gist extraction, and emotion regulation that are so deeply sleep-dependent.&nbsp;In this project, we explore ways to augment human creativity by extending, influencing, and capturing dreams in stage-1 sleep. It is currently impossible to force ourselves to be creative because so much creative idea association and creative incubation happens in the absence of executive control and directed attention. Sleep offers an opportunity for prompting creative thought in the absence of directed attention, if only dreams can be controlled.</p><p>During sleep onset, a window of opportunity arises in the form of hypnagogia, a semi-lucid sleep state where we all begin dreaming before we fall fully unconscious. Hypnagogia is characterized by phenomenological unpredictability, distorted perception of space and time, and spontaneous, fluid idea association. Edison, Tesla, Poe, and Dalí each accessed this state by napping with a steel ball in hand to capture creative ideas generated in hypnagogic microdreams when it dropped to the floor below.</p><p>In this project we modernize this technique, using an interactive social robot accompanied with an EEG system, muscular sleep stage tracking system, and auditory biofeedback. We are able to influence, extract information from, and extend hypnagogic microdreams for the first time: we found that active use of hypnagogia with the system can augment human creativity. This system enables future research into sleep, an underutilized and understudied state of mind vital for memory, learning, and creativity.</p><p>This work has been hugely collaborative. The following people, in alphabetical order by first name, have all made it possible: Abhinandan Jain, Eyal Perry, Ishaan Grover, Matthew Ha, Oscar Rosello, Pedro Reynolds-Cuéllar, Robert Stickgold, and Tomás Vega. For an in depth dive, see the FAQ below and see more on <a href=""http://adamjhh.com/dormio"">this website</a>.</p><br>",,,2019-03-26 14:01:38.378,True,2017-09-01,Dormio: Interfacing with Dreams,PUBLIC,http://www.adamjhh.com/,True,Fluid Interfaces,False
chill-out,tomasero,False,"<p>Temperature influences our perception and cognition both consciously and subconsciously. These effects are rooted in our bodily experiences and interactions with the environment, and are even embedded as metaphors in our language. By learning how temperature affects us in different contexts, we can make use of that knowledge to create interventions that help us with personal growth.</p><p>This project seeks to apply thermal interfaces to assist with emotion and attention regulation. Stress and attention levels can be inferred using implicit user inputs such as electrodermal activity, heart rate variability, and relative facial temperature. This information can then be used to determine appropriate thermal feedback to implicitly modify the user’s perception and aid with emotional and attention regulation in a minimally disruptive fashion.</p>",,,2018-04-25 20:05:40.570,True,2018-02-01,Chill.out,PUBLIC,,True,Fluid Interfaces,False
wearable-wisdom,tomasero,False,"<p>Having good mentors and role models is important for personal growth. Knowledge, advice, and inspiration from people a person admires can help motivate people when making life choices. Empirical research has shown that having a role model and insights from a mentor can positively affect the performance and progression of a person's career. However, such advice is not always available from the right people at the right time. Some of our personal heroes have passed away leaving only their writings and other artifacts. We present Wearable Wisdom, a context-aware, audio-based system in a glasses form factor for mediating wisdom from personal mentors to users. Our novel system offers just-in-time knowledge, advice, and inspiration from these mentors, based on the user's inquiry and current context. It does so by performing automated semantic analysis of the mentors' written text and selecting the most relevant quote to the user's inquiry.&nbsp;</p>",,,2019-04-28 03:26:25.734,True,2018-10-18,Wearable Wisdom,PUBLIC,,True,Fluid Interfaces,False
engineering-dreams,tomasero,False,"<p>Our dream is a sci-fi future, where dreams are controllable.&nbsp;</p><p>We are working to build technology that interfaces with the sleeping mind.&nbsp;As the dreamer descends into sleep, we track different sleep-stages using brain activity, muscle tension, heart rate, and movement data.&nbsp;External stimuli in the form of scent, audio, and muscle stimulation affect the content of the dreams. We are working on integrating multiple projects&nbsp; developed at the Fluid Interfaces group towards a vision where sleep is controllable.</p><p><a href=""http://dreams.media.mit.edu"">dreams.media.mit.edu</a><br></p>",,,2019-04-30 15:12:45.024,True,2018-03-21,Engineering Dreams,PUBLIC,,True,Fluid Interfaces,False
byte_it,tomasero,False,"<h2>Discreet Teeth Gestures for Mobile Device Interaction</h2><p>Byte.it is a miniaturized, discreet interface that uses teeth gestures for hands-free input for wearable computing.&nbsp;<br></p><p>As humans, we are constantly seeking to communicate and consume information, and mobile devices give us&nbsp; access the world wide web, our digital selves, and all our digital assets with the touch of our fingers . Context can temporarily reduce our abilities to perform certain activities, preventing us from having a fluid interaction with mobile computing. Hands are not always available, and sustained visual attention is often required for successful task performance and social norms. Current screen-based interfaces are not designed to be used by a person engaged in another attention demanding activity such as walking , talking, or driving, leading to ineffective interactions and even dangerous situations.</p><p>Audio interfaces are a potential solution as they can provide a high-bandwidth communication channel without requiring visual attention. Speech has been the predominant interaction modality for audio interfaces, but it can be ineffective in situations with loud environmental noise, or inappropriate in certain social or dynamic on-the-go contexts. Recent work has explored teeth gestures as a solution for interaction in these contexts, but these attempts are limited by the number of gestural primitives recognized (bandwidth) and the discreteness of the interfaces used to detect these gestures.</p><p>Byte.it expands on this work by exploring the use of a smaller and more unobtrusively positioned sensor (accelerometer and gyroscope) for detecting tooth clicks of different groups of teeth and bite slides for everyday human-computer interaction.&nbsp;Initial results show that an unobtrusive position on the lower mastoid close to the mandibular condyle can be used to classify teeth tapping of four different teeth groups&nbsp;(front, back, left, and right teeth click)&nbsp;with an accuracy of 89 percent, or an accuracy of 84 percent for seven different teeth clicking and bite sliding gestures (front,&nbsp;left, and right click, and front, back, left, and right slide).</p><p>The applications currently being explored are centered around dynamic, on-the-go, hands-and-eyes-free contexts. For example, (1) controlling the different commands of a media player, such as play/pause, volume, and current time of a song, podcast, or audiobook. Productivity-wise, being able to subtly (2) start and stop audio recordings of conversations or meetings, and tag relevant events that might be worth reviewing later. Teeth gestures could also allow for a discrete and rapid way to (3) accept or reject incoming &nbsp;alerts, notifications, and reminders, while minimizing task-switch time.&nbsp;A minimal set of teeth gestures could also enable the seamless&nbsp;(4)&nbsp;access of information streams such as &nbsp;messages, emails, news, or relevant notes about the person, place, and/or time of interest that could enhance the current interaction.</p><p>This research aims to investigate the following:</p><p>1) Understand how people could use teeth gestures to perform specific interaction commands in order to establish a standardized teeth gesture language.</p><p>2) Identify the optimal position of the sensor to achieve the highest gesture classification accuracy possible while ensuring a discreet form factor.</p><p>3) Measure the performance of our classification algorithm in the wild, while sitting, standing, walking, running, and cycling.</p><p>4) Assess the usability of the interface + applications in the wild.</p>",,,2019-05-09 16:26:43.481,True,2017-09-07,Byte.it,PUBLIC,http://www.tomasvega.com,True,Fluid Interfaces,False
cue,davidsu,False,"<h2>Exploring contextual multimodal cues as memory aids&nbsp;</h2><p><b>Objective&nbsp;<br></b></p><p>We are exploring the potential of proximity-triggered contextual audio and visual cues to help early-stage Alzheimer’s patients&nbsp;recall familiar people and places. In particular, we are using proximity beacons to determine when the user is physically close to another person, such as a loved one. The beacons will then trigger cues in the form of:</p><ol><li>audio conveying contextual information such as name, relationship, time/place/details of last interaction;</li><li>images and video (using AR) showing previous interactions along with text displaying contextual information; and</li><li>music in the form of specific songs associated with specific individuals.</li></ol><p><b>Research Questions</b></p><p>We’re interested in tackling the following questions:</p><ul><li>Which cue modalities are the most effective in improving recognition in early-stage Alzheimer’s patients?</li><li>What advantages and challenges are afforded by each of the different modalities?</li></ul>",2017-12-20,,2018-12-14 17:56:32.014,True,2017-09-22,Cue,LAB-INSIDERS,,False,Opera of the Future,False
seasons-change-together,davidsu,False,"<p><i>Seasons Change Together&nbsp;</i>is a collaborative song construction experience for multiple simultaneous participants. It represents a first step towards the creation of a framework, designed for interactive multiplayer musical experiences, that explores the potential for technology-enabled systems to facilitate collaborative creativity through expression, the emotional affordances of musical storytelling, and the spatiotemporal boundaries of co-presence.</p><p>Participants are presented with multiple interfaces that determine the musical texture, rhythmic patterns, and lyrical content of an interactive song. Individual participants can freely move between and share interfaces as they wish, allowing the experience to accommodate play sessions with variable user counts as well as encouraging participants to actively engage with all aspects of the song's construction through collaborative composition.</p><p>Drawing inspiration from improvisational practice in addition to game design and interactive storytelling, <i>Seasons Change Together</i> strives to open up new possibilities for experiencing music, narrative, and creativity in a social environment.</p>",,,2019-05-10 15:20:33.103,True,2018-09-01,Seasons Change Together,PUBLIC,,True,Opera of the Future,False
evergreen-blues,davidsu,False,"<p><i>Evergreen Blues</i> is a suite of interactive songs that together provide a collaborative musical narrative experience in the form of a multiplayer operatic game.</p><p><dfn class=""dictionary-of-numbers"">Two players simultaneously control </dfn>the construction and direction of a piece of music through the use of a real-time lyrical conversation system, allowing for granular control of musical expression. Choices made in <dfn class=""dictionary-of-numbers"">one song influence the </dfn>outcomes of the next, paving the way for multi-scene interactive experiences grounded in narrative principles of persistence and emotional consequence.</p><p>This work seeks to provide a novel means of creating and understanding multi-user, interactive music systems in which users participate in active and collaborative music-making in conjunction with narrative engagement. It is the goal that this work will open up new possibilities for experiencing music, narrative, and social interaction.&nbsp;</p>",,,2019-05-10 15:21:17.020,True,2019-01-01,Evergreen Blues,PUBLIC,,True,Opera of the Future,False
emotionally-intelligent-playback,davidsu,False,<p>Emotionally Intelligent Music Playback opens possibilities to various emotional trajectories through a piece of music. The listener can navigate through emotional territories via a touchscreen interface. The system transitions seamlessly to corresponding emotional interpretations extracted from various existing renditions of the same composition.</p>,,,2018-04-27 21:28:46.680,True,2017-09-01,Emotionally Intelligent Playback,PUBLIC,,True,Opera of the Future,False
joy-branch,davidsu,False,"<p>The Joy Branch project explores different user interfaces to allow parrots to shape their sonic environment.&nbsp;&nbsp;Animal agency—control of the environment—is an important and underutilized element of captive care.&nbsp;&nbsp;Parrot species are vocal learners, and as such are highly attuned to their sonic environment. Much of their brains are involved in the production and analysis of sound, and yet their sonic environment in managed care does not provide a rich experience. In this project, we assess the efficacy of new enrichment techniques that have the potential to improve the lives of these birds through music.&nbsp;&nbsp;The project involves the placement of sonic enrichment elements into the birds’ enclosures under controlled and supervised conditions.</p><p>The ""joystick branch"" element exposes only a standard wooden perch to the birds. The aim is to create naturalistic interactive methods for birds to generate sounds, and to assess their optional engagement with these new modes of control.&nbsp;</p>",2019-12-31,,2019-04-18 01:19:48.567,True,2019-01-01,Joy Branch,PUBLIC,,True,Opera of the Future,False
gammalan,davidsu,False,"<p>Gammalan is an interactive musical experience that uses music information retrieval techniques in conjunction with game design principles to engage audiences in creative behavior that combines the power of familiar songs with neural entrainment on multiple temporal scales.</p><p>A preliminary system analyzes and processes existing recordings, manipulating properties such as rhythm and harmony, while introducing synthesized frequencies in a musically informed manner. The recordings are then presented in an exploratory <dfn class=""dictionary-of-numbers"">3D game-like </dfn>environment that encourages active and playful engagement with the recorded music.</p>",,,2018-08-18 22:07:44.931,True,2018-02-01,Gammalan,PUBLIC,,True,Opera of the Future,False
adaptive-music-for-affect-improvement,davidsu,False,<p>Adaptive Music for Affect Improvement (AMAI) is a music generation and playback system with the goal of steering the listener toward a state of more positive affect. AMAI utilizes techniques from game music in order to adjust elements of the music being heard; such adjustments are made adaptively in response to the valence levels of the listener as measured via facial expression and emotion detection.</p>,,,2019-01-22 15:44:58.691,True,2017-10-01,Adaptive Music for Affect Improvement,PUBLIC,,True,Opera of the Future,False
cue,vparth,False,"<h2>Exploring contextual multimodal cues as memory aids&nbsp;</h2><p><b>Objective&nbsp;<br></b></p><p>We are exploring the potential of proximity-triggered contextual audio and visual cues to help early-stage Alzheimer’s patients&nbsp;recall familiar people and places. In particular, we are using proximity beacons to determine when the user is physically close to another person, such as a loved one. The beacons will then trigger cues in the form of:</p><ol><li>audio conveying contextual information such as name, relationship, time/place/details of last interaction;</li><li>images and video (using AR) showing previous interactions along with text displaying contextual information; and</li><li>music in the form of specific songs associated with specific individuals.</li></ol><p><b>Research Questions</b></p><p>We’re interested in tackling the following questions:</p><ul><li>Which cue modalities are the most effective in improving recognition in early-stage Alzheimer’s patients?</li><li>What advantages and challenges are afforded by each of the different modalities?</li></ul>",2017-12-20,,2018-12-14 17:56:32.014,True,2017-09-22,Cue,LAB-INSIDERS,,False,Object Based Media,False
large-user-interface-with-gesture-and-voice-feedback,vparth,False,"<p>LUI is a scalable, multimodal web-interface that uses a custom framework of nondiscrete, free-handed gestures and voice to control modular applications with a single stereo-camera and voice assistant. The gestures and voice input are mapped to ReactJS web elements to provide a highly-responsive and accessible user experience. This interface can be deployed on an AR or VR system, heads-up displays for autonomous vehicles, and everyday large displays.</p><p>Integrated applications include media browsing for photos and YouTube videos. Viewing and manipulating 3D models for engineering visualization are also in progress, with more applications to be added by developers in the longer-term. The LUI menu consists of a list of applications which the user can ""swipe"" and ""airtap"" to select an option. Each application has its unique set of non-discrete gestures to view and change content. If the user wants to find a specific application, they can also say a voice command to search or directly go to that application. Developers will be able to easily add more applications because of the modularity and extensibility of this web platform.</p><p>For more information, contact graduate researcher Vik Parthiban at vparth@mit.edu.</p><p>Advisors:<br>V. Michael Bove, Director, Object-Based Media group<br>Zach Lieberman, openFrameworks&nbsp;<br>John Underkoffler, CEO, Oblong Industries; Scientific advisor, <i>Minority Report</i> and <i>Iron Man</i> interface</p>",,,2019-04-21 20:40:17.351,True,2019-01-01,LUI: Large User Interface with Gesture and Voice Feedback,PUBLIC,https://vikparthiban.wordpress.com/,True,Object Based Media,False
mixed-reality-robots,vparth,False,"<p>This is a joint proposal from graduate students <a href=""https://www.media.mit.edu/people/vparth/overview/"">Vik Parthiban</a> (Media Lab) and <a href=""https://www.csail.mit.edu/person/andrew-spielberg"">Andrew Spielberg</a> (<a href=""https://www.csail.mit.edu/research/distributed-robotics-laboratory"">MIT CSAIL, Distributed Robotics Laboratory</a>).&nbsp;</p><p>With the release of the <a href=""https://www.magicleap.com/creator"">Magic Leap Creator</a>&nbsp;and <a href=""https://developer.leapmotion.com/northstar/"">Leap Motion NorthStar</a> platform for new mixed-reality and augmented-reality applications, we propose a new computer-aided design (CAD) tool for simulating and deploying robots. Our goal is to better understand locomotion in 3D space and diagnose character movement under multiple constraints.&nbsp;</p><p>We will develop the ""Magic Leap Design and Control Toolbox,"" a suite of new interactive algorithms and implementations to directly build structures within the environment using captured information about the world. A new system for gesturing the desired path or degrees of freedom of an object will automatically translate the gestures into robot and character control inputs. The system will be based on algorithms that synthesize natural gestural control based on the desired motion.</p>",,,2019-04-18 14:43:04.547,True,2018-05-04,Designing robots in mixed reality,PUBLIC,,True,Object Based Media,False
cosmetic-light,vparth,False,"<p>From skin to light: an artistic dialog between cosmetics, lasers, and lighting between the human and microscopic scale.&nbsp;</p><p>Cosmetics essentially diffract and scatter light in a way that makes the wearer appear different to the naked eye. In this project we propose an illumination grammar between cosmetic projects and&nbsp;coherent light lasers, as a suggestion for alternative interior lighting as well as artistic effect and expression. From skin to light, we explore these artistic archetypes.</p>",,,2019-02-07 15:55:16.524,True,2018-02-06,Cosmetic Light,PUBLIC,https://ninalutz.github.io,True,Object Based Media,False
zl-vortice,adelineg,False,"<p>This project is currently promoting a survey of data from the East Side (Zona Leste) of the city of Sao Paulo, Brazil. The aim is to detect the landscape dynamics: infrastructure and urban planning, critical landscapes, housing, productive territory, recycling, and public space. The material will be made available on a digital platform, accessible by computers and mobile devices: a tool specially developed to enable local communities to disseminate productive and creative practices that occur in the area, as well as to enable a greater participation in the formulation of public policies. ZL Vortice is an instrument that will serve to strengthen social, productive, and cultural networks of the region.</p>",2016-01-01,--Choose Location,2017-01-08 20:05:40.680,True,2015-01-01,ZL Vortice,PUBLIC,,False,Civic Media,False
interactive-journaling,sooyeon6,False,"<p>We developed a smartphone application that detects users’ affect and provides personalized positive psychology interventions in order to enhance users’ psychological wellbeing. Users’ emotional states were measured by analyzing facial expressions and the sentiment of SMS messages. A virtual character in the application prompted users to verbally journal about their day by providing three positive psychology interventions. The system used a Markov Decision Process (MDP) model and a State-Action-Reward-State-Action (SARSA) algorithm to learn users’ preferences about the positive psychology interventions. Nine participants were recruited for an experimental study to test the application. They used it daily for three weeks. The interactive journaling activity increased participants’ arousal and valence levels immediately following each interaction, and we saw a trend toward improved self-acceptance levels over the three week period. The interaction duration increased significantly throughout the study as well. The qualitative analysis on journal entries showed that the application users explored and reflected on various aspects of themselves by looking at daily events, and found novel appreciation for and meanings in their daily routine.</p>",2016-12-31,--Choose Location,2017-06-05 16:12:47.957,True,2015-01-01,Interactive Journaling,PUBLIC,,False,Personal Robots,False
robot-vocal-expressiveness,sooyeon6,False,"<p>Prior research with preschool children has established that book reading, especially when children are encouraged to actively process the story materials through dialogic reading, is an effective method for expanding young children’s vocabulary.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">A growing body of research also suggests that social robots have potential as learning companions and tutors for young children’s early language education. Social robots are new technologies that combine the adaptability, customizability, and scalability of technology with the embodied, situated world in which we operate.</span></p><p>In this project, we asked whether a social robot can effectively engage preschoolers in dialogic reading. Given that past work has shown that children can and do learn new words from social robots, we investigate what factors modulate their learning. In particular, we looked at whether the verbal expressiveness of the robot impacted children’s learning and engagement during a dialogic reading activity.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">This project was funded by an NSF Cyberlearning grant.</span></p>",2017-09-01,,2019-03-02 01:42:38.931,True,2015-09-01,Robot Expressiveness Affects Children's Learning,PUBLIC,http://robotic.media.mit.edu/portfolio/robot-vocal-expressivity,False,Personal Robots,False
children,sooyeon6,False,"<p>When learning from human partners, infants and young children will pay attention to nonverbal signals, such as gaze and bodily orientation, to figure out what a person is looking at and why. They may follow gaze to determine what object or event triggered another's emotion, or to learn about the goal of another's ongoing action. They also follow gaze in language learning, using the speaker's gaze to figure out what new objects are being referred to or named.</p><p>In this project, we examine whether young children will attend to the same social cues from a robot as from a human partner during a word learning task, specifically gaze and bodily orientation.</p>",2015-12-31,,2017-06-07 16:38:44.917,True,2014-09-01,Children Use Nonverbal Cues to Learn from Robots,PUBLIC,http://robotic.media.mit.edu/portfolio/children-use-nonverbal-cues-learn-robots/,False,Personal Robots,False
tega-a-new-robot-platform-for-long-term-interaction,sooyeon6,False,"<p>Tega is a new robot platform designed to support long-term, in-home interactions with children, with applications in early-literacy education from vocabulary to storytelling.<br></p>",,--Choose Location,2018-02-02 12:45:47.861,True,2015-01-01,Tega: A New Social Robot Platform,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,False
moodyboost,sooyeon6,False,"<p>Major depressive disorder is the leading cause of disability in the US for ages 15-44 and affects 14.8 million American adults, or about 6.7 percent of the US population age 18 and older. However, individual psychotherapy is still not yet widely accessible to the majority of people who need psychological interventions, and are not available to patients on a daily basis.&nbsp;</p><p>In order to increase accessibility to therapeutic interventions, we are developing a social robot that provides daily positive psychology interventions to&nbsp;enhance people's psychological wellbeing at their home. Our system uses multi-modal data&nbsp;and a reinforcement learning approach to personalize the interventions for each user based on his/her preferences, affect, and engagement.&nbsp; In our one-month deployment study, we expect to find higher engagement and better wellbeing from participants who are provided personalized interventions than from participants with non-personalized interventions.&nbsp;&nbsp;</p>",,,2019-05-20 18:58:40.920,True,2018-06-01,MoodyBoost,LAB-INSIDERS,,True,Personal Robots,False
personalized-emotional-wellness-coach,sooyeon6,False,"<p>The diagnosis and tracking of mood disorders still rely on clinical assessments, originating more than 50 years ago, of self-reported depressive symptoms via surveys and interviews. Such methods are known to provide limited accuracy and reliability in addition to being costly to track and scale. Once a problem is detected, providing personalized daily intervention and support is also too costly and does not scale. The goal of this pilot project is to develop a proof of concept of personalized emotional wellness coach focusing on key technology modules to create an emotionally intelligent social robot for this targeted domain. We shall also conduct a pilot evaluation with a five-week user study to evaluate the robot coach with respect to its ability to successfully sustain a user long-term adherence (i.e., daily self-report and efficacious advice)—with the expected result that it is more effective than state-of-the-art, gamified mobile apps currently used.&nbsp;</p>",2019-09-30,,2019-04-22 15:26:39.440,True,2018-09-01,Personalized Emotional Wellness Coach,PUBLIC,,True,Personal Robots,False
responsive-communities-pilot-project-in-jun-spain,billp,False,"<p>To gain insights into how digital technologies can make local governments more responsive and deepen citizen engagement, we are studying the Spanish town of Jun (population 3,500). For the last four years, Jun has been using Twitter as its principal medium for citizen-government communication. We are mapping the resulting social networks and analyzing the dynamics of the Twitter interactions, in order to better understand the initiative's impact on the town. Our long-term goal is to determine whether the system can be replicated at scale in larger communities, perhaps even major cities.</p>",2016-12-31,--Choose Location,2017-02-16 16:31:17.794,True,2014-09-01,"Responsive Communities: Pilot Project in Jun, Spain",PUBLIC,,False,Scalable Cooperation,False
the-electome-measuring-responsiveness-in-the-2016-election,billp,False,"<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “<a href=""http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/"">Horse Race of Ideas</a>”).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf"">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/""> share of conversation</a> or coverage that <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/"">any given issue </a>or candidate commands on Twitter and in the news media, respectively—and how the two platforms are aligned<br></li><li>which issues are <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/"">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of “<a href=""http://fusion.net/story/304384/election-incivility-on-twitter/"">incivility</a>” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=""http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/""> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM’s deployment of Electome analytics has been <a href=""http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will"">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets—including the <a href=""https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/"">Washington Post</a>, <a href=""http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps"">Bloomberg News</a>, <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf"">CNN Politics</a> and Fusion—as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=""http://www.debates.org/"">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates’ moderators and credentialed journalists<br></li><li>also collaborated with the <a href=""http://ropercenter.cornell.edu/"">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center’s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>",,--Choose Location,2019-04-19 18:40:58.879,True,2015-09-01,The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,True,Scalable Cooperation,False
responsive-communities-pilot-project-in-jun-spain,msaveski,False,"<p>To gain insights into how digital technologies can make local governments more responsive and deepen citizen engagement, we are studying the Spanish town of Jun (population 3,500). For the last four years, Jun has been using Twitter as its principal medium for citizen-government communication. We are mapping the resulting social networks and analyzing the dynamics of the Twitter interactions, in order to better understand the initiative's impact on the town. Our long-term goal is to determine whether the system can be replicated at scale in larger communities, perhaps even major cities.</p>",2016-12-31,--Choose Location,2017-02-16 16:31:17.794,True,2014-09-01,"Responsive Communities: Pilot Project in Jun, Spain",PUBLIC,,False,Social Machines,False
human-atlas,msaveski,False,"<p>This project aims to map and analyze the publicly knowable social connections of various communities, allowing us to gain unprecedented insights about the social dynamics in such communities. Most analyses of this sort map online social networks, such as Twitter, Facebook, or LinkedIn. While these networks encode important aspects of our lives (e.g., our professional connections) they fail to capture many real-world relationships. Most of these relationships are, in fact, public and known to the community members. By mapping this publicly knowable graph, we get a unique view of the community that allows us to gain deeper understanding of its social dynamics. To this end, we built a web-based tool that is simple, easy to use, and allows the community to map itself. Our goal is to deploy this tool in communities of different sizes, including the Media Lab community and the Spanish town of Jun.</p>",2016-12-31,--Choose Location,2017-04-11 21:40:23.904,True,2015-01-01,Human Atlas,LAB-INSIDERS,,False,Social Machines,False
flipfeed,msaveski,False,"<p class=""""><a href=""https://chrome.google.com/webstore/detail/flipfeed/glfjakcglibkihmaaekjcaefcbebgcfg?hl=en-US&amp;gl=US"">FlipFeed is a Google Chrome Extension</a> that enables Twitter users to replace their own feed with that of another real Twitter user. Powered by deep learning and social network analysis, feeds are selected based on inferred political ideology (""left"" or ""right"") and served to users of the extension. For example, a right-leaning user who uses FlipFeed may load and navigate a left-leaning user's feed to observe the news stories, commentary, and other content they consume. The user can then decide to flip back to their own feed or repeat the process with another feed. We hope tools like FlipFeed will enable us to explore how social media platforms can be used to mitigate, rather than exacerbate, ideological polarization by helping people explore and empathize with different perspectives.<br></p>",,,2017-03-08 16:10:50.709,True,2016-11-11,FlipFeed,PUBLIC,http://flipfeed.media.mit.edu,True,Social Machines,False
conversation-health-twitter-toxicity-network-structure,msaveski,False,"<p>We investigate the nature of toxic conversations on Twitter, focusing on the important subset of public discourse related to mainstream news. We analyze 71,000 conversations prompted by tweets from five major news outlets, comprising more than 12.3 million tweets and replies posted by 1.13 million users. We study the relationship between tweet toxicity, the reply-tree structure of the tweets, and the network of social connections between the tweet authors at three main levels: the individual level of the tweet authors, the dyadic level (between a tweet author and a reply author), and the overall reply-tree and follow graph structure of the conversation.</p>",,,2019-04-10 20:19:57.770,True,2018-09-01,Conversation health: Twitter toxicity and network structure,LAB-INSIDERS,,True,Social Machines,False
an-autonomous-running-exoskeleton-for-ankle-plantar-exion-assistance,xingbang,False,"<p>Lower-limb exoskeletons are of great interest in the robotics community because of their various applications in enhancement and rehabilitation. We design an autonomous exoskeleton for ankle plantarﬂexion assistance. The exoskeleton has a high efﬁciency transmission system, with reduction ratio up to 28.8 : 1 without the need of a gear box. This allows relocating the actuator to the hip to reduce device inertia. A simple feed-forward torque controller based on ﬁeld-oriented voltage control for controlling brushless DC motors is used to control the system. Through various performance tests, the exoskeleton was shown to provide a torque control bandwidth of 17.5Hz and can effectively track biological torque proﬁles. This exoskeleton establishes an autonomous platform for experiments involving ankle assistance, including testing control algorithms for metabolic cost reduction.<br></p>",2018-12-31,,2018-10-22 19:33:13.961,True,2017-12-01,An Autonomous Running Exoskeleton for Ankle Plantarﬂexion Assistance,LAB-INSIDERS,,False,Biomechatronics,False
equalais-adversarial-attacks-for-facial-recognition,ggreene,False,"<p>This 2018 AI and Governance Assembly project, <a href=""http://equalais.media.mit.edu/"">EqualAIs</a>, was a multifaceted investigation into the technical, policy and societal questions raised by the increasing capabilities of facial recognition software and the unprecedented  capability for automated, real time identification and tracking of  individuals.</p><p>Our team demonstrated the technical feasibility of facial recognition adversarial attacks through the creation of a working prototype, successfully attacking the major APIs' facial recognition classifiers. We filed a FOIA with partners including the ACLU for information about US Customs' use of facial recognition algorithms and we sought to encourage public dialogue about facial recognition policy choices through a series of public talks and the creation of open source resources.<br></p>",2018-04-20,,2018-10-18 01:27:03.683,True,2018-02-01,"EqualAIs: Facial Recognition, Adversarial Attacks and Policy Choice",PUBLIC,,False,Other,False
beeme,niccolop,False,"<p><a href=""https://twitter.com/beeme_mit"">BeeMe</a> a massive immersive social game that directly draws inspiration from<a href=""https://owa.exchange.mit.edu/owa/redir.aspx?C=ZxOiBK_RTx9yjsDV33RV7Hlj5grGY-DdQlJ9osuamIzqlCueCD7WCA..&amp;URL=https%3a%2f%2fwww.imdb.com%2ftitle%2ftt3531824%2f""> popular culture</a>,<a href=""https://owa.exchange.mit.edu/owa/redir.aspx?C=QFKraQ8VaK9kjKe7_S9IhtxaoVSC9OSCF9QKfrMOtGDqlCueCD7WCA..&amp;URL=https%3a%2f%2fwww.amazon.com%2fCircle-Dave-Eggers%2fdp%2f159413961X""> literature</a>,<a href=""https://owa.exchange.mit.edu/owa/redir.aspx?C=4UOPlp7_QKNzFMGuulv4ZaXzzgukEG_B2e8c1iqa8_XqlCueCD7WCA..&amp;URL=https%3a%2f%2fwww.theguardian.com%2fartanddesign%2f2014%2fmay%2f12%2fmarina-abramovic-ready-to-die-serpentine-gallery-512-hours""> performing arts</a>, gaming, and YouTube streaming culture.</p><p>On Halloween night (10/31/2018) at 11pm ET, an actor will give up their free will and let Internet users control their every action. The event will follow the story of an evil AI by the name of <i>Zookd, </i>who has accidentally been released online. Internet users will have to coordinate at scale and collectively help the actor (also a character in the story) to defeat <i>Zookd</i>. If they fail, the consequences could be disastrous.</p><p>To play, simply connect to<a href=""http://beeme.online""> beeme.online</a> on the night of Halloween at 11pm ET. We believe BeeMe will provide insight into ways we can fundamentally change the nature of online interactions and entertainment.&nbsp;</p>",2018-11-01,,2018-10-31 13:08:43.651,True,2018-10-01,BeeMe,PUBLIC,,False,Scalable Cooperation,False
black-rock-atlas,niccolop,False,"<p>Burning Man is a magical place that gets the best of human creativity and collaboration to flourish. To further understand what makes this magic happen, we are creating the first ever Black Rock Atlas–a map of the social patterns and networks that exist on the playa. </p><p>To do this, we are tracking the decentralized journey of a multitude of vessels through the gift economy of Burning Man with GPS technology and generative photography. The Atlas will explore new ways of community interaction, storytelling, and data visualization.</p>",2018-12-11,,2018-10-15 20:44:54.352,True,2018-08-15,Black Rock Atlas,PUBLIC,https://blackrockatlas.mit.edu,False,Scalable Cooperation,False
deep-angel-ai,niccolop,False,"<p><b>Deep Angel&nbsp;</b>is an artificial intelligence that erases objects from photographs. The algorithm is hosted on <a href=""http://deepangel.media.mit.edu"">http://deepangel.media.mit.edu</a>, which enables anyone&nbsp;to interact with the AI and explore what it can disappear.</p><p>Part philosophy, part technology, and part art, Deep Angel is designed to spark a series of conversations on technology in our daily lives and AI and media manipulation.&nbsp;&nbsp;</p><p>Deep Angel draws from&nbsp; Walter Benjamin's description of Paul Klee's Angelus Novus, the angel of history who has clairvoyance into the dark side of what appears to be progress. The angel sees the unravelling of all that matters in the world and would like to alert the world about his vision, but he's caught in the storm of progress and can't communicate any messages. The images that Deep Angel generates are intended to deliver the message that Angelus Novus would have sent if he could.&nbsp;</p><p>The algorithm applies computer vision techniques to automatically (1) detect and outline objects in images, (2) remove the outlined object from the image, and (3) imagine what the image would look like if that outlined object were removed from the image. Any image uploaded and transformed by Deep Angel can be published on the Deep Angel website by clicking the ""Publish to Deep Angel"" button.&nbsp;</p><p>The AI's performance varies across photographs. Sometimes, it's impossible to tell what has been disappeared. Other times, the images appear similar to the images from Adrian Piper's <i>Everything</i> series. The more people interact with the algorithm, the more attuned people will be to the potential and limitations of modern AI to manipulate the media. It's now possible to automate the vanishing commissar in Soviet photography, but the AI is not yet perfect. Below are two examples of the Deep Angel AI effect: (1) a gif generated by Deep Angel showing a father and daughter disappearing in the wilderness and (2) two images showing the before and after of Deep Angel peering into a photo of a professional surfer.&nbsp;</p>",,,2019-02-14 19:46:25.924,True,2018-08-06,Deep Angel: The AI behind the aesthetics of absence,PUBLIC,http://deepangel.media.mit.edu/,True,Scalable Cooperation,False
8k-time-into-space,ohmata,False,"<p>8K Time into Space is a user interface for a video exploration system with an 8K display. 8K is an ultra high-definition video system and it can present a huge amount of visual content on one display. In our system, video thumbnails with shifted playback time in chronological order are spaced out like tiles. The time range of a scene that a viewer wants to check can be adjusted with a touch interface, and resolution of the thumbnails is changed depending of the range. 8K Time into Space aims to provide responsive and intuitive experiences for video consumption.</p>",2018-12-31,--Choose Location,2019-04-17 22:34:57.493,True,2015-09-01,8K Time into Space,PUBLIC,,False,Viral Communications,True
election-arg,mhjiang,False,"<p>Only 40% of the eligible population votes in the typical US midterm election, and among young people turnout is even lower. In this experiment, we develop a game that encourages people to influence their friends to physically go to the polls. The system is reminiscent of <a href=""https://www.media.mit.edu/projects/fiftynifty/overview/"">Fifty Nifty</a>, where people competed to amass points by both calling representatives and spreading the message to others. In addition to awarding points, vote.lol aims to motivate players by allowing them influence over the outcome of a shared narrative that develops in real-time before (and during) the election. Interactive stories with real-world game mechanics are characteristic of alternate reality games (ARGs), which have received scholarly attention for their potential to instigate viral communications among players who self-organize to solve complex problems. The purpose of this study is to test whether ARG techniques can motivate gamers to solve the intractable problem of getting their peers to vote.</p>",2018-12-31,,2019-04-18 00:52:49.802,True,2018-10-01,Election ARG,PUBLIC,,False,Viral Communications,False
viralcasting,mhjiang,False,"<p>Ditch the truck. Live, collaborative broadcasting through mixed reality.</p>",,,2019-04-18 01:30:20.552,True,2016-09-06,Broadercasting,PUBLIC,,True,Viral Communications,False
let-s-see-a-game,mhjiang,False,"<p><b>In ""Let's see a game!"" we&nbsp;expose the different perspectives in&nbsp;TV&nbsp;sports and news in order to build broadcasting systems that unify rather than divide. We use the galvanizing impact of sports and live events as a forum, and then we add production and viewing opportunities to distinguish fact from opinion and to challenge the basis of those opinions.</b></p><p>In 1951, when the Dartmouth football team played against Princeton, there was deep disagreement between the two schools as to what had happened during the game. In ""They Saw a Game: A Case Study,"" the psychologists Albert Hastorf and Hadley Cantril found that when the <i>same </i>motion picture of the game was shown to a sample of undergraduates at each school,&nbsp; each individual <i>perceived a different game</i>, and their versions of the game was just as ""real"" as other versions were to other people.&nbsp;</p><p><b>However,&nbsp; little is known about whether and how broadcasting media are adding fuel to the fire. In order to study the relationship between storytelling/perspectives and opinion formation, we built the following two applications: ""Let's see a game!"" and ""Let's watch news!""</b></p><p>In the first step, we built an interactive application&nbsp;that exposes different perspectives in sports broadcasting.&nbsp;The application plays two broadcasts of the same game, created for each team's home audience.&nbsp;The user can tune into an audio channel by moving the slider. Additional buttons allow the user to take other actions.</p>",,,2019-04-18 14:04:57.014,True,2017-09-11,Let's see a game!,PUBLIC,,True,Viral Communications,False
enlightened,mhjiang,False,"<p>Media filter bubbles sacrifice shared reality amongst US citizens. We aim to burst these echo chambers by presenting short, automatically summarized news clips to users through a mobile app. The user watches these short clips in sequence and has the option to press a lightbulb that signifies whether the news segment have been enlightening. These clips are generated from SuperGlue metadata and is based on&nbsp;<a href=""https://www.media.mit.edu/projects/news-2/overview/"">News*2</a>. It uses an “Anti-recommender system” that actively expands the user’s horizon—contrary to traditional recommender systems that aim to thicken the walls of echo chambers.</p><p>Follow this <a href=""https://viral.pubpub.org/pub/enlightened/"">link</a> for more information.</p>",,,2019-04-18 01:36:42.924,True,2019-01-01,Enlightened: Broaden Your Views,PUBLIC,,True,Viral Communications,False
superglue,mhjiang,False,"<p>SuperGlue is a core news research initiative that is a ""digestion system"" and metadata generator for mass media. An evolving set of analysis modules annotate 14 DirecTV live news broadcast channels as well as web pages and tweets. The video is archived and synchronized with the analysis. Currently, the system provides named-entity extraction, audio expression markers, face detectors, scene/edit point locators, excitement trackers, and thumbnail summarization. We use this to organize material for presentation, analysis, and summarization. SuperGlue supports other news-related experiments.</p><p>SuperGlue is a framework for media digestion and metadata generation. The digestion work flow also has applications for media more broadly including conversational ecommerce.</p>",,--Choose Location,2019-04-08 16:48:12.894,True,2014-09-01,SuperGlue,PUBLIC,,True,Viral Communications,False
defacto,mhjiang,False,"<p>Despite recent efforts, current fact-checking organizations cannot keep up with the amount of information that is produced and spread throughout the internet [1]. One of the biggest challenges is to minimize the time it takes to verify the claims of a story. We are building a decentralized, crowd-sourced news verification system that aims to leverage the ""wisdom of crowds"" to generate timely labels that can be used to put a badge on news articles. The labels are stored on <a href=""https://ipfs.io/"">IPFS</a> using <a href=""https://underlay.mit.edu/"">The Underlay</a>—a protocol developed by the Knowledge Futures Group at MIT. Governance mechanisms and incentive structures are implemented to hold all parties accountable and to prevent unbalanced concentration of power.</p><p>Follow this&nbsp;<a href=""https://viral.pubpub.org/pub/defacto/"">link</a>&nbsp;for more information.</p><p>[1] <a href=""https://www.washingtonpost.com/news/fact-checker/wp/2018/06/25/rapidly-expanding-fact-checking-movement-faces-growing-pains/?utm_term=.069157def6eb"">https://www.washingtonpost.com/news/fact-checker/wp/2018/06/25/rapidly-expanding-fact-checking-movement-faces-growing-pains/?utm_term=.069157def6eb</a>.</p>",,,2019-04-18 14:34:43.198,True,2019-02-01,Defacto: Decentralized crowdsourced news verification system,PUBLIC,,True,Viral Communications,False
unspoken-news,mhjiang,False,"<p>How something is presented can be as important as the message itself.</p><p>In the age of political polarization and election meddling, it is of vital importance to understand which factors contribute to the formation of public opinions. Television is one of the main sources of information for a large portion of the general population.</p><p>In order to understand discrepancies in news perception, what they are caused by and which implications they might have on shaping public political debate, we first need to understand how television news are constructed and define presentational aspects of the news. Given that, we can build tools that analyze news consumption by the public.</p><p>We aim to use various artificial intelligence techniques to model the ""subcarriers of information"" present in a TV newscast, to automatically detect and understand visual and auditory cues beyond the spoken word including the layout of the set, the affect of the participants, the nature of the motion, and other cues. Our goal is to develop an algorithmic understanding of journalistic choices in the way news content is presented. We also attempt develop an understanding of higher-level characteristics of television news such as television set atmosphere or political bias. This altogether would enable a broad-range, comprehensive algorithmic analysis of <b>how news presentation is trying to shape the public political debate</b>.</p><p><a href=""https://viral.media.mit.edu/pub/unspoken"">Project updates</a> via PubPub</p>",,,2019-04-18 16:51:12.445,True,2018-11-01,Unspoken News,PUBLIC,,True,Viral Communications,False
youtune-1,mhjiang,False,"<p>A web app that is a television with topical tuning rather than channel tuning. You can select the subject, the slant, and the emotional intensity of the people and the captions. Our question is whether these controls will broaden people's perspective and help create a common framework with which we can discuss our attitudes, sentiments, and positions on public issues. This project is a visualization of the infrastructure of Unspoken News and Superglue. These are an evolving resource for broadcast news understanding.</p>",,,2019-04-18 16:50:41.771,False,2018-10-15,YouTune,LAB-INSIDERS,,True,Viral Communications,False
election-arg,smpsnr,False,"<p>Only 40% of the eligible population votes in the typical US midterm election, and among young people turnout is even lower. In this experiment, we develop a game that encourages people to influence their friends to physically go to the polls. The system is reminiscent of <a href=""https://www.media.mit.edu/projects/fiftynifty/overview/"">Fifty Nifty</a>, where people competed to amass points by both calling representatives and spreading the message to others. In addition to awarding points, vote.lol aims to motivate players by allowing them influence over the outcome of a shared narrative that develops in real-time before (and during) the election. Interactive stories with real-world game mechanics are characteristic of alternate reality games (ARGs), which have received scholarly attention for their potential to instigate viral communications among players who self-organize to solve complex problems. The purpose of this study is to test whether ARG techniques can motivate gamers to solve the intractable problem of getting their peers to vote.</p>",2018-12-31,,2019-04-18 00:52:49.802,True,2018-10-01,Election ARG,PUBLIC,,False,Viral Communications,False
civic-link,smpsnr,False,"<p>CivicLink is an online organization in a box. A leader or moderator plugs it in, invites members, and the tools needed to build community action are in place. It is grassroots mobilization recursed to the lower level: we envision extremely large networks of extremely local, single-issue orgs. Core elements are an events calendar and forum for each event. The link is a private server that retains all communications and personal information within it; there is no contribution to an online cloud. The architecture is extensible to add features such as mapping, canvassing, etc. It is designed to be used where groups physically meet and for access via a smartphone app.</p><p>Related work tests whether privacy is important to users, whether games can be used to promote actions, how web sites can be distributed offline via QR codes, and how this link can merge culturally unique resonances.</p>",,,2019-04-18 01:23:05.324,True,2018-09-04,CivicLink,PUBLIC,,True,Viral Communications,False
reality-editor,hobinjk,False,"<p>The Reality Editor is a new kind of tool for empowering you to connect and manipulate the functionality of physical objects. Just point the camera of your smartphone at an object and its invisible capabilities will become visible for you to edit. Drag a virtual line from one object to another and create a new relationship between these objects. With this simplicity, you are able to master the entire scope of connected objects.</p>",2017-06-30,--Choose Location,2018-04-30 14:19:37.322,True,2014-01-01,Reality Editor,PUBLIC,http://www.realityeditor.org,False,Fluid Interfaces,False
origami-robotics,lizbethb,False,"<p><span style=""font-size: 18px; font-weight: 400;"">Robots today can be made of various materials; we are no longer&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">limited to heavy metal parts and assemblies. This project explores ""informal"" materials for movement of&nbsp; structures created from a single sheet of paper. By relying on the flexibility or rigidity of various materials, we use origami folding techniques to achieve a precise movement.&nbsp; Kinematics and mechanical principles seen in pop-up and flat folding structures are implemented using a laminate assembly technique.&nbsp;</span><br></p>",2019-06-01,,2019-04-23 13:35:59.036,True,2019-01-01,Origami Robotics,PUBLIC,,False,Space Enabled,False
earthrise-a-50-year-contemplation,lizbethb,False,"<p>In December of 1968, the first human voyage to the moon catapulted the population of Earth into a new era of space exploration and self-reflection. It was during this voyage that astronauts Bill Anders and Jim Lovell recognized a familiar pale blue dot in the distance and snapped a photo, providing us with the first view of Earth from this distant vantage point. Since its release, this image has been the subject of various works of art and literature. After having seen the Earth from space, some astronauts reported a cognitive shift in awareness about the planet. This shift helped them recognize the fragility of Earth and has inspired feelings of global citizenship. Thanks to the writings of author Frank White, we now call this shift, “The Overview Effect.”<br></p><p>The Media Lab’s Space Enabled research group asks if it is possible to create a similar cognitive shift in Earthlings through an experiential installation piece meant to inspire global citizenship as well as universal citizenship. “Earthrise: A 50 Year Contemplation” will celebrate the original Earthrise photo by creating a meditative space of reflection where participants are transported to the surface of Earth’s moon to reflect on themselves, Earth, and the solar system. The viewer will be immersed in the sensory experience that surrounds them. Through artificially creating “The Overview Effect,” and altering our perspective, might we also inspire a more sustainable approach in our exploration of the solar system?</p><p>Frank White, author of <i>The Overview Effect: Space Exploration and Human Evolution</i>, is launching “The Human Space Program"" through his new book, <i>The Cosma Hypothesis: Implications of the Overview Effect</i> (Emergent Media; February 2019). The goal of the Human Space Program is to create a comprehensive, sustainable, and inclusive plan for exploring and developing the solar system. It is built around White’s “Cosma Hypothesis,” which addresses the question, “What is the purpose of human space exploration? &nbsp;Why has the evolutionary process brought humanity to the brink of becoming a spacefaring species?” White’s surprising conclusion: <i>Homo sapiens</i> have a very significant role to play in the evolution of the universe (Cosma). Space Enabled appreciates the opportunity to dialogue with Frank about these fundamental questions shaping the moral compass of human exploration beyond earth.</p><p><i>By Lizbeth B. De La Torre,&nbsp; Rachael Petersen, Frank White&nbsp; and&nbsp; Danielle Wood</i></p>",,,2019-01-08 17:44:59.020,True,2018-12-23,Earthrise | A 50 Year Contemplation,PUBLIC,,True,Space Enabled,False
design-for-a-citizen-science-and-public-engagement-project-celebrating-antarctica-and-the-southern-ocean,lizbethb,False,"<p>Antarctica is a unique and beautiful continent that is key to the global ecosystem and climate. Research based in Antarctica is helping scientists explore cutting-edge questions in oceanography, physics, climate science, ecology, and more. To most members of the public, however, Antarctica appears to be a cold and inhospitable place with little relevance to daily life. This project asks if combining a citizen science campaign and an immersive museum experience can increase a sense of stewardship to care for Antarctica within people around the world.&nbsp;</p><p>For this project, the Space Enabled research group designed a concept for an&nbsp;interactive, room-scale multi-sensory presentation providing education regarding the Antarctic.This presentation is designed to be placed in an informal education venue such as an aquarium or science museum.&nbsp;The immersive presentation helps participants understand this dynamic region through an experience that combines photos, video, sound, smell,&nbsp; and temperature changes.&nbsp;The information draws on multiple types of data from earth science satellites (i.e., ICESAT), airborne science platforms (i.e., IceBridge), and in-situ sensors (i.e., underwater video cameras and photographs). As participants go through the experience, they hear vignettes about specific research areas in Antarctica, including studies on penguin colonies, ice cores, meteorite searches, and ocean food chains.&nbsp;</p><p>In addition to the physical, interactive presentation, the second aspect of the project involves a companion Citizen Science campaign. Specifically, citizen scientists and students located in countries near the Southern Ocean (including Chile, South Africa, New Zealand, and Australia) are invited to participate in a data collection campaign about their part of the Southern Ocean. A proposed mobile application in both English and Spanish could invite participants to submit photographs of the shoreline on the Southern Ocean. Our team is prototyping a small, low-cost kit that allows citizen scientists to take measurements such as temperature, salinity and pH measurements from coastal areas facing the Southern Ocean. Information from these citizen scientists could be incorporated into the physical presentation in the museum. Meanwhile, the visual aspects of the presentation may be provided online as photo or video files that&nbsp; could be downloaded to host on their own platforms.&nbsp;<br></p>",,,2019-04-19 18:42:30.362,True,2018-03-09,Design for a citizen science and public engagement project celebrating Antarctica and the Southern Ocean,PUBLIC,,True,Space Enabled,False
nutrition-justice,lizbethb,False,"<p>The interactive Nutrition Justice art exhibit uses&nbsp;space-based assets, data science, and&nbsp;projection mapping to showcase local hunger in ways that inspire measurable local action&nbsp;in food insecure communities.</p><p>Hunger in the US has many faces: children, seniors, veterans, and people from rural and marginalized communities are not immune. And as humanity is urbanizing rapidly, so too are food deserts expanding and densifying. Several urban agriculture solutions bring new life to food deserts and serve as a driving force for community development. Yet we need all hands on deck&nbsp;to create inclusive food oases. </p><p>In the spirit of collective action, this exhibit visualizes local hunger to encourage the re-introduction of self-reliance in food deserts and marginalized communities.</p>",,,2019-01-14 19:56:36.741,True,2018-10-01,Nutrition Justice,LAB-INSIDERS,,True,Space Enabled,False
children-s-book-ch,lizbethb,False,"<p>OpenIdeo put out a challenge to create a book targeting children ages 0-3 that would inspire children and their caregivers to read together. The challenge encouraged writers to create stories that center underrepresented identities and reflect the lives of urban communities like Philadelphia.</p><p>We are creating a story that draws parallels between the life of a little Black girl and the life of an astronaut by juxtaposing typical activities like getting ready in the morning, taking the bus or having family gatherings with the activities that an astronaut is expected to conduct for their job requirements. The message of the story is that astronauts, while few in number, are still just normal human beings with a career that children of any identity should be able to see themselves in.</p>",,,2019-04-22 18:25:19.612,True,2019-04-12,Early Childhood Book Challenge,LAB-INSIDERS,,True,Space Enabled,False
origami-robotics,elenack,False,"<p><span style=""font-size: 18px; font-weight: 400;"">Robots today can be made of various materials; we are no longer&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">limited to heavy metal parts and assemblies. This project explores ""informal"" materials for movement of&nbsp; structures created from a single sheet of paper. By relying on the flexibility or rigidity of various materials, we use origami folding techniques to achieve a precise movement.&nbsp; Kinematics and mechanical principles seen in pop-up and flat folding structures are implemented using a laminate assembly technique.&nbsp;</span><br></p>",2019-06-01,,2019-04-23 13:35:59.036,True,2019-01-01,Origami Robotics,PUBLIC,,False,Responsive Environments,False
digital-spiritual-companions-enhancing-our-connection-with-nature,elenack,False,"<p>As AI and AR/VR become more prevalent, their impact on our daily lives grows. But, advances in these technologies come with a further distancing from nature. This research is exploring the possibilities of an integrated, personal companion AI to assist users with emotional and psychological health and balance. </p><p>An AI companion that grows with you, through its years of interaction, will become familiar with your moods, health, behavior, and habits. This knowledge will assist the AI companion in suggesting gentle interventions to improve your quality of life—perhaps some time spent out of doors, in nature, would benefit your concentration and mood. And if your companion is, say, a turtle, its reminder to visit the ocean or a lake holds an extra layer of meaning related to common aspects of the animal. Finally, additional layers of meaning relating to specific indigenous communities are also available. Thus, these digital spirit companions, while reconnecting the user with the natural world, can also provide links to an unknown culture, and open doors to that culture’s wisdom, history, and teachings.</p>",,,2019-04-25 01:28:14.694,True,2019-04-07,Digital Spiritual Companions: Enhancing our connection with nature,LAB-INSIDERS,,True,Responsive Environments,False
black-rock-atlas,zive,False,"<p>Burning Man is a magical place that gets the best of human creativity and collaboration to flourish. To further understand what makes this magic happen, we are creating the first ever Black Rock Atlas–a map of the social patterns and networks that exist on the playa. </p><p>To do this, we are tracking the decentralized journey of a multitude of vessels through the gift economy of Burning Man with GPS technology and generative photography. The Atlas will explore new ways of community interaction, storytelling, and data visualization.</p>",2018-12-11,,2018-10-15 20:44:54.352,True,2018-08-15,Black Rock Atlas,PUBLIC,https://blackrockatlas.mit.edu,False,Scalable Cooperation,False
turingbox,zive,False,"<p>TuringBox is a platform&nbsp;that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms.&nbsp;It is a two-sided marketplace. On one side, <i>AI contributors</i> upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">On the other side, </span><i style=""font-size: 18px; font-weight: 400;"">AI examiners</i><span style=""font-size: 18px; font-weight: 400;"">&nbsp;develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.</span></p>",,,2018-04-05 19:00:33.916,True,2018-03-21,TuringBox: Democratizing the study of AI,PUBLIC,http://turingbox.mit.edu,True,Scalable Cooperation,False
deep-angel-ai,zive,False,"<p><b>Deep Angel&nbsp;</b>is an artificial intelligence that erases objects from photographs. The algorithm is hosted on <a href=""http://deepangel.media.mit.edu"">http://deepangel.media.mit.edu</a>, which enables anyone&nbsp;to interact with the AI and explore what it can disappear.</p><p>Part philosophy, part technology, and part art, Deep Angel is designed to spark a series of conversations on technology in our daily lives and AI and media manipulation.&nbsp;&nbsp;</p><p>Deep Angel draws from&nbsp; Walter Benjamin's description of Paul Klee's Angelus Novus, the angel of history who has clairvoyance into the dark side of what appears to be progress. The angel sees the unravelling of all that matters in the world and would like to alert the world about his vision, but he's caught in the storm of progress and can't communicate any messages. The images that Deep Angel generates are intended to deliver the message that Angelus Novus would have sent if he could.&nbsp;</p><p>The algorithm applies computer vision techniques to automatically (1) detect and outline objects in images, (2) remove the outlined object from the image, and (3) imagine what the image would look like if that outlined object were removed from the image. Any image uploaded and transformed by Deep Angel can be published on the Deep Angel website by clicking the ""Publish to Deep Angel"" button.&nbsp;</p><p>The AI's performance varies across photographs. Sometimes, it's impossible to tell what has been disappeared. Other times, the images appear similar to the images from Adrian Piper's <i>Everything</i> series. The more people interact with the algorithm, the more attuned people will be to the potential and limitations of modern AI to manipulate the media. It's now possible to automate the vanishing commissar in Soviet photography, but the AI is not yet perfect. Below are two examples of the Deep Angel AI effect: (1) a gif generated by Deep Angel showing a father and daughter disappearing in the wilderness and (2) two images showing the before and after of Deep Angel peering into a photo of a professional surfer.&nbsp;</p>",,,2019-02-14 19:46:25.924,True,2018-08-06,Deep Angel: The AI behind the aesthetics of absence,PUBLIC,http://deepangel.media.mit.edu/,True,Scalable Cooperation,False
creaitivity,zive,False,<p>What it is.</p><p>Where it is going.</p><p>List the projects.</p>,,,2019-02-14 19:47:57.622,False,2018-01-01,creAItivity,PUBLIC,,True,Scalable Cooperation,False
ai-spirits,zive,False,,2019-10-31,,2018-11-14 21:20:32.287,False,2018-10-31,AI Spirits,PUBLIC,,True,Scalable Cooperation,False
black-rock-atlas,groh,False,"<p>Burning Man is a magical place that gets the best of human creativity and collaboration to flourish. To further understand what makes this magic happen, we are creating the first ever Black Rock Atlas–a map of the social patterns and networks that exist on the playa. </p><p>To do this, we are tracking the decentralized journey of a multitude of vessels through the gift economy of Burning Man with GPS technology and generative photography. The Atlas will explore new ways of community interaction, storytelling, and data visualization.</p>",2018-12-11,,2018-10-15 20:44:54.352,True,2018-08-15,Black Rock Atlas,PUBLIC,https://blackrockatlas.mit.edu,False,Scalable Cooperation,False
ai_spirits,groh,False,<p>Media manipulation technologies have the power to vanish people from photographs. Yet their souls live on in the deep memory of these algorithms of omission.</p>,,,2019-02-14 19:49:25.678,True,2018-10-22,AI Spirits,PUBLIC,http://spirits.media.mit.edu,True,Scalable Cooperation,False
towards-understanding-the-impact-of-ai-on-labor,groh,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-03-22 16:27:19.712,False,2018-03-01,Towards Understanding the Impact of AI on Labor,PUBLIC,http://www.media.mit.edu/~mrfrank,True,Scalable Cooperation,False
turingbox,groh,False,"<p>TuringBox is a platform&nbsp;that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms.&nbsp;It is a two-sided marketplace. On one side, <i>AI contributors</i> upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">On the other side, </span><i style=""font-size: 18px; font-weight: 400;"">AI examiners</i><span style=""font-size: 18px; font-weight: 400;"">&nbsp;develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.</span></p>",,,2018-04-05 19:00:33.916,True,2018-03-21,TuringBox: Democratizing the study of AI,PUBLIC,http://turingbox.mit.edu,True,Scalable Cooperation,False
deep-angel-ai,groh,False,"<p><b>Deep Angel&nbsp;</b>is an artificial intelligence that erases objects from photographs. The algorithm is hosted on <a href=""http://deepangel.media.mit.edu"">http://deepangel.media.mit.edu</a>, which enables anyone&nbsp;to interact with the AI and explore what it can disappear.</p><p>Part philosophy, part technology, and part art, Deep Angel is designed to spark a series of conversations on technology in our daily lives and AI and media manipulation.&nbsp;&nbsp;</p><p>Deep Angel draws from&nbsp; Walter Benjamin's description of Paul Klee's Angelus Novus, the angel of history who has clairvoyance into the dark side of what appears to be progress. The angel sees the unravelling of all that matters in the world and would like to alert the world about his vision, but he's caught in the storm of progress and can't communicate any messages. The images that Deep Angel generates are intended to deliver the message that Angelus Novus would have sent if he could.&nbsp;</p><p>The algorithm applies computer vision techniques to automatically (1) detect and outline objects in images, (2) remove the outlined object from the image, and (3) imagine what the image would look like if that outlined object were removed from the image. Any image uploaded and transformed by Deep Angel can be published on the Deep Angel website by clicking the ""Publish to Deep Angel"" button.&nbsp;</p><p>The AI's performance varies across photographs. Sometimes, it's impossible to tell what has been disappeared. Other times, the images appear similar to the images from Adrian Piper's <i>Everything</i> series. The more people interact with the algorithm, the more attuned people will be to the potential and limitations of modern AI to manipulate the media. It's now possible to automate the vanishing commissar in Soviet photography, but the AI is not yet perfect. Below are two examples of the Deep Angel AI effect: (1) a gif generated by Deep Angel showing a father and daughter disappearing in the wilderness and (2) two images showing the before and after of Deep Angel peering into a photo of a professional surfer.&nbsp;</p>",,,2019-02-14 19:46:25.924,True,2018-08-06,Deep Angel: The AI behind the aesthetics of absence,PUBLIC,http://deepangel.media.mit.edu/,True,Scalable Cooperation,False
creaitivity,groh,False,<p>What it is.</p><p>Where it is going.</p><p>List the projects.</p>,,,2019-02-14 19:47:57.622,False,2018-01-01,creAItivity,PUBLIC,,True,Scalable Cooperation,False
ai-spirits,groh,False,,2019-10-31,,2018-11-14 21:20:32.287,False,2018-10-31,AI Spirits,PUBLIC,,True,Scalable Cooperation,False
future-of-work-ai-automation-labor,groh,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-04-03 14:42:46.705,True,2017-06-06,"AI, Automation, Labor, and Cities: How to map the future of work",PUBLIC,,True,Scalable Cooperation,False
black-rock-atlas,emoro,False,"<p>Burning Man is a magical place that gets the best of human creativity and collaboration to flourish. To further understand what makes this magic happen, we are creating the first ever Black Rock Atlas–a map of the social patterns and networks that exist on the playa. </p><p>To do this, we are tracking the decentralized journey of a multitude of vessels through the gift economy of Burning Man with GPS technology and generative photography. The Atlas will explore new ways of community interaction, storytelling, and data visualization.</p>",2018-12-11,,2018-10-15 20:44:54.352,True,2018-08-15,Black Rock Atlas,PUBLIC,https://blackrockatlas.mit.edu,False,Human Dynamics,False
collective-sensemaking-in-cryptocurrency-community,emoro,False,"<p>Participants in cryptocurrency markets are in constant communication with each other about the latest coins&nbsp;and news releases. Do these conversations build hype through the contagiousness of excitement, help the&nbsp;community process information, or play some other role? Using a novel dataset from a major cryptocurrency&nbsp;forum, we conduct an exploratory study of the characteristics of online discussion around cryptocurrencies. We find that coins with more information available and higher levels of&nbsp;technical innovation are associated with higher quality discussion. People who talk about serious coins tend&nbsp;to participate in discussion displaying signatures of collective intelligence and information processing, while&nbsp;people who talk about less serious coins tend to display signatures of hype and naïvety. Interviews with&nbsp;experienced forum members also confirm these quantitative findings. These results highlight the varied roles&nbsp;of discussion in the cryptocurrency ecosystem and suggest that discussion of serious coins may be oriented&nbsp;towards earnest, perhaps more accurate, attempts at discovering which coins are likely to succeed.&nbsp;</p>",,,2019-04-19 14:41:02.807,True,2016-06-01,Collective sensemaking in cryptocurrency community,PUBLIC,,True,Human Dynamics,False
measuring-and-reducing-social-segregation-in-cities,emoro,False,"<p>We use high-resolution geospatial data collected from mobile phones to measure social segregation at an unprecedented resolution in cities across the United States. Social segregation happens when people of varying socioeconomic groups in a city have little opportunity to be exposed to people different than them.</p><p>To construct this measure, we aggregate high-resolution data from&nbsp;over 4.5 million users in the principal metro areas in the US to characterize places in the city by how mixed their visitors are by income. Using this measure, rather than traditional residential metrics, reveals that social exposure in third places is crucial to understanding economic segregation patterns in cities. In fact, the social segregation of different economic groups is dependent on an extremely small proportion of overall venues in a city.&nbsp;</p><p>We also look at how much individual citizens would need to change their behavior in order to make their patterns of exposure more integrated. Surprisingly, small changes in the amount of time people spend in different categories of places—changes as low as 2-5%—can reduce their social segregation by half.&nbsp;</p><p>We're currently working on finalizing these results and exploring how we might translate these findings into policy.</p>",,,2019-04-19 14:46:25.026,True,2017-10-01,Measuring and reducing social segregation in cities,PUBLIC,,True,Human Dynamics,False
the-ripple-effect-your-are-more-influential-than-you-think,emoro,False,"<p>The well-known ""small-world"" phenomenon indicates that an individual can be connected with any other in the world through a limited number of personal acquaintances. Furthermore, Nicholas and Fowler show that not only are we connected to each other, but we could also shape the behavior of our friends' friends. In this project, we are interested in understanding how social influence propagates and triggers behavioral change in social networks. Specifically, we analyze a large-scale, one-month international event held in the European country of Andorra using country-wide mobile phone data, and investigate the change in the likelihood of attending the event for people that have been&nbsp;<span style=""font-size: 18px; font-weight: normal;"">influenced by and are of different social distances from the attendees.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">Our results suggest that social influence exhibits the ripple effect, decaying across social distances from the source but persisting up to six degrees of separation. We further show that influence decays as communication delay increases and intensity decreases. Such ripple effect in social communication can lead to important policy implications in applications where it is critical to trigger behavior change in the population.</span></p>",,,2019-04-19 14:55:02.488,True,2016-08-01,The Ripple Effect: You are more influential than you think,PUBLIC,,True,Human Dynamics,False
towards-understanding-the-impact-of-ai-on-labor,emoro,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-03-22 16:27:19.712,False,2018-03-01,Towards Understanding the Impact of AI on Labor,PUBLIC,http://www.media.mit.edu/~mrfrank,True,Human Dynamics,False
evolution-strategies-applied-to-collective-intelligence,emoro,False,"<p>We build recommender bots that use machine learning and network analytics to create personalized recommendations for users on various social and financial platforms. We show that bots that work not just on the raw user data, but instead build on human intuition, do far better. We are in the process of live testing these bots on various platforms.&nbsp;</p>",,,2017-04-05 18:49:11.444,True,2017-03-10,Social Learning Recommender Bots,PUBLIC,,True,Human Dynamics,False
Reversed-Urbanism,emoro,False,"<h2>Predicting Urban Performance through Behavioral Patterns in Temporal Telecom Data</h2><p>This study explores a novel method to analyze diverse behavioral patterns in large urban populations and to associate them with discrete urban features. This work utilizes machine learning and anonymized telecom data to understand which fragments of the city has greater potential to attract dense and diverse populations over longer periods of time. Finally, this work suggests a road map for building spatial prediction tools in an effort to improve city-design and planning processes.&nbsp;&nbsp;</p><p><b><a href=""https://cityscope.github.io/CS_Andorra_RNC/"">Click here for an interactive visualization of this study</a>&nbsp;<br><br></b></p><p><b>Advisors:</b>&nbsp;Kent Larson&nbsp;and&nbsp;Esteban Moro<br><b>Thanks to</b> Andorra Telecom, ActuaTech,&nbsp;Núria Macià. <br>Data was&nbsp;obtained by Andorra Telecom as part of MIT Media Lab City Science and the State of Andorra collaboration.&nbsp;</p>",,,2019-02-24 23:21:12.068,True,2017-07-01,Reversed Urbanism,PUBLIC,http://ArielNoyman.com,True,Human Dynamics,False
social-ai-and-extended-intelligence,emoro,False,"<p>There is a deep fear that human jobs will be replaced by AI. Rather than racing against the machines, our aim is to show that a human-AI combination will perform better than humans and AI working alone. Although no man is better than a machine for some tasks, ""no machine is better than a man with a machine"" (Paul Tudor Jones) . Thus, by building ""bots"" that are compatible with human behavior, and specifically leverage the manner in which humans use social information, we have been able to build bots that extend human intelligence capabilities. In a large-scale financial trading experiment, we have shown that groups of humans and ""socially compatible"" AI bots can successfully incorporate human intuition into their decisions and consequently not only do better than humans alone, but also do better than similar AI bots that use only objective information.
                    
                </p>",,,2019-02-12 15:14:05.048,True,2016-07-01,Social AI and Extended Intelligence,PUBLIC,,True,Human Dynamics,False
identifying-the-human-impacts-of-climate-change,emoro,False,<p>Climate change is going to alter the environments that we depend on in myriad ways. We're using data to identify and quantify these potential human impacts.&nbsp;</p>,,,2019-04-19 17:44:02.613,True,2016-07-01,Identifying the human impacts of climate change,PUBLIC,,True,Human Dynamics,False
the-atlas-of-inequality,emoro,False,"<h2><b>Segregation is hurting our societies and especially our cities. But economic inequality isn't just limited to neighborhoods. The restaurants, stores, and other places we visit in cities are all unequal in their own way.&nbsp;</b></h2><p>The Atlas of Inequality &nbsp;shows the income inequality of people who visit different places in the Boston metro area. It uses aggregated anonymous location data from digital devices to estimate people's incomes and where they spend their time.&nbsp;Using that data, we've made our own <b>place inequality </b><b>metric</b> to capture how unequal the incomes of visitors to each place are. Economic inequality isn't just limited to neighborhoods; it's part of the places you visit every day.</p><p>Try it yourself here:</p><h2><a href=""http://inequality.media.mit.edu""><b>The Atlas of Inequality</b></a></h2><p>The Atlas of Inequality is a project from the Human Dynamics group at the <a href=""https://www.media.mit.edu/"">MIT Media Lab</a> and the Department of Mathematics at <a href=""http://www.uc3m.es/"">Universidad Carlos III de Madrid</a>.</p><p>It is part of a broader initiative to understand human behavior in our cities and how large-scale problems like transportation, housing, segregation, or inequality depend in part on the emergent patterns of people’s individual opportunities and choices.</p>",,,2019-03-20 17:07:49.005,True,2019-03-01,The Atlas of Inequality,PUBLIC,https://inequality.media.mit.edu,True,Human Dynamics,False
future-of-work-ai-automation-labor,emoro,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-04-03 14:42:46.705,True,2017-06-06,"AI, Automation, Labor, and Cities: How to map the future of work",PUBLIC,,True,Human Dynamics,False
gsk-manufacturing-initiative,nlutz,False,"<p>This project is the first of two projects in collaboration with GSK. We are developing a computational simulation that allows a human user (or AI) to test drug manufacturing investment scenarios for an entire portfolio over multiple years. We aspire to help decision-makers understand the possible impact of new techniques such as CBM on selected key performance metrics. This game like simulation allows various stakeholders to come together and make collaborative decisions regarding the entire supply chain. The software works dynamically with a Tactile Matrix, which is an interactive decision support system that allows users to instantly and collaboratively explore the models in an approachable, tangible way. </p><p>Screenshots courtesy of Ira Winder. Photos by Nina Lutz.</p>",2018-06-01,,2018-08-20 20:42:11.340,True,2017-01-09,GSK Manufacturing Initiative,PUBLIC,,False,Object Based Media,False
gsk-places-initiative,nlutz,False,"<p>This project is part of a parallel research endeavor with GSK Manufacturing. By simulating how scientists at the Upper Providence site interact with one another and the space around them, we hope to help assist future renovations in a range of GSK locations. This was motivated by GSK’s drive to improve spatial configuration within their organization, including various open office environments and even smart labs.&nbsp;</p><p>We hope this project will serve as both a packaged decision support system and a framework for GSK scientists and stakeholders to reconfigure with their own spatial inquiries and case studies. Our goals can be enumerated as below.&nbsp;</p><ol>
<li>Design a decision support tool that allows R&amp;D to understand how changes to physical environments and adjacencies may have an impact on key workplace indicators. Encourage data-driven demonstration and discussion of decisions related to spatial changes.&nbsp;</li>
<li>Employ spatial mathematical models to calculate key workplace indicators, including</li>
<ol>

<li>Space utilization</li>
<li>Time accessibility between amenities</li>
<li>Synergy, defined as potential for interaction among researchers</li>

</ol>
<li>Deploy the decision support tool as an evolving platform that exists in two forms:&nbsp;</li>
<ol>

<li>Tangible user interface “Tactile Matrix”</li>
<li>Traditional executable application that can be used on personal machines with mouse and keyboard interface.&nbsp;</li>
<li>Provide transparent source code and open source, off the shelf technologies that allows for adaptation and customization for various case studies and applications.&nbsp;</li>

</ol>
</ol><br><p>Screenshots courtesy of Nina Lutz.&nbsp;<br></p>",2018-06-01,,2018-08-20 20:43:34.774,True,2017-01-09,GSK Places Initiative,PUBLIC,,False,Object Based Media,False
geobits,nlutz,False,"<p>This is an open source geospatial exploration tool. Using various public APIs including Open Street Map and the United States Census, we can make dynamic, flexible models of how people are moving through the city. These models include accessibility in cities, multimodal transportation networks, and diversity. Overall this allows anyone with or without an urban planning background to build strong models with geospatial and urban data. This system works dynamically with a Tactile Matrix, which is an interactive decision support system that allows users to instantly and collaboratively explore the models in a tangible way.</p><p>Photos by Nina Lutz.&nbsp;</p>",2018-06-01,,2018-09-04 19:47:57.544,True,2016-07-11,GeoBits,PUBLIC,,False,Object Based Media,False
singapore-pedestrian-accessibility,nlutz,False,<p>This project focused on pedestrian accessibility in collaboration with Singapore Centre for Liveable Cities. Researchers and planners came together to design an interface that would allow both citizens and planners to interact with a model regarding pedestrian accessibility. The tangible interface allows users to come together to have conversations and make interventions to make the case study area more accessible for pedestrians.&nbsp;</p>,2018-06-01,,2018-08-22 03:18:47.497,True,2016-06-06,Singapore Pedestrian Accessibility,PUBLIC,,False,Object Based Media,False
connected-coral,nlutz,False,"<p>Connected Coral integrates physical and digital elements in a visualization of the environmental impacts on reefs. This complex projection mapping uses multiple projectors, angled mirrors, and a motion sensor to create an interactive digital skin on a complex three-dimensional surface.<br></p><p>To integrate the projected content with the physical design, the students fabricated the physical coral model based on photogrammetry scans of real coral, warped and blended the projected areas, and factored in hardware specifications. These modifications minimize visual distortion on the uneven surface and allow for an uninhibited interactive experience.</p><p>This project was created through the Open Ocean Initiative and will be on display at the MIT Museum through Spring 2019.</p>",2018-12-31,,2018-11-08 19:07:36.290,True,2018-03-29,Connected Coral,PUBLIC,,False,Object Based Media,False
pigmented-lumens,nlutz,False,"<p>This pedagogy of experiments seeks to accurately model and manufacture a new form of lighting fixtures.</p><p>These new fixtures are made of polyester resin and a particle distribution called a colloid. Unlike traditional light bulb models, rather than having illumination come from individual lighting fixtures that are wired to a common electric source, we are using one lighting source in the form of a laser to illuminate each fixture.</p><p>By using controlled resin casting alongside numerical simulation of structured colloids, we can program and evaluate different substances held in distribution with a 9 dimensionality lighting model. This allows us to understand the type of lighting output that can be achieved with various materials and shapes, as well as predict the aesthetic properties that these lighting sources may allow us. From various colors to different glittery reflects, this framework allows both technical and artistic framework.&nbsp;</p>",,,2019-02-07 23:14:01.769,True,2018-08-01,Pigments and Lumens,PUBLIC,https://ninalutz.github.io,True,Object Based Media,False
andorra-tourism,nlutz,False,"<p><span style=""font-size: 18px; font-weight: 400;""></span><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/""><span style=""font-size: 18px; font-weight: 400;"">View the main City Science Andorra project profile.</span></a><br></p><p><span style=""font-size: 18px; font-weight: 400;"">With more than eight million visitors a year, tourism represents almost 30% of the economy of Andorra. By gathering and analyzing data from social media, call detail records, and wifi, we can understand the country's dynamics of tourism and commerce as well as design interventions that can improve the experience for tourists, encouraging them to visit Andorra more frequently, stay longer, and increase spending.&nbsp;</span><br></p><h2><b>Current Projects</b></h2><ul><li>Event Analysis<br></li><li>Social Network<br></li><li>Location Recommendation system<br></li></ul><p> </p><h2><b>EVENT ANALYSIS</b></h2><p>Based on the analysis of call detail records and social media, the goal of this project is to understand the tourist behaviors in Andorra.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">After mining those anonymized data, we have been able to learn different patterns and behaviors of the tourism in Andorra thanks to an agent-based model developed in order to represent the flow of people. This simulation is also coupled with an interactive table called CityMatrix.</span></p>",,,2019-02-25 15:33:28.936,True,2015-08-01,Andorra | Tourism,PUBLIC,,True,Object Based Media,False
escape-pod-1,nlutz,False,"<p>The esc-Pod&nbsp; (or Escape Pod) is an exploratory platform for researchers investigating moments of refuge within our bustling work lives.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">The core of the esc-Pod consists of actuated work and rest surfaces. This allows for moments of productivity and relaxation to occur within a single space.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The outer skin provides variable transparency, enabling a spectrum of visibility settings according to privacy requirements.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The inner skin provides an infrastructure for the modulation of spatial experiences. Each panel is a pixel, connecting itself to the skin network, and can embody an array of senses.</span></p>",,,2019-04-08 17:01:14.555,True,2016-08-01,Escape Pod,PUBLIC,,True,Object Based Media,False
cosmetics-empowered-identities,nlutz,False,"<p>What if we could leverage technology and modern cosmetics for good? To empower people to explore identities? To give anyone, regardless of skill level, the tools to bend, express, and explore various gender identities and traits?</p><p>The goal of this project is to create a tool that will allow a user to add more feminine, masculine, or androgynous traits to themselves with cosmetics. After interacting with renders and the tool's preferences, the user will receive a list of cosmetics and then follow along with the tool in real time while they apply their makeup.</p><p>This tool is meant to be accessible, to be free and open source technology, and made for users of all skin tones, shapes, and preferences. To allow anyone to express their identities.</p><p>This work is ongoing and will have regular updates, beta releases, and user surveys coming soon.&nbsp;</p>",,,2019-02-07 16:07:17.285,True,2018-08-01,Cosmetics-Empowered Identities,PUBLIC,https://ninalutz.github.io,True,Object Based Media,False
cosmetic-light,nlutz,False,"<p>From skin to light: an artistic dialog between cosmetics, lasers, and lighting between the human and microscopic scale.&nbsp;</p><p>Cosmetics essentially diffract and scatter light in a way that makes the wearer appear different to the naked eye. In this project we propose an illumination grammar between cosmetic projects and&nbsp;coherent light lasers, as a suggestion for alternative interior lighting as well as artistic effect and expression. From skin to light, we explore these artistic archetypes.</p>",,,2019-02-07 15:55:16.524,True,2018-02-06,Cosmetic Light,PUBLIC,https://ninalutz.github.io,True,Object Based Media,False
soro,edith,False,<p>The Social Robot Toolkit aims to provide a platform for children to learn through playful interaction. The social robot (Soro) toolkit allows preschool children to experiment with computational concepts while teaching a social robot new rules. The toolkit also provides a platform for learning interpersonal skills through the use of storytelling that integrates interpersonal and computational concepts. This harnesses preschoolers' natural interest in social interaction to familiarize them with new concepts.</p>,2017-04-30,,2017-06-05 16:13:22.057,True,2014-08-23,Social Robot Toolkit,PUBLIC,,False,City Science,True
soro,randiw12,False,<p>The Social Robot Toolkit aims to provide a platform for children to learn through playful interaction. The social robot (Soro) toolkit allows preschool children to experiment with computational concepts while teaching a social robot new rules. The toolkit also provides a platform for learning interpersonal skills through the use of storytelling that integrates interpersonal and computational concepts. This harnesses preschoolers' natural interest in social interaction to familiarize them with new concepts.</p>,2017-04-30,,2017-06-05 16:13:22.057,True,2014-08-23,Social Robot Toolkit,PUBLIC,,False,Personal Robots,False
pop-kit,randiw12,False,"<h2>How can we add the missing ""T"" and ""E"" in preschool STEAM education?</h2>",,,2019-05-02 20:11:55.104,True,2016-10-01,PopBots: An early childhood AI curriculum,PUBLIC,,True,Personal Robots,False
relationship-assessments,randiw12,False,"<p>Social robots are increasingly being developed for long-term interactions with children in domains such as healthcare, education, therapy, and entertainment.&nbsp;In prior research, we have seen that children treat robots as more than mere artifacts, e.g., ascribing them mental states, psychological attributes, and moral standing. Thus, while children’s relationships with robots may not be like the relationships they have with their parents, pets, imaginary friends, or smart devices, they will form relationships of some kind.&nbsp;As such, we need to deeply understand how children’s relationships with robots develop through time, and find ways to characterize and measure these relationships.&nbsp;However, there are few validated assessments for measuring young children’s long-term relationships. Thus, we have adapted or created a variety of assessments for use in this context for children aged 5-6 years.&nbsp;</p><p><a href=""https://dam-prod.media.mit.edu/x/2018/04/25/KoryWestlund-IDC18.pdf"">Four of these assessments are presented in the associated paper.</a></p><p>This paper shows that children can appropriately respond to these assessments with reasonably high internal reliability, and that these assessments are able to capture child-robot relationship adjustments&nbsp; over a long-term interaction.</p>",,,2019-04-17 18:39:40.951,True,2017-04-01,Assessing Children's Relationships with Social Robots,PUBLIC,,True,Personal Robots,False
bot-blocks,randiw12,False,,,,2019-03-27 13:40:23.668,False,2018-01-05,Block-Based Programming for Robotics Education,PUBLIC,http://robotic.media.mit.edu/peple/randi-williams,True,Personal Robots,False
collaborative-robot-storyteller,randiw12,False,"<p>Could a social robot collaboratively exchange stories with children as a peer and help improve their linguistic and storytelling skills? Tega uses machine learning algorithms to learn actions that improve children's storytelling and keep them engaged. &nbsp;We are also interested in how Tega can personalize its interaction with each child over multiple encounters, because every child learns and engages differently.&nbsp;</p><p>In Spring 2017, Tega went to twelve preschool classrooms in the Greater Boston area for&nbsp;three months, pioneering the field of long-term human-robot interaction.&nbsp;Using Q-learning, a policy was trained to tell stories optimized for each child’s engagement and linguistic skill progression. Tega monitored children's affect signals and asked dialogic questions during storytelling to gauge their engagement. Tega also invited children to&nbsp;tell&nbsp;it stories, which Tega used to assess each child's linguistic skill development. Our results show robot's interaction policy indeed personalized to each child. At the end of the sessions, the policy significantly differed from one child to the other. Children who interacted and built relationships with a personalized robot showed higher engagement, learned and retained more vocabularies, and used more complex syntax structure in their speech compared to where they had started.</p>",,--Choose Location,2019-01-22 17:42:46.144,True,2015-09-01,Personalized Robot Storytelling Companion,PUBLIC,,True,Personal Robots,False
soro,haewon,False,<p>The Social Robot Toolkit aims to provide a platform for children to learn through playful interaction. The social robot (Soro) toolkit allows preschool children to experiment with computational concepts while teaching a social robot new rules. The toolkit also provides a platform for learning interpersonal skills through the use of storytelling that integrates interpersonal and computational concepts. This harnesses preschoolers' natural interest in social interaction to familiarize them with new concepts.</p>,2017-04-30,,2017-06-05 16:13:22.057,True,2014-08-23,Social Robot Toolkit,PUBLIC,,False,Personal Robots,False
robot-vocal-expressiveness,haewon,False,"<p>Prior research with preschool children has established that book reading, especially when children are encouraged to actively process the story materials through dialogic reading, is an effective method for expanding young children’s vocabulary.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">A growing body of research also suggests that social robots have potential as learning companions and tutors for young children’s early language education. Social robots are new technologies that combine the adaptability, customizability, and scalability of technology with the embodied, situated world in which we operate.</span></p><p>In this project, we asked whether a social robot can effectively engage preschoolers in dialogic reading. Given that past work has shown that children can and do learn new words from social robots, we investigate what factors modulate their learning. In particular, we looked at whether the verbal expressiveness of the robot impacted children’s learning and engagement during a dialogic reading activity.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">This project was funded by an NSF Cyberlearning grant.</span></p>",2017-09-01,,2019-03-02 01:42:38.931,True,2015-09-01,Robot Expressiveness Affects Children's Learning,PUBLIC,http://robotic.media.mit.edu/portfolio/robot-vocal-expressivity,False,Personal Robots,False
social-robots-in-zero-gravity-scenarios,haewon,False,"<p>Can we enable social connectivity between astronauts and people on Earth through an embodied agent?</p><p>Astronauts actively communicate with their families on Earth through several forms of digital and voice communication, including phone calls, video conferencing, and email. However, as astronaut Scott Kelly describes in the <i>Time</i> documentary <i>A Year in Space</i>, the experience can be incredibly isolating despite these affordances. Shortcomings of these modes of communication lie in their inability to translate emotion effectively, failure to facilitate shared experiences, lack of physical feedback, and the resulting perceived lack of control. The psychological effects of these limitations can become heightened over time, and peak during moments when the family on Earth is in need of support. As space becomes more accessible, it is important to consider how we design for social connectivity between people on Earth and in space.</p><p>What if embodied social agents, besides being the astronaut's personal sidekick, could help to facilitate a more connected experience between space and Earth? From C3PO in <i>Star Wars</i> to Rosie the Maid in <i>The Jetsons</i>, the idea of robots in space has been well explored in fiction universe. On Earth, embodied social agents have been shown to be effective in providing companionship, relieving stress and anxiety, and fostering connection among people. In this project to send an&nbsp;embodied social agent into zero gravity, we explore several key themes relating to the potential for this technology to offer better connection and shared experience between astronauts and people on Earth.</p><p>While in zero gravity, the embodied social agent interacts with people on cognitive, creative, and social tasks with varying degrees of proactive behavior. We collect physiological, audio, and video data of the experience as individuals complete a series of tasks with the agent with the goal of designing agents that can enable us to be more socially connected.</p>",2018-12-31,,2017-12-05 19:04:57.085,True,2017-10-01,Social Robots in Space: Initial Explorations,PUBLIC,,False,Personal Robots,False
designing-social-robots-for-older-adults,haewon,False,"<p>Most countries are projected to see the number of people ages 65 and older surpass the population under the age of 15 by 2050. The limitations of current solutions to assisting older adults, the increased social and emotional toll on caregivers, and the inability of institutions to create structural solutions in a timely manner calls for a paradigm shift in the way we approach aging.</p><p>As these new meanings of age, aged, and aging are re-negotiated at a personal and collective level, the <b>main goal of this research initiative is to&nbsp;study aging adults’ daily living assistance, social and emotional needs, and intergenerational connection</b> while exploring the optimized modalities for embodied agents to successfully deliver these interactions.&nbsp;We see embodied agents as a method to enable older adults to age-in-place, supporting them in ways such as promoting social connectedness, tracking vitals, coaching in emotional wellness, and assisting with medical adherence.</p><p>Our work is rooted in partnering with the community through co-design and participatory design methods to inform robot design by empowering older adults to engage in our research. We prioritize developing robot interactions that can be tested long-term in older adults’ homes to better inform how social robots can shape aging-in-place.<br></p><p>Currently, we are running a long-term codesign study with older adults. Over the course of the year, older adults will engage in interviews, interactive artwork, living with a robot, prototyping on a robot, and design guideline generation.&nbsp;</p><p>If you are 70 years of age or older and interested in participating in future study opportunities, please contact Anastasia Ostrowski (akostrow@media.mit.edu).</p>",,,2019-05-10 19:51:29.310,True,2017-06-01,Designing social robots for older adults,PUBLIC,,True,Personal Robots,False
jibo-research-platform,haewon,False,"<p>The Jibo Research Platform is an in-the-field deployable Social Robotics experimentation and data collection infrastructure. Built upon the world's first commercial social robot for the home, it extends Jibo's design, hardware, and data security for research purposes.</p>",,,2018-10-15 01:36:16.436,True,2018-08-01,Jibo Social Robotic Research Platform,PUBLIC,,True,Personal Robots,False
talking-machines-democratizing-the-design-of-voice-based-agents-for-the-home,haewon,False,"<p>Embodied voice-based agents, such as Amazon’s Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most, these agents represent their first experience of&nbsp;<i>living</i> with artificial intelligence in such private and personal spaces.&nbsp;</p><p>However, little is known about people’s desires, preferences, and boundaries for these technologies. This projects seeks to answer questions surrounding this space:&nbsp;<b>How do we live with voice-based agents in the home? How do different generations interact with voice-based agents? How should these technologies be designed to incorporate people’s preferences, desires, and boundaries? What tools can be used to understand this space?</b></p><p>This work presents insights from a long-term exploration with over 70 children, adults, and older adults over a one-year period to interact with, discover, experience, reflect upon, and design voice-based agents. In addition, design tools and learnings from the experience have been developed into an open-source design kit to enable designers and researchers to explore these ideas with the broader population.</p><p>For more information, please contact <b>Nikhita Singh (nikhita@media.mit.edu) </b>and <b>Anastasia Ostrowski (akostrow@media.mit.edu)</b>.</p>",,,2018-07-15 03:34:47.660,True,2017-09-01,Talking Machines: Democratizing the design of voice-based agents for the home,PUBLIC,,True,Personal Robots,False
robot-mindset-and-curiosity,haewon,False,"<h1>Young Learner's Companion&nbsp;</h1><h2>Developing robots' growth mindset and pro-curious behavior and fostering the same in young learners via long-term interaction</h2><p>A growth mindset and curiosity have significant impact on children's academic and social achievements. We are developing and evaluating a novel expressive cognitive-affective architecture that synergistically integrates models of curiosity, understanding of mindsets, and expressive social behaviors to advance the state-of the-art of robot companions. In doing so, we aim to contribute major advancements in the design of AI algorithms for artificial curiosity, artificial mindset, and their verbal and non-verbal expressiveness in a social robot companion for children. In our longitudinal study, we aim to evaluate the robot companion's ability to sustain engagement and promote children's curiosity and growth mindset for improved learning outcomes in an educational play context.<br></p>",,--Choose Location,2018-05-09 04:35:35.760,True,2015-09-01,Robot Mindset and Curiosity,PUBLIC,,True,Personal Robots,False
tega-a-new-robot-platform-for-long-term-interaction,haewon,False,"<p>Tega is a new robot platform designed to support long-term, in-home interactions with children, with applications in early-literacy education from vocabulary to storytelling.<br></p>",,--Choose Location,2018-02-02 12:45:47.861,True,2015-01-01,Tega: A New Social Robot Platform,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,False
text-to-motion-expressive-robot-motion-sequencing,haewon,False,"<p><b>Text-to-Motion</b> generates a sequence of contingent robot animations to accompany the sentiment analyzed from an input sentence and its spoken audio. We trained a linear classifier to transfer learn&nbsp;our&nbsp;corpus of animated robot speech from DeepMoji network,&nbsp; a long short-term memory (LSTM) network with an attention model trained on billion tweets.&nbsp;</p>",,,2019-04-17 18:48:02.883,True,2018-01-28,Text-to-Motion: Automatic sequencing of animative robot motions,PUBLIC,,True,Personal Robots,False
realtime-detection-of-social-cues,haewon,False,"<h2>Realtime detection of social cues in children’s voices</h2><p>In everyday conversation, people use what are known as backchannels to signal to someone that they are still listening, paying attention, and engaged. As listeners, we smile, nod, and say “uh-huh” to convey attentiveness, and we do this naturally with little thought. We give this feedback not randomly but at certain moments in the conversation because speakers give off social cues that signal upcoming backchanneling opportunities.</p>",,,2018-05-07 17:34:51.716,True,2017-03-01,Realtime Detection of Social Cues,PUBLIC,http://www.haewonpark.com/,True,Personal Robots,False
pop-kit,haewon,False,"<h2>How can we add the missing ""T"" and ""E"" in preschool STEAM education?</h2>",,,2019-05-02 20:11:55.104,True,2016-10-01,PopBots: An early childhood AI curriculum,PUBLIC,,True,Personal Robots,False
relationship-assessments,haewon,False,"<p>Social robots are increasingly being developed for long-term interactions with children in domains such as healthcare, education, therapy, and entertainment.&nbsp;In prior research, we have seen that children treat robots as more than mere artifacts, e.g., ascribing them mental states, psychological attributes, and moral standing. Thus, while children’s relationships with robots may not be like the relationships they have with their parents, pets, imaginary friends, or smart devices, they will form relationships of some kind.&nbsp;As such, we need to deeply understand how children’s relationships with robots develop through time, and find ways to characterize and measure these relationships.&nbsp;However, there are few validated assessments for measuring young children’s long-term relationships. Thus, we have adapted or created a variety of assessments for use in this context for children aged 5-6 years.&nbsp;</p><p><a href=""https://dam-prod.media.mit.edu/x/2018/04/25/KoryWestlund-IDC18.pdf"">Four of these assessments are presented in the associated paper.</a></p><p>This paper shows that children can appropriately respond to these assessments with reasonably high internal reliability, and that these assessments are able to capture child-robot relationship adjustments&nbsp; over a long-term interaction.</p>",,,2019-04-17 18:39:40.951,True,2017-04-01,Assessing Children's Relationships with Social Robots,PUBLIC,,True,Personal Robots,False
design-inquiry,haewon,False,"<p>Social robots are seen as a potential device to promote and enable older adults to age-in-place. The work here is a component of our long-term, co-design process with older adults happening in 2019.&nbsp; Through co-creating artworks of&nbsp; interactions with a robot with older adults, abstract technology concepts such as security and privacy, accountability, and autonomy are translated into icons that older adults can leverage to express their thoughts around various interactions. The interactions focused upon for this study include medical adherence, exercise and physical therapy, body signal monitoring, cognitive health monitoring, emotional wellness, social connectedness, and financial literacy.&nbsp;</p><p>In this component, participants create a self-representation and a representation of the robot using physical models. These models are then scanned into a digital space where older adults can scale the representations and add icons to represent their thoughts and desires around the interactions in the artwork.</p>",,,2019-04-18 14:28:23.440,True,2019-04-01,Design Inquiry: Translating abstract technology concepts to artwork,LAB-INSIDERS,,True,Personal Robots,False
personalized-interaction-for-language-learning,haewon,False,"<p>The process by which children learn native languages is markedly different from the process of learning a second, or non-native, language. Children are typically immersed in their native languages. They receive input from the adults and other children surrounding them, based on immediate need and interaction, during every waking hour. &nbsp;</p><p>Second language learners are exposed to input from the new language in very different ways, most commonly in a classroom setting.&nbsp;The second language learner relies heavily on memory skills with sparse interaction, in contrast to the first language learner that can rely on environmental reinforcement and social interaction to learn words.&nbsp;</p><p>Social robots have the potential to drastically improve on this paradigm, making the second-language learning experience more like the experience of learning a native language by engaging the child in a rich, interactive exposure to the target language, especially aspects not typically covered by traditional technological solutions, such as prosody, fundamental phonetics, common linguistic structures, etc.</p><p>Our project explores how to design child-robot &nbsp;interactions that encourage child-driven language learning, that adapt and personalize each child’s learning experience. We incorporate game design and machine learning into the child-robot interaction design. The child and robot play through a suite of educational games together. Using real-time sensor data and gameplay features, the robot constructs a model of each child's learning and emotional trajectory, then uses these models to inform its own decision making during the game. Thus, the robot's behaviors become personalized to individual children based on their learning style, personality and knowledge/emotional states during gameplay.&nbsp;</p><p></p><p></p><p></p><p></p><p></p><p></p>",,,2018-05-03 20:56:50.720,True,2017-01-01,Personalized  Interaction for Language Learning,PUBLIC,,True,Personal Robots,False
shaping-engagement,haewon,False,"<p>Voice-user interfaces (VUIs), such as Amazon Echo and&nbsp;Google Home, are increasingly becoming present in domestic&nbsp;environments. Users attribute agency and personality traits to these AI agents. Due to the social attributes of these technologies,&nbsp; users try to understand the agents' characteristics based on social norms. These factors affect user experience quality and overall engagement, which, when considering first experiences, can impact continuous usage and engagement with VUI technology.</p><p>Our work examines users’ first impressions and interactions&nbsp;with&nbsp; VUI agents, such as Google Home, Amazon Echo, and Jibo, with varying brands and modalities. Using personality and experience questionnaires, we seek to understand how VUI modalities, form, and personality affect engagement with VUIs.</p>",,,2018-10-09 20:17:18.440,True,2018-08-13,Engagement with Voice-User Interface Agents,PUBLIC,,True,Personal Robots,False
collaborative-robot-storyteller,haewon,False,"<p>Could a social robot collaboratively exchange stories with children as a peer and help improve their linguistic and storytelling skills? Tega uses machine learning algorithms to learn actions that improve children's storytelling and keep them engaged. &nbsp;We are also interested in how Tega can personalize its interaction with each child over multiple encounters, because every child learns and engages differently.&nbsp;</p><p>In Spring 2017, Tega went to twelve preschool classrooms in the Greater Boston area for&nbsp;three months, pioneering the field of long-term human-robot interaction.&nbsp;Using Q-learning, a policy was trained to tell stories optimized for each child’s engagement and linguistic skill progression. Tega monitored children's affect signals and asked dialogic questions during storytelling to gauge their engagement. Tega also invited children to&nbsp;tell&nbsp;it stories, which Tega used to assess each child's linguistic skill development. Our results show robot's interaction policy indeed personalized to each child. At the end of the sessions, the policy significantly differed from one child to the other. Children who interacted and built relationships with a personalized robot showed higher engagement, learned and retained more vocabularies, and used more complex syntax structure in their speech compared to where they had started.</p>",,--Choose Location,2019-01-22 17:42:46.144,True,2015-09-01,Personalized Robot Storytelling Companion,PUBLIC,,True,Personal Robots,False
tools-to-investigate-societal-impacts-of-robot-ai,haewon,False,"<p>Artificial intelligence (AI) agents in an embodied form, such as Jibo, Amazon Alexa, and Google Home, are increasingly becoming part of our daily lives and our homes. While there have been numerous studies in lab settings documenting short-term individual interactions with intelligent agents, we are at a point where we need to be exploring the larger impact of these technologies in the world, living with real people over longer periods of time.</p><p>From a design research perspective, understanding and developing robots and AI that intersect with society is a “wicked problem,” a problem with many components that cannot be solved without interdisciplinary approaches. Design research within interdisciplinary applications has sought to develop approaches, methods, tools, and techniques to investigate the impact of technologies and inform future development. This work focuses on developing tools for exploring robots’ and AI’s impact on daily lives to better inform the&nbsp;development of these technologies by elucidating academia’s and industry’s requirements of tools for this domain.</p><p>For more information, please contact Anastasia Ostrowski (<a href=""mailto:akostrow@media.mit.edu"">akostrow@media.mit.edu</a>).</p>",,,2019-04-17 18:48:44.577,True,2018-07-01,Tools to investigate societal impacts of robots and AI,PUBLIC,,True,Personal Robots,False
ai-ethics-for-middle-school,haewon,False,"<h2>How do we raise conscientious consumers and designers of AI?</h2><p>Children today live in the age of artificial intelligence. On average, US children tend to receive their first smartphone at age 10, and by age 12 over half of all children have their own social media account. Additionally, it's estimated that by 2022, there will be 58 million new jobs in the area of artificial intelligence. Thus, it's&nbsp;important that the youth of today are both conscientious consumers and designers of AI.&nbsp;</p><p>This project seeks to develop an open source curriculum for middle school students on the topic of artificial intelligence. Through a series of lessons and activities, students learn technical concepts—such as how to train a simple classifier—and the ethical implications those technical concepts entail, such as algorithmic bias.&nbsp;</p>",,,2019-04-17 18:36:01.110,True,2018-08-01,AI + Ethics Curriculum for Middle School,PUBLIC,http://blakeleyhoffman.me/,True,Personal Robots,False
personalized-emotional-wellness-coach,haewon,False,"<p>The diagnosis and tracking of mood disorders still rely on clinical assessments, originating more than 50 years ago, of self-reported depressive symptoms via surveys and interviews. Such methods are known to provide limited accuracy and reliability in addition to being costly to track and scale. Once a problem is detected, providing personalized daily intervention and support is also too costly and does not scale. The goal of this pilot project is to develop a proof of concept of personalized emotional wellness coach focusing on key technology modules to create an emotionally intelligent social robot for this targeted domain. We shall also conduct a pilot evaluation with a five-week user study to evaluate the robot coach with respect to its ability to successfully sustain a user long-term adherence (i.e., daily self-report and efficacious advice)—with the expected result that it is more effective than state-of-the-art, gamified mobile apps currently used.&nbsp;</p>",2019-09-30,,2019-04-22 15:26:39.440,True,2018-09-01,Personalized Emotional Wellness Coach,PUBLIC,,True,Personal Robots,False
perspectives,jasrub,False,"<p>The news is probably one of the first things people check in the morning, but how much does what you know and understand about the world depend on your news source? Will you view the world differently if you head over to CNN instead of BBC?</p><p>“Perspectives” presents top news stories from many points of view. The viewer can easily see the different perspectives and get the whole story.</p>",2018-06-06,,2018-10-17 23:15:04.211,True,2016-08-01,Perspectives,PUBLIC,,False,Viral Communications,False
panorama,jasrub,False,"<p>An interface for smashing filter bubbles,&nbsp;<a href=""http://panorama.um-dokku.media.mit.edu/"">Panorama</a> is built to allow open, transparent, and collaborative exploration of news from all across the political map. It presents different perspectives and encourages serendipity in news exploration, versus getting all of our news from one single source.&nbsp;Panorama is a human-in-the-loop interface.&nbsp;The computer processes more than 10,000 news stories each day, both broadcast and written, and it uses machine learning algorithms to decide  what topics each story is talking about and if the stories are positive, subjective, or trending.&nbsp;The machine learning process pours over massive datasets and learns to generalize in smart ways, but not in the same smart ways that humans generalize. As a result, it can be brilliant and also get very confused. With Panorama, some of the training data was a large open set of movie reviews, and while this is a great dataset to start with, it is not mapped so well to news stories.&nbsp;As humans interact with Panorama, they are encouraged to give better labels to stories; those labels are fed back into the algorithm to make it better.</p><p>Having a lot of information about each news story and all stories together allows us to create an open-box news aggregator. With most aggregators we use today (like the Facebook News feed), the user has no idea what are the algorithms and filters that decide what s/he will see. Panorama is open: the user can decide to view everything, or filter only to specific things that he s/he is interested in, by playing with the sliders and seeing in real time how the news feed changes accordingly.&nbsp;For example, you could easily get all stories about animals, from the right side of the political map, that are also positive and objective.&nbsp;Panorama also exposes interesting patterns, such as the topics that different news sources focus on every day, and what sources had many objective versus subjective stories.</p>",2018-06-06,,2018-10-17 23:55:32.429,True,2016-10-01,Panorama,PUBLIC,,False,Viral Communications,False
as-you-need-it,jasrub,False,"<p>Video or broadcast news is viewed in a far wider set of circumstances than it ever has been before. It is composed with the assumption of a complete, situated viewing, but in fact it is often grabbed on-the-fly as a momentary experience. As You Need It is a semantic summarizer that deconstructs a multi-part segment for presentation as ""chunks of importance."" We are learning if a story can be cut down to a useful update that takes less time than a traffic light, or as much time as a given user has. This project uses and contributes to another group project, SuperGlue.</p>",,--Choose Location,2016-12-05 00:16:49.848,True,2016-01-01,As You Need It,PUBLIC,,True,Viral Communications,False
gobo,jasrub,False,"<h1>Your social media. Your rules.</h1><p><a href=""http://gobo.social"">Gobo</a> is an experiment, not a startup. We’re building it to change the conversation on social media and imagine a better version of it. This is a technology-to-think-with—a tool we want you to play with and push against. Gobo is being built by a small team at <a href=""https://www.media.mit.edu/groups/civic-media/overview/"">MIT Media Lab's Center for Civic Media</a>, where we work on technologies for social change.</p><p>For questions, feedback, and musings, you can reach the Gobo team at <a href=""mailto:gobo@media.mit.edu"">gobo@media.mit.edu</a>.</p><h2>Control your own feed</h2><p>Social media companies use algorithms to control what we see on our feeds, but we don’t know how these algorithms work. &nbsp;As a result, we’re often unaware why certain posts show up in our feed while others don’t. Gobo allows you to control the algorithms, or a set of “rules,” so you can decide what gets shown on your feed and know why.</p><h2>Connect multiple platforms</h2><p>We believe that multiple social media platforms should exist to serve different purposes. However, it’s not easy to keep up with all these platforms, especially when your data can’t be easily shared between them. Gobo allows you to connect up to three platforms, so you can view all of your feeds in one place. </p><h2>See what gets hidden</h2><p>We believe that transparency can help you better understand what you see on social media and keep platforms accountable for algorithmic bias. Gobo tells you why certain posts are hidden based on the rules you set. It also shows you how many posts are hidden, so you can understand the overall impact of the rules you set.</p><h2>Expand your perspective</h2><p>Social media companies make assumptions about what we want to see based on what we read and click on. They tend to show us content we’re already engaging with, reinforcing our echo chambers. Instead of assuming what you want to see, Gobo allows you to add unfamiliar perspectives into your feed, so you can better understand the range of opinions that are shared online.</p>",,,2019-04-10 14:21:45.965,True,2017-09-01,Gobo,PUBLIC,https://gobo.social,True,Viral Communications,False
fiftynifty,jasrub,False,"<p>This is a grassroots challenge to get friends to participate in democracy by making calls to congresspeople in all 50 states. Live phone calls are the best way to directly express your opinion on an issue to your elected officials. Your mission is to pass message this along to friends who will make calls and also pass the message/link along to others who will do the same. It's a social chain letter and a call to action for a better participatory democracy. &nbsp;<span style=""font-size: 18px; font-weight: normal;"">We help you make your call and you pass on an invitation for your friends to do the same. Your invite can stress your opinion on a given issue.&nbsp;</span></p><p>The winners are the first ten chains to reach 50 states and accumulate the most challenge points. You get 250 points for making a call, 125 points for a call that your friend makes, 65 points for the call their friend makes, on and on. Everyone on the chain earns points. Points count for your first call to each of your two senators and your representative. You get a bonus for a ""grand slam""—a network that reaches all 435 representatives and 100 senators.</p><p>There is a leaderboard and a network view so you can track how you are doing. You can also see how much of the country your chain is covering.</p>",,,2019-06-04 20:46:21.258,True,2017-02-13,FiftyNifty,PUBLIC,https://fiftynifty.org,True,Viral Communications,False
news-graph,jasrub,False,"<p>This project aims to show a different picture of the data behind the news, looking at how we analyze, represent, and interact with it.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">Video content is constantly created and added to the public archives, but there is never time to watch it all. News Graph explores a new method for interacting with news media.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">By analyzing the words that are said, extracting entities that appear, and finding the connections between them, we are able to map connections between video segments. Each connection represents two entities that were mentioned in the same video segment, and a video segment can be mapped to a number of connections.</span></p>",2019-09-30,,2019-04-18 01:15:48.619,True,2016-04-01,News Graph,PUBLIC,http://news-graph.um-dokku.media.mit.edu,True,Viral Communications,False
openscope,nsavidis,False,"<p>OpenScope is an open source project that combines three components for anyone to explore the micro world anytime, anywhere. The 3D-printable open hardware turns your smartphone into a 200x microscope, the image processing application helps you recognize specific objects, and the online community allows you to share and contribute your findings from the microscope. OpenScope is expanding microscopy technologies beyond research laboratories and transforming the way we interact with the micro world.</p>",2017-01-01,--Choose Location,2018-05-04 10:52:12.271,True,2016-01-01,OpenScope,PUBLIC,http://oi7.me,False,Synthetic Neurobiology,False
printed-wearable-holographic-display,nsavidis,False,"<p>Holographic displays offer many advantages, including comfort and maximum realism. In this project we adapt our guided-wave light-modulator technology to see-through lenses to create a wearable 3D display suitable for augmented or virtual reality applications. As part of this work we also are developing a femtosecond-laser-based process that can fabricate the entire device by ""printing.""</p>",,--Choose Location,2016-12-05 00:16:56.157,True,2015-09-01,Printed Wearable Holographic Display,PUBLIC,,True,Synthetic Neurobiology,False
breathvr,xuhaixu,False,"<p>Breathing actions are used to augment controller-based input by giving superpowers to players in two VR games. Blowing out long and strong turns you into a fire-breathing dragon, while holding your breath sends you into stealth mode.&nbsp; By using&nbsp; breathing as a directly controlled physiological signal, BreathVR can facilitate unique and engaging play experiences through natural interaction in single and multiplayer virtual reality games. Paper available here:&nbsp;<a href=""http://web.media.mit.edu/~sra/breathvr.html"">http://web.media.mit.edu/~sra/breathvr.html</a></p>",2018-07-31,,2018-08-20 16:27:18.375,True,2017-08-16,BreathVR,PUBLIC,,False,Fluid Interfaces,False
galvr-a-novel-collaboration-interface-using-gvs,xuhaixu,False,"<p>GVS or galvanic vestibular stimulation is a technology that directly affects a user's vestibular system by altering their sense of balance and direction. It works through electrical stimulation via electrodes placed on the mastoid bones behind each ear. In standing users, GVS evokes a prolonged ""galvanic body sway."" In walking users, it affects balance and causes users to stagger in the anodal direction. However, in walking users, with their head pitched forward, it causes them to turn smoothly from their planned trajectory in the anodal direction. Dark Room is a cooperative asymmetrical ""escape the room"" style game played by a VR and a PC user, inspired by the single-player mobile game Dark Echo. The PC user controls the walking direction of the VR user to guide them around virtual or physical obstacles. The VR player uses echolocation to detect obstacles. Video and paper available here:&nbsp;<a href=""http://web.media.mit.edu/~sra/gvs.html"">http://web.media.mit.edu/~sra/gvs.html</a></p>",2018-07-31,,2019-04-19 17:08:16.864,True,2017-08-15,A VR Collaboration Interface Using Galvanic Vestibular Stimulation,PUBLIC,,False,Fluid Interfaces,False
robot-vocal-expressiveness,jakory,False,"<p>Prior research with preschool children has established that book reading, especially when children are encouraged to actively process the story materials through dialogic reading, is an effective method for expanding young children’s vocabulary.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">A growing body of research also suggests that social robots have potential as learning companions and tutors for young children’s early language education. Social robots are new technologies that combine the adaptability, customizability, and scalability of technology with the embodied, situated world in which we operate.</span></p><p>In this project, we asked whether a social robot can effectively engage preschoolers in dialogic reading. Given that past work has shown that children can and do learn new words from social robots, we investigate what factors modulate their learning. In particular, we looked at whether the verbal expressiveness of the robot impacted children’s learning and engagement during a dialogic reading activity.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">This project was funded by an NSF Cyberlearning grant.</span></p>",2017-09-01,,2019-03-02 01:42:38.931,True,2015-09-01,Robot Expressiveness Affects Children's Learning,PUBLIC,http://robotic.media.mit.edu/portfolio/robot-vocal-expressivity,False,Personal Robots,False
children,jakory,False,"<p>When learning from human partners, infants and young children will pay attention to nonverbal signals, such as gaze and bodily orientation, to figure out what a person is looking at and why. They may follow gaze to determine what object or event triggered another's emotion, or to learn about the goal of another's ongoing action. They also follow gaze in language learning, using the speaker's gaze to figure out what new objects are being referred to or named.</p><p>In this project, we examine whether young children will attend to the same social cues from a robot as from a human partner during a word learning task, specifically gaze and bodily orientation.</p>",2015-12-31,,2017-06-07 16:38:44.917,True,2014-09-01,Children Use Nonverbal Cues to Learn from Robots,PUBLIC,http://robotic.media.mit.edu/portfolio/children-use-nonverbal-cues-learn-robots/,False,Personal Robots,False
relational-ai,jakory,False,"<h2>Creating long-term interpersonal interaction and shared experiences with social robots&nbsp;<br></h2><p>Many of our current projects explore the use of social robots as a technology to support young children's early language development. In this project, instead of focusing on <i>how</i> to make social robots effective as an educational tools, we ask <i>why</i> they are effective. Based on our prior work, we hypothesize that a key aspect of why social robots can benefit children's learning is their nature as a<i> relational technology</i>—that is, a technology that can build long-term, social-emotional relationships with users. </p><p>Thus, in this project, our goals are twofold. First, we aim to understand how children conceptualize social robots as relational agents in learning contexts, and how children relate to these robots through time. Second, we explore the core nature of autonomous relational technologies, that is, relational AI. We will examine how adding features of relational AI to a social robot impacts longitudinal child-robot learning interactions, including children's learning, engagement, and relationships.</p><p>As part of this project, we are taking a second look at work we have done so far, this time through the lens of children's relationships. We are creating assessments for measuring young children's relationships. We are developing a computational relational AI model, which we will test during a longitudinal study with a social robot.</p><p><a href=""https://www.media.mit.edu/posts/making-new-robot-friends/"">Read more about children's relationships with robots here!</a><br></p>",,,2018-10-19 15:23:25.170,True,2016-09-01,Relational AI,PUBLIC,,True,Personal Robots,False
relationship-assessments,jakory,False,"<p>Social robots are increasingly being developed for long-term interactions with children in domains such as healthcare, education, therapy, and entertainment.&nbsp;In prior research, we have seen that children treat robots as more than mere artifacts, e.g., ascribing them mental states, psychological attributes, and moral standing. Thus, while children’s relationships with robots may not be like the relationships they have with their parents, pets, imaginary friends, or smart devices, they will form relationships of some kind.&nbsp;As such, we need to deeply understand how children’s relationships with robots develop through time, and find ways to characterize and measure these relationships.&nbsp;However, there are few validated assessments for measuring young children’s long-term relationships. Thus, we have adapted or created a variety of assessments for use in this context for children aged 5-6 years.&nbsp;</p><p><a href=""https://dam-prod.media.mit.edu/x/2018/04/25/KoryWestlund-IDC18.pdf"">Four of these assessments are presented in the associated paper.</a></p><p>This paper shows that children can appropriately respond to these assessments with reasonably high internal reliability, and that these assessments are able to capture child-robot relationship adjustments&nbsp; over a long-term interaction.</p>",,,2019-04-17 18:39:40.951,True,2017-04-01,Assessing Children's Relationships with Social Robots,PUBLIC,,True,Personal Robots,False
a-new-project,aamena,False,"<p>It is well established that countries, regions and institutions tend to develop towards related activities. This implies that, for instance, countries are more likely to enter a new activity that is closer/related with the activities it has already developed. An empirical fact that results from the overlapping of the necessary knowledge of each activity. In this context, the product space—a network relating countries economic activities—has been instrumental in capturing the role of relatedness in the economic development of countries. But, although relatedness seems to be a major driver for the diversification of countries exports and research activities, there are many instances when countries deviate from this norm, but to what extent do they benefit from such actions? Is it possible to pinpoint a particular stage of development of a country in which these exceptions are more likely to occur or are they purely at random?</p><p>Using 50 years of trade date we have analyzed how countries diversify their products portfolios in the context of the Economic Complexity and Product Space10. We have shown that 1) there is an intermediate and non-trivial stage of economic development at which countries are more likely to develop towards unrelated activities; that 2) countries that do so achieve a faster economic growth; and 3) that low and high developed economies are the ones that are more likely to diversify towards related varieties.</p><p>These results have significant implications in the literature of regional development. For instance, recently the European Union presented a regional plan of development, coined as Smart Specialization, which advocates for a one rule that fits all: regions should develop the most related and highest reward activities. Our results suggest more caution. Indeed, our findings point out that the development stage of a country, or a region, plays a determinant role in devising a development strategy. For instance, while low and highly developed regions should look forward to developing related activities, regions at an intermediate level of development should be incentivized to pursue the development of unrelated activities and diversify. These results build up to the conclusions of the previous project (2.1), in the sense that economies should adopt dynamical diversification strategies in which the big challenge is to identify the narrow window for unrelated diversification.</p>",2017-12-31,,2017-10-11 00:23:45.415,True,2016-06-01,Do countries benefit from jumping into unrelated varieties?,PUBLIC,,False,Collective Learning,False
strategic-diffusion,aamena,False,"<p>One of the eternal challenges of economic development is how to identify the economic activities that a country, city, or region should target. During recent years, a large body of research has shown that countries, regions, and cities, are more likely to enter economic activities that are related to the ones they already have. For instance, a region specialized in the exports of frozen fish and crustaceans can more easily start exporting fresh fish than heavy machinery. This research has illuminated a new chapter in the economic development literature, but has left an important question unanswered: what is the right strategy for countries wanting to diversify their economies?&nbsp;</p>",,,2019-04-17 19:27:50.930,True,2018-04-02,What is the optimal way to diversify an economy?,PUBLIC,,True,Collective Learning,False
relatedness-knowledge-diffusion-and-the-evolution-of-bilateral-trade,aamena,False,"<p>During the last few decades two important intellectual contributions have reshaped our understanding of international trade. On the one hand, work emphasizing trade frictions and extended gravity models has shown that countries trade more with those with whom they share a language, colonial past, or ethnic social relationships. This is interpreted as evidence of trade not being only about differences in factor endowments and transportation costs, but the result of complex social processes where information frictions and social networks play a key role. On the other hand, work emphasizing knowledge diffusion has shown that the probability that a country starts exporting a product increases with the number of related products it already exports. Yet, despite the importance of these two recent findings, little is known about their intersection: does knowledge on how to export to a destination also diffuses among related products? Here, we use bilateral trade data from 2000 to 2015, disaggregated into 1,242 product categories, to create an extended gravity model of bilateral trade that reproduces previous findings (effects of language, distance, colonial past, etc.) and shows that, in addition to these, countries are more likely to increase their exports of a product to a destination when: (i) they export related products to it, (ii) already export that product to some of its neighbors, and (iii) have neighbors who also export the same product to that destination. We interpret these findings as evidence of knowledge diffusion among related products and among geographic neighbors, both in the context of exporters and importers. Then, we explore the magnitude of these effects for new, nascent, and experienced exporters, and also, for groups of products classified according to Lall's technological classification of exports. We find that the effects of product and geographic relatedness are stronger for new exporters, and also, that the effect of product relatedness increases with the technological sophistication of products. These findings support the idea that international trade is shaped by knowledge and information frictions that are partially reduced in the presence of product relatedness.</p>",,,2017-10-11 16:57:36.018,True,2017-03-01,"Relatedness, Knowledge Diffusion, and the Evolution of Bilateral Trade",PUBLIC,,True,Collective Learning,False
a-new-project,hartmado,False,"<p>It is well established that countries, regions and institutions tend to develop towards related activities. This implies that, for instance, countries are more likely to enter a new activity that is closer/related with the activities it has already developed. An empirical fact that results from the overlapping of the necessary knowledge of each activity. In this context, the product space—a network relating countries economic activities—has been instrumental in capturing the role of relatedness in the economic development of countries. But, although relatedness seems to be a major driver for the diversification of countries exports and research activities, there are many instances when countries deviate from this norm, but to what extent do they benefit from such actions? Is it possible to pinpoint a particular stage of development of a country in which these exceptions are more likely to occur or are they purely at random?</p><p>Using 50 years of trade date we have analyzed how countries diversify their products portfolios in the context of the Economic Complexity and Product Space10. We have shown that 1) there is an intermediate and non-trivial stage of economic development at which countries are more likely to develop towards unrelated activities; that 2) countries that do so achieve a faster economic growth; and 3) that low and high developed economies are the ones that are more likely to diversify towards related varieties.</p><p>These results have significant implications in the literature of regional development. For instance, recently the European Union presented a regional plan of development, coined as Smart Specialization, which advocates for a one rule that fits all: regions should develop the most related and highest reward activities. Our results suggest more caution. Indeed, our findings point out that the development stage of a country, or a region, plays a determinant role in devising a development strategy. For instance, while low and highly developed regions should look forward to developing related activities, regions at an intermediate level of development should be incentivized to pursue the development of unrelated activities and diversify. These results build up to the conclusions of the previous project (2.1), in the sense that economies should adopt dynamical diversification strategies in which the big challenge is to identify the narrow window for unrelated diversification.</p>",2017-12-31,,2017-10-11 00:23:45.415,True,2016-06-01,Do countries benefit from jumping into unrelated varieties?,PUBLIC,,False,Other,False
economic-complexity-and-income-inequality,hartmado,False,"<p>Decades ago development scholars argued that the productive structure of a country (i. e. the mix of industries operating in the country) constrains its ability to generate and distribute income. They were correct! It was recently shown that the mix of products that a country exports is predictive of its future pattern of diversification and economic growth. But what is the link between a country's productive structure and its ability to distribute income?&nbsp;Here, we combine methods from econometrics, network science, and economic complexity, together with data on income inequality and world trade, to show that countries exporting complex products have lower levels of income inequality than countries exporting simpler products. Using multivariate regression analysis, we show that economic complexity is a significant and negative predictor of income inequality and that this relationship is robust to controlling for aggregate measures of income, institutions, export concentration, and human capital. Moreover, we introduce a measure that associates a product to a level of income inequality equal to the average GINI of the countries exporting that product (weighted by the share the product represents in that country’s export basket). The Product-GINI index, or PGI,&nbsp;can provide important insights on the constraints to inequality imposed by a country's productive structure. Finally, we integrate our results to the Observatory of Economic Complexity, an online resource that allows its users to visualize the structural transformation of over 150 countries&nbsp;and their associated changes in income inequality during 1963–2008.</p>",,--Choose Location,2017-10-10 16:03:06.641,True,2014-09-01,Inequality and the impact of industrial structures,PUBLIC,,True,Other,False
which-industries-follow-relatedness,hartmado,False,"<p>Industries are more likely to enter and less likely to exit regions that are densely populated by related industries. Unfortunately, the measures used to estimate the relatedness of industries often combine information about multiple forms of relatedness. Here, we use data on the entire formal sector economy of a large country to construct five different measures of relatedness and compare their ability to predict diversification events. We interpret differences in the ability of these metrics to predict entry events as evidence of the relative importance of each relatedness channel for specific industries. These findings advance our understanding of the forms of relatedness that are more likely to predict regional diversification events for specific industries.</p>",,,2018-05-03 20:55:25.540,True,2017-02-01,Untangling Relatedness: What forms of relatedness predict diversification?,LAB-INSIDERS,,True,Other,False
connected-coral,katybell,False,"<p>Connected Coral integrates physical and digital elements in a visualization of the environmental impacts on reefs. This complex projection mapping uses multiple projectors, angled mirrors, and a motion sensor to create an interactive digital skin on a complex three-dimensional surface.<br></p><p>To integrate the projected content with the physical design, the students fabricated the physical coral model based on photogrammetry scans of real coral, warped and blended the projected areas, and factored in hardware specifications. These modifications minimize visual distortion on the uneven surface and allow for an uninhibited interactive experience.</p><p>This project was created through the Open Ocean Initiative and will be on display at the MIT Museum through Spring 2019.</p>",2018-12-31,,2018-11-08 19:07:36.290,True,2018-03-29,Connected Coral,PUBLIC,,False,Other,False
neaq-2069,katybell,False,"<p>The New England Aquarium was&nbsp;one of the world’s first modern aquariums when it opened its doors in Boston in 1969.&nbsp; Throughout its history, the aquarium has been a leader in innovative ways to share the ocean with the public, including the creation of the&nbsp;&nbsp;Giant Ocean Tank, the largest circular saltwater tank in the world when it opened in 1970.&nbsp;</p><p>Approaching its 50th anniversary, the New England Aquarium is working with the Open Ocean initiative and&nbsp;MIT Design Lab to develop future scenarios depicting what the experience of the aquarium will be in the next 50 years.</p>",,,2018-04-30 16:04:10.248,True,2018-01-22,NEAQ 2069: Envisioning the Future Aquarium Experience,PUBLIC,,True,Other,False
project-prometheus,katybell,False,"<p>Most of the underwater world remains far off the map. For many of the most exciting exploration challenges—from Maya cenotes to urban aquifers to archaeological treasures to coral reefs—map-making remains largely pre-industrial and time consuming. The difficulty and expense of mapping these spaces is a major barrier to storytelling for science, conservation, and stewardship. While many tools exist for open-ocean bathymetry (such as multibeam sonars), cost-effective diver-deployable tools for rapidly mapping complex and enclosed spaces are sorely lacking. Our goal is to create diver-deployable tools that are orders of magnitude faster, more precise, and less expensive than current practice–to enable mapping and imaging of these underwater resources at a societal scale.</p><p>To this end we are developing low-cost, high-precision, diver-deployable underwater LIDAR and Depth-Imaging systems—3D scanning and navigation systems with which to quickly, safely, and beautifully map caves, aquifers, coral reefs, sunken cities, and other large-scale underwater spaces. To satisfy scientific and storytelling needs, these devices must be easy to use, have fine spatial resolution, map at swimming speed, produce data in industry-standard formats, and be completely open source at both hardware and software levels.</p>",,,2019-04-22 17:55:43.363,True,2018-04-02,Project Prometheus,PUBLIC,,True,Other,False
my-deepsea-mybackyard,katybell,False,"<p>Seventy percent of nations have deep-sea environments within their maritime Exclusive Economic Zones (EEZs), yet only 16 percent of them are able to explore those environments. This is especially true for less economically developed countries. The dearth of technological capability and knowledge leads to a lack of exploration, inappropriate or inadequate management decisions, and unaware populations. Our goal is to empower countries to explore their own deep-sea backyards using low-cost technology, while building lasting in-country capacity. </p><p>Our project takes place in two small island developing states—the Republic of Kiribati, and Trinidad and Tobago. It utilizes Deep-Sea Drop Cameras developed by National Geographic’s Exploration Technology team (ExTech) and OpenROV’s Trident Remotely Operated Vehicles. Both technologies collect compelling imagery, but require minimal resources and expertise. In our pilot study during summer 2018, an engineer from ExTech and another team member traveled to each country to train a group of scientists, students, and communicators in the use of these technologies, which are to be left in-country until a scientist, a student, and a communicator from each country travel to the USA for further training in data analysis and creating outreach materials.</p>",,,2018-10-22 17:59:56.835,True,2018-04-02,"My Deep Sea, My Backyard",PUBLIC,,True,Other,False
journal-of-open-exploration,katybell,False,"<p>In collaboration with&nbsp;<a href=""https://www.media.mit.edu/groups/viral-communications/overview/"">Viral Communications</a>&nbsp;and the&nbsp;<a href=""https://www.media.mit.edu/groups/space-exploration/overview/"">Space Exploration initiative</a>, Open Ocean is using the&nbsp;<a href=""https://www.media.mit.edu/projects/pubpub/overview/"">PubPub</a>&nbsp;platform&nbsp; to launch the Journal of Open Exploration. We want to make the process and results of exploration collaborative, open, playful, and–most importantly–shared with a wider audience than traditional academic journals.</p>",,,2018-09-25 20:43:42.274,True,2018-02-26,Journal of Open Exploration,LAB,https://explore.pubpub.org/,True,Other,False
lego-wayfinder,katybell,False,"<p>The LEGO Wayfinder project combines LEGO, robotics, and seawater into a playground of project-based learning and citizen science for budding engineers and explorers. As part of this outreach program, our team has developed a first prototype of a buildable LEGO marine exploration vehicle kit—addressing some of the design challenges of building for the underwater context.</p><p>Our aim is to build an awareness of the state of the aquatic environment and instill a greater responsibility in shaping our interactions with the environment. To do so, young people will view underwater wonders of the world with their robots and get outside to explore their local waterway. Our approach embraces Seymour Papert’s model of ""low floors"" (where getting started is easy), and ""high ceilings,"" where students can pour their time and collaborative work efforts into creative engineering solutions to carry out a marine science experiment of their own design in the field.</p>",,,2019-04-22 17:15:27.304,True,2018-04-01,LEGO Wayfinder,PUBLIC,,True,Other,False
big-ocean-big-data,katybell,False,"<p>More ocean data has been collected in the last two years than in all previous years combined, and we are on a path to continue to break that record. More than ever, we need to establish a solid foundation for processing this ceaseless stream of data. This is especially true for visual data, where ocean-going platforms are beginning to integrate multi-camera feeds for observation and navigation.&nbsp;Techniques to efficiently process and utilize visual datasets with machine learning exist and continue to be transformative, but have had limited success in the ocean world due to:</p><ul><li>Lack of data set standardization;</li><li>Sparse annotation tools for the wider oceanographic community; and&nbsp;</li><li>Insufficient formatting of existing, expertly curated imagery for use by data scientists.&nbsp;</li></ul><p>Building on successes of the machine learning community, we are developing a public platform that makes use of existing (and future) expertly curated data. Our efforts will establish a new baseline dataset, optimized to directly accelerate development of modern, intelligent, automated analysis of underwater visual data. This effort will ultimately enable scientists, explorers, policymakers, storytellers, and the public to know what’s in the ocean and where it is for effective and responsible marine stewardship.</p>",,,2019-04-22 17:49:21.666,True,2018-04-02,"Big Ocean, Big Data",PUBLIC,,True,Other,False
connected-coral,emilysa,False,"<p>Connected Coral integrates physical and digital elements in a visualization of the environmental impacts on reefs. This complex projection mapping uses multiple projectors, angled mirrors, and a motion sensor to create an interactive digital skin on a complex three-dimensional surface.<br></p><p>To integrate the projected content with the physical design, the students fabricated the physical coral model based on photogrammetry scans of real coral, warped and blended the projected areas, and factored in hardware specifications. These modifications minimize visual distortion on the uneven surface and allow for an uninhibited interactive experience.</p><p>This project was created through the Open Ocean Initiative and will be on display at the MIT Museum through Spring 2019.</p>",2018-12-31,,2018-11-08 19:07:36.290,True,2018-03-29,Connected Coral,PUBLIC,,False,Object Based Media,False
neaq-2069,emilysa,False,"<p>The New England Aquarium was&nbsp;one of the world’s first modern aquariums when it opened its doors in Boston in 1969.&nbsp; Throughout its history, the aquarium has been a leader in innovative ways to share the ocean with the public, including the creation of the&nbsp;&nbsp;Giant Ocean Tank, the largest circular saltwater tank in the world when it opened in 1970.&nbsp;</p><p>Approaching its 50th anniversary, the New England Aquarium is working with the Open Ocean initiative and&nbsp;MIT Design Lab to develop future scenarios depicting what the experience of the aquarium will be in the next 50 years.</p>",,,2018-04-30 16:04:10.248,True,2018-01-22,NEAQ 2069: Envisioning the Future Aquarium Experience,PUBLIC,,True,Object Based Media,False
funnel-vision,emilysa,False,"<p><b>Motivation</b></p><p>The need for inexpensive, reliable 3D, 360-degree display technologies grows as augmented reality applications continue to increase in popularity. &nbsp;There is room for innovation in the field, as many volumetric displays have moving components, are prohibitively expensive, aren’t 360-degree, or some combination of those factors. &nbsp;By creating a 360-degree autostereoscopic display that is cost-effective and reliable, we could expand the audience who would benefit from collaborative, interactive experiences. Additionally, while augmented reality headsets are increasingly popular, a volumetric display readily encourages accessible, collaborative interaction without separating users by wearable technology. &nbsp;This technology could impact a variety of industries and demographics as the connected media landscape continues to expand.</p><p><b>Approach -&nbsp;</b><b>Hardware</b></p><p>This system is comprised on three main components. &nbsp;First, a 4K monitor displays properly distorted, lenticularized content using a custom shader. &nbsp;That light-field information is then partitioned by a refracting medium, either a radial lenticular array or a holographic optical element. &nbsp;After the rays of light from the pixels bend through the refracting medium, the rays then bounce off the conical mirror, producing the proper image for each viewing angle.</p><p><b>Hardware - Radial Lenticular</b></p><p>The radial lenticular design is dictated by several parameters which all trade-off with one another. &nbsp;For the design of this lenticular, I plan to focus on maximizing the resolution and size of the image rendered in the cone, while also maintaining a reasonable number of viewing angles. &nbsp;Variables that can be changed include the number of cameras (which corresponds to the number of discreet viewing zones), the number of views per lenticular slice, and the number of times a particular sequence of views should repeat. &nbsp;Those variables dictate the minimum viewing radius size, the angular size of each lenticular slice, the angle of influence for each viewing zone and the size of the image rendered in the cone.  It is important to consider what a user can reasonably see reflected in the cone, before the rays no longer bounce back to the user.</p><br><p>Resolution is one over the number of views per each lenticular wedge. &nbsp;As the user moves radially around the display, the rendered views that are opposite the user (i.e. on the other side of the radial mirror) cannot be seen. &nbsp;Because of this assertion that the user doesn’t need to see all of the potential views from a static position, radial lenticulars operate differently than linear lenticulars. &nbsp;With a linear lenticular, every view that will be displayed needs to exist under every lenticular lens for the effect to work.  For example, if there are 8 potential images lenticularized, there would be a slice of each of those 8 views underneath each lenticular lens. &nbsp;However, with this setup, it is not</p><p>necessary to produce every view under every lenticular lens because each lenticular lens only needs to display the views that the user could potentially see from that lens. &nbsp;&nbsp;Because of that, there is a rolling priority system for which views to display under each lenticular lens.  If the user is standing directly in front of where a viewing zone is located for a particular image, the lenticular lenses closest to the user should have that view centrally located under the lenticular. &nbsp;The lenticulars that are adjacent to that area should still have information that can reach the user, however the views have to shift to accommodate the off-axis position.  In that way, there is a rolling priority system, where the generated view that is closer to the user will appear centrally under the lenticular lens and incrementally move off-center and eventually disappear as the user moves radially around the display.</p><p><b>Hardware - Interactivity</b></p><p>For hardware integration for interactivity, I plan to use Intel Realsense cameras, microphones and arduino powered LEDs that all communicate with Unity. &nbsp;There is a plugin to use Intel Realsense in the unity environment, along with libraries that allow for more detailed signal processing and pose analysis.  I will use serial messaging in unity to send message to the arduino that will control the LEDs that will wrap around the housing of the device. &nbsp;The LEDs will be used to indicate important signals to users.  Those signals include that the system can see the user and whether the user appears to be engaging as well as can signal when the program is performing a request that requires time.  Additionally, I will use unity to receive and process microphone inputs.</p><p><b>Software -&nbsp;</b><b>Custom Shader</b></p><p>Unity has built in support for creating complex shaders that can be applied real-time. &nbsp;In Unity, I will write a shader that appropriately partitions the views based on the number of cameras in the scene, the number of views per each lenticular lens, and the number of times each sequence of views should repeat. &nbsp;Before the views can be lenticularized the media from the camera must be rotated around the display to match where it will physically appear on the reflected cone.  If you don’t do this step, all of the views will be rendered on top of each other, resulting in an incorrect result. &nbsp;By rotating the views to their appropriate location, the imagery you should see when standing on the left of the display will appear on the left and same for all other</p><p>directions. &nbsp;After the views have been rotated to the appropriate location, each of the camera’s views are sliced and reordered within the shader to produce the lenticularized result.</p><br><p><b>Software - Interactivity</b></p><br><p>Intel Realsense and similar depth sensing devices like Kinect and Leap Motion provide SDKs that allow developers to readily stream depth and color data into Unity. &nbsp;The Intel Realsense data can be further processed and interpreted using software like Nuitrack to do skeleton tracking which exposes body pose and joint positions.  This can be used to allow the system to know when a user is close enough to the display or standing a particular place. &nbsp;This is good for gesture tracking as well.  The color data from the Intel Realsense camera can be processed in OpenCV which has a Unity SDK.  With OpenCV, the color data can be analyzed for object detection, face detection and recognition and face pose analysis.  This allows the system to recognize objects, people and face pose (which could be used to interpret affective state). &nbsp;Face and gaze detection could be used in lieu of trigger words like “OK Google” and “Alexa” as presumably the users have intent to interact with the system if they’re looking at the character/sensors.</p><p>The sound recordings are used to detect volume, pitch and speech. &nbsp;The speech is analyzed using a cloud-based service, which then streams the input and a response back to unity to influence how the character animates and responds. &nbsp;The speech analysis could be used to interpret special assigned words to trigger activities, games or special animations or content in the display.  The response generated by the cloud-service can be used to animate the characters mouth if the character audibly responds to users. &nbsp;The coupling of depth/RGB data and audio allow for more nuanced understanding of a user’s intent and affective state.  In combination, this could be used to drive sympathetic animations from the character.  Because the RGB data allows for face recognition, the character can potentially store information about users to be retrieved whenever that user interacts with the system.</p><br><p><b>Software - Procedural Character</b></p><br><p>I chose an animated dog character as the embodied representative for several key affordances. &nbsp;The face and body of this character have already been rigged and those parameters are accessible in Unity. &nbsp;The design of the character is ideal for this system and for expressing emotion because the head size is large relative to the body and the facial features of the character are contrasted which will read better with this device. &nbsp;Additionally, because this character is familiar and has recognizable physical embodiments of emotion, the user will more readily understand key animation poses.  For example, if the character is happy, it will wag its tail and stick out its tongue, but if it’s nervous or upset, it will put its head and ears down. &nbsp;Based on the signals received and processed from the Intel Realsense camera and microphone, the dog character will animate.  Those inputs will impact the character’s emotional pose, LookAt( ) behavior and special animation sequences.</p><br><p><b>Evaluation</b></p><p>The ultimate goal of this project is to produce a low-cost, reliable, 3D, 360-degree autostereoscopic display. &nbsp;I will visually inspect the hardware/software implementation to determine if the desired 360-degree display is produced and measure the specifications of this display to other closely related devices. &nbsp;I will evaluate our software/hardware implementation by creating test patterns to discretize each individual view zone.  Additionally, we will perform user studies to verify that the interaction system is intuitive and engaging.</p><br><p><b>User Study</b></p><p>The questions I’d like to answer about this project lie at the intersection of the affordances of the display itself and the media users can interact with. &nbsp;Because I’m designing a procedural character for this display, I want to explore believability and engagement through the user study as well as whether placing a character in-situ in a space impacts user experience. &nbsp;The nearest neighbor devices for the study will be an AR headset and a 2D monitor.  In this individual-use user study, participants will engage in activities that evaluate the character’s ability to engage.  The participants will complete a close ended warm-up task both meant to acclimate and provide a concrete objective. &nbsp;After the warm-up task is completed (e.g. playing catch), the participants will be asked to engage in a more exploratory capacity with the character on each of the displays.  The activity will involve engaging the character’s LookAt( ) behavior, object detection, face-pose detection, speech recognition, and visual accessories (like the LED light-strip). &nbsp;The order in which the participants use each display will be randomized to avoid conflating intuitive use with prior experience with the activity they’re asked to compete.</p><br><p>If this device were to become commercially viable, it would have to be accessible for a wide variety of ages and expertise level. &nbsp;Because of that, I will look for a diverse group of participants with varying degrees of prior experience with technologies like video games, AR/VR, and voice assistants. &nbsp;As I design and implement features for my project, I will carefully prioritize scope that builds towards the user-study experience.</p><br><p><b>Future Work</b></p><p>If the radial lenticular works optimally, it would be amazing to explore a portable version of this novel volumetric display. &nbsp;Because the optical elements of this device have no moving parts and are lightweight, this display would be a great candidate for portability. &nbsp;It’d be fascinating to expand on the AI character’s capabilities by adding context aware infrastructure that changes how the character responds depending on localized metadata curation. &nbsp;</p><br><p>Ethically, this technology has the potential to make volumetric displays incredibly accessible at scale, because both the mylar cone and radial lenticular would cost less than one dollar to fabricate when productized. &nbsp;This would severely reduce cost on a market where displays typically cost thousands of dollars per unit.<br></p>",,,2019-01-21 23:00:29.197,True,2018-02-01,Funnel Vision,PUBLIC,,True,Object Based Media,False
ASAP,amosg,False,"<p><b>ASAP&nbsp;&nbsp;</b>is a natural extension of the <a href=""https://www.media.mit.edu/projects/leakyphones/overview/"">LeakyPhones project</a>.&nbsp;&nbsp;It is a rapid and <b>functional</b> “cardboard prototyping” platform for auditory augmented reality (AAR), location-based sound (LBS), and object&nbsp; sonification.&nbsp;The ASAP system is simple and versatile. It could be used by designers, doctors, educators and others with minimal prototyping and computer skills as a simple tool to rapidly <b>ideate</b>, <b>prototype,</b> and <b>test</b> ideas with real users.</p>",2018-08-01,,2018-05-07 13:44:21.752,True,2018-03-01,ASAP: A Spatial Auditory Platform for rapid prototyping of sonic user experiences,LAB-INSIDERS,,False,Tangible Media,False
lift-liquid-resin-injection-fabrication-technology-in-micro-gravity,amosg,False,"<p>The future of human life outside of Earth will heavily depend on the ability to fabricate and manufacture things. Yet fabrication in space poses numerous difficulties. Some of these challenges include storage space in vehicles, availability of raw materials, lack of machines, and shortage of manpower.&nbsp;</p><p>&nbsp;Other challenges in fabricating objects in space are simply a result of &nbsp;the &nbsp;different &nbsp;physical environment; &nbsp;the lack of gravity introduces unexpected material &nbsp;behaviour, as other forces aside from gravity become dominate. Surface tension, for example, becomes very dominant in determining the shape of liquid materials and adhesion between liquids and other materials also plays a more dominant role.&nbsp;</p><p>Because of the reasons stated above, 3D printing in space&nbsp;was conceptually limited to fused deposition modeling (FDM) technologies, which are less susceptible to problems resulting from the harsh conditions. Liquid- or powder-based printing technologies are assumed to be very problematic for space fabrication because of liquid behavior in microgravity conditions. On the other hand, FDM technologies have a lot of limitations such as the inability to create transparent &nbsp;structures or layerless shapes with defined smooth curvatures.</p><p>In this experiment, we would like to harness surface tension's dominance in liquid behavior under zero gravity conditions &nbsp;to create various controllable and accurate, layerless and transparent geometries using UV-curable resin. The resin will be hardened using a high-power UV light source.</p><p>We will focus on rapid fabrication (in under 17 seconds) of the following shapes:</p><ol><li>Shapes that are hard to make on Earth without special machinery, e.g., perfect lenses.</li><li>Shapes and materials that could be necessary in the space environment and are hard to make with existing methods available in space, such as ball bearings.&nbsp;</li></ol>",,,2019-04-17 19:30:16.889,True,2017-09-01,Liquid resin fabrication in microgravity,PUBLIC,,True,Tangible Media,False
auto-inflatables,amosg,False,,,,2017-11-25 17:16:39.275,True,2017-01-01,Auto-Inflatables,PUBLIC,,True,Tangible Media,False
leakyphones,amosg,False,"<p><strong>LeakyPhones</strong>&nbsp;is a public/private headset that was designed to encourage face-to-face interactions, curiosity, and&nbsp;healthier&nbsp;social skills by letting users ""peek"" into each other's music just by looking at one another.&nbsp;</p><p>Gaze is an important social signal in human interaction.&nbsp;Though its interpretation may vary across cultures, it is generally agreed that eye contact indicates interest&nbsp;and the point of attention in a conversation. Despite this, many common personal computing technologies, such as our smartphones and headphones, require significant visual and auditory attention thereby inhibiting our ability&nbsp;to interact with others. LeakyPhones offers a new approach for addressing this challenge.&nbsp;</p>",,,2019-02-26 20:28:41.011,True,2017-03-16,LeakyPhones,PUBLIC,,True,Tangible Media,False
grouploop-a-collaborative-network-enabled-audio-feedback-instrument,dramsay,False,"<p>GroupLoop is a browser-based, collaborative audio feedback control system for musical performance. Upon logging in, GroupLoop users send their microphone stream to other participants while simultaneously controlling the mix of other users' streams played through their speakers. Complex feedback loops involving several participants are possible by working together—in some cases, multiple feedback paths may overlap and interact. Users are able to shape the feedback sounds in real time by adjusting delay, EQ, and gain, as well as by manipulating the acoustics of their portion of the audio feedback path.&nbsp;</p><p>GroupLoop is capable of diverse and unexpected sounds, immeasurable reconfigurability, and in some cases, unrepeatable complexity. It creates new topologies for collaboration in performance, and invites thoughtful reflection on future topologies for real-time music collaboration over distance.</p><p>Try it at https://feedback.davidbramsay.com<span style=""font-size: 18px; font-weight: normal;"">.</span></p>",2015-01-01,--Choose Location,2016-12-05 00:17:13.639,True,2015-09-01,"GroupLoop: A Collaborative, Network-Enabled Audio Feedback Instrument",PUBLIC,https://feedback.davidbramsay.com,False,Responsive Environments,False
cognitive-audio,dramsay,False,"<p>When we form memories, not everything that we perceive is noticed; not  everything that we notice is remembered. Humans are excellent at  filtering and retaining only the most important parts of their  experience—what if our audio compression had the same ability?&nbsp;</p><p>Our goal is to understand what makes sound memorable. With this  work, we hope to gain insight into the cognitive processes that drive  auditory perception and predict the memorability of sounds in the world  around us more accurately than ever before.  Ultimately, these models  will give us the ability to generate and manipulate the sounds that  surround us to be more or less memorable.&nbsp;</p><p>We envision this research introducing new paradigms into the  space of audio compression, attention-driven user interactions, and  auditory AR, amongst others.</p>",,,2018-05-06 23:28:44.204,True,2017-11-01,Cognitive Audio,PUBLIC,,True,Responsive Environments,False
compression-by-content-curation,dramsay,False,"<p>As we move towards an increasingly IoT-enabled ecosystem, we find that it is easier than ever before to capture vast amounts of audio data.  However, there are many scenarios in which we may seek a ""compressed"" representation of an audio stream, consisting of an intentional curation of content to achieve a specific presentation—a background soundtrack for studying or working; a summary of salient events over the course of a day; or an aesthetic soundscape that evokes nostalgia of a time and place.  In this work, we present a novel, automated approach to the task of content-driven ""compression,"" built upon the tenets of auditory cognition, attention, and memory.  We expand upon our previous experimental findings, which demonstrate the relative importance of higher-level gestalt and lower level spectral principles in determining auditory memory, to design corresponding computational implementations enabled by auditory saliency models, deep neural networks for audio classification, and spectral feature extraction.  We demonstrate the approach by generating a number of 30 second binaural mixes from eight-hour recordings captured in three contrasting locations at the Media Lab, and conduct a qualitative evaluation illustrating the relationship between our feature space and a user's perception of the resulting presentations.  Through this work, we suggest rethinking traditional paradigms of compression in favor of an approach that is goal-oriented and modulated by human perception.</p>",,,2019-04-16 20:15:50.983,True,2019-03-01,Cognition-driven audio summarization,PUBLIC,,True,Responsive Environments,False
journalism-mapping-and-analytics-project-jmap,pernghwa,False,"<p>Over the last two decades, digital technologies have flattened old hierarchies in the news business and opened the conversation to a multitude of new voices. To help comprehend this promising but chaotic new public sphere, we're building a ""social news machine"" that will provide a structured view of the place where journalism meets social media. The basis of our project is a two-headed data ingest. On one side, all the news published online 24/7 by a sample group of influential US media outlets. On the other, all Twitter comments of the journalists who produced the stories. The two streams will be joined through network analysis and algorithmic inference. In future work we plan to expand the analysis to include all the journalism produced by major news outlets and the overall public response on Twitter, shedding new light on such issues as bias, originality, credibility, and impact.</p>",2016-05-31,--Choose Location,2016-12-05 00:17:15.736,True,2014-09-01,Journalism Mapping and Analytics Project (JMAP),PUBLIC,,False,Social Machines,False
the-electome-measuring-responsiveness-in-the-2016-election,pernghwa,False,"<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “<a href=""http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/"">Horse Race of Ideas</a>”).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf"">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/""> share of conversation</a> or coverage that <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/"">any given issue </a>or candidate commands on Twitter and in the news media, respectively—and how the two platforms are aligned<br></li><li>which issues are <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/"">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of “<a href=""http://fusion.net/story/304384/election-incivility-on-twitter/"">incivility</a>” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=""http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/""> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM’s deployment of Electome analytics has been <a href=""http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will"">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets—including the <a href=""https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/"">Washington Post</a>, <a href=""http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps"">Bloomberg News</a>, <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf"">CNN Politics</a> and Fusion—as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=""http://www.debates.org/"">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates’ moderators and credentialed journalists<br></li><li>also collaborated with the <a href=""http://ropercenter.cornell.edu/"">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center’s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>",,--Choose Location,2019-04-19 18:40:58.879,True,2015-09-01,The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,True,Social Machines,False
journalism-mapping-and-analytics-project-jmap,soph,False,"<p>Over the last two decades, digital technologies have flattened old hierarchies in the news business and opened the conversation to a multitude of new voices. To help comprehend this promising but chaotic new public sphere, we're building a ""social news machine"" that will provide a structured view of the place where journalism meets social media. The basis of our project is a two-headed data ingest. On one side, all the news published online 24/7 by a sample group of influential US media outlets. On the other, all Twitter comments of the journalists who produced the stories. The two streams will be joined through network analysis and algorithmic inference. In future work we plan to expand the analysis to include all the journalism produced by major news outlets and the overall public response on Twitter, shedding new light on such issues as bias, originality, credibility, and impact.</p>",2016-05-31,--Choose Location,2016-12-05 00:17:15.736,True,2014-09-01,Journalism Mapping and Analytics Project (JMAP),PUBLIC,,False,Social Machines,False
the-electome-measuring-responsiveness-in-the-2016-election,soph,False,"<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “<a href=""http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/"">Horse Race of Ideas</a>”).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf"">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/""> share of conversation</a> or coverage that <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/"">any given issue </a>or candidate commands on Twitter and in the news media, respectively—and how the two platforms are aligned<br></li><li>which issues are <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/"">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of “<a href=""http://fusion.net/story/304384/election-incivility-on-twitter/"">incivility</a>” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=""http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/""> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM’s deployment of Electome analytics has been <a href=""http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will"">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets—including the <a href=""https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/"">Washington Post</a>, <a href=""http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps"">Bloomberg News</a>, <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf"">CNN Politics</a> and Fusion—as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=""http://www.debates.org/"">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates’ moderators and credentialed journalists<br></li><li>also collaborated with the <a href=""http://ropercenter.cornell.edu/"">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center’s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>",,--Choose Location,2019-04-19 18:40:58.879,True,2015-09-01,The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,True,Social Machines,False
maestro-myth-exploring-the-impact-of-conducting-gestures-on-musicians-body-and-sounding-result,platte,False,"<p>Expert or fraud, the powerful person in front of an orchestra or choir attracts both hate and admiration. But what is the actual influence a conductor has on the musician and the sounding result? To throw light on the fundamental principles of this special gestural language, we try to prove a direct correlation between the conductor's gestures, muscle tension, and the physically measurable reactions of musicians in onset-precision, muscle tension, and sound quality. We also measure whether the mere form of these gestures causes different levels of stress or arousal. With this research we aim not only to contribute to the development of a theoretical framework on conducting, but also to enable a precise mapping of gestural parameters in order to develop and demonstrate a new system to the optional enhancement of musical learning, performance, and expression.</p>",2016-06-30,--Choose Location,2016-12-05 00:17:16.936,True,2015-01-01,Maestro Myth: Exploring the Impact of Conducting Gestures on Musician's Body and Sounding Result,PUBLIC,,False,Opera of the Future,False
empathy-and-the-future-of-experience,platte,False,"<p>Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of–as well as long-term commitment to–empathic communication.</p>",,--Choose Location,2019-04-17 19:59:42.795,True,2015-01-01,Empathy and the future of experience,PUBLIC,,True,Opera of the Future,False
mapping-the-globe,dignazio,False,<p>Mapping the Globe is an interactive tool and map that helps us understand where the <i>Boston Globe</i> directs its attention. Media attention matters—in quantity and quality. It helps determine what we talk about as a public and how we talk about it. Mapping the Globe tracks where the paper's attention goes and what that attention looks like across different regional geographies in combination with diverse data sets like population and income. Produced in collaboration with the <i>Boston Glob</i>e.</p>,2015-09-01,--Choose Location,2016-12-05 00:17:16.960,True,2014-09-01,Mapping the Globe,PUBLIC,,False,Civic Media,True
newspix,dignazio,False,"<p>NewsPix is a simple news-engagement application that helps users encounter breaking news in the form of high-impact photos. It is currently a Chrome browser extension (mobile app to come) that is customizable for small and large news organizations. Currently, when users open a new, blank page in Chrome, they get a new tab with tiles that show recently visited pages. NewsPix replaces that view with a high-quality picture from a news site. Users interested in more information about the photo can click through to the news site. News organizations can upload photos ranging from breaking news to historic sporting events, with photos changing every time a new tab is clicked.</p>",,--Choose Location,2016-12-05 00:17:03.242,True,2014-01-01,NewsPix,PUBLIC,,True,Civic Media,True
databasic,dignazio,False,"<p>DataBasic is a suite of web-based tools that give people fun and relevant ways learn how to work with data. Existing tools focus on operating on data quickly to create some output, rather than focusing on helping learners understand how to work with data. This fails the huge population of data literacy learners, who are trying to build their capacity in various ways. Our tools focus on the user as learner. They provide introductory activities, connect to people with fun sample datasets, and connect to other tools and techniques for working with data. We strongly believe in building tools focused on learners, and are putting those ideas into practice on these tools and activities. Visit <a href=""https://databasic.io"">databasic.io</a> today to try it out!</p>",,--Choose Location,2018-04-30 15:30:39.371,True,2015-09-01,DataBasic,PUBLIC,,True,Civic Media,True
the-babbling-brook,dignazio,False,"<p>The Babbling Brook is an unnamed neighborhood creek in Waltham, MA, that winds its way to the Charles River. With the help of networked sensors and real-time processing, the brook constantly tweets about the status of its water quality, including thoughts and bad jokes about its own environmental and ontological condition. Currently, the Babbling Brook senses temperature and depth and cross-references that information with real-time weather data to come up with extremely bad comedy. Thanks to Brian Mayton, the Responsive Environments group, and Tidmarsh Farms Living Observatory for their support.</p>",,--Choose Location,2016-12-05 00:16:54.870,True,2014-09-01,The Babbling Brook,PUBLIC,,True,Civic Media,True
open-water-project,dignazio,False,"<p>The Open Water Project aims to develop and curate a set of low-cost, open source tools enabling communities everywhere to collect, interpret, and share their water quality data. Traditional water monitoring uses expensive, proprietary technology, severely limiting the scope and accessibility of water quality data. Homeowners interested in testing well water, watershed managers concerned about fish migration and health, and other groups could benefit from an open source, inexpensive, accessible approach to water quality monitoring. We're developing low-cost, open source hardware devices that will measure some of the most common water quality parameters, using designs that makes it possible for anyone to build, modify, and deploy water quality sensors in their own neighborhood.</p>",,--Choose Location,2016-12-05 00:16:38.427,True,2014-01-01,Open Water Project,PUBLIC,,True,Civic Media,True
make-the-breast-pump-not-suck-hackathon-2018,dignazio,False,"<h1><strong>A Case for Breastfeeding Innovation</strong></h1><h2><strong>Breastfeeding saves lives.</strong></h2><p>If women globally were able to meet the WHO's public health goal to exclusively breastfeed for the first six months, we would prevent 823,000 infant deaths. For every 597 women who optimally breastfeed, one maternal or child death is prevented.</p><h2><strong>Breastfeeding promotes long-term wellness for mother and baby.</strong></h2><p>Breastfeeding protects against child infections and malocclusion, increases intelligence, and reduces the risk of obesity and diabetes for children¹. Breastfeeding decreases mothers' risk of breast cancer and optimal breastfeeding would lead to 20,000 fewer cases every year¹. It may also protect against ovarian cancer and diabetes¹. Women who are supported to successfully establish breastfeeding in early months have a lower risk for postpartum depression.</p><p><strong>Breastfeeding saves healthcare costs.</strong></p><p>If women in the US were able to meet the WHO's public health goal to exclusively breastfeed for the first six months, we would save $17.2 billion dollars in annual costs treating preventable events, including infant and maternal deaths, SIDS, ear infections and necrotizing enterocolitis in babies, and heart attacks, diabetes and breast cancer in mothers.</p><h2><strong>Work environments and policies in the US are hostile to breastfeeding.</strong></h2><p>The US is one of only three nations worldwide without paid parental leave. The other countries in this club are Papua New Guinea and Lesotho⁴. Women's return to work outside the home is the leading factor for early weaning⁵. Most US work environments do not provide material or policy-based support for breastfeeding women, including parental leave, flexible schedules, on-site daycare, breaks and spaces for nursing and pumping.</p><h2>The Hackathon</h2><p><a href=""https://www.makethebreastpumpnotsuck.com/team"">Our team</a>&nbsp;is thrilled to produce a weekend with the leading innovators in breastfeeding and postpartum health, along with many mamas, papas, babies, students, and newcomers. This time around we have a focus on equity and inclusive innovation in breastfeeding. We want to catalyze the development of tech, products, spaces, clothing, programs and services that have an eye on affordability and access as well as cultural diversity.</p><p>REFERENCES:</p><ol><li>Bartick, M. C., Schwarz, E. B., Green, B. D., Jegier, B. J., Reinhold, A. G., Colaizy, T. T., Stuebe, A. M. (2017). Suboptimal breastfeeding in the United States: Maternal and pediatric health outcomes and costs. Maternal &amp; Child Nutrition, 13(1), e12366.&nbsp;<a href=""http://doi.org/10.1111/mcn.12366"">http://doi.org/10.1111/mcn.12366</a></li><li>Watkins, S., Meltzer-Brody, S., Zolnoun, D., &amp; Stuebe, A. (2011). Early Breastfeeding Experiences and Postpartum Depression. Obstetrics &amp; Gynecology, 118(2, Part 1), 214–221.&nbsp;<a href=""http://doi.org/10.1097/AOG.0b013e3182260a2d"">http://doi.org/10.1097/AOG.0b013e3182260a2d</a></li><li>Breastfeeding in the 21st century: epidemiology, mechanisms, and lifelong effect. Victora, Cesar G et al. The Lancet, Volume 387, Issue 10017, 475 - 490.</li><li><a href=""https://en.wikipedia.org/wiki/Parental_leave"">https://en.wikipedia.org/wiki/Parental_leave</a></li><li>Why invest, and what it will take to improve breastfeeding practices? Rollins, Nigel C et al. The Lancet , Volume 387, Issue 10017, 491 - 504.</li></ol>",,,2019-04-19 19:02:17.039,True,2017-04-02,Make the Breast Pump Not Suck Hackathon 2018,PUBLIC,,True,Civic Media,True
mobile-wearable-sensor-data-visualization,gershon,False,"<p>As part of the Living Observatory ecological sensing initiative, we've been developing new approaches to mobile, wearable sensor data visualization. The Tidmarsh app for Google Glass visualizes real-time sensor network data based on the wearer's location and gaze. A user can approach a sensor node to see 2D plots of its real-time data stream, and look across an expanse to see 3D plots encompassing multiple devices. On the back-end, the app showcases our Chain API, crawling linked data resources to build a dynamic picture of the sensor network. Besides development of new visualizations, we are building in support for voice queries, and exploring ways to encourage distributed data collection by users.</p>",2015-01-01,--Choose Location,2016-12-05 00:17:17.896,True,2014-01-01,"Mobile, Wearable Sensor Data Visualization",PUBLIC,,False,Responsive Environments,False
hearthere-ubiquitous-sonic-overlay,gershon,False,"<p class="""">With our Ubiquitous Sonic Overlay, we are working to place virtual sounds in the user's environment, fixing them in space even as the user moves. We are working toward creating a seamless auditory display, indistinguishable from the user's actual surroundings. Between bone-conduction headphones, small and cheap orientation sensors, and ubiquitous GPS, a confluence of fundamental technologies is in place. However, existing head-tracking systems either limit the motion space to a small area (e.g., Oculus Rift), or sacrifice precision for scale using technologies like GPS. We are seeking to bridge the gap to create large outdoor spaces of sonic objects.</p>",,--Choose Location,2018-06-07 19:16:45.828,True,2014-09-01,HearThere: Ubiquitous Sonic Overlay,PUBLIC,,True,Responsive Environments,False
fingersynth-wearable-transducers-for-exploring-the-environment-through-sound,gershon,False,"<p>The FingerSynth is a wearable musical instrument made up of a bracelet and set of rings that enables its players to produce sound by touching nearly any surface in their environments. Each ring contains a small, independently controlled audio exciter transducer. The rings sound loudly when they touch a hard object, and are silent otherwise. When a wearer touches their own (or someone else's) head, the contacted person hears sound through bone conduction, inaudible to others. A microcontroller generates a separate audio signal for each ring, and can take user input through an accelerometer in the form of taps, flicks, and other gestures. The player controls the envelope and timbre of the sound by varying the physical pressure and the angle of their finger on the surface, or by touching differently resonant surfaces. The FingerSynth encourages players to experiment with the materials around them and with one another.</p>",,--Choose Location,2019-04-19 14:26:25.030,True,2014-01-01,FingerSynth: Wearable transducers for exploring the environment through sound,PUBLIC,,True,Responsive Environments,False
tidzam,gershon,False,"<p>Tid'Zam is an ambient sound analysis system for outdoor environments. It is a component of the Tidmarsh Farms project which monitors the environmental evolution of an industrial cranberry farm during its ecological restoration of wetland. Tid'Zam analyzes the audio streams generated by the deployed microphones in the wild in order to detect the sonic events happening on the site, such as bird calls, insects, frogs, rain, storms, car noise, human voices, and more.&nbsp;</p><p>This system is used to cross-validate other sensors for weather monitoring to identify, geolocalize, and track present wildlife and bird specimens over time. It also controls the audio mixers in order to mute or change the gain on noisy microphones.</p>",,--Choose Location,2017-10-02 15:14:53.312,True,2016-01-01,Tid'Zam,PUBLIC,http://deep-resenv.media.mit.edu:8080,True,Responsive Environments,False
cilllia-3d-printed-micro-pillar-structures-for-surface-texture-actuation-and-sensing,gershon,False,"<p>In nature, hair has numerous functions such as providing warmth, adhesion, locomotion, sensing, and a sense of touch, as well as its well-known aesthetic qualities. This work presents a computational method of 3D printing hair structures. It allows us to design and generate hair geometry at 50 micrometer resolution and assign various functionalities to the hair. The ability to fabricate customized hair structures enables us to create superfine surface texture, mechanical adhesion properties, new passive actuators, and touch sensors on a 3D-printed artifact. We also present several applications to show how the 3D-printed hair can be used for designing everyday interactive objects.</p>",,--Choose Location,2019-05-23 19:18:17.084,True,2015-01-01,"Cilllia: 3D-printed micro pillar structures for surface texture, actuation, and sensing",PUBLIC,,True,Responsive Environments,False
living-distance,gershon,False,"<p>Living Distance is a mission and a fantasy realized, in which a wisdom tooth is sent to outer space and back down to Earth again. Carried by a crystalline robotic device called EBIFA, the tooth tells the inconsequential but unique story of a person in this universe.&nbsp;EBIFA's form and function follow an unusually personal approach to our technological space futures, one centered on visceral, active, empathic, and poetic engagement.<br></p>",,,2019-06-03 13:32:35.379,True,2018-09-03,Living Distance,PUBLIC,http://xxxxxxxxxinliu.com,True,Responsive Environments,False
doppelmarsh-cross-reality-environmental-sensor-data-browser,gershon,False,"<p>Doppelmarsh is a cross-reality sensor data browser built for experimenting with presence and multimodal sensory experiences. Built on evolving terrain data from a physical wetland landscape, the software integrates real-time data from an environmental sensor network with real-time audio streams and other media from the site. Sensor data is rendered in the scene in both visual representations and as 3D sonification. Users can explore this data by walking on the virtual terrain in a first person view, or flying high above it. This flexibility allows Doppelmarsh to serve as an interface to other research platforms on the site, such as Quadrasense, an augmented reality UAV system that blends a flying live camera view with a virtual camera from Doppelmarsh. We are currently investigating methods for representing subsurface data, such as soil and water temperatures at depth, as well as automation in scene and terrain painting.</p>",,--Choose Location,2019-04-19 14:24:11.949,True,2014-09-01,Doppelmarsh: Cross-reality environmental sensor data browser,PUBLIC,,True,Responsive Environments,False
mobile-wearable-sensor-data-visualization,bmayton,False,"<p>As part of the Living Observatory ecological sensing initiative, we've been developing new approaches to mobile, wearable sensor data visualization. The Tidmarsh app for Google Glass visualizes real-time sensor network data based on the wearer's location and gaze. A user can approach a sensor node to see 2D plots of its real-time data stream, and look across an expanse to see 3D plots encompassing multiple devices. On the back-end, the app showcases our Chain API, crawling linked data resources to build a dynamic picture of the sensor network. Besides development of new visualizations, we are building in support for voice queries, and exploring ways to encourage distributed data collection by users.</p>",2015-01-01,--Choose Location,2016-12-05 00:17:17.896,True,2014-01-01,"Mobile, Wearable Sensor Data Visualization",PUBLIC,,False,Responsive Environments,False
low-power-wireless-environmental-sensor-node,bmayton,False,"<p>Tidmarsh is a 600-acre former cranberry farm near Plymouth, MA that has undergone a restoration to wetland. We have instrumented the site with an extensive network of custom low-power environmental sensor nodes, microphones, and cameras. The data from the network is made available in real time and has enabled a number of explorations into the ways that people can experience and learn from large-scale, long-term sensor installations.</p><p><a href=""https://tidmarsh.media.mit.edu"">See sensor data, listen to live audio, and watch live camera feeds on the Tidmarsh website.</a><br></p>",,,2019-04-19 14:30:37.248,True,2016-06-01,Low-power wireless environmental sensor network,PUBLIC,https://tidmarsh.media.mit.edu,True,Responsive Environments,False
tidmarsh-living-observatory-portal,bmayton,False,"<p>The Tidmarsh Living Observatory Portal is a research project that focuses on the realization of a pavilion that will&nbsp;generate an immersive experience about the Tidmarsh Living Observatory. This&nbsp;site has been restored from a former cranberry farm to natural wetland.&nbsp;Through an extensive Responsive Environments research, this networked&nbsp; and&nbsp;outdoor instrumented site streams live&nbsp; data that will be part of the portal&nbsp;experience.</p>",,,2019-04-19 18:16:55.498,False,2019-04-19,Tidmarsh Living Observatory Portal,PUBLIC,,True,Responsive Environments,False
doppelmarsh-cross-reality-environmental-sensor-data-browser,bmayton,False,"<p>Doppelmarsh is a cross-reality sensor data browser built for experimenting with presence and multimodal sensory experiences. Built on evolving terrain data from a physical wetland landscape, the software integrates real-time data from an environmental sensor network with real-time audio streams and other media from the site. Sensor data is rendered in the scene in both visual representations and as 3D sonification. Users can explore this data by walking on the virtual terrain in a first person view, or flying high above it. This flexibility allows Doppelmarsh to serve as an interface to other research platforms on the site, such as Quadrasense, an augmented reality UAV system that blends a flying live camera view with a virtual camera from Doppelmarsh. We are currently investigating methods for representing subsurface data, such as soil and water temperatures at depth, as well as automation in scene and terrain painting.</p>",,--Choose Location,2019-04-19 14:24:11.949,True,2014-09-01,Doppelmarsh: Cross-reality environmental sensor data browser,PUBLIC,,True,Responsive Environments,False
mobile-wearable-sensor-data-visualization,sfr,False,"<p>As part of the Living Observatory ecological sensing initiative, we've been developing new approaches to mobile, wearable sensor data visualization. The Tidmarsh app for Google Glass visualizes real-time sensor network data based on the wearer's location and gaze. A user can approach a sensor node to see 2D plots of its real-time data stream, and look across an expanse to see 3D plots encompassing multiple devices. On the back-end, the app showcases our Chain API, crawling linked data resources to build a dynamic picture of the sensor network. Besides development of new visualizations, we are building in support for voice queries, and exploring ways to encourage distributed data collection by users.</p>",2015-01-01,--Choose Location,2016-12-05 00:17:17.896,True,2014-01-01,"Mobile, Wearable Sensor Data Visualization",PUBLIC,,False,Responsive Environments,False
hearthere-ubiquitous-sonic-overlay,sfr,False,"<p class="""">With our Ubiquitous Sonic Overlay, we are working to place virtual sounds in the user's environment, fixing them in space even as the user moves. We are working toward creating a seamless auditory display, indistinguishable from the user's actual surroundings. Between bone-conduction headphones, small and cheap orientation sensors, and ubiquitous GPS, a confluence of fundamental technologies is in place. However, existing head-tracking systems either limit the motion space to a small area (e.g., Oculus Rift), or sacrifice precision for scale using technologies like GPS. We are seeking to bridge the gap to create large outdoor spaces of sonic objects.</p>",,--Choose Location,2018-06-07 19:16:45.828,True,2014-09-01,HearThere: Ubiquitous Sonic Overlay,PUBLIC,,True,Responsive Environments,False
tidzam,sfr,False,"<p>Tid'Zam is an ambient sound analysis system for outdoor environments. It is a component of the Tidmarsh Farms project which monitors the environmental evolution of an industrial cranberry farm during its ecological restoration of wetland. Tid'Zam analyzes the audio streams generated by the deployed microphones in the wild in order to detect the sonic events happening on the site, such as bird calls, insects, frogs, rain, storms, car noise, human voices, and more.&nbsp;</p><p>This system is used to cross-validate other sensors for weather monitoring to identify, geolocalize, and track present wildlife and bird specimens over time. It also controls the audio mixers in order to mute or change the gain on noisy microphones.</p>",,--Choose Location,2017-10-02 15:14:53.312,True,2016-01-01,Tid'Zam,PUBLIC,http://deep-resenv.media.mit.edu:8080,True,Responsive Environments,False
doppelmarsh-cross-reality-environmental-sensor-data-browser,sfr,False,"<p>Doppelmarsh is a cross-reality sensor data browser built for experimenting with presence and multimodal sensory experiences. Built on evolving terrain data from a physical wetland landscape, the software integrates real-time data from an environmental sensor network with real-time audio streams and other media from the site. Sensor data is rendered in the scene in both visual representations and as 3D sonification. Users can explore this data by walking on the virtual terrain in a first person view, or flying high above it. This flexibility allows Doppelmarsh to serve as an interface to other research platforms on the site, such as Quadrasense, an augmented reality UAV system that blends a flying live camera view with a virtual camera from Doppelmarsh. We are currently investigating methods for representing subsurface data, such as soil and water temperatures at depth, as well as automation in scene and terrain painting.</p>",,--Choose Location,2019-04-19 14:24:11.949,True,2014-09-01,Doppelmarsh: Cross-reality environmental sensor data browser,PUBLIC,,True,Responsive Environments,False
tattio,cindykao,False,"<p>We present Tattio, a fabrication process that draws from current body decoration processes (i.e., jewelry such as metallic temporary tattoos) for the creation of on-skin technology. The fabrication process generates functional components such as Near Field Communication (NFC) tags, while maintaining the aesthetics and user experience of existing metallic temporary tattoos. The fabrication process is low-cost, accessible, and customizable; we seek to enable individuals to design, make, and wear their own skin technology creations.</p>",2016-03-01,--Choose Location,2016-12-05 00:17:23.072,True,2015-09-01,Tattio,PUBLIC,,False,Speech + Mobility,False
sensortape-modular-and-programmable-3d-aware-dense-sensor-network-on-a-tape,cindykao,False,"<p>SensorTape is a modular and dense sensor network in a form factor of a tape. SensorTape is composed of interconnected and programmable sensor nodes on a flexible electronics sub-strate. Each node can sense its orientation with an inertial measurement unit, allowing deformation self-sensing of the whole tape. Also, nodes sense proximity using time-of-flight infrared. We developed network architecture to automatically determine the location of each sensor node, as SensorTape is cut and rejoined. We also made an intuitive graphical interface to program the tape. Our user study suggested that SensorTape enables users with different skill sets to intuitively create and program large sensor network arrays. We developed diverse applications ranging from wearables to home sensing, to show low-deployment effort required by the user. We showed how SensorTape could be produced at scale and made a 2.3-meter long prototype.</p>",,--Choose Location,2019-04-19 14:34:20.486,True,2016-01-01,SensorTape: Modular and programmable 3D-aware dense sensor network on a tape,PUBLIC,,True,Speech + Mobility,False
kino-kinetic-wearable,cindykao,False,"<p><span style=""font-size: 18px; font-weight: 400;"">This work explores a dynamic future in which the accessories we wear are no longer static, but are instead mobile, living objects on the body. Engineered with the functionality of miniaturized robotics, this ""living"" jewelry roams on unmodified clothing, changing location and reconfiguring appearance according to social context and enabling multiple presentations of self. With the addition of sensor devices, they can actively respond to environmental conditions. They can also be paired with existing mobile devices to become personalized on-body assistants to help complete tasks. Attached to garments, they generate shape-changing clothing and kinetic pattern designs—creating a new, dynamic fashion.</span><br></p><p> </p><p><span style=""font-size: 18px; font-weight: 400;"">It is our vision that in the future, these robots will be miniaturized to the extent that they can be seamlessly integrated into existing practices of body ornamentation. With the addition of kinetic capabilities, traditionally static jewelry and accessories will start displaying life-like qualities, learning, shifting, and reconfiguring to the needs and preferences of the wearer, also assisting in fluid presentation of self. With wearables that possess hybrid qualities of the living and the crafted, we explore a new on-body ecology for human-wearable symbiosis.</span><span style=""font-size: 18px; font-weight: 400;"">&nbsp;</span></p>",,--Choose Location,2017-10-17 03:54:26.236,True,2015-01-01,Kino,PUBLIC,,True,Speech + Mobility,False
rovables,cindykao,False,"<p>We introduce Rovables, a miniature robot that can move freely on unmodified clothing. The robots are held in place by magnetic wheels, and can climb vertically. The robots are untethered and have an onboard battery, microcontroller, and wireless communications. They also contain a low-power localization system that uses wheel encoders and IMU, allowing Rovables to perform limited autonomous navigation on the body. In the technical evaluations, we found that Rovables can operate continuously for 45 minutes and can carry up to 1.5N. We propose an interaction space for mobile on-body devices spanning sensing, actuation, and interfaces, and develop application scenarios in that space. Our applications include on-body sensing, modular displays, tactile feedback and interactive clothing and jewelry.
                    
                </p>",,,2017-03-03 18:38:10.500,True,2016-09-01,Rovables,PUBLIC,,True,Speech + Mobility,False
nailo,cindykao,False,"<p>NailO is a wearable input device in the form of a commercialized nail art sticker. It works as a miniaturized trackpad the size and thickness of a fingernail that can connect to your mobile devices; it also enables wearers to customize the device to fit the wearer’s personal style. NailO allows wearers to perform different functions on a phone or PC with different gestures, and the wearer can easily alter its appearance with a nail art design layer, creating a combination of functionality and aesthetics.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">From the fashion-conscious, to techies, and anyone in between, NailO can make a style, art, or a design statement; but in its more neutral, natural-looking example it can be worn and used only for its functionality. As a nail art sticker, NailO is small, discreet, and removable. Interactions through NailO can be private and subtle, for example attracting minimal attention when you are in a meeting but need to reply to an urgent text message. Mimicking the form of a cosmetic extension, NailO blends into and decorates one’s body when attached, yet remains removable at the wearer’s discretion, giving the wearer power and control over the level of intimacy of the device to one’s body.</span></p>",,--Choose Location,2017-03-27 21:12:03.637,True,2014-09-01,NailO,PUBLIC,http://nailo.media.mit.edu,True,Speech + Mobility,False
2ndskin,cindykao,False,"<p>DuoSkin is a fabrication process that enables anyone to create customized functional devices that can be attached directly to the skin. Using gold metal leaf, a material that is cheap, skin-friendly, and robust for everyday wear, we demonstrate three types of on-skin interfaces: sensing touch input, displaying output, and wireless communication. DuoSkin draws from the aesthetics found in metallic jewelry-like temporary tattoos to create on-skin devices which resemble jewelry. DuoSkin devices enable users to control their mobile devices, display information, and store information on their skin while serving as a statement of personal style. We believe that in the future, on-skin electronics will no longer be black-boxed and mystified; instead, they will converge towards the user friendliness, extensibility, and aesthetics of body decorations, forming a DuoSkin integrated to the extent that it has seemingly disappeared.</p><p><span style=""font-size: 18px; font-weight: normal;"">Credits:<br></span><span style=""font-size: 18px; font-weight: normal;"">Cindy Hsin-Liu Kao, Asta Roseway*, Christian Holz*, Paul Johns*, Andres Calvo, Chris Schmandt<br></span><span style=""font-size: 18px; font-weight: 400;"">MIT Media Lab in collaboration with Microsoft Research*</span></p>",,--Choose Location,2017-10-17 18:38:36.804,True,2015-09-01,DuoSkin,PUBLIC,http://duoskin.media.mit.edu,True,Speech + Mobility,False
chromoskin,cindykao,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Makeup has long been used as a body decoration process for self-expression and for the transformation of one's appearance. While the material composition and processes for creating makeup products have evolved, they still remain static and non-interactive. But our social contexts demand different representations of ourselves; thus, we propose ChromoSkin, a dynamic color-changing makeup system that gives the wearer ability to alter seamlessly their appearance. We prototyped an interactive eye shadow tattoo composed of thermochromic pigments activated by electronics or ambient temperature conditions. We present the design and fabrication of these interactive cosmetics, and the challenges in creating skin interfaces that are seamless, dynamic, and fashionable.</span></p>",,--Choose Location,2018-02-08 20:50:38.533,True,2015-12-01,ChromoSkin,PUBLIC,https://vimeo.com/155460417,True,Speech + Mobility,False
invisible-ink,amirl,False,"<p>Invisible Ink is a certified mail application that demonstrates the utility of the blockchain for maintaining a public ledger of transactions while keeping the content of those transactions private. In this case, the idea is a method for guaranteeing the delivery, receipt and existence of email messages. It archives the transaction in the Bitcoin blockchain and uses secure off-chain storage for the other details. Invisible Ink demonstrates the extensibility of this distributed technology for contracts, audits, and recovery of sensitive information. It is an evolution of work begun as the Ethos project. Since Bitcoin has shown that a distributed system of trust can be workable for irreversibly storing time-stamped information, these extensions and applications are potentially important for a wide variety of cases from finance to personal information.</p>",2015-01-01,--Choose Location,2017-06-25 16:35:11.855,True,2015-01-01,Invisible Ink,PUBLIC,,False,Viral Communications,False
ethos,amirl,False,"<p>Ethos is a decentralized, Bitcoin-like network for storing and sharing valuable information. We provide transparency, control, and ownership over personal data and its distribution. Validation and maintenance is distributed throughout the data community and automatically maintained without needing a safe deposit box or a commercial site. What Bitcoin has done for currency and BitTorrent for media, Ethos does for personal data. Nodes in the network are incentivized by collecting transaction fees, coinbase transactions (""finding blocks""), and proof-of-storage fees to sustain the distribution of personal data. Fees are paid with the underlying cryptocurrency represented by the network, also known as ""PrivacyCoin."" The role of nodes, besides the usual proof-of-work, which protects against ""double spending,"" is to maintain shredded pieces of information and present them to the network on-demand. </p>",2015-09-01,--Choose Location,2017-06-25 16:32:52.708,True,2014-01-01,Ethos,PUBLIC,,False,Viral Communications,False
ethos,guyzys,False,"<p>Ethos is a decentralized, Bitcoin-like network for storing and sharing valuable information. We provide transparency, control, and ownership over personal data and its distribution. Validation and maintenance is distributed throughout the data community and automatically maintained without needing a safe deposit box or a commercial site. What Bitcoin has done for currency and BitTorrent for media, Ethos does for personal data. Nodes in the network are incentivized by collecting transaction fees, coinbase transactions (""finding blocks""), and proof-of-storage fees to sustain the distribution of personal data. Fees are paid with the underlying cryptocurrency represented by the network, also known as ""PrivacyCoin."" The role of nodes, besides the usual proof-of-work, which protects against ""double spending,"" is to maintain shredded pieces of information and present them to the network on-demand. </p>",2015-09-01,--Choose Location,2017-06-25 16:32:52.708,True,2014-01-01,Ethos,PUBLIC,,False,Human Dynamics,False
enigma,guyzys,False,"<p>A peer-to-peer network, enabling different parties to jointly store and run computations on data while keeping the data completely private. Enigma's computational model is based on a highly optimized version of secure multi-party computation, guaranteed by a verifiable secret-sharing scheme. For storage, we use a modified distributed hashtable for holding secret-shared data. An external blockchain is utilized as the controller of the network, manages access control and identities, and serves as a tamper-proof log of events. Security deposits and fees incentivize operation, correctness, and fairness of the system. Similar to Bitcoin, Enigma removes the need for a trusted third party, enabling autonomous control of personal data. For the first time, users are able to share their data with cryptographic guarantees regarding their privacy.</p>",,--Choose Location,2018-02-12 20:37:17.158,True,2015-01-01,Enigma,PUBLIC,,True,Human Dynamics,False
media-lab-digital-certificates,guyzys,False,"<p class="""">Blockchain Certificates is a set of tools, software, and strategies to store and manage digital credentials. Certificates are registered on the bitcoin blockchain, cryptographically signed, and tamper-proof. They can represent or recognize many different types of achievements. After a number of prototypes (we issued digital credentials to Media Lab Director's Fellows and Media Lab alumni) we published our code under an open-source license to enable others to deploy the tools we developed. More information at&nbsp;<a href=""http://blockcerts.org"" class="""">http://blockcerts.org</a>.&nbsp;</p>",,--Choose Location,2019-01-31 17:23:57.299,True,2015-01-01,Digital Academic Credentials,PUBLIC,,True,Human Dynamics,False
visualizing-patterns-of-segregation,eaman,False,"<p>Segregation, or income inequality, is one of the major problems of our society. Residential segregation has long been of research interest in fields such as sociology, economics, and psychology. But our behaviors are also segregated. In this project we want to visualize how human behaviors—like conversations in Twitter, mobility around the cities, or purchasing—show patterns of segregation. Our objective is to allow people to understand the segregation of behaviors in their cities to increase the awareness of that problem, but also to show how we can address potential solutions by using different layers of big datasets.</p>",2017-06-30,,2017-04-05 18:53:49.527,True,2017-03-27,Visualizing patterns of segregation,PUBLIC,,False,Human Dynamics,False
collective-sensemaking-in-cryptocurrency-community,eaman,False,"<p>Participants in cryptocurrency markets are in constant communication with each other about the latest coins&nbsp;and news releases. Do these conversations build hype through the contagiousness of excitement, help the&nbsp;community process information, or play some other role? Using a novel dataset from a major cryptocurrency&nbsp;forum, we conduct an exploratory study of the characteristics of online discussion around cryptocurrencies. We find that coins with more information available and higher levels of&nbsp;technical innovation are associated with higher quality discussion. People who talk about serious coins tend&nbsp;to participate in discussion displaying signatures of collective intelligence and information processing, while&nbsp;people who talk about less serious coins tend to display signatures of hype and naïvety. Interviews with&nbsp;experienced forum members also confirm these quantitative findings. These results highlight the varied roles&nbsp;of discussion in the cryptocurrency ecosystem and suggest that discussion of serious coins may be oriented&nbsp;towards earnest, perhaps more accurate, attempts at discovering which coins are likely to succeed.&nbsp;</p>",,,2019-04-19 14:41:02.807,True,2016-06-01,Collective sensemaking in cryptocurrency community,PUBLIC,,True,Human Dynamics,False
improving-official-statistics-in-emerging-markets-using-machine-learning-and-mobile-phone-data,eaman,False,"<p><span style=""font-size: 18px; font-weight: 400;"">Mobile phones are one of the fastest growing technologies in the developing world with global penetration rates reaching 90%. Mobile phone data, also called CDR, are generated every time phones are used and recorded by carriers at scale. CDR have generated groundbreaking insights in public health, official statistics, and logistics. However, the fact that most phones in developing countries are prepaid means that the data lacks key information about the user, including gender and other demographic variables. This precludes numerous uses of this data in social science and development economic research. It furthermore severely prevents the development of humanitarian applications such as the use of mobile phone data to target aid towards the most vulnerable groups during crisis.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: 400;"">We developed a framework to extract more than 1,400 features from standard mobile phone data and used them to predict useful individual characteristics and group estimates. We here present a systematic cross-country study of the applicability of machine learning for dataset augmentation at low cost. We validate our framework by showing how it can be used to reliably predict gender and other information for more than half a million people in two countries. We show how standard machine learning algorithms trained on only 10,000 users are sufficient to predict individual’s gender with an accuracy ranging from 74.3 to 88.4% in a developed country and from 74.5 to 79.7% in a developing country using only metadata. This is significantly higher than previous approaches and, once calibrated, gives highly accurate estimates of gender balance in groups. Performance suffers only marginally if we reduce the training size to 5,000, but significantly decreases in a smaller training set. We finally show that our indicators capture a large range of behavioral traits using factor analysis and that the framework can be used to predict other indicators of vulnerability such as age or socio-economic status. Mobile phone data has a great potential for good and our framework allows this data to be augmented with vulnerability and other information at a fraction of the cost.</span></p>",,,2018-10-19 20:56:21.936,True,2016-06-01,Improving official statistics in emerging markets using machine learning and mobile phone data,PUBLIC,,True,Human Dynamics,False
visualizing-patterns-of-segregation,alotaibi,False,"<p>Segregation, or income inequality, is one of the major problems of our society. Residential segregation has long been of research interest in fields such as sociology, economics, and psychology. But our behaviors are also segregated. In this project we want to visualize how human behaviors—like conversations in Twitter, mobility around the cities, or purchasing—show patterns of segregation. Our objective is to allow people to understand the segregation of behaviors in their cities to increase the awareness of that problem, but also to show how we can address potential solutions by using different layers of big datasets.</p>",2017-06-30,,2017-04-05 18:53:49.527,True,2017-03-27,Visualizing patterns of segregation,PUBLIC,,False,Human Dynamics,False
opal-health,alotaibi,False,<h1><b>Open Algorithms (OPAL)</b></h1>,,,2018-10-19 21:07:55.443,True,2017-11-01,OPAL 4 Health,PUBLIC,https://www.shadaalsalamah.com/,True,Human Dynamics,False
smart-2-opal,alotaibi,False,"<p>Privacy-preserving mHealth application using Open Algorithm (OPAL) architecture to address urgent care challenges in Riyadh, Saudi Arabia.</p>",,,2019-04-01 17:39:27.492,False,2017-11-01,SMART^2 OPAL,PUBLIC,,True,Human Dynamics,False
visualizing-patterns-of-segregation,alfredom,False,"<p>Segregation, or income inequality, is one of the major problems of our society. Residential segregation has long been of research interest in fields such as sociology, economics, and psychology. But our behaviors are also segregated. In this project we want to visualize how human behaviors—like conversations in Twitter, mobility around the cities, or purchasing—show patterns of segregation. Our objective is to allow people to understand the segregation of behaviors in their cities to increase the awareness of that problem, but also to show how we can address potential solutions by using different layers of big datasets.</p>",2017-06-30,,2017-04-05 18:53:49.527,True,2017-03-27,Visualizing patterns of segregation,PUBLIC,,False,Human Dynamics,True
data-for-refugees,alfredom,False,"<p>Data for refugees is a big data challenge whereby Turk Telekom opens a large dataset of anonymized mobile phone records to research groups for the purpose of providing better living conditions to Syrian refugees in Turkey.&nbsp;&nbsp;</p><p>We introduce different measures extracted from mobile phone metadata to study the integration of refugees along three dimensions: (1) social integration, (2) spatial integration, and (3) economic integration through signatures of employment activity. We use these measures to compare integration across different regions in Turkey and find striking differences both in the distributions of these dimensions and the relations between them.&nbsp;<br></p><p>The paper is currently under review but will be shared soon.&nbsp;</p>",,,2019-04-19 14:42:49.278,True,2018-06-01,Data for Refugees,PUBLIC,,True,Human Dynamics,True
visualizing-patterns-of-segregation,xdong,False,"<p>Segregation, or income inequality, is one of the major problems of our society. Residential segregation has long been of research interest in fields such as sociology, economics, and psychology. But our behaviors are also segregated. In this project we want to visualize how human behaviors—like conversations in Twitter, mobility around the cities, or purchasing—show patterns of segregation. Our objective is to allow people to understand the segregation of behaviors in their cities to increase the awareness of that problem, but also to show how we can address potential solutions by using different layers of big datasets.</p>",2017-06-30,,2017-04-05 18:53:49.527,True,2017-03-27,Visualizing patterns of segregation,PUBLIC,,False,Human Dynamics,True
deepshop-understanding-purchase-patterns-via-deep-learning,xdong,False,"<p>The recent availability of quantitative behavioral data provides an opportunity to study human behavior at unprecedented scale. Using large-scale financial transaction data, we propose a novel deep learning framework for understanding human purchase patterns and testing the link between them and the existence of individual financial troubles. Our work opens new possibilities in studying human behavioral traits using state-of-the-art machine learning techniques, without the need for hand-engineered features.</p>",,--Choose Location,2019-04-19 14:43:35.517,True,2016-01-01,DeepShop: Understanding purchase patterns via deep learning,PUBLIC,,True,Human Dynamics,True
measuring-and-reducing-social-segregation-in-cities,xdong,False,"<p>We use high-resolution geospatial data collected from mobile phones to measure social segregation at an unprecedented resolution in cities across the United States. Social segregation happens when people of varying socioeconomic groups in a city have little opportunity to be exposed to people different than them.</p><p>To construct this measure, we aggregate high-resolution data from&nbsp;over 4.5 million users in the principal metro areas in the US to characterize places in the city by how mixed their visitors are by income. Using this measure, rather than traditional residential metrics, reveals that social exposure in third places is crucial to understanding economic segregation patterns in cities. In fact, the social segregation of different economic groups is dependent on an extremely small proportion of overall venues in a city.&nbsp;</p><p>We also look at how much individual citizens would need to change their behavior in order to make their patterns of exposure more integrated. Surprisingly, small changes in the amount of time people spend in different categories of places—changes as low as 2-5%—can reduce their social segregation by half.&nbsp;</p><p>We're currently working on finalizing these results and exploring how we might translate these findings into policy.</p>",,,2019-04-19 14:46:25.026,True,2017-10-01,Measuring and reducing social segregation in cities,PUBLIC,,True,Human Dynamics,True
the-ripple-effect-your-are-more-influential-than-you-think,xdong,False,"<p>The well-known ""small-world"" phenomenon indicates that an individual can be connected with any other in the world through a limited number of personal acquaintances. Furthermore, Nicholas and Fowler show that not only are we connected to each other, but we could also shape the behavior of our friends' friends. In this project, we are interested in understanding how social influence propagates and triggers behavioral change in social networks. Specifically, we analyze a large-scale, one-month international event held in the European country of Andorra using country-wide mobile phone data, and investigate the change in the likelihood of attending the event for people that have been&nbsp;<span style=""font-size: 18px; font-weight: normal;"">influenced by and are of different social distances from the attendees.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">Our results suggest that social influence exhibits the ripple effect, decaying across social distances from the source but persisting up to six degrees of separation. We further show that influence decays as communication delay increases and intensity decreases. Such ripple effect in social communication can lead to important policy implications in applications where it is critical to trigger behavior change in the population.</span></p>",,,2019-04-19 14:55:02.488,True,2016-08-01,The Ripple Effect: You are more influential than you think,PUBLIC,,True,Human Dynamics,True
social-bridges-in-community-purchase-behavior,xdong,False,"<p>The understanding and modeling of social influence on human economic behavior in city environments can have important implications. In this project, we study human purchase behavior at a community level and argue that people who live in different communities but work at similar locations could act as ""social bridges"" that link their respective communities and make the community purchase behavior similar through the possibility of social learning through face-to-face interactions.</p>",,--Choose Location,2016-12-05 00:17:04.354,True,2015-09-01,Social Bridges in Community Purchase Behavior,PUBLIC,,True,Human Dynamics,True
the-atlas-of-inequality,xdong,False,"<h2><b>Segregation is hurting our societies and especially our cities. But economic inequality isn't just limited to neighborhoods. The restaurants, stores, and other places we visit in cities are all unequal in their own way.&nbsp;</b></h2><p>The Atlas of Inequality &nbsp;shows the income inequality of people who visit different places in the Boston metro area. It uses aggregated anonymous location data from digital devices to estimate people's incomes and where they spend their time.&nbsp;Using that data, we've made our own <b>place inequality </b><b>metric</b> to capture how unequal the incomes of visitors to each place are. Economic inequality isn't just limited to neighborhoods; it's part of the places you visit every day.</p><p>Try it yourself here:</p><h2><a href=""http://inequality.media.mit.edu""><b>The Atlas of Inequality</b></a></h2><p>The Atlas of Inequality is a project from the Human Dynamics group at the <a href=""https://www.media.mit.edu/"">MIT Media Lab</a> and the Department of Mathematics at <a href=""http://www.uc3m.es/"">Universidad Carlos III de Madrid</a>.</p><p>It is part of a broader initiative to understand human behavior in our cities and how large-scale problems like transportation, housing, segregation, or inequality depend in part on the emergent patterns of people’s individual opportunities and choices.</p>",,,2019-03-20 17:07:49.005,True,2019-03-01,The Atlas of Inequality,PUBLIC,https://inequality.media.mit.edu,True,Human Dynamics,True
rock-porosity-measurement-using-optical-scattering,barmak,False,"<p>We use terahertz transmission through sedimentary rock samples to assess the macro and micro porosity. We exploit the notable water absorption in terahertz spectrum to interact with the pores that are two orders of magnitude smaller than the terahertz wavelength. Terahertz water sensitivity provides us with the dehydration profile of the rock samples. The results show that there is a correlation between such a profile and distribution of micro to macro porosity of the rock. The study further estimates the absolute value of total porosity based on diffusion theory. We compare our results with mercury injection capillary pressure as a benchmark to confirm our analytic framework. This porosimetry method can set a foundation for a more affordable, less invasive porosimetry that can be used in geological studies and in other industries without the need for hazardous mercury or ionizing radiation. 
                    
                </p>",2017-04-06,,2017-04-03 20:57:41.280,True,2016-07-06,Rock porosity measurement using optical scattering,PUBLIC,http://web.media.mit.edu/~barmak/,False,Camera Culture,True
future-of-batteries-and-future-with-better-batteries,barmak,False,"<p>This is the 3rd concept exhibition of imaginarium of technology which explores the future with advancement of batteries and how battery technology may be improved. 
                    
                </p>",2017-04-28,,2017-04-03 20:58:19.940,True,2016-09-09,Future of batteries and future with better batteries,PUBLIC,,False,Camera Culture,True
a-practical-look-at-the-future-of-artificial-intelligence-concept-exhibition,barmak,False,<p>This exciting concept exhibition which is the 4th concept exhibition of the imaginarium of technology series will explore the practical possibilities for the future of artificial intelligence. &nbsp;Check it out at E14-274</p>,2017-05-31,,2017-04-03 20:58:05.877,True,2017-04-05,A practical look at the future of Artificial Intelligence- iMT 4th Concept exhibition,PUBLIC,http://imtspace.com/index.php,False,Camera Culture,True
barmaks-untitled-project-2,barmak,False,"<p>We exploit the sub-picosecond time resolution along with spectral resolution provided by terahertz time-domain spectroscopy to extract occluding content from layers whose thicknesses are wavelength comparable. The method uses the statistics of the THz E-field at subwavelength gaps to lock into each layer position and then uses a time-gated spectral kurtosis to tune to highest spectral contrast of the content on that specific layer. To demonstrate, occluding textual content was successfully extracted from a sample similar to a closed book down to nine pages without human supervision. The method provides over an order of magnitude enhancement in the signal contrast and can impact inspection of structural defects in wooden objects, plastic components, composites, drugs, and especially cultural artifacts with subwavelength or wavelength comparable layers.</p>",,--Choose Location,2016-10-24 19:44:56.028,False,2015-09-01,Reading through Closed Books: THz Time-Gated Spectral Imaging for Content Extraction through Layered Structures,PUBLIC,,True,Camera Culture,True
single-photon-sensitive-ultrafast-imaging,barmak,False,"<p>The ability to record images with extreme temporal resolution enables a diverse range of applications, such as time-of-flight depth imaging and characterization of ultrafast processes. Here we present a demonstration of the potential of single-photon detector arrays for visualization and rapid characterization of events evolving on picosecond time scales. The single-photon sensitivity, temporal resolution, and full-field imaging capability enables the observation of light-in-flight in air, as well as the measurement of laser-induced plasma formation and dynamics in its natural environment. The extreme sensitivity and short acquisition times pave the way for real-time imaging of ultrafast processes or visualization and tracking of objects hidden from view. </p>",,--Choose Location,2016-12-05 00:17:02.706,True,2015-01-01,Single-Photon Sensitive Ultrafast Imaging,PUBLIC,,True,Camera Culture,True
beyond-the-self-driving-car,barmak,False,<p>This concept gallery shows the chain of startups and ideas that will follow after the emergence of self-driving cars.</p>,,--Choose Location,2016-12-05 00:16:14.777,True,2016-01-01,Beyond the Self-Driving Car ,PUBLIC,,True,Camera Culture,True
the-next-30-years-of-vr,barmak,False,"<p>In this visual brainstorming, we present the next 30 years of VR in a set of concept designs.</p>",,--Choose Location,2016-12-05 00:16:55.169,True,2016-01-01,The Next 30 Years of VR ,PUBLIC,,True,Camera Culture,True
imaging-behind-diffusive-layers,barmak,False,"<h2><span style=""font-weight: normal;"">Locating and classifying florescent tags behind turbid layers using time-resovled inversion&nbsp;</span></h2><p>Using time resolved and sparse optimization framework to locate and classify fluorescent markers hidden behind turbid layer:&nbsp;<span style=""font-size: 18px; font-weight: normal;"">The use of fluorescent probes and the recovery of their lifetimes allow for significant advances in many imaging systems, in particular medical imaging systems. Here, we propose and experimentally demonstrate reconstructing the locations and lifetimes of fluorescent markers hidden behind a turbid layer. This opens the door to various applications for non-invasive diagnosis, analysis, flowmetry, and inspection. The method is based on a time-resolved measurement which captures information about both fluorescence lifetime and spatial position of the probes. To reconstruct the scene, the method relies on a sparse optimization framework to invert time-resolved measurements. This wide-angle technique does not rely on coherence, and does not require the probes to be directly in line of sight of the camera, making it potentially suitable for long-range imaging.</span></p><p>More details:<br>http://web.media.mit.edu/~guysatat/project_scattering.html&nbsp;<br>http://web.media.mit.edu/~guysatat/fl/<br></p>",,--Choose Location,2017-04-05 01:48:57.383,True,2015-01-01,Imaging Behind Diffusive Layers,PUBLIC,,True,Camera Culture,True
imaging-with-all-photons,barmak,False,"<h2><span style=""font-weight: normal;"">How to see through tissue</span></h2><p>We demonstrate a new method to image through scattering materials like tissue and fog. The demonstration includes imaging an object hidden behind 1.5cm of tissue; it's like imaging through the palm of a hand. Our optical method is based on measuring and using all photons in the signal (as opposed to traditional methods, which use only part of the signal). Specifically, we use a time-resolved method that allows us to distinguish between photons that travel different paths in the tissue. Combining this unique measurement process with novel algorithms allows us to recover the hidden objects. This technique can be used in biomedical imaging, as well as imaging through fog and clouds.</p>",,--Choose Location,2017-04-05 01:49:57.109,True,2015-09-01,Imaging with All Photons,PUBLIC,,True,Camera Culture,True
reading-through-a-closed-book,barmak,False,<p>Terahertz time-gated spectral imaging for content extraction through layered structures.<br></p>,,,2017-10-10 19:20:00.582,True,2016-09-01,Reading Through a Closed Book,PUBLIC,,True,Camera Culture,True
towards-in-vivo-biopsy,barmak,False,"<p>A new method to detect and distinguish between different types of fluorescent materials. The suggested technique has provided a dramatically larger depth range compared to previous methods; thus it enables medical diagnosis of body tissues without removing the tissue from the body, which is the current medical standard. It uses fluorescent probes, which are commonly used in medical diagnosis. One of these parameters is the fluorescence lifetime, that is the average time the fluorescence emission lasts. The new method can distinguish between different fluorescence lifetimes, which allows diagnosis of deep tissues. Locating fluorescence probes in the body using this method can, for example, indicate the location of a tumor in deep tissue, and classify it as malignant or benign according to the fluorescence lifetime, thus eliminating the need for X-ray or biopsy.</p>",,--Choose Location,2018-03-29 20:34:54.336,True,2014-09-01,Towards In-Vivo Biopsy,PUBLIC,,True,Camera Culture,True
time-folded-optics,barmak,False,<h2>Rethinking photography optics in the time dimension</h2><p><i>What if we could design optics in time instead of space?</i></p>,,,2018-09-24 18:15:42.266,True,2018-08-01,Time-folded optics,PUBLIC,http://web.media.mit.edu/~barmak/Time-folded.html,True,Camera Culture,True
calibration-invariant-i,barmak,False,"<p><b>Object Classification through Scattering Media&nbsp;with Deep Learning</b></p><p>A method for classifying objects hidden behind a scattering layer with a neural network. Training on synthetic data with variations in calibration parameters allows the network to learn a model that doesn't require calibration during lab experiments.<br></p><p>Traditional techniques to see through scattering media rely on a physical model that needs to be precisely calibrated. Computationally overcoming the scattering relies heavily on accurately calibrated physical models. Thus, such systems are extremely sensitive to a precise and lengthy calibration process. </p><p>In this work we overcome this bottleneck by utilizing neural networks and their ability to learn models that are invariant to data transformation. In our case, the transformations are variations in the imaging system calibration parameters. To that end, we create a synthetic dataset that contains variations in all calibration parameters (we use a Monte Carlo forward model to render the measurements). The system is then tested on actual lab experiments without specific calibration or tuning.</p>",,,2018-10-20 00:40:19.319,True,2016-09-01,Calibration Invariant Imaging,PUBLIC,,True,Camera Culture,True
smart-communal-spaces,kapeloni,False,"<p><b>Smart Communal Spaces </b>is a project from MIT Media Lab Dubai Workshop 2016, which was deployed and filmed at the Dubai Museum of the Future. It explores how Mixed Reality could enhance team communication in shared office spaces by looking into the interplay of Slack and HoloLens. While existing co-working spaces provide the benefits of openness and flexibility, they sacrifice our privacy and personal experience. However, Mixed Reality enables us to project holograms into our physical reality, visually and auditorily reorganizing how bits and atoms exist around us. In the demo video, we try to imagine and tell a story of how human-computer interactions might look like in an office when team communication tools are operated in a spatial context without any streaming device. We broke down the most important elements in the office space by researching the core functions of a team communication software and several holographic applications.
                    
                </p><p><b>Design Lead:</b> Chrisoula Kapelonis,&nbsp;</p><p><b>Technology Lead:</b> Poseidon Hai-Chi Ho&nbsp;</p><p><b>Students:&nbsp;</b>Rajeev Mylapalli, Yazan Fanous, Lamees Alhashimi, Moza Al Naimi, Esra'a Alsanie, Asalah Aranki</p><p><b>Special Thanks: </b>Joichi Ito, Noah Raford, Nick DuPey, Ashley Shaffer<br>MIT Media Lab, IDEO, Wamda, Dubai Museum of the Future</p>",2017-08-29,,2018-05-04 10:51:42.799,True,2016-08-29,Smart Communal Spaces,PUBLIC,,False,City Science,False
spatial-flux,kapeloni,False,"<p>Structurally, zero gravity means that we do not have to contend with architecture's greatest arch-nemesis, gravity. This opens up a new world of possibilities where we can deploy structures that no longer have to counteract/resist gravitational force. We would like to explore new forms of rapid inflatable prototyping. Most importantly, this prototype explores surfaces utilizing materials that would normally fail on Earth, yet flourish in zero gravity.</p><p>This year the MIT Media Lab's City Science group had an opportunity to think of architecture at the scale of the body that was literally out of this world. These are the results.</p>",,,2019-04-17 19:45:01.556,True,2017-06-01,Spatial Flux: Body and architecture in space,PUBLIC,,True,City Science,False
escape-pod,kapeloni,False,,,,2017-10-13 16:05:41.451,False,2017-10-10,Escape Pod,PUBLIC,,True,City Science,False
receptive-skins,kapeloni,False,"<p><br>In architecture, the building skin is the primary interface for mediating the environment of the external with the internal. But today, this mediation is mechanical, deterministic, and static—often seeing the human as a generalizable and problematic input. With advances in material science however, there is great potential to disrupt these traditional manufactured environments of architecture and turn them into responsive mediated environments. What this thesis aims to explore is this idea of the receptive skin—a sensate and dynamic multi-material interface for environmental mediation. This suggests that by departing from the view that buildings are static artifacts, we may instead begin to see buildings as organic, living entities.</p><p>Through the development of a working prototype, this project explores how such an interface may manifest itself, through dynamic material composites, instead of mechanical and electronic means. The final prototype is a “proof of concept,” a built example of this novel design methodology, which unites material performance with sensate technologies, as a way to enable new interactions between building and environment.&nbsp;</p>",,,2018-06-26 21:12:49.214,True,2017-05-01,Receptive Skins: The Breathing Wall,PUBLIC,http://www.chrisoulakapelonis.com,True,City Science,False
escape-pod-1,kapeloni,False,"<p>The esc-Pod&nbsp; (or Escape Pod) is an exploratory platform for researchers investigating moments of refuge within our bustling work lives.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">The core of the esc-Pod consists of actuated work and rest surfaces. This allows for moments of productivity and relaxation to occur within a single space.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The outer skin provides variable transparency, enabling a spectrum of visibility settings according to privacy requirements.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The inner skin provides an infrastructure for the modulation of spatial experiences. Each panel is a pixel, connecting itself to the skin network, and can embody an array of senses.</span></p>",,,2019-04-08 17:01:14.555,True,2016-08-01,Escape Pod,PUBLIC,,True,City Science,False
social-robots-in-zero-gravity-scenarios,igrover,False,"<p>Can we enable social connectivity between astronauts and people on Earth through an embodied agent?</p><p>Astronauts actively communicate with their families on Earth through several forms of digital and voice communication, including phone calls, video conferencing, and email. However, as astronaut Scott Kelly describes in the <i>Time</i> documentary <i>A Year in Space</i>, the experience can be incredibly isolating despite these affordances. Shortcomings of these modes of communication lie in their inability to translate emotion effectively, failure to facilitate shared experiences, lack of physical feedback, and the resulting perceived lack of control. The psychological effects of these limitations can become heightened over time, and peak during moments when the family on Earth is in need of support. As space becomes more accessible, it is important to consider how we design for social connectivity between people on Earth and in space.</p><p>What if embodied social agents, besides being the astronaut's personal sidekick, could help to facilitate a more connected experience between space and Earth? From C3PO in <i>Star Wars</i> to Rosie the Maid in <i>The Jetsons</i>, the idea of robots in space has been well explored in fiction universe. On Earth, embodied social agents have been shown to be effective in providing companionship, relieving stress and anxiety, and fostering connection among people. In this project to send an&nbsp;embodied social agent into zero gravity, we explore several key themes relating to the potential for this technology to offer better connection and shared experience between astronauts and people on Earth.</p><p>While in zero gravity, the embodied social agent interacts with people on cognitive, creative, and social tasks with varying degrees of proactive behavior. We collect physiological, audio, and video data of the experience as individuals complete a series of tasks with the agent with the goal of designing agents that can enable us to be more socially connected.</p>",2018-12-31,,2017-12-05 19:04:57.085,True,2017-10-01,Social Robots in Space: Initial Explorations,PUBLIC,,False,Personal Robots,False
sleep-creativity,igrover,False,"<p>Sleep is a forgotten country of the mind. A vast majority of our technologies are built for our waking state, even though a third of our lives are spent asleep. Current technological interfaces miss an opportunity to access the unique, imaginative, elastic cognition ongoing during dreams and semi-lucid states. In turn, each of us misses an opportunity to use interfaces to influence our own processes of memory consolidation, creative insight generation, gist extraction, and emotion regulation that are so deeply sleep-dependent.&nbsp;In this project, we explore ways to augment human creativity by extending, influencing, and capturing dreams in stage-1 sleep. It is currently impossible to force ourselves to be creative because so much creative idea association and creative incubation happens in the absence of executive control and directed attention. Sleep offers an opportunity for prompting creative thought in the absence of directed attention, if only dreams can be controlled.</p><p>During sleep onset, a window of opportunity arises in the form of hypnagogia, a semi-lucid sleep state where we all begin dreaming before we fall fully unconscious. Hypnagogia is characterized by phenomenological unpredictability, distorted perception of space and time, and spontaneous, fluid idea association. Edison, Tesla, Poe, and Dalí each accessed this state by napping with a steel ball in hand to capture creative ideas generated in hypnagogic microdreams when it dropped to the floor below.</p><p>In this project we modernize this technique, using an interactive social robot accompanied with an EEG system, muscular sleep stage tracking system, and auditory biofeedback. We are able to influence, extract information from, and extend hypnagogic microdreams for the first time: we found that active use of hypnagogia with the system can augment human creativity. This system enables future research into sleep, an underutilized and understudied state of mind vital for memory, learning, and creativity.</p><p>This work has been hugely collaborative. The following people, in alphabetical order by first name, have all made it possible: Abhinandan Jain, Eyal Perry, Ishaan Grover, Matthew Ha, Oscar Rosello, Pedro Reynolds-Cuéllar, Robert Stickgold, and Tomás Vega. For an in depth dive, see the FAQ below and see more on <a href=""http://adamjhh.com/dormio"">this website</a>.</p><br>",,,2019-03-26 14:01:38.378,True,2017-09-01,Dormio: Interfacing with Dreams,PUBLIC,http://www.adamjhh.com/,True,Personal Robots,False
data-of-children-storytelling,igrover,False,<p>Collecting real world data to understand social interactions!</p>,,,2018-01-22 06:31:46.892,True,2017-05-01,Data of Children Storytelling,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,False
collaborative-robot-storyteller,igrover,False,"<p>Could a social robot collaboratively exchange stories with children as a peer and help improve their linguistic and storytelling skills? Tega uses machine learning algorithms to learn actions that improve children's storytelling and keep them engaged. &nbsp;We are also interested in how Tega can personalize its interaction with each child over multiple encounters, because every child learns and engages differently.&nbsp;</p><p>In Spring 2017, Tega went to twelve preschool classrooms in the Greater Boston area for&nbsp;three months, pioneering the field of long-term human-robot interaction.&nbsp;Using Q-learning, a policy was trained to tell stories optimized for each child’s engagement and linguistic skill progression. Tega monitored children's affect signals and asked dialogic questions during storytelling to gauge their engagement. Tega also invited children to&nbsp;tell&nbsp;it stories, which Tega used to assess each child's linguistic skill development. Our results show robot's interaction policy indeed personalized to each child. At the end of the sessions, the policy significantly differed from one child to the other. Children who interacted and built relationships with a personalized robot showed higher engagement, learned and retained more vocabularies, and used more complex syntax structure in their speech compared to where they had started.</p>",,--Choose Location,2019-01-22 17:42:46.144,True,2015-09-01,Personalized Robot Storytelling Companion,PUBLIC,,True,Personal Robots,False
p2pstory,igrover,False,"<p>Understanding social-emotional behaviors in storytelling interactions plays a critical role in the development of interactive and educational technologies for children. A challenge when designing for such interactions using technologies like social robots, virtual agents, and tablets is understanding the social-emotional behaviors pertinent to the storytelling context—especially when emulating a natural peer-to-peer relationship between the child and the technology.&nbsp;&nbsp;We present P2PSTORY, a dataset of young children (5-6 years old) engaging in natural peer-to-peer storytelling interactions with fellow classmates.&nbsp;The dataset contains 58 recorded storytelling sessions along with a diverse set of behavioral annotations as well as developmental and demographic profiles of each child participant.&nbsp;</p><p>The CHI 2018 paper presenting this dataset can be found here:&nbsp;<br><b><a href=""https://www.media.mit.edu/publications/p2pstory-dataset-of-children-storytelling-and-listening-in-peer-to-peer-interactions/"">Nikhita Singh, Jin Joo Lee, Ishaan Grover, and Cynthia Breazeal (2018). P2PSTORY: Dataset of Children Storytelling and Listening in Peer-to-Peer Interactions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems.</a></b></p><br><p>See below for instructions on how to access the dataset.</p>",,,2019-06-10 09:19:51.212,True,2018-01-22,P2PSTORY: Dataset of children as storytellers and listeners in peer-to-peer interactions,PUBLIC,,True,Personal Robots,False
social-robots-in-zero-gravity-scenarios,nikhita,False,"<p>Can we enable social connectivity between astronauts and people on Earth through an embodied agent?</p><p>Astronauts actively communicate with their families on Earth through several forms of digital and voice communication, including phone calls, video conferencing, and email. However, as astronaut Scott Kelly describes in the <i>Time</i> documentary <i>A Year in Space</i>, the experience can be incredibly isolating despite these affordances. Shortcomings of these modes of communication lie in their inability to translate emotion effectively, failure to facilitate shared experiences, lack of physical feedback, and the resulting perceived lack of control. The psychological effects of these limitations can become heightened over time, and peak during moments when the family on Earth is in need of support. As space becomes more accessible, it is important to consider how we design for social connectivity between people on Earth and in space.</p><p>What if embodied social agents, besides being the astronaut's personal sidekick, could help to facilitate a more connected experience between space and Earth? From C3PO in <i>Star Wars</i> to Rosie the Maid in <i>The Jetsons</i>, the idea of robots in space has been well explored in fiction universe. On Earth, embodied social agents have been shown to be effective in providing companionship, relieving stress and anxiety, and fostering connection among people. In this project to send an&nbsp;embodied social agent into zero gravity, we explore several key themes relating to the potential for this technology to offer better connection and shared experience between astronauts and people on Earth.</p><p>While in zero gravity, the embodied social agent interacts with people on cognitive, creative, and social tasks with varying degrees of proactive behavior. We collect physiological, audio, and video data of the experience as individuals complete a series of tasks with the agent with the goal of designing agents that can enable us to be more socially connected.</p>",2018-12-31,,2017-12-05 19:04:57.085,True,2017-10-01,Social Robots in Space: Initial Explorations,PUBLIC,,False,Personal Robots,False
designing-social-robots-for-older-adults,nikhita,False,"<p>Most countries are projected to see the number of people ages 65 and older surpass the population under the age of 15 by 2050. The limitations of current solutions to assisting older adults, the increased social and emotional toll on caregivers, and the inability of institutions to create structural solutions in a timely manner calls for a paradigm shift in the way we approach aging.</p><p>As these new meanings of age, aged, and aging are re-negotiated at a personal and collective level, the <b>main goal of this research initiative is to&nbsp;study aging adults’ daily living assistance, social and emotional needs, and intergenerational connection</b> while exploring the optimized modalities for embodied agents to successfully deliver these interactions.&nbsp;We see embodied agents as a method to enable older adults to age-in-place, supporting them in ways such as promoting social connectedness, tracking vitals, coaching in emotional wellness, and assisting with medical adherence.</p><p>Our work is rooted in partnering with the community through co-design and participatory design methods to inform robot design by empowering older adults to engage in our research. We prioritize developing robot interactions that can be tested long-term in older adults’ homes to better inform how social robots can shape aging-in-place.<br></p><p>Currently, we are running a long-term codesign study with older adults. Over the course of the year, older adults will engage in interviews, interactive artwork, living with a robot, prototyping on a robot, and design guideline generation.&nbsp;</p><p>If you are 70 years of age or older and interested in participating in future study opportunities, please contact Anastasia Ostrowski (akostrow@media.mit.edu).</p>",,,2019-05-10 19:51:29.310,True,2017-06-01,Designing social robots for older adults,PUBLIC,,True,Personal Robots,False
proactive-connected-spaces,nikhita,False,"<p>With families living further apart, it has become increasingly difficult for people to stay connected—particularly in the case of grandparents and children. The challenge lies in deciding when, how and what to engage on. <b>Can technology act as a proactive facilitator of human-human connection?</b></p><p>Social robots are uniquely positioned to act as active facilitators of human-human connection. However, in order to do so, they require the ability to be <b>proactive</b>. Proactivity demands that an agent not only respond to it's environment, but also exhibit goal-directed behavior by taking the initiative. In this work, an ecosystem for connected social robots to utilize the surfaces of the home as a canvas for expression in order to engage grandparents and grandkids in human-human interaction is proposed. </p><p><b>This work models proactivity in an agent as a function of understanding the context, proposing a goal, and taking initiative through an interaction.&nbsp;</b>Human studies will be conducted in order to understand and draw inspiration from human behavior to drive how a robot gets an individual's attention. Further interaction studies will serve to design and evaluate the form of expression (robot, environment, or both) most relevant for given contexts.</p>",,,2018-04-25 03:28:31.115,True,2017-08-01,"Proactive, Connected Spaces",PUBLIC,,True,Personal Robots,False
data-of-children-storytelling,nikhita,False,<p>Collecting real world data to understand social interactions!</p>,,,2018-01-22 06:31:46.892,True,2017-05-01,Data of Children Storytelling,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,False
talking-machines-democratizing-the-design-of-voice-based-agents-for-the-home,nikhita,False,"<p>Embodied voice-based agents, such as Amazon’s Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most, these agents represent their first experience of&nbsp;<i>living</i> with artificial intelligence in such private and personal spaces.&nbsp;</p><p>However, little is known about people’s desires, preferences, and boundaries for these technologies. This projects seeks to answer questions surrounding this space:&nbsp;<b>How do we live with voice-based agents in the home? How do different generations interact with voice-based agents? How should these technologies be designed to incorporate people’s preferences, desires, and boundaries? What tools can be used to understand this space?</b></p><p>This work presents insights from a long-term exploration with over 70 children, adults, and older adults over a one-year period to interact with, discover, experience, reflect upon, and design voice-based agents. In addition, design tools and learnings from the experience have been developed into an open-source design kit to enable designers and researchers to explore these ideas with the broader population.</p><p>For more information, please contact <b>Nikhita Singh (nikhita@media.mit.edu) </b>and <b>Anastasia Ostrowski (akostrow@media.mit.edu)</b>.</p>",,,2018-07-15 03:34:47.660,True,2017-09-01,Talking Machines: Democratizing the design of voice-based agents for the home,PUBLIC,,True,Personal Robots,False
talking-machines-democratizing-the-design-of-voice-controlled-agents-in-the-home,nikhita,False,"<p>Embodied voice-based agents, such as Amazon’s Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most people, these agents represent their first experience <i>living</i> with artificial intelligence in such private and personal spaces. However, little is known about people’s desires, preferences, and boundaries for these technologies. This project seeks to answer questions surrounding this space. How do we live with voice-based agents in the home? How do different generations interact with voice-based agents? How should these technologies be designed to incorporate people’s preferences, desires, and boundaries? What tools can be used to understand this space?</p>",,,2018-07-15 03:35:18.476,False,2017-05-01,Talking Machines: Democratizing the Design of Voice-Based Agents in the Home,PUBLIC,,True,Personal Robots,False
curious-learning,nikhita,False,"<p>Early literacy plays an important role in a child's future. However, the reality is that over 57 million children have no access to a school and another 100 million attend such inadequate schools that they will remain functionally non-literate.</p><p>Curious Learning is an open platform that addresses the deployment and learning challenges faced by under-resourced communities, particularly their limited access to literacy instruction.</p><p>We are developing a system of early literacy apps, games, toys, and robots that will triage how children are learning, diagnose literacy deficits, and deploy dosages of content to encourage app play using a mentoring algorithm that recommends an appropriate activity given a child's progress. Currently, over 200 Android-based tablets have been sent to children around the world; these devices are instrumented to provide a very detailed picture of how kids are using these technologies. We are using this big data to discover usage and learning models that will inform future educational development.&nbsp; The open-source software enables any Android device to be transformed into a literacy mentor. This platform is presently deployed in Ethiopia, Uganda, India, South Africa, and rural United States.</p><p>The open-source tablet software enables data collection across the deployment sites.  By employing a data-driven approach to understanding learning behaviors across cultures and contexts, this project seeks to design and develop a personalized, adaptive learning platform.&nbsp;</p>",,,2019-04-17 18:41:03.019,True,2016-09-01,Curious Learning: Understanding learning behaviors for early literacy,PUBLIC,http://www.curiouslearning.org/,True,Personal Robots,False
collaborative-robot-storyteller,nikhita,False,"<p>Could a social robot collaboratively exchange stories with children as a peer and help improve their linguistic and storytelling skills? Tega uses machine learning algorithms to learn actions that improve children's storytelling and keep them engaged. &nbsp;We are also interested in how Tega can personalize its interaction with each child over multiple encounters, because every child learns and engages differently.&nbsp;</p><p>In Spring 2017, Tega went to twelve preschool classrooms in the Greater Boston area for&nbsp;three months, pioneering the field of long-term human-robot interaction.&nbsp;Using Q-learning, a policy was trained to tell stories optimized for each child’s engagement and linguistic skill progression. Tega monitored children's affect signals and asked dialogic questions during storytelling to gauge their engagement. Tega also invited children to&nbsp;tell&nbsp;it stories, which Tega used to assess each child's linguistic skill development. Our results show robot's interaction policy indeed personalized to each child. At the end of the sessions, the policy significantly differed from one child to the other. Children who interacted and built relationships with a personalized robot showed higher engagement, learned and retained more vocabularies, and used more complex syntax structure in their speech compared to where they had started.</p>",,--Choose Location,2019-01-22 17:42:46.144,True,2015-09-01,Personalized Robot Storytelling Companion,PUBLIC,,True,Personal Robots,False
p2pstory,nikhita,False,"<p>Understanding social-emotional behaviors in storytelling interactions plays a critical role in the development of interactive and educational technologies for children. A challenge when designing for such interactions using technologies like social robots, virtual agents, and tablets is understanding the social-emotional behaviors pertinent to the storytelling context—especially when emulating a natural peer-to-peer relationship between the child and the technology.&nbsp;&nbsp;We present P2PSTORY, a dataset of young children (5-6 years old) engaging in natural peer-to-peer storytelling interactions with fellow classmates.&nbsp;The dataset contains 58 recorded storytelling sessions along with a diverse set of behavioral annotations as well as developmental and demographic profiles of each child participant.&nbsp;</p><p>The CHI 2018 paper presenting this dataset can be found here:&nbsp;<br><b><a href=""https://www.media.mit.edu/publications/p2pstory-dataset-of-children-storytelling-and-listening-in-peer-to-peer-interactions/"">Nikhita Singh, Jin Joo Lee, Ishaan Grover, and Cynthia Breazeal (2018). P2PSTORY: Dataset of Children Storytelling and Listening in Peer-to-Peer Interactions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems.</a></b></p><br><p>See below for instructions on how to access the dataset.</p>",,,2019-06-10 09:19:51.212,True,2018-01-22,P2PSTORY: Dataset of children as storytellers and listeners in peer-to-peer interactions,PUBLIC,,True,Personal Robots,False
social-robots-in-zero-gravity-scenarios,pcuellar,False,"<p>Can we enable social connectivity between astronauts and people on Earth through an embodied agent?</p><p>Astronauts actively communicate with their families on Earth through several forms of digital and voice communication, including phone calls, video conferencing, and email. However, as astronaut Scott Kelly describes in the <i>Time</i> documentary <i>A Year in Space</i>, the experience can be incredibly isolating despite these affordances. Shortcomings of these modes of communication lie in their inability to translate emotion effectively, failure to facilitate shared experiences, lack of physical feedback, and the resulting perceived lack of control. The psychological effects of these limitations can become heightened over time, and peak during moments when the family on Earth is in need of support. As space becomes more accessible, it is important to consider how we design for social connectivity between people on Earth and in space.</p><p>What if embodied social agents, besides being the astronaut's personal sidekick, could help to facilitate a more connected experience between space and Earth? From C3PO in <i>Star Wars</i> to Rosie the Maid in <i>The Jetsons</i>, the idea of robots in space has been well explored in fiction universe. On Earth, embodied social agents have been shown to be effective in providing companionship, relieving stress and anxiety, and fostering connection among people. In this project to send an&nbsp;embodied social agent into zero gravity, we explore several key themes relating to the potential for this technology to offer better connection and shared experience between astronauts and people on Earth.</p><p>While in zero gravity, the embodied social agent interacts with people on cognitive, creative, and social tasks with varying degrees of proactive behavior. We collect physiological, audio, and video data of the experience as individuals complete a series of tasks with the agent with the goal of designing agents that can enable us to be more socially connected.</p>",2018-12-31,,2017-12-05 19:04:57.085,True,2017-10-01,Social Robots in Space: Initial Explorations,PUBLIC,,False,Civic Media,False
sleep-creativity,pcuellar,False,"<p>Sleep is a forgotten country of the mind. A vast majority of our technologies are built for our waking state, even though a third of our lives are spent asleep. Current technological interfaces miss an opportunity to access the unique, imaginative, elastic cognition ongoing during dreams and semi-lucid states. In turn, each of us misses an opportunity to use interfaces to influence our own processes of memory consolidation, creative insight generation, gist extraction, and emotion regulation that are so deeply sleep-dependent.&nbsp;In this project, we explore ways to augment human creativity by extending, influencing, and capturing dreams in stage-1 sleep. It is currently impossible to force ourselves to be creative because so much creative idea association and creative incubation happens in the absence of executive control and directed attention. Sleep offers an opportunity for prompting creative thought in the absence of directed attention, if only dreams can be controlled.</p><p>During sleep onset, a window of opportunity arises in the form of hypnagogia, a semi-lucid sleep state where we all begin dreaming before we fall fully unconscious. Hypnagogia is characterized by phenomenological unpredictability, distorted perception of space and time, and spontaneous, fluid idea association. Edison, Tesla, Poe, and Dalí each accessed this state by napping with a steel ball in hand to capture creative ideas generated in hypnagogic microdreams when it dropped to the floor below.</p><p>In this project we modernize this technique, using an interactive social robot accompanied with an EEG system, muscular sleep stage tracking system, and auditory biofeedback. We are able to influence, extract information from, and extend hypnagogic microdreams for the first time: we found that active use of hypnagogia with the system can augment human creativity. This system enables future research into sleep, an underutilized and understudied state of mind vital for memory, learning, and creativity.</p><p>This work has been hugely collaborative. The following people, in alphabetical order by first name, have all made it possible: Abhinandan Jain, Eyal Perry, Ishaan Grover, Matthew Ha, Oscar Rosello, Pedro Reynolds-Cuéllar, Robert Stickgold, and Tomás Vega. For an in depth dive, see the FAQ below and see more on <a href=""http://adamjhh.com/dormio"">this website</a>.</p><br>",,,2019-03-26 14:01:38.378,True,2017-09-01,Dormio: Interfacing with Dreams,PUBLIC,http://www.adamjhh.com/,True,Civic Media,False
empowered-brains,pcuellar,False,"<h2><b>EEEeb Spring 2019:&nbsp;&nbsp;Urban Oceans</b></h2><p>March 24,&nbsp;April 7 and 21,&nbsp;May 19,&nbsp;June 2&nbsp;<br><span style=""font-size: 18px; font-weight: 400;"">To register, please </span><a href=""https://www.eventbrite.com/e/ecology-evolution-and-engineering-for-empowered-brains-spring-2019-tickets-54681164836"" style=""font-size: 18px; font-weight: 400;"">visit this link</a><span style=""font-size: 18px; font-weight: 400;"">.&nbsp;</span></p><p>Sponsored and run by members of the MIT Media Lab and the <a href=""http://www.empoweredbrain.org/"">Empowered Brain Institute</a>,&nbsp;<i>Ecology, Evolution, and Engineering for Empowered Brains</i> is an eight-week, sensory-friendly series of related educational workshops for neurodiverse individuals (ages 8 - 14) which aims to hone skills in understanding, interpreting, and protecting the natural environment. Through creative, hands-on teaching exercises and field visits, participants become comfortable with basic ecological principles, as well as emerging technologies used to sculpt ecological and evolutionary processes. We discuss contemporary issues related to conservation and highlight engineering strategies with which to address these obstacles. Through project-based learning, students will have the opportunity to develop understanding by experimentation—or play—and workshops will emphasize immersion, rather than memorization.  Wholly, we seek to foster a safe and creative learning space in which students are able to develop the necessary technical literacy to become future leaders in the myriad realms of environmental science.&nbsp;</p><p>For questions, please contact Avery Normandin (ave@media.mit.edu).<br></p>",,,2019-01-10 16:02:03.396,True,2019-01-10,"Ecology, Evolution, and Engineering for Empowered Brains",PUBLIC,,True,Civic Media,False
technology-design-for-coffee-production,pcuellar,False,"<p>In the Civic Media group, we are exploring new theoretical frameworks for design, and novel design education methodologies that serve a future celebrating the pluriversal rather than the universal.</p><p>The “Technology Design for Coffee Production in Colombia” is an educational intervention exploring this concept. The course convened 16 participants from seven countries to explore technological, social, and business solutions for small-scale coffee production along with coffee growers in rural Colombia.&nbsp;The course was a unique, multidisciplinary, and multicultural design experience in which people came together to co-design technologies that connected with rural coffee growers invention practices in the Sumapaz region in Colombia.</p><p>The course immersed participants into different agricultural practices, primarily coffee growing, as well as in the ontologies traditional to these practices. In an effort to expose participants to non-mainstream design methods and mechanisms of invention, our research team focused on surfacing local knowledge through research materials and hands-on activities.&nbsp;</p>",2020-01-31,,2019-06-05 18:40:55.600,True,2019-01-07,Technology Design for Coffee Production,PUBLIC,https://reynoldscuellar.com,True,Civic Media,False
curious-learning,pcuellar,False,"<p>Early literacy plays an important role in a child's future. However, the reality is that over 57 million children have no access to a school and another 100 million attend such inadequate schools that they will remain functionally non-literate.</p><p>Curious Learning is an open platform that addresses the deployment and learning challenges faced by under-resourced communities, particularly their limited access to literacy instruction.</p><p>We are developing a system of early literacy apps, games, toys, and robots that will triage how children are learning, diagnose literacy deficits, and deploy dosages of content to encourage app play using a mentoring algorithm that recommends an appropriate activity given a child's progress. Currently, over 200 Android-based tablets have been sent to children around the world; these devices are instrumented to provide a very detailed picture of how kids are using these technologies. We are using this big data to discover usage and learning models that will inform future educational development.&nbsp; The open-source software enables any Android device to be transformed into a literacy mentor. This platform is presently deployed in Ethiopia, Uganda, India, South Africa, and rural United States.</p><p>The open-source tablet software enables data collection across the deployment sites.  By employing a data-driven approach to understanding learning behaviors across cultures and contexts, this project seeks to design and develop a personalized, adaptive learning platform.&nbsp;</p>",,,2019-04-17 18:41:03.019,True,2016-09-01,Curious Learning: Understanding learning behaviors for early literacy,PUBLIC,http://www.curiouslearning.org/,True,Civic Media,False
collaborative-robot-storyteller,pcuellar,False,"<p>Could a social robot collaboratively exchange stories with children as a peer and help improve their linguistic and storytelling skills? Tega uses machine learning algorithms to learn actions that improve children's storytelling and keep them engaged. &nbsp;We are also interested in how Tega can personalize its interaction with each child over multiple encounters, because every child learns and engages differently.&nbsp;</p><p>In Spring 2017, Tega went to twelve preschool classrooms in the Greater Boston area for&nbsp;three months, pioneering the field of long-term human-robot interaction.&nbsp;Using Q-learning, a policy was trained to tell stories optimized for each child’s engagement and linguistic skill progression. Tega monitored children's affect signals and asked dialogic questions during storytelling to gauge their engagement. Tega also invited children to&nbsp;tell&nbsp;it stories, which Tega used to assess each child's linguistic skill development. Our results show robot's interaction policy indeed personalized to each child. At the end of the sessions, the policy significantly differed from one child to the other. Children who interacted and built relationships with a personalized robot showed higher engagement, learned and retained more vocabularies, and used more complex syntax structure in their speech compared to where they had started.</p>",,--Choose Location,2019-01-22 17:42:46.144,True,2015-09-01,Personalized Robot Storytelling Companion,PUBLIC,,True,Civic Media,False
children-s-perception-of-empathy-in-robot-robot-scenarios,pcuellar,False,"<p>Empathy is a core human skill. From early stages of our lives, being able to understand and behave with empathy is fundamental to our social experience. Research in the field of social robotics suggests that given a set of behaviors from a social robot, a child can perceive this agent as empathic. In this project, we explore a novel approach to modeling empathy in children using a social robot. Two social robots were programmed to have conversations containing interactions depicting empathic and non-empathic behaviors. Children were provided with opportunities to act on these interactions as well as to comment on the robot's behavior afterward.</p>",,,2019-04-17 18:49:27.482,True,2017-09-14,The role of social robots in fostering human empathy,PUBLIC,,True,Civic Media,False
mission-wildlife,gabem,False,"<p>Mission Wildlife is a research collaboration between San&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Diego Zoo Global and the MIT Center for Civic Media to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">explore the potential for interactive technologies in&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">conservation education. In particular, we used&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">augmented reality (AR) to focus visitors’ attention&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">towards survival threats to endangered species. The&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">project was deployed during the 100th anniversary&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">year (2016) of the San Diego Zoo. Visitors competed&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">against each other to trigger 3D animations from&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">animal signage in the zoo and shared results on social&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">media to spread awareness about conservation issues.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">This case study demonstrates how AR can be tied to&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">efforts to expand awareness of social issues.</span></p>",2017-06-20,,2018-05-04 10:49:30.350,True,2016-06-20,Mission Wildlife,PUBLIC,http://oi7.me,False,Playful Systems,True
joy-branch,gabem,False,"<p>The Joy Branch project explores different user interfaces to allow parrots to shape their sonic environment.&nbsp;&nbsp;Animal agency—control of the environment—is an important and underutilized element of captive care.&nbsp;&nbsp;Parrot species are vocal learners, and as such are highly attuned to their sonic environment. Much of their brains are involved in the production and analysis of sound, and yet their sonic environment in managed care does not provide a rich experience. In this project, we assess the efficacy of new enrichment techniques that have the potential to improve the lives of these birds through music.&nbsp;&nbsp;The project involves the placement of sonic enrichment elements into the birds’ enclosures under controlled and supervised conditions.</p><p>The ""joystick branch"" element exposes only a standard wooden perch to the birds. The aim is to create naturalistic interactive methods for birds to generate sounds, and to assess their optional engagement with these new modes of control.&nbsp;</p>",2019-12-31,,2019-04-18 01:19:48.567,True,2019-01-01,Joy Branch,PUBLIC,,True,Playful Systems,True
sonic-enrichment-at-the-zoo,gabem,False,<p>This project is a collaboration between the MIT Media Lab and the San Diego Zoo to design and build interactive sonic enrichment systems for animals in managed care. Our approach is based on the potential of animal-animal and human-animal relationship as an environmental enrichment for the welfare of zoo-housed animals specifically in terms of animal vocal communication.&nbsp; Enrichment is a way for caregivers to provide animals with the opportunity to express natural behaviors and reduce stereotypic behaviors.&nbsp;</p>,,,2019-04-17 20:03:16.443,True,2018-04-01,Sonic enrichment at the zoo,PUBLIC,,True,Playful Systems,True
human-atlas,echu,False,"<p>This project aims to map and analyze the publicly knowable social connections of various communities, allowing us to gain unprecedented insights about the social dynamics in such communities. Most analyses of this sort map online social networks, such as Twitter, Facebook, or LinkedIn. While these networks encode important aspects of our lives (e.g., our professional connections) they fail to capture many real-world relationships. Most of these relationships are, in fact, public and known to the community members. By mapping this publicly knowable graph, we get a unique view of the community that allows us to gain deeper understanding of its social dynamics. To this end, we built a web-based tool that is simple, easy to use, and allows the community to map itself. Our goal is to deploy this tool in communities of different sizes, including the Media Lab community and the Spanish town of Jun.</p>",2016-12-31,--Choose Location,2017-04-11 21:40:23.904,True,2015-01-01,Human Atlas,LAB-INSIDERS,,False,Social Machines,False
chatterbox,echu,False,"<p>Speech synthesis in tutor mode. Using phones for literacy learning is an empowering application of mobile technology, but there are elements of the human tutor that have yet to be replicated in current apps. Namely, when reading a story, a tutor is likely to be more expressive and colorful in tone. When encountering a new word, a tutor might emphasize the vowel phoneme or stress a consonant pair the child has yet to master. By modeling speech with deep neural networks, our speech synthesizer will be able to interpolate between speaking styles, switching from 'normal' mode to 'tutor' mode as needed.
                    
                </p>",,,2016-12-05 00:17:02.173,True,2016-04-01,ChatterBox,PUBLIC,,True,Social Machines,False
playful-words,echu,False,"<p>While there are a number of literacy technology solutions developed for individuals, the role of social—or networked—literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.</p><p>By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.</p><p>To learn more about this project, please check out:&nbsp;<a href=""http://playfulwords.org/"">http://playfulwords.org/</a></p>",,--Choose Location,2018-04-30 20:28:15.298,True,2014-09-01,Playful Words,PUBLIC,,True,Social Machines,False
play-analytics,echu,False,"<p>Analyzing detailed data from SpeechBlocks&nbsp;to understand how kids engage with constructionist literacy learning technologies, with the goal of empowering caregivers (e.g. parents, older siblings, tutors) with these insights.</p>",,,2018-04-30 20:56:57.445,True,2016-02-01,Play Analytics,PUBLIC,,True,Social Machines,False
story-learning-machine,echu,False,"<p>The Storytelling project uses machine-based analytics to identify the qualities of engaging and marketable media. By developing models with the ability to “read” emotional arcs and semantic narrative video content, our researchers aim to map video story structure across many story types and formats.</p><p>To complement this content-based analysis, our researchers are also developing methods to analyze how emotional and semantic narratives affect viewer engagement with these stories. By tracking “referrals” of video URLs on social media networks, our researchers hope to identify how stories of different types and genres diffuse across networks, who influences this spread, and how video story distribution might be optimized. Given this project’s two-pronged strategy, our hope is to develop a robust story learning machine that uniquely maps the relationship between story structure and engagement across networks.</p>",,,2018-06-12 19:49:38.670,True,2017-02-23,The Story Learning Machine,PUBLIC,,True,Social Machines,False
language-simplification,echu,False,"<p><span style=""font-size: 18px; font-weight: 400;"">The Language Simplification project is developing automatic methods to simplify complex texts to be more easily read and understood by a broader audience, such as children and non-native English speakers. Using neural networks, complex words and phrases can be substituted, the sentence can be split and rephrased, and the overall text can be summarized and compressed.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">These capabilities can be wrapped into a reading assistance tool for end users, or as a pre-processing step for other NLP tasks.</span></p>",,,2019-04-17 20:51:49.717,True,2019-02-01,Language Simplification,PUBLIC,,True,Social Machines,False
newspix,matt54,False,"<p>NewsPix is a simple news-engagement application that helps users encounter breaking news in the form of high-impact photos. It is currently a Chrome browser extension (mobile app to come) that is customizable for small and large news organizations. Currently, when users open a new, blank page in Chrome, they get a new tab with tiles that show recently visited pages. NewsPix replaces that view with a high-quality picture from a news site. Users interested in more information about the photo can click through to the news site. News organizations can upload photos ranging from breaking news to historic sporting events, with photos changing every time a new tab is clicked.</p>",,--Choose Location,2016-12-05 00:17:03.242,True,2014-01-01,NewsPix,PUBLIC,,True,Civic Media,False
first-upload,matt54,False,"<p>First Upload is a tool for verifying the authenticity of news imagery. It helps find the first upload of imagery, particularly videos. Finding the person who uploaded a video is a key to determining authenticity, because often it is necessary to contact that person directly. It is being developed with input from YouTube and Bloomberg. Currently we have a working prototype, built for the YouTube site.</p>",,--Choose Location,2016-12-05 00:16:25.181,True,2015-01-01,First Upload,PUBLIC,,True,Civic Media,False
fold,matt54,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Some readers require greater context to understand complex stories.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">FOLD (</span><a href=""http://fold.cm"" style=""font-size: 18px; font-weight: normal;"">fold.cm</a><span style=""font-size: 18px; font-weight: normal;"">) is an open publishing platform with a unique structure that lets writers link media cards to the text of their stories.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">Media cards can contain videos, maps, tweets, music, interactive visualizations, and more.&nbsp;</span></p><p>FOLD is used by journalists, educators, and storytellers around the world.&nbsp;<br></p>",,--Choose Location,2016-12-15 02:27:31.751,True,2014-01-01,FOLD,PUBLIC,,True,Civic Media,False
aithpaos-untitled-project-2,aithpao,False,..,,--Choose Location,2016-09-27 20:24:23.900,False,2014-01-01,aithpao's untitled project,LAB,,True,Affective Computing,True
3d-mobility,agrignar,False,"<p><i style=""font-size: 18px; font-weight: 400;"">Unfolding the way we move.&nbsp;</i><br></p><p>Mobility has shaped the built environment since humans started settling together. From industrial towns to post-industrial innovation and service hubs, the mobility mode of the era was key in shaping not only the physical attributes of cities, but also the efficacy. In order to allocate the massive migration from rural areas, cities are growing and becoming more dense. Although high density can minimize transportation cost and energy, several problems start to appear if they are not planned carefully. Urban ventilation potential is reduced and open seen spaces are limited, compromising our experience and life quality. Residential, office, and retail get closer but remain arranged in conventional ways. A two-dimensional street that organizes the way we live and keeps transportation methods are in permanent conflict. Too fast for those who live in it, and too slow and congested for those that go by.</p><p>Urban mobility is becoming more electric, more autonomous, more shared, and more connected, indicators that call for a mobility revolution, and designers have the chance to reinvent the way city is experienced.</p><p>Today, more than ever, the scale and rate of urban expansion is making mobility solutions a key concern, which will impact large segments of the global population since it is estimated that by 2050, more that two thirds of the global population will be living in cities. We&nbsp; propose a new experience and mobility around cities, unfolding the city networks and using its third dimensions, different mobility, speed modes (static, mass transportation, internal transportation), public areas appearing in rooftops, and mix-use spaces in&nbsp;intersticial&nbsp;parts of buildings. Through simulation as a tool, we can understand the impact of this new disrupting mobility system, avoiding to repeat mistakes like those made in the past.&nbsp;</p>",,,2019-06-12 14:47:59.274,True,2018-07-09,3D Mobility,PUBLIC,https://guadalupebabio.com,True,City Science,False
cityscope-hamburg,agrignar,False,"<p><a href=""https://medium.com/mit-media-lab/shifting-priorities-finding-places-9ad3bdbe38b8"">Read more about this project here</a></p><p>MIT City Science is working with HafenCity University to develop CityScope for the neighborhood of Rothenburgsort in Hamburg, Germany. The goal is to create an interactive stakeholder engagement tool that also serves as the platform for joint research of modules for city simulation. Researchers are developing modules for walkability, neighborhood connectivity, energy efficiency, and economic activity, among others.</p>",,--Choose Location,2019-05-10 19:07:36.178,True,2015-01-01,City Science Lab Hamburg,PUBLIC,,True,City Science,False
traffic-andorra,agrignar,False,"<h2>Data Fusion&nbsp;for Dynamic Traffic Prediction</h2><p>Traffic congestion has huge negative impacts on the productivity, health and personal lives of city dwellers. To manage this problem&nbsp;effectively, transportation engineers need to predict traffic congestion throughout the road network at all hours of the day.&nbsp;Prediction of traffic typically involves travel surveys that are expensive, time consuming and do not capture temporal variation in travel demand.&nbsp;However,&nbsp;anonymised&nbsp;location data from mobile phones present an alternative source of data which is passively collected, widely available and naturally captures temporal trends.&nbsp;On the other hand, these data contain other biases and so if we use these data for transportation models, we must take care to correct for these biases using more reliable data. As part of the City Science collaboration with Andorra, we used&nbsp;a&nbsp;Bayesian network to build a calibrated transportation model for the country based on&nbsp;geolocated telecoms data and validated using a small sample of traffic counts.</p>",,,2019-04-17 19:43:06.032,True,2017-01-01,Dynamic Traffic Prediction in Andorra: a Bayesian network approach,PUBLIC,,True,City Science,False
andorra-innovation,agrignar,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>The MIT Media Lab's City Science research group, the University of Andorra, and national and international companies are collaborating in order to bring an innovative ecosystem into the capital of Andorra. This innovation district aims to engage local citizens, researchers, and R&amp;D from the companies in order to build together an Andorran living lab, an ""innovation district"" where national and international companies can test and deploy their products and ideas and cultivate human capital.</p><p><b>Current Projects</b></p><ul><li>Andorra Innovation Space</li><li>Andorra Cultural Heritage</li><li>Drones patterns and flows, collaboration living lab<br></li><li>Young Future</li></ul>",,,2018-07-09 18:49:41.844,True,2016-09-01,Andorra | Innovation,PUBLIC,,True,City Science,False
andorra-dynamic-urban-planning,agrignar,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>",,,2019-02-25 15:17:09.063,True,2016-09-01,Andorra | Dynamic Urban Planning,PUBLIC,,True,City Science,False
city-science-lab-aalto,agrignar,False,"<p>Aalto University, Finland, and the MIT Media Lab’s City Science group are co-developing a version of the MIT CityScope platform for urban analysis, efficient resource utilization, and spatial programming for campus development, using Otaniemi as a testbed. Aalto joins a network of City Science collaborators which includes Tongji University (Shanghai), Taipei Tech (Taiwan), HafenCity University (Hamburg), and ActuaTech (Andorra).</p>",,,2019-05-07 19:59:14.315,True,2017-05-01,City Science Lab Aalto,PUBLIC,,True,City Science,False
citymatrix,agrignar,False,"<h2><br>An Urban Decision-Support System Augmented by Artificial Intelligence<br></h2><p>The decision-making process in urban design and urban planning is outdated. Currently, urban decision-making is mostly a top-down process, with community participation only in its late stages. Furthermore, many design decisions are subjective, rather than based on quantifiable performance and data. Current tools for urban planning do not allow both expert and non-expert stakeholders to explore a range of complex scenarios rapidly with real-time feedback.&nbsp;</p><p>CityMatrix was an effort towards evidence-based, democratic decision-making. Its contributions lie in the application of Machine Learning as a versatile, quick, accurate, and low-cost approach to enable real-time feedback of complex urban simulations and the implementation of the optimization searching algorithms to provide open-ended decision-making suggestions.&nbsp;The goals of CityMatrix were:&nbsp;</p><br><ol><li><i>Designing an intuitive Tangible User Interface (TUI) to improve the accessibility of the decision-making process for non-experts.&nbsp;</i></li><li><i>Creating real-time feedback on multi-objective urban performances to help users evaluate their decisions, thus to enable rapid, collaborative decision-making.&nbsp;</i></li><li><i>Constructing a suggestion-making system that frees stakeholders from excessive, quantitative considerations and allows them to focus on the qualitative aspects of the city, thus helping them define and achieve their goals more efficiently.</i></li></ol><p>CityMatrix was augmented by Artificial Intelligence (AI) techniques including Machine Learning simulation predictions and optimization search algorithms. The hypothesis explored in this work was that the decision quality could be improved by the organic combination of both strengths of human intelligence and machine intelligence.</p><p>The system was pilot-tested and evaluated by comparing the problem-solving results of volunteers, with or without AI suggestions. Both quantitative and qualitative analytic results showed that CityMatrix is a promising tool that helps both professional and non-professional users understand the city better to make more collaborative and better-informed decisions.&nbsp;</p>",,,2018-10-17 18:10:46.064,True,2016-02-26,CityMatrix,PUBLIC,https://media.mit.edu/people/ryanz,True,City Science,False
andorra-living-lab,agrignar,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,City Science,False
Reversed-Urbanism,agrignar,False,"<h2>Predicting Urban Performance through Behavioral Patterns in Temporal Telecom Data</h2><p>This study explores a novel method to analyze diverse behavioral patterns in large urban populations and to associate them with discrete urban features. This work utilizes machine learning and anonymized telecom data to understand which fragments of the city has greater potential to attract dense and diverse populations over longer periods of time. Finally, this work suggests a road map for building spatial prediction tools in an effort to improve city-design and planning processes.&nbsp;&nbsp;</p><p><b><a href=""https://cityscope.github.io/CS_Andorra_RNC/"">Click here for an interactive visualization of this study</a>&nbsp;<br><br></b></p><p><b>Advisors:</b>&nbsp;Kent Larson&nbsp;and&nbsp;Esteban Moro<br><b>Thanks to</b> Andorra Telecom, ActuaTech,&nbsp;Núria Macià. <br>Data was&nbsp;obtained by Andorra Telecom as part of MIT Media Lab City Science and the State of Andorra collaboration.&nbsp;</p>",,,2019-02-24 23:21:12.068,True,2017-07-01,Reversed Urbanism,PUBLIC,http://ArielNoyman.com,True,City Science,False
cityscope-volpe,agrignar,False,"<p>CityScope Volpe is demonstrating most of the urban planning, analysis, and prediction features developed for the CityScope project. The site, a 14-acre parcel on the northern part of MIT/Kendall Square area of Cambridge, has been acquired and is&nbsp; being developed by MIT. City Science researchers designed and built a CityScope urban performance tool that is aiming to predict the outcomes of multiple planning and development scenarios.&nbsp;</p>",,,2019-02-25 15:14:11.403,True,2016-11-01,CityScope Volpe,PUBLIC,,True,City Science,False
andorra-mobility,agrignar,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile</a></p><p>With no airport or train service, most of the 8 million tourists who visit Andorra each year arrive by car, making traffic management and parking some of the country's most important challenges. We are currently developing different projects spanning from data science to the deployment of autonomous vehicles to help address these issues.<br></p>",,,2017-10-25 05:56:26.309,True,2016-09-01,Andorra | Mobility,PUBLIC,,True,City Science,False
urban-swarms,agrignar,False,"<p>Modern cities have to respond to the growing demands of more efficient and sustainable urban development, as well as an increased quality of life. In this context, the cities of the future will need the ability to gain insight about current urban conditions and react dynamically to them. According to this view, ""smart cities"" can be seen as cybernetic urban environments in which different agents (e.g., citizens) and actuators (e.g., robots) exploit the city-wide infrastructure as a medium to operate synergistically.<b><i> Urban Swarms</i></b> explores the feasibility of swarm robotics systems in urban environments. By using bio-inspired methods, a swarm of robots is able to handle important urban systems and infrastructures, improving their efficiency and autonomy. A diverse set of simulation experiments were designed and conducted using real-world GIS data. Results show that the proposed combination is able to outperform current approaches. <i><b>Urban Swarms</b></i> not only aims to show the efficiency of our proposed solution, but also to give insights about how to design and customize these systems.&nbsp;<a href=""https://www.media.mit.edu/projects/cityscope-volpe/overview/"" style=""font-size: 18px; font-weight: 400;"">CityScope</a><span style=""font-size: 18px; font-weight: 400;"">&nbsp;Volpe ABM model has been customized to integrate Swarm behavior using the </span><a href=""https://gama-platform.github.io/"" style=""font-size: 18px; font-weight: 400;"">Gama Platform</a><span style=""font-size: 18px; font-weight: 400;""> as an </span><a href=""https://github.com/mitmedialab/UrbanSwarms"" style=""font-size: 18px; font-weight: 400;"">open source project</a><span style=""font-size: 18px; font-weight: 400;"">.&nbsp;</span></p>",,,2019-03-12 15:37:36.573,True,2018-10-01,Urban Swarms,PUBLIC,http://www.eduardocastello.com,True,City Science,False
city-science-lab-shanghai,agrignar,False,"<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>",,,2019-05-16 20:43:38.677,True,2017-02-14,City Science Lab Shanghai,PUBLIC,,True,City Science,False
city-science-guadalajara,agrignar,False,"<p>The University of Guadalajara, referred to as UdeG, is a university network composed of 15 campuses within the state of Jalisco and one online system. The University offers undergraduate and graduate studies to around 130,000 students. UdeG strives to understand urban performance metrics using evidence-based decision making tools, facilitated through a collaboration with the MIT Media Lab City Science group.</p>",,,2019-05-10 19:44:20.937,True,2018-11-01,City Science Collaboration Guadalajara,PUBLIC,,True,City Science,False
andorra-tourism,agrignar,False,"<p><span style=""font-size: 18px; font-weight: 400;""></span><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/""><span style=""font-size: 18px; font-weight: 400;"">View the main City Science Andorra project profile.</span></a><br></p><p><span style=""font-size: 18px; font-weight: 400;"">With more than eight million visitors a year, tourism represents almost 30% of the economy of Andorra. By gathering and analyzing data from social media, call detail records, and wifi, we can understand the country's dynamics of tourism and commerce as well as design interventions that can improve the experience for tourists, encouraging them to visit Andorra more frequently, stay longer, and increase spending.&nbsp;</span><br></p><h2><b>Current Projects</b></h2><ul><li>Event Analysis<br></li><li>Social Network<br></li><li>Location Recommendation system<br></li></ul><p> </p><h2><b>EVENT ANALYSIS</b></h2><p>Based on the analysis of call detail records and social media, the goal of this project is to understand the tourist behaviors in Andorra.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">After mining those anonymized data, we have been able to learn different patterns and behaviors of the tourism in Andorra thanks to an agent-based model developed in order to represent the flow of people. This simulation is also coupled with an interactive table called CityMatrix.</span></p>",,,2019-02-25 15:33:28.936,True,2015-08-01,Andorra | Tourism,PUBLIC,,True,City Science,False
andorra-energy-environment,agrignar,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p>",,,2018-10-22 21:46:23.783,True,2016-09-02,Andorra | Energy + Environment,PUBLIC,,True,City Science,False
basic,agrignar,False,"<p>Autonomous vehicles (AVs), drones, and robots will revolutionize our way of traveling and understanding urban space. In order to operate, all of these devices are expected to collect and analyze a lot of sensitive data about our daily activities. However, current operational models for these devices have extensively relied on centralized models of managing these data. The security of these models unveiled significant issues.</p><p>This project&nbsp; proposes BASIC, the Blockchained Agent-based Simulator for Cities. This tool aims to verify the feasibility of the use of blockchain in simulated urban scenarios by considering the communication between agents through&nbsp;<i>smart contracts</i>. In order to test the proposed tool, we implemented a car-sharing model within the city of Cambridge (Massachusetts, USA). In this research, the relevant literature was explored, new methods were developed, and different solutions were designed and tested. Finally, conclusions about the feasibility of the combination between blockchain technology and agent-based simulations were drawn.</p><p>Developed using&nbsp;<a href=""https://gama-platform.github.io/"">Gama Platform</a>.&nbsp;&nbsp;</p><p>Click <a href=""https://github.com/mitmedialab/Basic"">here</a> for the Open Source Repository.</p>",,,2019-04-23 20:08:21.338,True,2018-09-03,BASIC: Blockchained Agent-based Simulator for Cities,PUBLIC,,True,City Science,False
abmobility,agrignar,False,"<p>Mobility is a key issue for city planners. Being able to evaluate the impact of its evolution is complex and involves many factors including new technologies like electric cars, autonomous vehicles and also new social habits like vehicle sharing. We need a better understanding of different scenarios to improve the quality of long-term decisions. Computer simulations can be a tool to better understand this evolution, to discuss different solutions and to communicate the implications of different decisions. In this paper, we propose a new generic model that creates an artificial micro-world which allows the modeler to create and modify new mobility scenarios in a quick and easy way. This not only helps to better understand the impact of new mobility modes on a city, but also fosters a better-informed discussion of different futures. Our model is based on the agent-based paradigm using the GAMA Platform. It takes into account different mobility modes, people profiles, congestion and traffic patterns. In this paper, we review an application of the model of the city of Cambridge.</p>",,,2019-03-27 20:04:59.772,True,2016-04-15,ABMobility,PUBLIC,,True,City Science,False
cityscope-cooper-hewitt,agrignar,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
city-science-andorra,agrignar,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
cityscope,agrignar,False,"<p>City Science researchers are developing a slew of tangible and digital platforms dedicated to solving spatial design and urban planning challenges. The tools range from simulations that quantify the impact of disruptive interventions in cities to communicable collaboration applications. We develop and deploy these tools around the world and maintain open source repositories for the majority of deployments. ""CityScope"" is a concept for shared, interactive computation for urban planning.</p><p>All current CityScope development, tools, and software are open source <a href=""https://cityscope.github.io/"">here</a>.&nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2019-05-16 20:42:57.474,True,2017-08-01,Theme | CityScope,PUBLIC,,True,City Science,False
3d-mobility,gbabio,False,"<p><i style=""font-size: 18px; font-weight: 400;"">Unfolding the way we move.&nbsp;</i><br></p><p>Mobility has shaped the built environment since humans started settling together. From industrial towns to post-industrial innovation and service hubs, the mobility mode of the era was key in shaping not only the physical attributes of cities, but also the efficacy. In order to allocate the massive migration from rural areas, cities are growing and becoming more dense. Although high density can minimize transportation cost and energy, several problems start to appear if they are not planned carefully. Urban ventilation potential is reduced and open seen spaces are limited, compromising our experience and life quality. Residential, office, and retail get closer but remain arranged in conventional ways. A two-dimensional street that organizes the way we live and keeps transportation methods are in permanent conflict. Too fast for those who live in it, and too slow and congested for those that go by.</p><p>Urban mobility is becoming more electric, more autonomous, more shared, and more connected, indicators that call for a mobility revolution, and designers have the chance to reinvent the way city is experienced.</p><p>Today, more than ever, the scale and rate of urban expansion is making mobility solutions a key concern, which will impact large segments of the global population since it is estimated that by 2050, more that two thirds of the global population will be living in cities. We&nbsp; propose a new experience and mobility around cities, unfolding the city networks and using its third dimensions, different mobility, speed modes (static, mass transportation, internal transportation), public areas appearing in rooftops, and mix-use spaces in&nbsp;intersticial&nbsp;parts of buildings. Through simulation as a tool, we can understand the impact of this new disrupting mobility system, avoiding to repeat mistakes like those made in the past.&nbsp;</p>",,,2019-06-12 14:47:59.274,True,2018-07-09,3D Mobility,PUBLIC,https://guadalupebabio.com,True,City Science,False
cityscope-cooper-hewitt,gbabio,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
cityscope,gbabio,False,"<p>City Science researchers are developing a slew of tangible and digital platforms dedicated to solving spatial design and urban planning challenges. The tools range from simulations that quantify the impact of disruptive interventions in cities to communicable collaboration applications. We develop and deploy these tools around the world and maintain open source repositories for the majority of deployments. ""CityScope"" is a concept for shared, interactive computation for urban planning.</p><p>All current CityScope development, tools, and software are open source <a href=""https://cityscope.github.io/"">here</a>.&nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2019-05-16 20:42:57.474,True,2017-08-01,Theme | CityScope,PUBLIC,,True,City Science,False
troxes,jbobrow,False,"<p>The building blocks we grow up with and the coordinate systems we are introduced to at an early age shape the design space with which we think. Complex systems are difficult to understand because they often require transition from one coordinate system to another. We could even begin to say that empathy is precisely this ability to map easily to many different coordinates. Troxes is a building blocks kit based on the triangle, where kids get to build their building blocks and then assemble Platonic and Archimedean solids.</p>",,--Choose Location,2016-12-15 03:10:14.309,True,2014-09-01,Troxes,PUBLIC,,True,Playful Systems,False
automatiles,jbobrow,False,"<p>A tabletop set of cellular automata ready to exhibit complex systems through simple behaviors, AutomaTiles explores emergent behavior through tangible objects. Individually they live as simple organisms, imbued with a simple personality; together they exhibit something ""other"" than the sum of their parts. Through communication with their neighbors, complex interactions arise. What will you discover with AutomaTiles?</p>",,--Choose Location,2017-01-05 22:03:10.469,True,2015-01-01,AutomaTiles,PUBLIC,,True,Playful Systems,False
dice,jbobrow,False,"<p>Today, algorithms drive our cars, our economy, what we read, and how we play. Modern-day computer games utilize weighted probabilities to make games more competitive, fun, and addicting. In casinos, slot machines--once a product of simple probability--employ similar algorithms to keep players playing. Dice++ takes the seemingly straight probability of rolling a die and determines an outcome with algorithms of its own.</p>",,--Choose Location,2016-12-15 02:58:30.762,True,2014-09-01,Dice++,PUBLIC,,True,Playful Systems,False
storyboards,jbobrow,False,"<p>Giving opaque technology a glass house, Storyboards present the tinkerers or owners of electronic devices with stories of how their devices work. Just as the circuit board is a story of star-crossed lovers—Anode and Cathode—with its cast of characters (resistor, capacitor, transistor), Storyboards have their own characters driving a parallel visual narrative.</p>",,--Choose Location,2017-04-03 01:11:34.352,True,2014-09-01,Storyboards,PUBLIC,,True,Playful Systems,False
sensortape-modular-and-programmable-3d-aware-dense-sensor-network-on-a-tape,artemd,False,"<p>SensorTape is a modular and dense sensor network in a form factor of a tape. SensorTape is composed of interconnected and programmable sensor nodes on a flexible electronics sub-strate. Each node can sense its orientation with an inertial measurement unit, allowing deformation self-sensing of the whole tape. Also, nodes sense proximity using time-of-flight infrared. We developed network architecture to automatically determine the location of each sensor node, as SensorTape is cut and rejoined. We also made an intuitive graphical interface to program the tape. Our user study suggested that SensorTape enables users with different skill sets to intuitively create and program large sensor network arrays. We developed diverse applications ranging from wearables to home sensing, to show low-deployment effort required by the user. We showed how SensorTape could be produced at scale and made a 2.3-meter long prototype.</p>",,--Choose Location,2019-04-19 14:34:20.486,True,2016-01-01,SensorTape: Modular and programmable 3D-aware dense sensor network on a tape,PUBLIC,,True,Responsive Environments,False
artemds-untitled-project-2,artemd,False,..,,--Choose Location,2016-09-27 20:24:24.795,False,2014-01-01,artemd's untitled project,LAB,,True,Responsive Environments,False
kino-kinetic-wearable,artemd,False,"<p><span style=""font-size: 18px; font-weight: 400;"">This work explores a dynamic future in which the accessories we wear are no longer static, but are instead mobile, living objects on the body. Engineered with the functionality of miniaturized robotics, this ""living"" jewelry roams on unmodified clothing, changing location and reconfiguring appearance according to social context and enabling multiple presentations of self. With the addition of sensor devices, they can actively respond to environmental conditions. They can also be paired with existing mobile devices to become personalized on-body assistants to help complete tasks. Attached to garments, they generate shape-changing clothing and kinetic pattern designs—creating a new, dynamic fashion.</span><br></p><p> </p><p><span style=""font-size: 18px; font-weight: 400;"">It is our vision that in the future, these robots will be miniaturized to the extent that they can be seamlessly integrated into existing practices of body ornamentation. With the addition of kinetic capabilities, traditionally static jewelry and accessories will start displaying life-like qualities, learning, shifting, and reconfiguring to the needs and preferences of the wearer, also assisting in fluid presentation of self. With wearables that possess hybrid qualities of the living and the crafted, we explore a new on-body ecology for human-wearable symbiosis.</span><span style=""font-size: 18px; font-weight: 400;"">&nbsp;</span></p>",,--Choose Location,2017-10-17 03:54:26.236,True,2015-01-01,Kino,PUBLIC,,True,Responsive Environments,False
bioessence,artemd,False,<h2><b>A wearable olfactory display that monitors cardio-respiratory information to support mental wellbeing.</b></h2><p>BioEssence is a novel wearable olfactory&nbsp;display that provides just-in-time release of scents based on&nbsp;the physiological state of the wearer. The device can release up&nbsp;to three scents and passively captures subtle chest vibrations&nbsp;associated with the beating of the heart and respiration through&nbsp;clothes.&nbsp;<br></p>,,,2019-05-11 00:16:22.469,True,2017-12-01,BioEssence,PUBLIC,http://www.judithamores.com/essence,True,Responsive Environments,False
circuit-robots,artemd,False,"<h2>Integrating sensors and actuators using flexible electronics</h2><p>Currently, the manufacturing of self-actuating and self-sensing robots requires non-standard manufacturing techniques and assembly steps to integrate electrical and mechanical systems. In this work, we developed a novel manufacturing technique, where such robots can be produced at a flexible electronics factory. We developed the technique using standard industrial machines, processes, and materials. Using a lamination process, we were able to integrate air pouches or shape memory alloy (SMA) inside a polyamide-based flexible circuit to produce bending actuators. The bend angle of the actuators is sensed with a chain of inertial measurement units integrated on the actuator. Air-pouch actuators can produce a force of a 2.24N, and a maximum bend angle of 74 degrees. To demonstrate, we manufactured a five-legged robot with the developed actuators and bend sensors, with all the supporting electronics (e.g., microcontrollers, radio) directly integrated into the flexible printed circuit. Such robots are flat and lightweight (15 grams) and thus conveniently compact for transportation and storage. We believe that our technique can allow inexpensive and fast prototyping and deployment of self-actuating and self-sensing robots.<br></p>",,,2019-04-17 19:28:42.580,True,2017-08-01,Circuit Robots: Mass manufacturing of self-actuating robots,PUBLIC,,True,Responsive Environments,False
low-power-gesture-input-with-wrist-worn-pressure-sensors,artemd,False,"<p>We demonstrate an always-available, on-body gestural interface. Using an array of pressure sensors worn around the wrist, it can distinguish subtle finger pinch gestures with high accuracy (&gt;80?). We demonstrate that it is a complete system that works wirelessly in real time. The device is simple and light-weight in terms of power consumption and computational overhead. Prototype's sensor power consumption is 89uW, allowing the prototype to last more then a week on a small lithium polymer battery. Also, device is small and non-obtrusive, and can be integrated into a wristwatch or a bracelet. Custom pressure sensors can be printed with off-the-shelf conductive ink-jet technology. We demonstrate that number of gestures can be greatly extended by adding orientation data from an accelerometer. Also, we explore various usage scenarios with the device.</p>",,--Choose Location,2019-04-19 14:29:45.955,True,2014-01-01,Low-power gesture input with wrist-worn pressure sensors,PUBLIC,,True,Responsive Environments,False
chainform,artemd,False,"<p>ChainFORM is a modular hardware system for designing linear shape-changing interfaces. Each module is developed based on a servo motor with added flexible circuit board, and is capable of touch detection, visual output, angular sensing, and motor actuation. Moreover, because each module can communicate with other modules linearly, it allows users and designers to adjust and customize the length of the interface. Using the functionality of the hardware system, we propose a wide range of applications, including line-based shape changing display, reconfigurable stylus, rapid prototyping tool for actuated crafts, and customizable haptic glove. We conducted a technical evaluation and a user study to explore capabilities and potential requirements for future improvement.</p>",,--Choose Location,2018-05-04 15:33:24.390,True,2015-09-01,ChainFORM,PUBLIC,,True,Responsive Environments,False
rovables,artemd,False,"<p>We introduce Rovables, a miniature robot that can move freely on unmodified clothing. The robots are held in place by magnetic wheels, and can climb vertically. The robots are untethered and have an onboard battery, microcontroller, and wireless communications. They also contain a low-power localization system that uses wheel encoders and IMU, allowing Rovables to perform limited autonomous navigation on the body. In the technical evaluations, we found that Rovables can operate continuously for 45 minutes and can carry up to 1.5N. We propose an interaction space for mobile on-body devices spanning sensing, actuation, and interfaces, and develop application scenarios in that space. Our applications include on-body sensing, modular displays, tactile feedback and interactive clothing and jewelry.
                    
                </p>",,,2017-03-03 18:38:10.500,True,2016-09-01,Rovables,PUBLIC,,True,Responsive Environments,False
nailo,artemd,False,"<p>NailO is a wearable input device in the form of a commercialized nail art sticker. It works as a miniaturized trackpad the size and thickness of a fingernail that can connect to your mobile devices; it also enables wearers to customize the device to fit the wearer’s personal style. NailO allows wearers to perform different functions on a phone or PC with different gestures, and the wearer can easily alter its appearance with a nail art design layer, creating a combination of functionality and aesthetics.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">From the fashion-conscious, to techies, and anyone in between, NailO can make a style, art, or a design statement; but in its more neutral, natural-looking example it can be worn and used only for its functionality. As a nail art sticker, NailO is small, discreet, and removable. Interactions through NailO can be private and subtle, for example attracting minimal attention when you are in a meeting but need to reply to an urgent text message. Mimicking the form of a cosmetic extension, NailO blends into and decorates one’s body when attached, yet remains removable at the wearer’s discretion, giving the wearer power and control over the level of intimacy of the device to one’s body.</span></p>",,--Choose Location,2017-03-27 21:12:03.637,True,2014-09-01,NailO,PUBLIC,http://nailo.media.mit.edu,True,Responsive Environments,False
skinbot-a-wearable-skin-climbing-robot,artemd,False,"<p>We introduce SkinBot: a lightweight robot that moves over the skin's surface with a two-legged suction-based locomotion mechanism and captures a wide range of body parameters with an exchangeable multipurpose sensing module. We believe that robots that live on our skin, such as SkinBot, will enable a more systematic study of the human body and offer great opportunities to advance our knowledge in many areas such as telemedicine, human-computer interfaces, body care, and fashion.</p>",,,2019-04-19 14:35:05.713,True,2016-11-01,"SkinBot: A wearable, skin-climbing robot",PUBLIC,http://www.artemdementyev.com,True,Responsive Environments,False
eternal,holbrow,False,"<p>The web enables massive realtime communication and collaboration, but most media on the web does not take advantage of these features. Media on the internet typically uses the web only as a distribution medium.</p><p>If we are going to make next-generation internet media, we need to think about how to integrate the unique properties of the web into the media itself. This involves rethinking the role and design of web servers so they facilitate realtime interaction instead of serving requests.</p><p>Models for internet-enabled interaction and collaboration like forums, chatroom, live documents, metrics and A/B testing, are not designed with interactive media in mind.&nbsp;</p><p>This project is our very first exploration using custom web server technology and a new interaction model to facilitate online collaboration.</p>",,,2018-05-01 13:57:18.927,True,2017-10-01,Eternal,PUBLIC,,True,Opera of the Future,False
imagining-the-future-of,holbrow,False,"<p>The internet&nbsp; changed&nbsp; how we create, distribute, and consume music and media. Modern digital tools for creating music and media provide ""cloud enabled"" features like automatic backups, an asset marketplace, and real-time collaboration. Despite these features, current&nbsp; tools for creating music are still based on the personal computing paradigms of the 20th century. How will media production change in the 21st century? <br></p><p>Internet engineering introduced the concept of cloud-native  applications. What would it mean for end-user experience to be truly cloud-native? This project shows a very early prototype that illustrates some of the possibilities. In this approach, the assets that make up a music project are securely exposed to the Internet, where they can be accessed and manipulated by digital services&nbsp; and and human collaborators.</p><p>The long-term goal is to give individual content creators control of their data, and a share of the benefits provided by machine learning analytics. A longer description and technical blueprint can be found in&nbsp; <a href=""https://medium.com/@charlesholbrow/turning-the-daw-inside-out-54834d0b674a"">Turning the Digital Audio Workstation Inside Out</a>.<br></p>",,,2018-10-22 00:44:23.233,True,2018-10-01,The SciFi Audio Workstation,PUBLIC,,True,Opera of the Future,False
evolving-media,holbrow,False,"<p>Songs released on music streaming services are static, never changing after their initial release.&nbsp;Evolving Media proposes a content production and publishing pipeline, enabling artists and content creators to release media that evolves and matures as it is consumed. To take advantage of this capability, we are re-thinking the tools and processes used to create and update media content.&nbsp;</p><p>The current implementation integrates our custom augmented reality stack to rapidly iterate and publish synchronized audio/video content to the web.&nbsp;</p>",,,2019-04-24 13:30:28.162,True,2019-03-01,Evolving Media on the Internet,PUBLIC,,True,Opera of the Future,False
sound-cycles,holbrow,False,"<p>Sound Cycles is a new interface for exploring, re-mixing, and composing with large volumes of audio content. The project presents a simple and intuitive interface for scanning through long audio files or pre-recorded music. Sound Cycles integrates with the existing Digital Audio Workstation for on-the-fly editing, audio analysis, and feature extraction.</p>",,--Choose Location,2016-12-05 00:17:04.683,True,2015-01-01,Sound Cycles,PUBLIC,,True,Opera of the Future,False
ambisonic-surround-sound-audio-compression,holbrow,False,"<p>Traditional music production and studio engineering depends on dynamic range compression audio signal processors that precisely and dynamically control the gain of an audio signal in the time domain. This project expands on the traditional dynamic range compression model by adding a spatial dimension. Ambisonic Compression allows audio engineers to dynamically control the spatial properties of a three-dimensional sound field, opening new possibilities for surround-sound design and spatial music performance.</p>",,--Choose Location,2019-04-17 19:58:29.909,True,2015-01-01,Ambisonic surround-sound audio compression,PUBLIC,,True,Opera of the Future,False
empathy-and-the-future-of-experience,holbrow,False,"<p>Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of–as well as long-term commitment to–empathic communication.</p>",,--Choose Location,2019-04-17 19:59:42.795,True,2015-01-01,Empathy and the future of experience,PUBLIC,,True,Opera of the Future,False
taking-a-line-on-a-walk,holbrow,False,"<p>Bauhaus artist Paul Klee considered a line to be&nbsp; one of the atomic elements of art and architecture. He famously described a line drawing as ""<a href=""https://www.artsy.net/article/artsy-editorial-how-to-be-an-artist-according-to-paul-klee"">a dot that went for a walk</a>.""&nbsp; What would a dot on a walk look like in three dimensions?&nbsp;</p><p>This project uses our custom high-resolution AR rig to find out.&nbsp; See the very first test footage:</p>",,,2019-04-18 14:10:20.018,True,2018-12-02,Taking a line on a walk,PUBLIC,http://CharlesHolbrow.com,True,Opera of the Future,False
preventing-tick-borne-disease-by-permanently-immunizing-mice,buchthal,False,"<p>Lyme disease is the most common vector-borne infection in North America. People are infected when bitten by ticks; ticks are typically infected when they bite white-footed mice, the primary ""reservoir"" of the disease. We are exploring the possibility of permanently immunizing mouse populations to block transmission by making and releasing mice that produce protective mouse antibodies from birth and pass immunity on to their pups. The project has been guided by representatives in offshore island communities from inception. Communities will choose which type of antibodies, pick uninhabited islands to serve as field trial sites, select independent monitors, and ultimately decide whether to volunteer their own islands for the next stage. If successful, prevention could be expanded to the mainland using local or global gene drive systems. Whether or not communities decide to proceed, we hope the process will become a model for responsive science worldwide.</p>",,--Choose Location,2019-04-17 18:53:26.237,True,2016-01-01,Preventing tick-borne disease by permanently immunizing mice,PUBLIC,,True,Sculpting Evolution,False
sculpting-evolution-test-project-joanna-tuesday,buchthal,False,<p>test placeholder text</p>,,,2017-05-01 19:55:53.618,True,2017-03-01,Sculpting Evolution test project Joanna/Tuesday,LAB,,True,Sculpting Evolution,False
daisydrives,buchthal,False,"<p>Who should decide whether, when, and how to alter the environment? These are hard questions, especially when the decisions will impact people in many different communities or nations. Daisy drive systems may help by empowering local communities to make decisions concerning their local environments without imposing them on anyone else.<br></p><p>The problem with current CRISPR-based gene drive systems is that they can spread indefinitely—potentially affecting every population of the target species throughout the world. It's unclear how such ""global"" drives can be safely tested, much less whether nations will ever agree to use them.&nbsp;To return power to the hands of local communities, we devised a new form of drive system called a ""daisy drive"" that can only affect local environments. The trick was to teach DNA to count&nbsp;and limit gene drive spreading to a pre-programmed number of generations.&nbsp;We hope that daisy drives will simplify decision-making and promote responsible use by allowing local communities to decide how to solve their own ecological problems.</p>",,,2019-04-03 16:07:53.609,True,2016-06-06,Daisy Drives,PUBLIC,http://www.sculptingevolution.org/daisydrives,True,Sculpting Evolution,False
preventing-tick-borne-disease-by-permanently-immunizing-mice,esvelt,False,"<p>Lyme disease is the most common vector-borne infection in North America. People are infected when bitten by ticks; ticks are typically infected when they bite white-footed mice, the primary ""reservoir"" of the disease. We are exploring the possibility of permanently immunizing mouse populations to block transmission by making and releasing mice that produce protective mouse antibodies from birth and pass immunity on to their pups. The project has been guided by representatives in offshore island communities from inception. Communities will choose which type of antibodies, pick uninhabited islands to serve as field trial sites, select independent monitors, and ultimately decide whether to volunteer their own islands for the next stage. If successful, prevention could be expanded to the mainland using local or global gene drive systems. Whether or not communities decide to proceed, we hope the process will become a model for responsive science worldwide.</p>",,--Choose Location,2019-04-17 18:53:26.237,True,2016-01-01,Preventing tick-borne disease by permanently immunizing mice,PUBLIC,,True,Sculpting Evolution,False
reducing-suffering-in-laboratory-animals-3,esvelt,False,"<p>The world uses an estimated 20 million mice in laboratory research experiments each year. These experiments are monitored and regulated to protect animal welfare whenever possible, including the use of painkillers where appropriate. However, analgesics cannot completely eliminate suffering, and many studies cannot use opiates or anti-inflammatory drugs because they would interfere with the biological process being studied. The benefits of animal research may outweigh the cost in animal suffering, but it would be better to perform these experiments without animal suffering. This project seeks to develop strains of mice that experience far less pain and suffering than current animals but are equally suited to laboratory and medical research. If successful, widespread adoption of these mice could drastically reduce the total amount of animal suffering in laboratories worldwide.</p>",,--Choose Location,2019-04-17 18:55:43.303,True,2016-01-01,Reducing suffering in laboratory animals,PUBLIC,,True,Sculpting Evolution,False
studying-the-evolution-of-gene-drive-systems,esvelt,False,"<p>How will gene drive systems evolve once released into the wild? Can they be reliably overwritten and blocked by immunizing reversal drives? Might they spread into related species? These are difficult questions because wild populations are so much larger than laboratory colonies, meaning critical evolutionary events would never be observed in the lab. We seek to develop nematode worms as a model system to help answer these questions. Nematodes are genetically tractable, reproduce twice each week, and are readily grown in populations numbering in the billions. This allows us to study drive systems intended for other organisms in nematodes. Synthetic site targeting, split drives, and ecological confinement will prevent spread into wild nematodes. Because nematodes are easy to culture and count using Foldscope microscopes, we intend to work with educators to enable students, museum-goers, and citizen scientists to participate in gene drive research.</p>",,--Choose Location,2019-04-17 18:56:53.592,True,2016-01-01,Studying the evolution of gene drive systems,PUBLIC,,True,Sculpting Evolution,False
reducing-suffering-in-laboratory-animals-2,esvelt,False,"<p>The world uses an estimated 20 million mice in laboratory research experiments each year. These experiments are monitored and regulated to protect animal welfare whenever possible, including the use of painkillers where appropriate. However, analgesics cannot completely eliminate suffering, and many studies cannot use opiates or anti-inflammatory drugs because they would interfere with the biological process being studied. The benefits of animal research may outweigh the cost in animal suffering, but it would be better to perform these experiments without animal suffering. This project seeks to develop strains of mice that experience far less pain and suffering than current animals but are equally suited to laboratory and medical research. If successful, widespread adoption of these mice could drastically reduce the total amount of animal suffering in laboratories worldwide.</p>",,--Choose Location,2016-12-05 00:17:21.835,True,2016-01-01,Reducing Suffering in Laboratory Animals,LAB,,True,Sculpting Evolution,False
computer-assisted-transgenesis,esvelt,False,"<p>This is a new platform to automate experiments in genetic engineering and bring large-scale moonshot projects within reach. Too often, lab experiments are limited in scale by human fatigue and costs associated with manual labor. In particular, the process of delivering genetic materials via manual microinjection remains a long-standing bottleneck. We are developing a computer-assisted microinjection platform to streamline the production of transgenic organisms. Briefly, organisms are immobilized in a gel and microinjections are performed using precision robotics using computer vision algorithms. This platform demonstrated high-throughput gene editing in an animal model (C. elegans) for the first time. We will use this technology to refine and create safeguards for our gene drive technology.</p>",,--Choose Location,2016-12-05 00:17:09.401,True,2016-01-01,Computer-Assisted Transgenesis,PUBLIC,,True,Sculpting Evolution,False
reducing-suffering-in-laboratory-animals,esvelt,False,"<p>The world uses an estimated 20 million mice in laboratory research experiments each year. These experiments are monitored and regulated to protect animal welfare whenever possible. However, analgesics cannot completely eliminate suffering, and many studies cannot use opiates or anti-inflammatory drugs because they would interfere with the biological process being studied. The benefits of animal research may outweigh the cost in animal suffering, but it would be better to perform these experiments without animal suffering. This project seeks to develop strains of mice that experience far less pain and suffering than current animals, but that are equally suited to laboratory and medical research. If successful, widespread adoption of these mice could drastically reduce animal suffering in laboratories worldwide.</p>",,--Choose Location,2017-10-03 18:17:03.994,True,2016-01-01,Reducing Suffering in Laboratory Animals,PUBLIC,,True,Sculpting Evolution,False
gene-editing-and-biomedical-ethics,esvelt,False,"<p>The initiative, working with George Church, professor of genetics at Harvard Medical School, and Kevin Esvelt, head of the Sculpting Evolution Group &nbsp;at the Media Lab, will explore the broader ethical dimensions for developing tools that involve CRISPR and the expansion of gene editing technologies. This work aims to create a dialog between scientists and citizens, and will include spiritual leaders, religious leaders, and community leaders in a wider conversation about the ethical implications and potential repercussions of the introduction and deployment of these emerging technological interventions. This project is exploring novel ways to inform citizens about science and how they can affect policy based on news scientific developments.
                    
                </p>",,,2017-05-30 15:47:22.536,True,2017-05-01,Gene Editing and Biomedical Ethics,PUBLIC,,True,Sculpting Evolution,False
sculpting-evolution-test-project-joanna-tuesday,esvelt,False,<p>test placeholder text</p>,,,2017-05-01 19:55:53.618,True,2017-03-01,Sculpting Evolution test project Joanna/Tuesday,LAB,,True,Sculpting Evolution,False
engineering-microbial-ecosystems,esvelt,False,"<p>We are developing methods of controlling the genetic and cellular composition of microbial communities in the gut. Stably colonized microbes could be engineered to sense disease, resist pathogen invasion, and release appropriate therapeutics in situ.</p><p><br></p>",,--Choose Location,2017-07-11 00:30:42.637,True,2016-01-01,Engineering Microbial Ecosystems,PUBLIC,,True,Sculpting Evolution,False
understanding-molecular-evolution,esvelt,False,"<p>Humanity has harnessed evolution to sculpt domesticated animals, crops, and molecules, but the process remains a black box. Which combinations of evolutionary parameters will enable us to discover the best solutions? We plan to answer this question by performing massively parallel directed evolution experiments. Our system will use phage-assisted continuous evolution (PACE), a method of building synthetic ecosystems in which billions of fast-replicating viruses compete to optimize a molecular function of our choice. We are developing methods of running many experiments in parallel, each with real-time fitness monitoring and customized evolutionary conditions such as mutation rate, selection stringency, and evolutionary goal-switching. We will use these methods to systematically characterize the relationship between evolutionary parameters and outcomes. </p>",,--Choose Location,2017-03-23 17:11:16.348,True,2016-01-01,Understanding Molecular Evolution,PUBLIC,,True,Sculpting Evolution,False
daisydrives,esvelt,False,"<p>Who should decide whether, when, and how to alter the environment? These are hard questions, especially when the decisions will impact people in many different communities or nations. Daisy drive systems may help by empowering local communities to make decisions concerning their local environments without imposing them on anyone else.<br></p><p>The problem with current CRISPR-based gene drive systems is that they can spread indefinitely—potentially affecting every population of the target species throughout the world. It's unclear how such ""global"" drives can be safely tested, much less whether nations will ever agree to use them.&nbsp;To return power to the hands of local communities, we devised a new form of drive system called a ""daisy drive"" that can only affect local environments. The trick was to teach DNA to count&nbsp;and limit gene drive spreading to a pre-programmed number of generations.&nbsp;We hope that daisy drives will simplify decision-making and promote responsible use by allowing local communities to decide how to solve their own ecological problems.</p>",,,2019-04-03 16:07:53.609,True,2016-06-06,Daisy Drives,PUBLIC,http://www.sculptingevolution.org/daisydrives,True,Sculpting Evolution,False
deep-reality,richer,False,"<p>We present an interactive virtual reality (VR) experience&nbsp;that uses biometric information for reflection and relaxation.&nbsp;We monitor in real-time brain activity using a modified version&nbsp;of the Muse EEG and track heart rate (HR) and electro&nbsp;dermal activity (EDA) using an Empatica E4 wristband. We&nbsp;use this data to procedurally generate 3D creatures and&nbsp;change the lighting of the environment to reflect the internal&nbsp;state of the viewer in a set of visuals depicting an underwater&nbsp;audiovisual composition. These 3D creatures are&nbsp;created to unconsciously influence the body signals of the&nbsp;observer via subtle pulses of light, movement and sound.&nbsp;We aim to decrease heart rate and respiration by subtle,&nbsp;almost imperceptible light flickering, sound pulsations and&nbsp;slow movements of these creatures to increase relaxation.</p>",,,2019-05-09 19:18:17.333,True,2017-04-01,"Deep Reality: An underwater VR experience to promote relaxation by unconscious HR, EDA and brain activity biofeedback",PUBLIC,,True,Responsive Environments,False
mediated-atmospheres,richer,False,"<h2>The Mediated Atmosphere project envisions a smart office that is capable of dynamically transforming itself to enhance occupants' work experience.</h2><p>In the knowledge economy, worker satisfaction is paramount to retention and productivity. Recent studies have identified a decline in workplace satisfaction. Our research demonstrates how Mediated Atmosphere address this growing need.&nbsp;We created a workspace prototype equipped with a modular real-time control infrastructure, integrating biosignal sensors, controllable lighting, projection, and sound.</p>",,,2018-07-16 12:52:08.873,True,2015-09-01,Mediated Atmosphere,PUBLIC,,True,Responsive Environments,False
masca,irmandy,False,"<p>Masca is a flexible mask for sleep stage detection. Our device adapts to the human body using conformable piezoresistive fabric and silicone, enabling eyelid motion detection in a comfortable, affordable form factor.&nbsp; <a href=""https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.1995.tb03410.x"">Eyes and eyelids change movement frequency predictably</a>&nbsp;as sleep stage transitions occur, allowing for a simpler, more portable system than the typical high-density EEG required for sleep tracking. Tracking and influencing of sleep cognition opens up doors to targeted reactivation of daytime experience during sleep: a future in which the consolidation of emotion, memory, and learning in sleep is rendered controllable by wearable electronics.&nbsp;</p><p>This device is modeled after Prof. Robert Stickgold's <a href=""https://www.sciencedirect.com/science/article/pii/S1053810084710026"">Nightcap</a>, and we are grateful for his ongoing assistance with this project, aiming at further extending the benefits of sleep neuroscience.<br></p>",,,2019-03-26 13:32:12.436,True,2018-04-01,Masca,PUBLIC,,True,Responsive Environments,False
FabricKeyboard,irmandy,False,"<h2>Multimodal textile sensate media as an expressive and deformable musical interface</h2><p>In the area of intelligent textiles, we are exploring a multi-modal, fabric-based, stretchable sensate surface for physical interaction media, specifically as&nbsp;deformable musical interface.&nbsp;</p><p>The fabric keyboard consists of multi-layer textile sensors machine-sewn in a keyboard pattern, and it detects different stimuli such as touch, pressure, stretch, proximity, and electric field. This allows users to explore physical and non-contact gestures for expressive on-body and on-surface musical performance. We've also developed additional textile-based inputs such as ribbon controller, trackpad, and fur for more expressive control. This soft sensate surface contributes toward developing seamless, self-aware, and washable media.</p>",,,2018-01-04 23:22:07.671,True,2016-05-27,FabricKeyboard,PUBLIC,,True,Responsive Environments,False
perform-1,irmandy,False,"<p>PerForm explores the intuitive meanings associated with the shape of objects, and how a shape-changing tool can allow for different forms of tangible interaction.&nbsp;Is it possible to map how ideas feel, and use the connections between senses to create more intuitive interfaces?&nbsp;PerForm addresses that question by allowing users to transform a physical tool&nbsp;to fit their intentions. This way, a user can play different musical instruments or take different actions in games, simply by varying the shape of the tool. Since the meanings associated with the shapes would be dependent on context, we are giving special focus to studying possible mappings of between the perception of sound and shape.</p><p><b>SOUND-SHAPE CORRESPONDENCES</b></p><p><b>“<i>Music is not limited to the world of sound. There exists a music of the visual world.</i>”</b></p><p><b>—Oskar Fischinger, 1951.</b></p><p>When the German-American animator and filmmaker Oskar Fischinger created musically inspired animations and works of art, he touched on the intuitive associations our minds make between all the different sensory stimuli received from the environment.&nbsp;There is strong evidence that our brains forge relationships between shapes and seemingly corresponding sounds.</p><p>PerForm explores how the associations between visual and auditory perception can be used in interaction design. We developed a physical interface that users can transform by bending to create geometric shapes or symbols.&nbsp;By investigating possible correlations, natural or forged, between perceptual components of shape and its correlates in sound, we enable the&nbsp;tool to become a new instrument, with different sound timbre depending on the geometry of the object.</p><p><b>A SHAPE-SHIFTING GAMING CONTROLLER</b></p><p>One of the applications of this shape-shifting device would be to enable different modes of interaction through changes in shape. Instead of having to buy multiple controller devices for each genre of gaming or kind of interaction, or simply using a single, fixed-form controller that limits the embodied experience, a device capable of transformation would enable users to have a more imaginative and creative gaming experience, even enabling new kinds of games in which the user can invent tools by varying shapes.</p>",2019-09-01,,2019-04-18 14:46:14.807,True,2018-05-01,PerForm,PUBLIC,,True,Responsive Environments,False
masca,adamjhh,False,"<p>Masca is a flexible mask for sleep stage detection. Our device adapts to the human body using conformable piezoresistive fabric and silicone, enabling eyelid motion detection in a comfortable, affordable form factor.&nbsp; <a href=""https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.1995.tb03410.x"">Eyes and eyelids change movement frequency predictably</a>&nbsp;as sleep stage transitions occur, allowing for a simpler, more portable system than the typical high-density EEG required for sleep tracking. Tracking and influencing of sleep cognition opens up doors to targeted reactivation of daytime experience during sleep: a future in which the consolidation of emotion, memory, and learning in sleep is rendered controllable by wearable electronics.&nbsp;</p><p>This device is modeled after Prof. Robert Stickgold's <a href=""https://www.sciencedirect.com/science/article/pii/S1053810084710026"">Nightcap</a>, and we are grateful for his ongoing assistance with this project, aiming at further extending the benefits of sleep neuroscience.<br></p>",,,2019-03-26 13:32:12.436,True,2018-04-01,Masca,PUBLIC,,True,Fluid Interfaces,False
frisson,adamjhh,False,"<p>This project unites embodied cognition and on-body device design to ask questions about the origin of emotions and the potential to hack our brains and behavior by hacking the body.&nbsp;</p><p>Frisson, also known as aesthetic chills, are the wave of chills you get while experience peak emotional moments in a song or peak meaning-making moments in a speech.&nbsp;&nbsp;At once transcendent and physiological, the subtle signals of beauty and semantics meet mechanism as the sublime literally cascades across skin. Conveniently, frisson seems to be an<a href=""https://www.researchgate.net/publication/227131588_Aesthetic_Chills_as_a_Universal_Marker_of_Openness_to_Experience"">&nbsp;almost universal marker</a>&nbsp;of peak emotional experiences across a wide range of cultures and continents.&nbsp;Embodied cognition has done much to illuminate links between our physical experience and our psychological experience, and studies on<a href=""https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2420110403"">&nbsp;misattribution of arousal</a>&nbsp;show us we can drive cognition by driving physical sensation. This points to opportunities: if we can drive frisson, perhaps we can also drive the openness to experience, relief in stress, increase in empathy, and experience of meaning that frisson has been tied to in past research.</p><p>So we built a device to trigger frisson.&nbsp;Alongside&nbsp;<a href=""https://www.researchgate.net/publication/305892666_Aesthetic_Chills_Knowledge-Acquisition_Meaning-Making_and_Aesthetic_Emotions"">Félix Schoeller</a>, first we're testing whether inducing chills can increase&nbsp;<a href=""http://journals.sagepub.com/doi/abs/10.1177/0305735615572358"">deep attention and openness to experiences.&nbsp;</a>&nbsp;Psychophysiology driving thought from the spine upwards!</p>",,,2019-03-26 13:36:01.178,True,2019-01-01,Frisson,PUBLIC,,True,Fluid Interfaces,False
sleep-creativity,adamjhh,False,"<p>Sleep is a forgotten country of the mind. A vast majority of our technologies are built for our waking state, even though a third of our lives are spent asleep. Current technological interfaces miss an opportunity to access the unique, imaginative, elastic cognition ongoing during dreams and semi-lucid states. In turn, each of us misses an opportunity to use interfaces to influence our own processes of memory consolidation, creative insight generation, gist extraction, and emotion regulation that are so deeply sleep-dependent.&nbsp;In this project, we explore ways to augment human creativity by extending, influencing, and capturing dreams in stage-1 sleep. It is currently impossible to force ourselves to be creative because so much creative idea association and creative incubation happens in the absence of executive control and directed attention. Sleep offers an opportunity for prompting creative thought in the absence of directed attention, if only dreams can be controlled.</p><p>During sleep onset, a window of opportunity arises in the form of hypnagogia, a semi-lucid sleep state where we all begin dreaming before we fall fully unconscious. Hypnagogia is characterized by phenomenological unpredictability, distorted perception of space and time, and spontaneous, fluid idea association. Edison, Tesla, Poe, and Dalí each accessed this state by napping with a steel ball in hand to capture creative ideas generated in hypnagogic microdreams when it dropped to the floor below.</p><p>In this project we modernize this technique, using an interactive social robot accompanied with an EEG system, muscular sleep stage tracking system, and auditory biofeedback. We are able to influence, extract information from, and extend hypnagogic microdreams for the first time: we found that active use of hypnagogia with the system can augment human creativity. This system enables future research into sleep, an underutilized and understudied state of mind vital for memory, learning, and creativity.</p><p>This work has been hugely collaborative. The following people, in alphabetical order by first name, have all made it possible: Abhinandan Jain, Eyal Perry, Ishaan Grover, Matthew Ha, Oscar Rosello, Pedro Reynolds-Cuéllar, Robert Stickgold, and Tomás Vega. For an in depth dive, see the FAQ below and see more on <a href=""http://adamjhh.com/dormio"">this website</a>.</p><br>",,,2019-03-26 14:01:38.378,True,2017-09-01,Dormio: Interfacing with Dreams,PUBLIC,http://www.adamjhh.com/,True,Fluid Interfaces,False
the-blank-canvas,adamjhh,False,"<p>The Blank Canvas directs immersion inwards using virtual reality, augmenting awareness of the microscopic worlds inside each of us and the science that is changing them today. It has been shown at Cannes Film Festival, Vision Summit, VR Sci Fest and the World Economic Forum.</p><p><a href=""https://www.youtube.com/watch?v=wo7iz8UgkYI"">This is the first episode of <i>The Blank Canvas</i></a>, a VR platform that showcases the future of science and scientific communication. So many of the brilliant contemporary innovations in science are lost to the general public because they happen at scales so small we can barely comprehend them. The Blank Canvas leverages the power of immersive technologies &nbsp;to make these ideas come to life in macro planetary scale, explaining themes like DNA editing, hacked viruses and CRISPR. We build collaborations between scientists and engineers for accurate, inspirational science storytelling that turns textbooks into experience.&nbsp;</p>",,,2018-10-20 17:52:17.331,True,2017-01-01,The Blank Canvas,PUBLIC,http://www.adamjhh.com/,True,Fluid Interfaces,False
engineering-dreams,adamjhh,False,"<p>Our dream is a sci-fi future, where dreams are controllable.&nbsp;</p><p>We are working to build technology that interfaces with the sleeping mind.&nbsp;As the dreamer descends into sleep, we track different sleep-stages using brain activity, muscle tension, heart rate, and movement data.&nbsp;External stimuli in the form of scent, audio, and muscle stimulation affect the content of the dreams. We are working on integrating multiple projects&nbsp; developed at the Fluid Interfaces group towards a vision where sleep is controllable.</p><p><a href=""http://dreams.media.mit.edu"">dreams.media.mit.edu</a><br></p>",,,2019-04-30 15:12:45.024,True,2018-03-21,Engineering Dreams,PUBLIC,,True,Fluid Interfaces,False
masca,rosello,False,"<p>Masca is a flexible mask for sleep stage detection. Our device adapts to the human body using conformable piezoresistive fabric and silicone, enabling eyelid motion detection in a comfortable, affordable form factor.&nbsp; <a href=""https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.1995.tb03410.x"">Eyes and eyelids change movement frequency predictably</a>&nbsp;as sleep stage transitions occur, allowing for a simpler, more portable system than the typical high-density EEG required for sleep tracking. Tracking and influencing of sleep cognition opens up doors to targeted reactivation of daytime experience during sleep: a future in which the consolidation of emotion, memory, and learning in sleep is rendered controllable by wearable electronics.&nbsp;</p><p>This device is modeled after Prof. Robert Stickgold's <a href=""https://www.sciencedirect.com/science/article/pii/S1053810084710026"">Nightcap</a>, and we are grateful for his ongoing assistance with this project, aiming at further extending the benefits of sleep neuroscience.<br></p>",,,2019-03-26 13:32:12.436,True,2018-04-01,Masca,PUBLIC,,True,Fluid Interfaces,False
sleep-creativity,rosello,False,"<p>Sleep is a forgotten country of the mind. A vast majority of our technologies are built for our waking state, even though a third of our lives are spent asleep. Current technological interfaces miss an opportunity to access the unique, imaginative, elastic cognition ongoing during dreams and semi-lucid states. In turn, each of us misses an opportunity to use interfaces to influence our own processes of memory consolidation, creative insight generation, gist extraction, and emotion regulation that are so deeply sleep-dependent.&nbsp;In this project, we explore ways to augment human creativity by extending, influencing, and capturing dreams in stage-1 sleep. It is currently impossible to force ourselves to be creative because so much creative idea association and creative incubation happens in the absence of executive control and directed attention. Sleep offers an opportunity for prompting creative thought in the absence of directed attention, if only dreams can be controlled.</p><p>During sleep onset, a window of opportunity arises in the form of hypnagogia, a semi-lucid sleep state where we all begin dreaming before we fall fully unconscious. Hypnagogia is characterized by phenomenological unpredictability, distorted perception of space and time, and spontaneous, fluid idea association. Edison, Tesla, Poe, and Dalí each accessed this state by napping with a steel ball in hand to capture creative ideas generated in hypnagogic microdreams when it dropped to the floor below.</p><p>In this project we modernize this technique, using an interactive social robot accompanied with an EEG system, muscular sleep stage tracking system, and auditory biofeedback. We are able to influence, extract information from, and extend hypnagogic microdreams for the first time: we found that active use of hypnagogia with the system can augment human creativity. This system enables future research into sleep, an underutilized and understudied state of mind vital for memory, learning, and creativity.</p><p>This work has been hugely collaborative. The following people, in alphabetical order by first name, have all made it possible: Abhinandan Jain, Eyal Perry, Ishaan Grover, Matthew Ha, Oscar Rosello, Pedro Reynolds-Cuéllar, Robert Stickgold, and Tomás Vega. For an in depth dive, see the FAQ below and see more on <a href=""http://adamjhh.com/dormio"">this website</a>.</p><br>",,,2019-03-26 14:01:38.378,True,2017-09-01,Dormio: Interfacing with Dreams,PUBLIC,http://www.adamjhh.com/,True,Fluid Interfaces,False
heartbit,rosello,False,"<p>HeartBit is an interface designed for haptic heart rate biofeedback. A handheld heart beats alongside your own, mirroring the size, weight, and movement of a hidden internal organ, now external and tangible in real-time. HeartBit offers a medium for users to self-regulate in moments of stress, anxiety or exertion: Control your heart to control your breath and body—for relaxation, performance enhancement, or augmented self-awareness.</p>",,,2019-04-18 17:08:14.374,True,2018-05-01,HeartBit,PUBLIC,,True,Fluid Interfaces,False
nevermind,rosello,False,"<p>NeverMind is an interface and application designed to support human memory. We combine the memory palace memorization method with augmented reality technology to create a tool to help anyone memorize more effectively. Early experiments conducted with a prototype of NeverMind suggest that the long-term memory recall accuracy of sequences of items is nearly tripled compared to paper-based memorization tasks. With this project, we hope to make the memory palace method accessible to novices and demonstrate one way augmented reality can support learning.</p>",,,2019-04-18 17:07:28.479,True,2016-09-01,NeverMind: Using AR for memorization,PUBLIC,,True,Fluid Interfaces,False
wearable-biotech,rosello,False,"<p>Wearable Biocomputer explores the intersection of wearable computation and biological computation. We designed on-body interfaces for culturing genetically engineered bacteria to sense, process, and actuate.&nbsp;</p>",,,2019-04-29 15:14:59.273,True,2019-04-27,Wearable Biocomputer,LAB,,True,Fluid Interfaces,False
Biological-Enhancement,rosello,False,"<h2><b>Lab on Body, Synthetic Biology, and Bio-Digital Systems for Health and Human Enhancement</b></h2>",,,2019-05-10 15:15:39.601,True,2019-02-03,Theme | Wearable Biotech Enhancement,PUBLIC,,True,Fluid Interfaces,False
engineering-dreams,rosello,False,"<p>Our dream is a sci-fi future, where dreams are controllable.&nbsp;</p><p>We are working to build technology that interfaces with the sleeping mind.&nbsp;As the dreamer descends into sleep, we track different sleep-stages using brain activity, muscle tension, heart rate, and movement data.&nbsp;External stimuli in the form of scent, audio, and muscle stimulation affect the content of the dreams. We are working on integrating multiple projects&nbsp; developed at the Fluid Interfaces group towards a vision where sleep is controllable.</p><p><a href=""http://dreams.media.mit.edu"">dreams.media.mit.edu</a><br></p>",,,2019-04-30 15:12:45.024,True,2018-03-21,Engineering Dreams,PUBLIC,,True,Fluid Interfaces,False
mod,lukeji,False,"<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>’s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>",,,2019-04-23 14:03:41.132,True,2017-08-01,Theme   |   Mobility On-Demand,PUBLIC,,True,City Science,True
hmi,lukeji,False,"<h2><i>Facilitating coexistence, trust-building, and collaboration among people and machines.</i></h2><p>New modes of 21st century urban transportation are becoming increasingly lightweight, electrified, connected, shared, and autonomous. Cohabitation of humans and machines is an increasingly important question, and one which requires careful attention and design.&nbsp; We strive to enable new forms of human-machine co-existence, trust, and collaboration.</p><h2>This work focuses on enabling:</h2><ol><li>Intuitive and effective two-way communication between vehicles and pedestrians;</li><li>Street safety and traffic-yielding mechanisms; and</li><li>Behavior change related to the adoption of active mobility mode, or electric assist.</li></ol>",,,2019-04-17 19:49:25.786,True,2016-11-01,Human-machine cooperation (HMC) for lightweight autonomous robots,PUBLIC,,True,City Science,True
pev,lukeji,False,"<h1><b>An Alternative Autonomous Revolution&nbsp;</b></h1><h2><i>System design for emerging urban contexts and societal aspirations</i></h2><p>The Persuasive Electric Vehicle (PEV) aims to solve urban mobility challenges with a healthy, convenient, sustainable alternative to cars. The PEV is a low-cost, agile, shared-use autonomous bike that can be either an electrically assisted tricycle for passenger commuting or an autonomous carrier for package delivery.</p><p>The PEV uses standard bicycle components and is lightweight (&lt;50kg) yet robust. Its sensors are easy to reconfigure and it has a 250W mid-drive electric motor and 10Ah battery pack that provides 25 miles of travel per charge and a top speed of 20 miles per hour.</p><p>Our vision for the PEV: a rider summons the PEV through a phone app, and the nearest available PEV arrives autonomously to meet the rider. Upon completing the trip, the PEV simply moves on to its next passenger or package pickup.&nbsp; The PEV can be autonomous, operated by the rider, or provide the rider with an electric assist. PEV's operate in bike lanes, avoiding the congestion and adding incentives to make more bikeable cities.</p>",,--Choose Location,2018-07-19 21:12:27.919,True,2014-09-01,Persuasive Electric Vehicle (PEV),PUBLIC,,True,City Science,True
torque,lukeji,False,"<h1>Open-Source Autonomous Platform for Educational and Service Design Applications</h1><h2><i>How can new technologies respond to society’s diverse industrial, socio-economic, and educational needs?</i></h2><p>Despite AI and robotics being widely trumpeted as keys to the new Industrial Revolution, access to their development remains largely restricted to companies and institutions that are rich in capital and/or data, potentially further deepening the socio-economic disparity observed across continents. As a likely result, these new technologies generate limited positive externalities. For instance, are automobiles really the most critical area in need of self-driving technology? Where else might AI and robotics be applied to lead to increased urban livability, socioeconomic equity, and the vibrancy of local businesses?</p><p>Building upon the architecture of MIT’s open-source race car platform, the City Science group introduces&nbsp;a new open-ended and heavy-duty self-driving platform.&nbsp;&nbsp;Torque is intended to be used by educators and makers and is ideal for hackathons and classroom instruction. Torque will soon allow rapid prototyping of usage scenarios and services for various contexts and needs.&nbsp;</p>",,,2018-05-04 15:23:33.802,True,2017-07-01,Torque,PUBLIC,,True,City Science,True
mod,cq_zhang,False,"<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>’s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>",,,2019-04-23 14:03:41.132,True,2017-08-01,Theme   |   Mobility On-Demand,PUBLIC,,True,City Science,True
hmi,cq_zhang,False,"<h2><i>Facilitating coexistence, trust-building, and collaboration among people and machines.</i></h2><p>New modes of 21st century urban transportation are becoming increasingly lightweight, electrified, connected, shared, and autonomous. Cohabitation of humans and machines is an increasingly important question, and one which requires careful attention and design.&nbsp; We strive to enable new forms of human-machine co-existence, trust, and collaboration.</p><h2>This work focuses on enabling:</h2><ol><li>Intuitive and effective two-way communication between vehicles and pedestrians;</li><li>Street safety and traffic-yielding mechanisms; and</li><li>Behavior change related to the adoption of active mobility mode, or electric assist.</li></ol>",,,2019-04-17 19:49:25.786,True,2016-11-01,Human-machine cooperation (HMC) for lightweight autonomous robots,PUBLIC,,True,City Science,True
pev,cq_zhang,False,"<h1><b>An Alternative Autonomous Revolution&nbsp;</b></h1><h2><i>System design for emerging urban contexts and societal aspirations</i></h2><p>The Persuasive Electric Vehicle (PEV) aims to solve urban mobility challenges with a healthy, convenient, sustainable alternative to cars. The PEV is a low-cost, agile, shared-use autonomous bike that can be either an electrically assisted tricycle for passenger commuting or an autonomous carrier for package delivery.</p><p>The PEV uses standard bicycle components and is lightweight (&lt;50kg) yet robust. Its sensors are easy to reconfigure and it has a 250W mid-drive electric motor and 10Ah battery pack that provides 25 miles of travel per charge and a top speed of 20 miles per hour.</p><p>Our vision for the PEV: a rider summons the PEV through a phone app, and the nearest available PEV arrives autonomously to meet the rider. Upon completing the trip, the PEV simply moves on to its next passenger or package pickup.&nbsp; The PEV can be autonomous, operated by the rider, or provide the rider with an electric assist. PEV's operate in bike lanes, avoiding the congestion and adding incentives to make more bikeable cities.</p>",,--Choose Location,2018-07-19 21:12:27.919,True,2014-09-01,Persuasive Electric Vehicle (PEV),PUBLIC,,True,City Science,True
torque,cq_zhang,False,"<h1>Open-Source Autonomous Platform for Educational and Service Design Applications</h1><h2><i>How can new technologies respond to society’s diverse industrial, socio-economic, and educational needs?</i></h2><p>Despite AI and robotics being widely trumpeted as keys to the new Industrial Revolution, access to their development remains largely restricted to companies and institutions that are rich in capital and/or data, potentially further deepening the socio-economic disparity observed across continents. As a likely result, these new technologies generate limited positive externalities. For instance, are automobiles really the most critical area in need of self-driving technology? Where else might AI and robotics be applied to lead to increased urban livability, socioeconomic equity, and the vibrancy of local businesses?</p><p>Building upon the architecture of MIT’s open-source race car platform, the City Science group introduces&nbsp;a new open-ended and heavy-duty self-driving platform.&nbsp;&nbsp;Torque is intended to be used by educators and makers and is ideal for hackathons and classroom instruction. Torque will soon allow rapid prototyping of usage scenarios and services for various contexts and needs.&nbsp;</p>",,,2018-05-04 15:23:33.802,True,2017-07-01,Torque,PUBLIC,,True,City Science,True
mod,jerryao,False,"<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>’s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>",,,2019-04-23 14:03:41.132,True,2017-08-01,Theme   |   Mobility On-Demand,PUBLIC,,True,City Science,False
hmi,jerryao,False,"<h2><i>Facilitating coexistence, trust-building, and collaboration among people and machines.</i></h2><p>New modes of 21st century urban transportation are becoming increasingly lightweight, electrified, connected, shared, and autonomous. Cohabitation of humans and machines is an increasingly important question, and one which requires careful attention and design.&nbsp; We strive to enable new forms of human-machine co-existence, trust, and collaboration.</p><h2>This work focuses on enabling:</h2><ol><li>Intuitive and effective two-way communication between vehicles and pedestrians;</li><li>Street safety and traffic-yielding mechanisms; and</li><li>Behavior change related to the adoption of active mobility mode, or electric assist.</li></ol>",,,2019-04-17 19:49:25.786,True,2016-11-01,Human-machine cooperation (HMC) for lightweight autonomous robots,PUBLIC,,True,City Science,False
mod,yagol,False,"<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>’s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>",,,2019-04-23 14:03:41.132,True,2017-08-01,Theme   |   Mobility On-Demand,PUBLIC,,True,City Science,False
hmi,yagol,False,"<h2><i>Facilitating coexistence, trust-building, and collaboration among people and machines.</i></h2><p>New modes of 21st century urban transportation are becoming increasingly lightweight, electrified, connected, shared, and autonomous. Cohabitation of humans and machines is an increasingly important question, and one which requires careful attention and design.&nbsp; We strive to enable new forms of human-machine co-existence, trust, and collaboration.</p><h2>This work focuses on enabling:</h2><ol><li>Intuitive and effective two-way communication between vehicles and pedestrians;</li><li>Street safety and traffic-yielding mechanisms; and</li><li>Behavior change related to the adoption of active mobility mode, or electric assist.</li></ol>",,,2019-04-17 19:49:25.786,True,2016-11-01,Human-machine cooperation (HMC) for lightweight autonomous robots,PUBLIC,,True,City Science,False
mod,cassiano,False,"<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>’s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>",,,2019-04-23 14:03:41.132,True,2017-08-01,Theme   |   Mobility On-Demand,PUBLIC,,True,City Science,False
hmi,cassiano,False,"<h2><i>Facilitating coexistence, trust-building, and collaboration among people and machines.</i></h2><p>New modes of 21st century urban transportation are becoming increasingly lightweight, electrified, connected, shared, and autonomous. Cohabitation of humans and machines is an increasingly important question, and one which requires careful attention and design.&nbsp; We strive to enable new forms of human-machine co-existence, trust, and collaboration.</p><h2>This work focuses on enabling:</h2><ol><li>Intuitive and effective two-way communication between vehicles and pedestrians;</li><li>Street safety and traffic-yielding mechanisms; and</li><li>Behavior change related to the adoption of active mobility mode, or electric assist.</li></ol>",,,2019-04-17 19:49:25.786,True,2016-11-01,Human-machine cooperation (HMC) for lightweight autonomous robots,PUBLIC,,True,City Science,False
termites,cassiano,False,"<p>TerMITes are wireless environmental sensors that capture data to help us better understand our environments and human behavior. The sensor data is time-stamped and place-tagged, but otherwise hardware agnostic. TerMITes support multi-modal sensor attachments using common protocols and can be attached to objects in the home such as doors, windows, drawers, cabinets, tables, and chairs to register object usage. TerMITes directly log on to the Internet via low-power Wi-Fi for ease of connection and automatically upload&nbsp;to a centralized database. TerMITes bridge existing methods for qualitative inquiry about our experiences in various planes to quantitative recording based on sensor input. TerMITes are currently used to gather data on humidity, presence detection, ambient light, motion, carbon dioxide, and temperature.&nbsp;</p>",,,2019-04-08 16:56:32.037,True,2017-09-01,TerMITes,PUBLIC,http://termites.synthetic.space/,True,City Science,False
escape-pod-1,cassiano,False,"<p>The esc-Pod&nbsp; (or Escape Pod) is an exploratory platform for researchers investigating moments of refuge within our bustling work lives.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">The core of the esc-Pod consists of actuated work and rest surfaces. This allows for moments of productivity and relaxation to occur within a single space.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The outer skin provides variable transparency, enabling a spectrum of visibility settings according to privacy requirements.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">The inner skin provides an infrastructure for the modulation of spatial experiences. Each panel is a pixel, connecting itself to the skin network, and can embody an array of senses.</span></p>",,,2019-04-08 17:01:14.555,True,2016-08-01,Escape Pod,PUBLIC,,True,City Science,False
mod,taiyu,False,"<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>’s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>",,,2019-04-23 14:03:41.132,True,2017-08-01,Theme   |   Mobility On-Demand,PUBLIC,,True,City Science,False
pev,taiyu,False,"<h1><b>An Alternative Autonomous Revolution&nbsp;</b></h1><h2><i>System design for emerging urban contexts and societal aspirations</i></h2><p>The Persuasive Electric Vehicle (PEV) aims to solve urban mobility challenges with a healthy, convenient, sustainable alternative to cars. The PEV is a low-cost, agile, shared-use autonomous bike that can be either an electrically assisted tricycle for passenger commuting or an autonomous carrier for package delivery.</p><p>The PEV uses standard bicycle components and is lightweight (&lt;50kg) yet robust. Its sensors are easy to reconfigure and it has a 250W mid-drive electric motor and 10Ah battery pack that provides 25 miles of travel per charge and a top speed of 20 miles per hour.</p><p>Our vision for the PEV: a rider summons the PEV through a phone app, and the nearest available PEV arrives autonomously to meet the rider. Upon completing the trip, the PEV simply moves on to its next passenger or package pickup.&nbsp; The PEV can be autonomous, operated by the rider, or provide the rider with an electric assist. PEV's operate in bike lanes, avoiding the congestion and adding incentives to make more bikeable cities.</p>",,--Choose Location,2018-07-19 21:12:27.919,True,2014-09-01,Persuasive Electric Vehicle (PEV),PUBLIC,,True,City Science,False
smart-infra,taiyu,False,"<h1>Scalable Urban Infrastructure for Human-Machine Cohabitation</h1><h2><i>New infrastructure to help sustain public-sector participation and operation, and maximize public interest and safety.</i></h2><p>Advancements in autonomous technology have led automobile makers and tech companies to focus on reinventing the automobile—increasing computational capability and enhancing sensor systems. But due to strict road-safety regulations, this vehicle-centric, inside-out approach may take years to materialize, and when it does, restricting “autonomy” to selected vehicles will limit autonomy’s impact on street safety and accessibility.</p><p>To address current issues, The City Science group focuses on ways to offload often-heavy computational requirements from the vehicle through affordable interventions to street infrastructure by creating human-machine readable traffic signs and urban markers.</p><p>With the support of a new genre of smart urban infrastructure, we believe this “autonomy-lite” approach will soon allow lightweight autonomous vehicles to be widely deployed and navigate smoothly in most urban environments.&nbsp;</p>",,,2018-03-28 22:18:15.132,True,2017-05-01,Urban Tattoo,PUBLIC,,True,City Science,False
mod,abhia,False,"<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>’s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>",,,2019-04-23 14:03:41.132,True,2017-08-01,Theme   |   Mobility On-Demand,PUBLIC,,True,City Science,False
pev,abhia,False,"<h1><b>An Alternative Autonomous Revolution&nbsp;</b></h1><h2><i>System design for emerging urban contexts and societal aspirations</i></h2><p>The Persuasive Electric Vehicle (PEV) aims to solve urban mobility challenges with a healthy, convenient, sustainable alternative to cars. The PEV is a low-cost, agile, shared-use autonomous bike that can be either an electrically assisted tricycle for passenger commuting or an autonomous carrier for package delivery.</p><p>The PEV uses standard bicycle components and is lightweight (&lt;50kg) yet robust. Its sensors are easy to reconfigure and it has a 250W mid-drive electric motor and 10Ah battery pack that provides 25 miles of travel per charge and a top speed of 20 miles per hour.</p><p>Our vision for the PEV: a rider summons the PEV through a phone app, and the nearest available PEV arrives autonomously to meet the rider. Upon completing the trip, the PEV simply moves on to its next passenger or package pickup.&nbsp; The PEV can be autonomous, operated by the rider, or provide the rider with an electric assist. PEV's operate in bike lanes, avoiding the congestion and adding incentives to make more bikeable cities.</p>",,--Choose Location,2018-07-19 21:12:27.919,True,2014-09-01,Persuasive Electric Vehicle (PEV),PUBLIC,,True,City Science,False
sample-project-for-neri,j_duro,False,<p>fja;sldjfa;boq</p>,,,2016-10-06 17:12:11.589,False,2016-10-06,Sample project for Neri,PUBLIC,,True,Mediated Matter,False
aguahoja,j_duro,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
water-based-additive-manufacturing,j_duro,False,"<p>This research presents water-based robotic fabrication as a design approach and enabling technology for additive manufacturing (AM) of biodegradable hydrogel composites. We focus on expanding the dimensions of the fabrication envelope, developing structural materials for additive deposition, incorporating material-property gradients, and manufacturing architectural-scale biodegradable systems. The technology includes a robotically controlled AM system to produce biodegradable composite objects, combining natural hydrogels with other organic aggregates. It demonstrates the approach by designing, building, and evaluating the mechanics and controls of a multi-chamber extrusion system. Finally, it provides evidence of large-scale composite objects fabricated by our technology that display graded properties and feature sizes ranging from micro- to macro-scale. Fabricated objects may be chemically stabilized or dissolved in water and recycled within minutes. Applications include the fabrication of fully recyclable products or temporary architectural components, such as tent structures with graded mechanical and optical properties.</p>",,--Choose Location,2019-06-04 21:35:42.589,True,2014-01-01,Water-Based Additive Manufacturing,PUBLIC,,True,Mediated Matter,False
synthetic-apiary,j_duro,False,"<p>The Synthetic Apiary proposes a new kind of environment, bridging urban and organismic scales by exploring one of the most important organisms for both the human species and our planet: bees. We explore the cohabitation of humans and other species through the creation of a controlled atmosphere and associated behavioral paradigms. The project facilitates Mediated Matter's ongoing research into biologically augmented digital fabrication with eusocial insect communities in architectural, and possibly urban, scales. Many animal communities in nature present collective behaviors known as ""swarming,"" prioritizing group survival over individuals, and constantly working to achieve a common goal. Often, swarms of organisms are skilled builders; for example, ants can create extremely complex networks by tunneling, and wasps can generate intricate paper nests with materials sourced from local areas.</p>",,--Choose Location,2017-10-13 23:33:55.831,True,2016-01-01,Synthetic Apiary,PUBLIC,,True,Mediated Matter,False
frisson,abyjain,False,"<p>This project unites embodied cognition and on-body device design to ask questions about the origin of emotions and the potential to hack our brains and behavior by hacking the body.&nbsp;</p><p>Frisson, also known as aesthetic chills, are the wave of chills you get while experience peak emotional moments in a song or peak meaning-making moments in a speech.&nbsp;&nbsp;At once transcendent and physiological, the subtle signals of beauty and semantics meet mechanism as the sublime literally cascades across skin. Conveniently, frisson seems to be an<a href=""https://www.researchgate.net/publication/227131588_Aesthetic_Chills_as_a_Universal_Marker_of_Openness_to_Experience"">&nbsp;almost universal marker</a>&nbsp;of peak emotional experiences across a wide range of cultures and continents.&nbsp;Embodied cognition has done much to illuminate links between our physical experience and our psychological experience, and studies on<a href=""https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2420110403"">&nbsp;misattribution of arousal</a>&nbsp;show us we can drive cognition by driving physical sensation. This points to opportunities: if we can drive frisson, perhaps we can also drive the openness to experience, relief in stress, increase in empathy, and experience of meaning that frisson has been tied to in past research.</p><p>So we built a device to trigger frisson.&nbsp;Alongside&nbsp;<a href=""https://www.researchgate.net/publication/305892666_Aesthetic_Chills_Knowledge-Acquisition_Meaning-Making_and_Aesthetic_Emotions"">Félix Schoeller</a>, first we're testing whether inducing chills can increase&nbsp;<a href=""http://journals.sagepub.com/doi/abs/10.1177/0305735615572358"">deep attention and openness to experiences.&nbsp;</a>&nbsp;Psychophysiology driving thought from the spine upwards!</p>",,,2019-03-26 13:36:01.178,True,2019-01-01,Frisson,PUBLIC,,True,Fluid Interfaces,False
wearable-lab-on-body,abyjain,False,"<p>Wearables are being widely researched for monitoring individual's health and wellbeing. Current generation wearable devices sense an individual's physiological data such as heart rate, respiration, electrodermal activity, and EEG,  but lack in sensing their biological counterparts, which drive the majority of an individual's physiological signals. On the other hand, biosensors for detecting biochemical markers are currently limited to one-time use, are non-continuous, and don't provide flexibility in choosing which biomarker they sense. We present ""wearable lab on body,"" a platform for active continuous monitoring of human biomarkers from the biological fluid.&nbsp;<br></p><p><i>To appear in IEEE Engineering for Biology and Medicine Society (EMBC) - Pataranutaporn et. al., 2019</i></p>",,,2019-05-10 04:27:17.379,True,2018-08-30,Wearable Lab on Body,PUBLIC,,True,Fluid Interfaces,False
sleep-creativity,abyjain,False,"<p>Sleep is a forgotten country of the mind. A vast majority of our technologies are built for our waking state, even though a third of our lives are spent asleep. Current technological interfaces miss an opportunity to access the unique, imaginative, elastic cognition ongoing during dreams and semi-lucid states. In turn, each of us misses an opportunity to use interfaces to influence our own processes of memory consolidation, creative insight generation, gist extraction, and emotion regulation that are so deeply sleep-dependent.&nbsp;In this project, we explore ways to augment human creativity by extending, influencing, and capturing dreams in stage-1 sleep. It is currently impossible to force ourselves to be creative because so much creative idea association and creative incubation happens in the absence of executive control and directed attention. Sleep offers an opportunity for prompting creative thought in the absence of directed attention, if only dreams can be controlled.</p><p>During sleep onset, a window of opportunity arises in the form of hypnagogia, a semi-lucid sleep state where we all begin dreaming before we fall fully unconscious. Hypnagogia is characterized by phenomenological unpredictability, distorted perception of space and time, and spontaneous, fluid idea association. Edison, Tesla, Poe, and Dalí each accessed this state by napping with a steel ball in hand to capture creative ideas generated in hypnagogic microdreams when it dropped to the floor below.</p><p>In this project we modernize this technique, using an interactive social robot accompanied with an EEG system, muscular sleep stage tracking system, and auditory biofeedback. We are able to influence, extract information from, and extend hypnagogic microdreams for the first time: we found that active use of hypnagogia with the system can augment human creativity. This system enables future research into sleep, an underutilized and understudied state of mind vital for memory, learning, and creativity.</p><p>This work has been hugely collaborative. The following people, in alphabetical order by first name, have all made it possible: Abhinandan Jain, Eyal Perry, Ishaan Grover, Matthew Ha, Oscar Rosello, Pedro Reynolds-Cuéllar, Robert Stickgold, and Tomás Vega. For an in depth dive, see the FAQ below and see more on <a href=""http://adamjhh.com/dormio"">this website</a>.</p><br>",,,2019-03-26 14:01:38.378,True,2017-09-01,Dormio: Interfacing with Dreams,PUBLIC,http://www.adamjhh.com/,True,Fluid Interfaces,False
move-u,abyjain,False,"<p>MoveU is a wearable vestibular stimulation device for providing proprioceptive haptic feedback in virtual reality (VR).&nbsp; The device induces sensations of motion corresponding to virtual motion, thereby increasing immersion in VR and reducing cybersickness.&nbsp;</p><p>MoveU non-invasively stimulates the vestibular system using a technique called galvanic vestibular stimulation (GVS).&nbsp;GVS is a specific way to elicit vestibular reflexes&nbsp;<span style=""font-size: 18px; font-weight: 400;"">using electrical current&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">that has been used for over a century to study the function of the vestibular system. In addition to GVS, the device supports physiological sensing by connecting heart rate, electrodermal activity, and other sensors&nbsp; using a plug and play mechanism.&nbsp;MoveU supports multiple categories of virtual reality applications with different types of virtual motions such as driving, navigating by flying, teleporting, or riding.&nbsp;</span></p>",,,2019-05-06 20:24:10.520,True,2018-07-15,MoveU,PUBLIC,,True,Fluid Interfaces,False
Biological-Enhancement,abyjain,False,"<h2><b>Lab on Body, Synthetic Biology, and Bio-Digital Systems for Health and Human Enhancement</b></h2>",,,2019-05-10 15:15:39.601,True,2019-02-03,Theme | Wearable Biotech Enhancement,PUBLIC,,True,Fluid Interfaces,False
SensorySynchrony,abyjain,False,"<p>Space and space flight are extreme environments for the human body due to exposure to microgravity and high radiation levels. While the brain is neuroplastic and adapts to different habitats by learning over time, sudden changes in the environment and unpreparedness for it can totally hinder the functioning of the individual.&nbsp;The physiological changes caused by microgravity include vestibular problems causing space motion sickness, bone demineralization, skeletal muscle atrophy, cardiovascular problems, and more.&nbsp;The primary goal of this research project is to investigate vestibular system stimulation techniques to combat motion sickness and create more intuitive experiences when being in non-natural gravity environments.</p><p>Motion sickness is theorized to be either a cause of sensory mismatch between visual and vestibular afferent nerves(inter-sensory) or between semicircular and otolith nerve in the vestibular system (intra-sensory). The magnitude of alteration and the latency between the sensory inputs also contributes to the severeness of the motion sickness.&nbsp;To combat the non-congruent changes in sensory signals while transitioning into space, we propose to investigate vestibular neuromodulation techniques for facilitating adaptation in a more natural way, appeasing the effects of motion sickness and use the altered gravity to create novel experiences in virtual/augmented reality devices.</p><p>We built a prototype for multipole vestibular stimulation for simulating acceleration in roll and pitch axis. The prototype will be tested on the upcoming zero gravity flight for minimizing the effects of alterations between micro and hyper gravity phases.</p>",,,2019-05-31 13:37:03.535,True,2018-09-24,Sensory Synchrony,LAB,,True,Fluid Interfaces,False
openag-flavor-ecology,rebekahj,False,"<p><b>Flavor</b>, in addition to making our food delicious, is one<b>&nbsp;</b>way of <b>sensing biochemical richness</b>. A highly flavorful plant generally contains a greater quantity and diversity of molecules—often with useful functional roles in our own metabolism—than a bland-tasting plant.&nbsp;</p><p>Flavor is a built-in reward for eating plants that has fueled our drive to domesticate and breed a massive biodiversity of vegetation over the last 10,000 years. OpenAg is going deep into the biochemical <b>machinery, evolution, and ecology of plants&nbsp;</b>to make growing food for the optimization of specific chemical profiles (flavor, pharmaceutical properties, nutrition) a reality.</p><p>Plants rely on <b>rich and diverse chemistry&nbsp;</b>for self-defense and stress adaptation. OpenAg is working <b>to induce a plant to synthesize these molecules&nbsp;</b>by adding specific stresses to the plant’s environment and measuring chemical shifts.</p><p>These <b>specialized metabolite molecules&nbsp;</b>can manifest as flavor, pharmaceutical compounds, and rich nutritional profiles. Flavor itself is frequently tied to additional healthful bio-activities for humans—such as <b>vitamins, antioxidants, stimulants, and nutrients.</b></p>",,,2019-04-23 21:12:54.149,True,2017-10-02,"Optimizing plants for flavor, nutrition, and pharmaceutical content",PUBLIC,https://www.linkedin.com/in/johndelaparra/,True,Open Agriculture,False
npcp,rebekahj,False,"<p>Chemical constituents are the most essential components of the fresh produce items consumed by people around the world. That chemistry can be extremely variable based on a number of factors—but wealthy or poor, privileged or vulnerable, we humans are ultimately subject to the destiny meted out by the chemistry of the food we consume.<br></p><p>Indeed, nutritional content, flavor, color, texture, aroma, and even medicinal benefits are dictated by this chemistry, yet there has never been a thorough and comprehensive analysis of that chemistry in fresh produce items.&nbsp;The National Produce Chemotyping Project sets the stage to assess the chemical variability, or chemotypes, of those items. Ultimately this project will lead to:</p><ol><li>Unprecedented studies seeking to correlate specific chemical attributes of food to health and well-being for all people.</li><li>Improved understanding of how the agricultural pipeline impacts food at the point of consumption, with the specific inclusion of food sources located in underserved communities.</li><li>The piloting of a free, public-facing resource (The National Produce Chemotyping Database) with interpreted data available in a format that is user-friendly for even the most vulnerable populations.</li></ol>",,,2019-04-18 13:44:15.691,True,2019-01-01,National produce chemotyping project,PUBLIC,,True,Open Agriculture,False
tree-computer,rebekahj,False,"<p>The&nbsp;OpenAg™&nbsp;<b>Tree Computer</b> is a food computer built to explore plant stress, health, longevity, and productivity across a wide range of crops.&nbsp;</p><p>The Tree Computer modulates&nbsp;<b>photosynthetic intensity</b>,&nbsp;<b>spectral range</b>,&nbsp;<b>wind velocity</b>,&nbsp;<b>nutrient levels, </b><b>soil and ambient temperature, and moisture content </b>in order&nbsp;to&nbsp;replicate growing conditions around the globe.&nbsp;</p><p>By reproducing actual and predicted weather patterns in near real time, the Tree Computer&nbsp;helps farmers determine what to plant, where&nbsp;to plant, when to harvest, and how to recover from drought, flood, frost, and disease.&nbsp;</p>",,,2019-04-16 14:37:15.732,True,2017-10-10,Tree Computer,PUBLIC,,True,Open Agriculture,False
personal-food-computer,rebekahj,False,"<p>The OpenAg™ Personal Food Computer is a tabletop-sized, controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber. Climate variables such as carbon dioxide, air temperature, humidity, dissolved oxygen, potential hydrogen, electrical conductivity, and root-zone temperature are among the many conditions that can be controlled and monitored within the growing chamber to yield various phenotypic expressions in the plants.&nbsp;</p><p>Our latest version—the <a href=""https://wiki.openag.media.mit.edu/pfc_edu_3.0""><b>PFC v3.0, or ""PFC_EDU""</b></a>—has been scaled down from previous PFC's in terms of cost, size, and complexity, and designed specifically with educators and children aged 8-14 in mind. It offers a spectrum of control so that users can make their growing experience as manual or as automated as they would like.</p><p><b><a href=""https://www.media.mit.edu/posts/build-a-food-computer/"">Click here to Build a Food Computer</a>.</b></p><p>Like all OpenAg's Food Computers, the PFC_EDU is open source and can be made from easily accessible components so that #nerdfarmers&nbsp;with a broad spectrum of skills, resources, and interests can build, modify, share, and upgrade over time. Build instructions, design files, and helpful resources for all our OpenAg™ Personal Food Computers are on our <a href=""https://wiki.openag.media.mit.edu/start""><b>OpenAg Wiki</b></a>, <a href=""https://github.com/OpenAgInitiative""><b>OpenAg Github</b></a>&nbsp;so&nbsp;nerd farmers can band together (using the <a href=""http://forum.openag.media.mit.edu/""><b>OpenAg Forum</b></a>) to conduct scientifically rigorous citizen-science experimentation, all over the world.</p>",,--Choose Location,2019-05-23 17:57:23.969,True,2015-09-01,Personal Food Computer,PUBLIC,,True,Open Agriculture,False
adrena,rebekahj,False,"<h1><b>aDRENA</b></h1><h2><br></h2><h2>A Digital Research and Experimentation Network for Agriculture </h2><p>Addressing complex and interlinked challenges requires a globally collaborative, digitally-enabled agricultural revolution.&nbsp;</p><p>This project focuses on three main elements:</p><ol><li><b>The Platform:</b> building an open access platform of &nbsp;hardware, software, and data libraries</li><li><b>The Community: </b>developing and engaging a community of people around the world who are collaborating and sharing data via a networked approach</li><li><b>OpenAg Proliferation:</b> supporting the launch of the OpenAg platform in 10 diverse locations around the world, creating a global network of research, replication, data collection, sharing, and learning from various applications.</li></ol><p>The three main elements enable the networking, scaling, and deployment of agricultural research while simultaneously educating and building capacity for the next generation of digital farmers. In addition, this approach to data collection and sharing creates a de facto standard to ensure interoperability of agricultural and climatic data sets that are open and accessible. &nbsp;</p>",,,2019-04-28 23:47:42.260,True,2019-01-01,aDRENA,PUBLIC,,True,Open Agriculture,False
food-server,rebekahj,False,"<p>The OpenAg™ Food Server is a shipping container-sized, <b>controlled environment agriculture technology</b> that can be built to utilize hydroponic or aeroponic technology. It can serve as both a<b> research platform for simulating precise environments at scale</b> (see <a href=""https://www.media.mit.edu/projects/openag-flavor-ecology/overview/"">Flavor, environment, and the phenome</a>) , and a <b>production unit</b> for any specified crop of interest. It is intended to produce <b>larger quantities of food</b> than a <a href=""https://www.media.mit.edu/projects/personal-food-computer/overview/"">Personal Food Computer</a> and appeals to <b>interdisciplinary researchers</b> as well as&nbsp;<b>small-scale cafeterias, restaurants, and boutique operators.</b></p>",,--Choose Location,2019-03-20 12:41:06.829,True,2015-09-01,Food Server,PUBLIC,https://www.linkedin.com/in/johndelaparra/,True,Open Agriculture,False
openag-flavor-ecology,delapa,False,"<p><b>Flavor</b>, in addition to making our food delicious, is one<b>&nbsp;</b>way of <b>sensing biochemical richness</b>. A highly flavorful plant generally contains a greater quantity and diversity of molecules—often with useful functional roles in our own metabolism—than a bland-tasting plant.&nbsp;</p><p>Flavor is a built-in reward for eating plants that has fueled our drive to domesticate and breed a massive biodiversity of vegetation over the last 10,000 years. OpenAg is going deep into the biochemical <b>machinery, evolution, and ecology of plants&nbsp;</b>to make growing food for the optimization of specific chemical profiles (flavor, pharmaceutical properties, nutrition) a reality.</p><p>Plants rely on <b>rich and diverse chemistry&nbsp;</b>for self-defense and stress adaptation. OpenAg is working <b>to induce a plant to synthesize these molecules&nbsp;</b>by adding specific stresses to the plant’s environment and measuring chemical shifts.</p><p>These <b>specialized metabolite molecules&nbsp;</b>can manifest as flavor, pharmaceutical compounds, and rich nutritional profiles. Flavor itself is frequently tied to additional healthful bio-activities for humans—such as <b>vitamins, antioxidants, stimulants, and nutrients.</b></p>",,,2019-04-23 21:12:54.149,True,2017-10-02,"Optimizing plants for flavor, nutrition, and pharmaceutical content",PUBLIC,https://www.linkedin.com/in/johndelaparra/,True,Open Agriculture,False
npcp,delapa,False,"<p>Chemical constituents are the most essential components of the fresh produce items consumed by people around the world. That chemistry can be extremely variable based on a number of factors—but wealthy or poor, privileged or vulnerable, we humans are ultimately subject to the destiny meted out by the chemistry of the food we consume.<br></p><p>Indeed, nutritional content, flavor, color, texture, aroma, and even medicinal benefits are dictated by this chemistry, yet there has never been a thorough and comprehensive analysis of that chemistry in fresh produce items.&nbsp;The National Produce Chemotyping Project sets the stage to assess the chemical variability, or chemotypes, of those items. Ultimately this project will lead to:</p><ol><li>Unprecedented studies seeking to correlate specific chemical attributes of food to health and well-being for all people.</li><li>Improved understanding of how the agricultural pipeline impacts food at the point of consumption, with the specific inclusion of food sources located in underserved communities.</li><li>The piloting of a free, public-facing resource (The National Produce Chemotyping Database) with interpreted data available in a format that is user-friendly for even the most vulnerable populations.</li></ol>",,,2019-04-18 13:44:15.691,True,2019-01-01,National produce chemotyping project,PUBLIC,,True,Open Agriculture,False
tree-computer,delapa,False,"<p>The&nbsp;OpenAg™&nbsp;<b>Tree Computer</b> is a food computer built to explore plant stress, health, longevity, and productivity across a wide range of crops.&nbsp;</p><p>The Tree Computer modulates&nbsp;<b>photosynthetic intensity</b>,&nbsp;<b>spectral range</b>,&nbsp;<b>wind velocity</b>,&nbsp;<b>nutrient levels, </b><b>soil and ambient temperature, and moisture content </b>in order&nbsp;to&nbsp;replicate growing conditions around the globe.&nbsp;</p><p>By reproducing actual and predicted weather patterns in near real time, the Tree Computer&nbsp;helps farmers determine what to plant, where&nbsp;to plant, when to harvest, and how to recover from drought, flood, frost, and disease.&nbsp;</p>",,,2019-04-16 14:37:15.732,True,2017-10-10,Tree Computer,PUBLIC,,True,Open Agriculture,False
open-phenome-project,delapa,False,"<p>The aim of this project is to create an open source digital library&nbsp;with open data sets that cross link phenotypic response in plants (<i style=""font-size: 18px; font-weight: 400;"">taste, nutrition, etc</i>) to environmental variables, biologic variables, genetic variables, and resources required in cultivation (<i style=""font-size: 18px; font-weight: 400;"">inputs).</i>&nbsp;While plants can be altered genetically to produce different or more desirable traits, plants with the same genetics may naturally vary in color, size, texture growth rate, yield, flavor, and nutrient density depending on the environmental conditions in which they are grown.&nbsp;</p><p>Each specific set of conditions can be thought of as a ""Climate Recipe"" that produces unique phenotypic results. As users experiment with new Climate Recipes, their input data and phenotypic results will be recorded and filed in an open source digital platform so that it can be shared, borrowed, scaled up, and improved upon around the world, instantly.&nbsp;</p>",,--Choose Location,2019-05-14 14:19:49.983,True,2015-01-01,Open Phenome Project,PUBLIC,,True,Open Agriculture,False
adrena,delapa,False,"<h1><b>aDRENA</b></h1><h2><br></h2><h2>A Digital Research and Experimentation Network for Agriculture </h2><p>Addressing complex and interlinked challenges requires a globally collaborative, digitally-enabled agricultural revolution.&nbsp;</p><p>This project focuses on three main elements:</p><ol><li><b>The Platform:</b> building an open access platform of &nbsp;hardware, software, and data libraries</li><li><b>The Community: </b>developing and engaging a community of people around the world who are collaborating and sharing data via a networked approach</li><li><b>OpenAg Proliferation:</b> supporting the launch of the OpenAg platform in 10 diverse locations around the world, creating a global network of research, replication, data collection, sharing, and learning from various applications.</li></ol><p>The three main elements enable the networking, scaling, and deployment of agricultural research while simultaneously educating and building capacity for the next generation of digital farmers. In addition, this approach to data collection and sharing creates a de facto standard to ensure interoperability of agricultural and climatic data sets that are open and accessible. &nbsp;</p>",,,2019-04-28 23:47:42.260,True,2019-01-01,aDRENA,PUBLIC,,True,Open Agriculture,False
food-server,delapa,False,"<p>The OpenAg™ Food Server is a shipping container-sized, <b>controlled environment agriculture technology</b> that can be built to utilize hydroponic or aeroponic technology. It can serve as both a<b> research platform for simulating precise environments at scale</b> (see <a href=""https://www.media.mit.edu/projects/openag-flavor-ecology/overview/"">Flavor, environment, and the phenome</a>) , and a <b>production unit</b> for any specified crop of interest. It is intended to produce <b>larger quantities of food</b> than a <a href=""https://www.media.mit.edu/projects/personal-food-computer/overview/"">Personal Food Computer</a> and appeals to <b>interdisciplinary researchers</b> as well as&nbsp;<b>small-scale cafeterias, restaurants, and boutique operators.</b></p>",,--Choose Location,2019-03-20 12:41:06.829,True,2015-09-01,Food Server,PUBLIC,https://www.linkedin.com/in/johndelaparra/,True,Open Agriculture,False
openpharm,delapa,False,"<h1><b>OpenPharm</b></h1><h2><b>The Future of Pharmacy&nbsp;</b></h2><p>What if we could produce <b>new and complex plant-derived pharmaceuticals</b> with the same <b>reliability and reproducibility </b>that has been developed with traditional bioreactor technology—without the time and expense of genetic manipulation and specialized cell culture protocols?&nbsp;</p><p><a href=""http://openag.mit.edu"">MIT’s Open Agriculture Initiative </a>is developing just such a solution for <b>whole medicinal plants</b> by precisely tuning the entire organism’s metabolism in modified <a href=""https://www.media.mit.edu/projects/food-server/overview/"">Food Servers</a>,&nbsp; highly controlled environments tied to an<b> integrated bioinformatics platform </b>and the <a href=""https://www.media.mit.edu/projects/open-phenome-project/overview/"">Open Phenome</a>.&nbsp;</p><p>Data derived from these networked environments allows for <b>computer modeling, machine learning, and simulations</b>—all working to predict drug production optimization within the plant. By embracing the complexity of big data we are able to discern the specifics of environmental stimuli and a plant’s precise metabolic response.</p><p>This technology also has exciting <b>global health implications</b>. The WHO estimates that <b>over 80 percent of the world relies on plant-based medicine</b> for some part of primary healthcare. However, many of these plants can be rare, highly endemic, or endangered species with limited natural population ranges—for which bulk collections would not be sustainable.&nbsp;</p><p>At OpenAg it becomes possible to <b>grow a wide variety of medicinal plants in precisely controlled environments</b>, elicit with environmental cues to spike and diversify metabolite levels, and then extract higher yields for further analysis and production.</p>",,,2018-10-20 16:49:29.218,True,2018-08-27,OpenPharm,LAB-INSIDERS,https://www.linkedin.com/in/johndelaparra/,True,Open Agriculture,False
bike-swarm,aberke,False,"<p>As bikes navigate city streets after dark, they are often equipped with lights. The lights make the bikes visible to cars or other bikers, and the hazards of traffic less dangerous.</p><p>Imagine that as solitary bikes come together, their lights begin to pulsate at the same cadence. The bikers may not know each other, or may only be passing each other briefly, but for the moments they are together, their lights synchronize. The effect is a visually united presence, as groups of bikes illuminate themselves with a gently pulsing, collective light source.</p>",,,2019-06-11 16:37:33.942,True,2019-02-01,[bike] swarm,PUBLIC,http://aberke.com,True,City Science,False
ar-enhanced-wall-plants-escape-pod,aberke,False,<p>An environment of plants and mirrors that extends beyond the terrarium walls.&nbsp;</p>,,,2019-05-07 19:24:13.501,True,2018-11-30,AR Enhanced Wall Plants: Escape Pod,PUBLIC,http://aberke.com,True,City Science,False
a-future-forward-proposal-for-a-system-of-streets-and-autonomous-vehicles,aberke,False,"<p>ABSTRACT</p><p>The impending introduction of autonomous vehicles (AVs) has posed regulatory and ethical questions regarding how they should operate. Much of the previous literature on this subject has explored these questions with an underlying model of streets based on the present.</p><p>This paper takes a different approach by putting forward a future vision for streets where privately owned and operated vehicles are no longer dominant and shared transit is more pervasive. In doing so, this paper expands the current discussions around individual AVs to the system of streets they will occupy. &nbsp;It views the topology of streets and the rules that govern them, coupled with the vehicles that move through the streets, as an autonomous system, or machine.  This project proposes updates to this autonomous system in order to build a more equitable system for a future where AVs will be ubiquitous.  The paper presents a design of two parts in order to ensure that AVs operate in the public’s best interests:</p><ol><li>An update to the laws that govern the use of roads, vehicle regulations and safety standards.&nbsp;&nbsp;</li><li>A requirement that AV decision making code be open sourced.</li></ol>",2020-09-01,,2019-01-08 20:15:32.709,False,2018-12-01,A Future-Forward Proposal for a System of Streets and Autonomous Vehicles,PUBLIC,https://github.com/aberke/moral-machine-simulation,True,City Science,False
the-trade-off-between-the-utility-and-privacy-risks-of-location-data-and-implications-for-data-as-a-public-good,aberke,False,"<p><i>Paper presented at the ""Connected Life 2019: Data &amp; Disorder"" conference at the Oxford Internet Institute.</i></p><p>High-resolution individual geolocation data passively collected from mobile phones is increasingly sold in private markets and shared with researchers.</p><p>This data poses significant security, privacy, and ethical risks: it’s been shown that users can be re-identified in such datasets, and its collection rarely involves their full consent or knowledge. This data is valuable to private firms (e.g. targeted marketing) but also presents clear value as a public good. Recent public interest research has demonstrated that high-resolution location data can more accurately measure segregation in cities and provide inexpensive transit modeling. But as data is aggregated to mitigate its re-identifiability risk, its value as a good diminishes. How do we rectify the clear security and safety risks of this data, its high market value, and its potential as a resource for public good? We extend the recently proposed concept of a tradeoff curve that illustrates the relationship between dataset utility and privacy. We then hypothesize how this tradeoff differs between private market use and its potential use for public good. We further provide real-world examples of how high resolution location data, aggregated to varying degrees of privacy protection, can be used in the public sphere and how it is currently used by private firms.</p>",,,2019-05-24 15:26:51.045,False,2019-03-01,The Tradeoff Between the Utility and Risk of Location Data and Implications for Public Good,LAB,,True,City Science,False
cityscope-cooper-hewitt,aberke,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
augmented-eternity,rahnama,False,"<p>Have you ever wondered what a friend&nbsp;would&nbsp;do if she was in your decision-making situation? Or thought about where a family member might go if he was visiting a travel destination with you? In many cases, you can only guess what a person would do if they were in your shoes. But now you may be able to securely ""<b>borrow their&nbsp; identity</b>"" and ask a question with the confidence of receiving a relevant and &nbsp;valuable answer.&nbsp;</p><p>Can software agents become our digital heirs? Can a head of state, a scientist, or a business owner leverage machine intelligence to complement succession planning?&nbsp; What if you could select the digital identity of a deceased person from a social network and activate it as a pluggable ontology into your iPhone’s Siri and ask a question?</p><p>Our digital identity has become so rich and intrinsic that without it, it may feel like a part of us is missing. The number of sensors we carry daily and the digital footprints we leave behind have given us enough granular patterns and data clusters that we can now use them for prediction and reasoning on behalf of an individual. We believe that by enabling our digital identity to perpetuate, we can significantly contribute to global expertise and enable a new form of an intergenerational collective intelligence.</p>",,,2019-04-19 14:37:08.576,True,2017-05-22,Augmented Eternity and Swappable Identities,PUBLIC,,True,Human Dynamics,False
city-science-lab-toronto,rahnama,False,"<p>City Science Lab Toronto was established in cooperation with Ryerson University in Toronto, Canada.&nbsp; It&nbsp; started in 2018 and is the newest city in the City Science Network. The lab will be embedded in&nbsp;the Faculty of Communications and Design and will be part of the University's Paradox Initiative.&nbsp;The two groups plan to build and work on&nbsp;the development and simulation of urban interventions, such as&nbsp; micro-units for young people, shared work and collaboration spaces, educational facilities, financial services innovations, and new mobility and parking systems. This information will be analyzed&nbsp;and visualized using different platforms including the CityScope. The two groups&nbsp; plan to&nbsp;&nbsp;define new parameters &nbsp;which may include financial modeling, design of innovation flow,&nbsp; public health, new mobility systems, and/or energy networks. A number of large financial institutions, telecommunications companies, and hospitality groups are the founding corporate supporters of the initiative. The lab's director is Professor Hossein Rahnama, who is also a visiting faculty member at the MIT Media Lab.&nbsp;</p>",,,2018-11-16 14:46:12.537,True,2018-03-01,City Science Lab Toronto,PUBLIC,,True,Human Dynamics,False
apedagogyofnoise,nsingh1,False,"<p><i>A Pedagogy of Noise</i>&nbsp;(in development)&nbsp;is a&nbsp;project-based approach to learning about sound, and to using sound to learn about the world around us. This is an ongoing project consisting of models, software systems, and more that are intended to enable this kind of creative learning. <i>Noise</i>&nbsp;here refers to the emergent texture of the sounds around us, in combination. This dense surface, especially in urban environments, can be perceived as noise, but there is a rich network of sounds that generate this texture.</p><p>Working with sound teaches us about sound itself, as well as engages us in creative processes, allows for artistic expression, and builds skills in a variety of traditional disciplines.&nbsp;Engaging with sound can also serve as a way of studying objects, environments, and more, as well as a medium for creative storytelling.</p><p>As we continue to make sound and noise as participants in our acoustical environments, and also continue to generate large quantities of audio for a variety of reasons, I'm very interested in how we can effectively tap into these as resources for creative projects, play, interaction, and learning.</p><p><b>In progress:</b>&nbsp;A construction kit for creative sound-based project-oriented learning.</p><h2>Example Projects</h2><p><b>Kronospaces</b></p><p><i>Kronospaces</i> (2018) is an interactive experience created using excerpts from the Kronos Quartet's recorded catalog, which spans about 40 years.</p><p>This project initially involved Investigating this catalog to find moments that could be excerpted to represent the range of music in the quartet's repertoire, to create a database of musical motifs and gestures.</p><p>Custom software was written to extract information about these audio excerpts, to organize them into 6 different ""spaces"" or categories using this extracted information, and to turn these into an interactive experience using concatenative synthesis to retrieve these excerpts from the database and stitch them into a sound ""mosaic"" in real-time based on the user's navigation of the visual interface, which also consists of custom software written for this project.</p><p><i>Kronospaces</i>&nbsp;is a playful, interactive window into the depth and diversity of the Kronos Quartet's history.</p><p><b>Scream</b></p><p><i>Scream</i> (2018) is an audiovisual installation that uses face-tracking as an input to control an instrument built from the audio portion of <a href=""https://zenodo.org/record/1188976"">The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)</a> by Livingstone &amp; Russo (licensed under <a href=""https://creativecommons.org/licenses/by-nc-sa/4.0/"">CC BY-NA-SC 4.0</a>). Facial features and gestures are used to construct textures from the database, involving custom audio and visual software.</p><p>This work is part of an ongoing series exploring artistic applications of audio datasets, bodily interfaces to recorded sound, and new visual and physical interfaces to the exploration of sound corpuses.</p><p><i>Scream&nbsp;</i>uses perhaps the most primal sound-making device, the mouth, in a silent, visual capacity. It creates a one-to-many mapping in terms of people, allowing the gestures of one person to be expressed through the voices of many.</p><p>The installation’s title doubles as an instruction, encouraging participants to scream, for whatever reason they may want to, through the voices of others without needing to make any sound acoustically.&nbsp;</p>",,,2019-04-18 19:20:06.357,True,2019-04-18,A Pedagogy of Noise,LAB,,True,Opera of the Future,False
food-attack,nfarve,False,"<p>The rise in wearable devices and the desire to quantify various aspects of everyday activities has provided the opportunity to offer just-in-time triggers to aid in achieving pre-determined goals. While a lot is known about the effectiveness of messaging in marketing efforts, less is known about the effectiveness of these marketing techniques on in-the-moment decision-making. We designed an experiment to determine if a simple solution of using just-in-time persuasive messaging could influence participants' eating habits and what types of messaging could be most effective in this effort. Our solution utilizes a head-mounted display to present health-based messages to users as they make real-time snack choices. We are able show that this method is effective and more feasible than current efforts to influence eating habits.</p>",,--Choose Location,2016-12-05 00:16:44.086,True,2015-01-01,Food Attack,PUBLIC,,True,Fluid Interfaces,False
smilecatcher,nfarve,False,"<p>Our hectic and increasingly digital lives can have a negative effect on our health and wellbeing. Some authors have argued that we socialize less frequently with other people in person and that people feel increasingly lonely. Loneliness has been shown to significantly affect health and wellbeing in a negative way. To combat this, we designed a game, SmileCatcher, which encourages players to engage in in-person, social interactions and get others to smile. Participants wear a device that takes regular pictures of what is in front of them and the system analyzes the pictures captured to detect the number of smiles.</p>",,--Choose Location,2016-12-05 00:16:44.188,True,2014-01-01,SmileCatcher,PUBLIC,,True,Fluid Interfaces,False
watch,nfarve,False,"<p>WATCH is a system that attempts to measure the possible influence that a new time-management interface will have on improving the habits of a user. Users set goals for each of the activities detected by the app. Detected activities include physical activity and time spent in pre-defined locations. An Andriod app (WATCH) on their personal phones is able to track their activities (running, walking, and sitting) as well as their GPS location. Their progress in comparison to their goals is displayed on their home screens as a pie chart.</p>",,--Choose Location,2016-12-05 00:16:58.002,True,2015-01-01,WATCH,PUBLIC,,True,Fluid Interfaces,False
the-challenge,nfarve,False,"<p>Mental wellbeing is intimately tied to both social support and physical activity. The Challenge is a tool aimed at promoting social connections and decreasing sedentary activity in a workplace environment. Our system asks participants to sign up for short physical challenges and pairs them with a partner to perform the activity. Social obligation and social consensus are leveraged to promote participation. Two experiments were conducted in which participants' overall activity levels were monitored with a fitness tracker. In the first study, we show that the system can improve users' physical activity, decrease sedentary time, and promote social connection. As part of the second study, we provide a detailed social network analysis of the participants, demonstrating that users' physical activity and participation depends strongly on their social community.</p>",,--Choose Location,2016-12-05 00:16:31.390,True,2015-01-01,The Challenge,PUBLIC,,True,Fluid Interfaces,False
creating-new-jazz-musical-instruments-and-new-jazz-idioms-1,tcarew,False,"<p><b>The Launch Team: </b>Christian McBride (winner of six Grammy Awards), David Gage, Mas Hino, and Topper Carew.</p>",,,2018-05-08 12:03:39.097,True,2018-05-07,Creating New Jazz Musical Instruments and New Jazz Idioms,PUBLIC,,True,Other,False
nccu-the-creation-of-new-black-musical-instruments-and-new-black-musical-idioms,tcarew,False,"<p>NCCU: music is a strong and universally accepted aspect of black culture. It has given birth to the genres of blues, jazz, gospel, rap, house, disco, funk, soul, trap, ragga, ska, dub, grime, reggae, calypso, hip hop, r&amp;b, dubstep, soul, and neo-soul. We believe that there are new instruments and ideas to be discovered. We are beginning this paradigmatic journey at an innovation center we launched at North Carolina Central University.&nbsp;</p>",,,2018-05-08 12:02:11.010,True,2015-03-01,NCCU: The Design and Fabrication of New Black Musical Instruments,PUBLIC,,True,Other,False
creating-new-jazz-mus,tcarew,False,"<p>&nbsp;For 18 months, the Smithsonian has been collecting art, photos, films, and interviews about the New Thing Art and Architecture Center, a Washington, DC inner-city cultural center that Topper Carew founded immediately after college. The work will be installed in the Smithsonian's personal collection.<br></p>",,,2018-05-08 13:54:00.724,True,2018-05-07,New Thing Art and Architecture Center at the Smithsonian,PUBLIC,,True,Other,False
icenters-the-design-of-a-methodology-to-encourage-a-new-generation-of-computer-scientists-inventors-and-innovators,tcarew,False,"<p>iCenters: We work with a select group of Historically Black Colleges and Universities (HBCUs) to encourage and support the development of iCenters (Innovation Centers). The model for the iCenters is greatly influenced by the South End Technology Center (SETC). SETC was the first community located Maker Space to spin out of the Media Lab’s Bits and Atoms Lab. It is helmed by MIT Professor Emeritus Mel King. iCenters emphasize experiential learning by making and doing. By learning to code and learning to make and do with fabrication tools, we capture and retain interest in technology and facilitate closure of the Technology gap.</p>",,,2017-04-04 21:48:44.601,True,2014-01-01,"iCenters : The Design of a Methodology to Encourage  a New Generation of Computer Scientists, Inventors, and Innovators",PUBLIC,,True,Other,False
l2d-d2l-high-school-students-develop-coding-and-fabrication-learning-activities-for-their-peers,tcarew,False,"<p><b>L2D:D2L:</b> Learn to Develop:Develop to Learn. Much of our work is with young people from underserved communities. To better understand the age group as end users, we added a youth cohort to our Content Development staff. Their responsibility is to learn the nuances of Content Development, recommend Best Practices, create Prototypes, and develop Peer Appropriate Learning Activities.</p>",,,2017-04-04 21:27:36.398,True,2016-10-17,L2D:D2L : High School Students Develop Coding and Fabrication Learning Activities For Their Peers,PUBLIC,,True,Other,False
spelman-the-design-of-a-two-way-television-studio-in-a-suitcase-broadcast-system-to-teach-fabrication-and-prototyping-remotely,tcarew,False,"<p><b style=""font-size: 18px;"">Spelman</b>: Spelman College is a distinguished Black Women’s college in Atlanta. Nationally, twenty-three percent (23%) of the Black Women in STEM graduate from Spelman College. With their Innovation Center at Spelman, we piloted a two-way Long Distance Learning course in coding, microcontrollers (MaKey MaKey and Arduino), and digital fabrication.&nbsp;Students received credit for the course. For the course, we authored and designed &nbsp;Experiential Learning Tutorials which&nbsp;we first taught to our Spelman teaching counterpart, Dr. Jerry Volcy. He heads Spelman’s Innovation Center. The final products of the course were fabricated interactive lamps, and &nbsp;fabricated digital musical instruments.<br></p><p>The course allowed us to test our Pilot design for a “Television Studio in a Suitcase” (TSS) broadcast system. TSS is an affordable, miniaturized and portable interactive broadcast system. It enables us to teach technology online from anywhere. It works. In the next series at Spelman, we will teach Robotics and Mechatronics.</p>",,,2017-10-11 17:49:30.834,True,2016-09-15,Spelman: The Design of a two-way ‘Television Studio in a Suitcase’ Broadcast System to Teach Fabrication and Prototyping Remotely,PUBLIC,,True,Other,False
the-qty-code-a-simple-tool-for-protein-design,shuguang,False,"<p>Structure and function studies of membrane proteins, particularly G protein-coupled receptors (GPCRs) and multipass transmembrane proteins, require detergents after removing them from cell membranes. We have invented a simple tool, the QTY code, that is named for three amino acids: glutamine (Q), threonine (T), and tyrosine (Y) for making water-insoluble domains become water-soluble without detergents. Despite substantial transmembrane domain changes, the detergent-free QTY variants maintain stable structures and ligand-binding activities. We believe the QTY code will be useful for designing water-soluble variants of membrane proteins and other water-insoluble aggregated proteins. The QTY code designed detergent-free chemokine receptors may be useful in many applications. The QTY variants may not only be useful as reagents in deorphanization studies, but also for designing biologic drugs to treat cancer, autoimmune, or infectious diseases. The QTY code allows membrane proteins to be systematically designed through simple, specific amino acid substitutions. The QTY code is robust and straightforward: it is the simplest tool to carry out membrane protein design without sophisticated computer algorithms. Thus, it can be used broadly. The QTY code has implications for designing additional GPCRs and other membrane proteins, or for rendering water-insoluble and aggregated proteins to become water soluble.</p>",,,2019-06-12 19:13:17.729,True,2019-04-01,The QTY code: A simple tool for protein design,LAB-INSIDERS,,True,Molecular Machines,False
membrane-receptor-based-high-molecular-density-bioelectronic-platform,shuguang,False,"<p>In collaboration with Prof. Uwe Sleytr and Dr. Andreas Breitwieser (BOKU, Vienna, Austria).</p><p>Bioelectronics is an emerging antidisciplinary field which utilizes biomolecules in electronics, mimics biological architectures, or builds electronic-living organism interfaces. One important aspect of the field is to fabricate sensors for label-free biomolecules detection. Researchers previously designed sensors based on i) metal-oxide-semiconductor (MOSFET), ii) polymers, and iii) inorganic crystalline materials which produce good sensitivity, but lack selectivity. Recent efforts are devoted to directly connecting biological receptors with electronic systems. G protein-coupled receptors (GPCRs) serve as suitable candidates as they are the largest family of membrane receptors that detect information (molecules and lights) and transduce to cell internal signals to regulate body functions.&nbsp;</p><p>There are ~1,000 GPCR proteins in human cells, each one highly specific to a particular signal. QTY-designed, detergent-free GPCR receptors can be modified and attached to recombinant-SbpA proteins, which are capable of reproducing two-dimensional crystalline monolayers on various electronic surfaces, as demonstrated by Prof. Uwe Sleytr in Vienna, Austria. SbpA 2D crystalline guides the orientation of the attached GPCRQTY receptors and exposes their active binding sites. The self-assembly yields functional molecules with high density &gt;1012 molecule/cm2. The bioelectronics platform yields detectable electrical, electrochemical, or optical signals in response to the biological stimuli from the receptor layer. When coupled with different types of receptors, this approach may be a platform for bioelectronics and ultrasensitive-sensing systems.</p>",,,2019-04-19 18:43:36.215,True,2019-04-01,Membrane receptor-based high molecular density bioelectronic platform,LAB-INSIDERS,,True,Molecular Machines,False
qty-code-designs-for-water-soluble-barrel-transmembrane-proteins,shuguang,False,"<p>In collaboration with Prof. Tao Fei’s lab (Shanghai Jiaotong University, China)</p><p>In addition to a large number of membrane proteins that comprise most alpha-helix transmembrane segments, there are also many transmembrane proteins with mostly beta-sheets, called beta-barrel membrane proteins. These beta-barrel proteins are often involved in molecular transports.&nbsp;Some are enzymes that are involved in lipid metabolism. We now use the QTY code to design water-soluble beta-barrel transmembrane proteins. Such water-soluble, beta-barrel proteins will facilitate studies of the molecular mechanisms of high selectivity of molecular transport and how these beta-barrel membrane proteins carry out lipid catalysis and metabolism. New insight gained from these studies may be useful for further designs of new molecular devices. &nbsp;</p>",,,2019-04-19 18:45:38.116,True,2019-04-01,QTY code designs for water-soluble beta-barrel transmembrane proteins,LAB-INSIDERS,,True,Molecular Machines,False
qty-code-for-highly-water-soluble-abeta42-involved-in-alzheimer-s-disease,shuguang,False,"<p>In collaboration with Professor Sara Linse’s laboratory (Lund University, Sweden)</p><p>Although much is known about the 42-residue Abeta peptide (Abeta42) and its detailed molecular structure, there is still no effective remedy for treatment of devastating Alzheimer’s disease that costs the world enormously economically. Thus, additional efforts and innovative ways are needed to find some effective treatment. In collaboration with Professor Sara Linse’s lab in Lund University, we will use the QTY code to design more water-soluble variants of Abeta42. QTY variants have the potential to retain their molecular structures, interact, and incorporate into the natural Abeta42 amyloid fibrils.&nbsp; Such molecular interactions may de-stabilize the natural Abeta42 fibrils that pack very tightly to form plaques in the Alzheimer’s disease brain. &nbsp;With further design, the water-soluble Abeta42 may fuse with specific protein degrading enzymes or other cleaning systems to disrupt and remove pathogenic amyloid fibrilsor plaques. &nbsp;</p>",,,2019-04-19 18:46:20.877,True,2019-06-03,QTY code for highly water-soluble Abeta42 involved in Alzheimer’s disease,LAB-INSIDERS,,True,Molecular Machines,False
water-soluble-transmembrane-protein-receptors-with-exchangeable-tunable-ligand-affinity,shuguang,False,"<p>QTY-designed, detergent-free chemokine receptors have been expressed in SF9 insect cells, as well as produced using a low cost and simple <i>E.coli&nbsp;</i>system with much higher throughput. These QTY-designed receptor variants exhibit remarkable heat stability in the presence of arginine additive, retaining ligand binding activity after 100°C treatment. New protein variants can also be designed using the same alpha-helical segments but switching the extracellular (EC) loop, e.g., using variant B’s EC loop to directly attach to variant A’s alpha-helical segments. This approach helps our understanding of the binding mechanism of QTY variants and natural membrane receptors, as well as enables a novel pathway for the design and production of multi-functional, water-soluble membrane receptors, with tunable properties for <i>in vitro&nbsp;</i>and <i>in vivo&nbsp;</i>applications.&nbsp;</p>",,,2019-04-17 02:42:22.369,True,2019-04-01,Water-soluble transmembrane protein receptors with exchangeable and tunable ligand affinity,LAB-INSIDERS,,True,Molecular Machines,False
cxcr4-designed-using-qty-code-becomes-more-hydrophilic-and-retains-cell-signaling-activity,shuguang,False,"<p>In collaboration with Prof. Horst Vogel and Dr. Horst Pick (EPFL, Lausanne, Switzerland).</p><p>G protein-coupled receptors (GPCRs) are vital for diverse biological functions, including vision, smell, and aging. They are also involved in a wide range of diseases, and are among the most important targets of medicinal drugs. Tools that facilitate GPCR studies or GPCR-based technologies or therapies are thus critical to develop. We used QTY code (glutamine, threonine, tyrosine) to systematically replace 29 membrane-facing leucine (L), isoleucine (I), valine (V), and phenylalanine (F) residues in the transmembrane alpha-helices of the GPCR CXCR4. This variant, CXCR4QTY29, became more water-soluble, while retaining the ability to bind its natural ligand CXCL12. When transfected CXCR4QTY29 gene into HEK293 cells, the translated CXCR4QTY29 receptor inserted into the cell membrane and retained its cellular signaling activity. This QTY code can significantly improve GPCR and membrane protein studies by making it possible to design functional hydrophilic receptors. The QTY code can be applied to diverse a-helical transmembrane proteins, and may aid in the development of other applications, including clinical therapies.&nbsp;</p>",,,2019-04-01 17:45:11.000,True,2019-04-01,QTY CXCR4 becomes more hydrophilic and retains cell signaling activity,LAB-INSIDERS,,True,Molecular Machines,False
the-qty-code-a-simple-tool-for-protein-design,vex,False,"<p>Structure and function studies of membrane proteins, particularly G protein-coupled receptors (GPCRs) and multipass transmembrane proteins, require detergents after removing them from cell membranes. We have invented a simple tool, the QTY code, that is named for three amino acids: glutamine (Q), threonine (T), and tyrosine (Y) for making water-insoluble domains become water-soluble without detergents. Despite substantial transmembrane domain changes, the detergent-free QTY variants maintain stable structures and ligand-binding activities. We believe the QTY code will be useful for designing water-soluble variants of membrane proteins and other water-insoluble aggregated proteins. The QTY code designed detergent-free chemokine receptors may be useful in many applications. The QTY variants may not only be useful as reagents in deorphanization studies, but also for designing biologic drugs to treat cancer, autoimmune, or infectious diseases. The QTY code allows membrane proteins to be systematically designed through simple, specific amino acid substitutions. The QTY code is robust and straightforward: it is the simplest tool to carry out membrane protein design without sophisticated computer algorithms. Thus, it can be used broadly. The QTY code has implications for designing additional GPCRs and other membrane proteins, or for rendering water-insoluble and aggregated proteins to become water soluble.</p>",,,2019-06-12 19:13:17.729,True,2019-04-01,The QTY code: A simple tool for protein design,LAB-INSIDERS,,True,Synthetic Neurobiology,False
the-qty-code-a-simple-tool-for-protein-design,junni,False,"<p>Structure and function studies of membrane proteins, particularly G protein-coupled receptors (GPCRs) and multipass transmembrane proteins, require detergents after removing them from cell membranes. We have invented a simple tool, the QTY code, that is named for three amino acids: glutamine (Q), threonine (T), and tyrosine (Y) for making water-insoluble domains become water-soluble without detergents. Despite substantial transmembrane domain changes, the detergent-free QTY variants maintain stable structures and ligand-binding activities. We believe the QTY code will be useful for designing water-soluble variants of membrane proteins and other water-insoluble aggregated proteins. The QTY code designed detergent-free chemokine receptors may be useful in many applications. The QTY variants may not only be useful as reagents in deorphanization studies, but also for designing biologic drugs to treat cancer, autoimmune, or infectious diseases. The QTY code allows membrane proteins to be systematically designed through simple, specific amino acid substitutions. The QTY code is robust and straightforward: it is the simplest tool to carry out membrane protein design without sophisticated computer algorithms. Thus, it can be used broadly. The QTY code has implications for designing additional GPCRs and other membrane proteins, or for rendering water-insoluble and aggregated proteins to become water soluble.</p>",,,2019-06-12 19:13:17.729,True,2019-04-01,The QTY code: A simple tool for protein design,LAB-INSIDERS,,True,Center for Bits and Atoms,False
the-qty-code-a-simple-tool-for-protein-design,ruiqing,False,"<p>Structure and function studies of membrane proteins, particularly G protein-coupled receptors (GPCRs) and multipass transmembrane proteins, require detergents after removing them from cell membranes. We have invented a simple tool, the QTY code, that is named for three amino acids: glutamine (Q), threonine (T), and tyrosine (Y) for making water-insoluble domains become water-soluble without detergents. Despite substantial transmembrane domain changes, the detergent-free QTY variants maintain stable structures and ligand-binding activities. We believe the QTY code will be useful for designing water-soluble variants of membrane proteins and other water-insoluble aggregated proteins. The QTY code designed detergent-free chemokine receptors may be useful in many applications. The QTY variants may not only be useful as reagents in deorphanization studies, but also for designing biologic drugs to treat cancer, autoimmune, or infectious diseases. The QTY code allows membrane proteins to be systematically designed through simple, specific amino acid substitutions. The QTY code is robust and straightforward: it is the simplest tool to carry out membrane protein design without sophisticated computer algorithms. Thus, it can be used broadly. The QTY code has implications for designing additional GPCRs and other membrane proteins, or for rendering water-insoluble and aggregated proteins to become water soluble.</p>",,,2019-06-12 19:13:17.729,True,2019-04-01,The QTY code: A simple tool for protein design,LAB-INSIDERS,,True,Center for Bits and Atoms,False
membrane-receptor-based-high-molecular-density-bioelectronic-platform,ruiqing,False,"<p>In collaboration with Prof. Uwe Sleytr and Dr. Andreas Breitwieser (BOKU, Vienna, Austria).</p><p>Bioelectronics is an emerging antidisciplinary field which utilizes biomolecules in electronics, mimics biological architectures, or builds electronic-living organism interfaces. One important aspect of the field is to fabricate sensors for label-free biomolecules detection. Researchers previously designed sensors based on i) metal-oxide-semiconductor (MOSFET), ii) polymers, and iii) inorganic crystalline materials which produce good sensitivity, but lack selectivity. Recent efforts are devoted to directly connecting biological receptors with electronic systems. G protein-coupled receptors (GPCRs) serve as suitable candidates as they are the largest family of membrane receptors that detect information (molecules and lights) and transduce to cell internal signals to regulate body functions.&nbsp;</p><p>There are ~1,000 GPCR proteins in human cells, each one highly specific to a particular signal. QTY-designed, detergent-free GPCR receptors can be modified and attached to recombinant-SbpA proteins, which are capable of reproducing two-dimensional crystalline monolayers on various electronic surfaces, as demonstrated by Prof. Uwe Sleytr in Vienna, Austria. SbpA 2D crystalline guides the orientation of the attached GPCRQTY receptors and exposes their active binding sites. The self-assembly yields functional molecules with high density &gt;1012 molecule/cm2. The bioelectronics platform yields detectable electrical, electrochemical, or optical signals in response to the biological stimuli from the receptor layer. When coupled with different types of receptors, this approach may be a platform for bioelectronics and ultrasensitive-sensing systems.</p>",,,2019-04-19 18:43:36.215,True,2019-04-01,Membrane receptor-based high molecular density bioelectronic platform,LAB-INSIDERS,,True,Center for Bits and Atoms,False
water-soluble-transmembrane-protein-receptors-with-exchangeable-tunable-ligand-affinity,ruiqing,False,"<p>QTY-designed, detergent-free chemokine receptors have been expressed in SF9 insect cells, as well as produced using a low cost and simple <i>E.coli&nbsp;</i>system with much higher throughput. These QTY-designed receptor variants exhibit remarkable heat stability in the presence of arginine additive, retaining ligand binding activity after 100°C treatment. New protein variants can also be designed using the same alpha-helical segments but switching the extracellular (EC) loop, e.g., using variant B’s EC loop to directly attach to variant A’s alpha-helical segments. This approach helps our understanding of the binding mechanism of QTY variants and natural membrane receptors, as well as enables a novel pathway for the design and production of multi-functional, water-soluble membrane receptors, with tunable properties for <i>in vitro&nbsp;</i>and <i>in vivo&nbsp;</i>applications.&nbsp;</p>",,,2019-04-17 02:42:22.369,True,2019-04-01,Water-soluble transmembrane protein receptors with exchangeable and tunable ligand affinity,LAB-INSIDERS,,True,Center for Bits and Atoms,False
barmaks-untitled-project-2,aghasi,False,"<p>We exploit the sub-picosecond time resolution along with spectral resolution provided by terahertz time-domain spectroscopy to extract occluding content from layers whose thicknesses are wavelength comparable. The method uses the statistics of the THz E-field at subwavelength gaps to lock into each layer position and then uses a time-gated spectral kurtosis to tune to highest spectral contrast of the content on that specific layer. To demonstrate, occluding textual content was successfully extracted from a sample similar to a closed book down to nine pages without human supervision. The method provides over an order of magnitude enhancement in the signal contrast and can impact inspection of structural defects in wooden objects, plastic components, composites, drugs, and especially cultural artifacts with subwavelength or wavelength comparable layers.</p>",,--Choose Location,2016-10-24 19:44:56.028,False,2015-09-01,Reading through Closed Books: THz Time-Gated Spectral Imaging for Content Extraction through Layered Structures,PUBLIC,,True,Camera Culture,False
barmaks-untitled-project-2,redosan,False,"<p>We exploit the sub-picosecond time resolution along with spectral resolution provided by terahertz time-domain spectroscopy to extract occluding content from layers whose thicknesses are wavelength comparable. The method uses the statistics of the THz E-field at subwavelength gaps to lock into each layer position and then uses a time-gated spectral kurtosis to tune to highest spectral contrast of the content on that specific layer. To demonstrate, occluding textual content was successfully extracted from a sample similar to a closed book down to nine pages without human supervision. The method provides over an order of magnitude enhancement in the signal contrast and can impact inspection of structural defects in wooden objects, plastic components, composites, drugs, and especially cultural artifacts with subwavelength or wavelength comparable layers.</p>",,--Choose Location,2016-10-24 19:44:56.028,False,2015-09-01,Reading through Closed Books: THz Time-Gated Spectral Imaging for Content Extraction through Layered Structures,PUBLIC,,True,Camera Culture,False
l-evolved,harshit,False,"<p>Ubiquitous computing has been focusing on creating smart agents that are submerged into everyday environments, however, recent development on physical computing is demanding a shift from calm computing to a physically engaging form. Computing is no more limited to increasing our comfort through passive and pervasive deployment, they can now be created as being more actively and physically intermeshed into our tasks. We present L’evolved, autonomous ubiquitous utilities that assist in user tasks through active physical participation. They not only dynamically adapt to individual user needs and actions, but also work in close tandem with the users. Among explorations on potential applications, we harness drone technology to realize the design and implementation of example utilities that afford free motions and computational controls. Through various use scenarios of those exemplary utilities, we show how this new form of smart agents promises new ways of interacting with our physical environments. We also discuss design implications and technical details of our implementations.</p>",,,2018-10-12 19:55:34.922,False,2015-09-11,L'evolved,PUBLIC,,True,Fluid Interfaces,False
leveraging-leadership-expertise-more-effectively-in-organizations,dhaval,False,"<p>We believe that the narrative of only listening to experts or trusting the wisdom of the crowd blindly is flawed. Instead we have developed a system that weighs experts and lay-people differently and dynamically and show that a good balance is required. We show that our methodology leads to a 15 percent improvement in mean performance, 15 percent decrease in variance, and almost 30 percent increase in Sharpe-type ratio in a real online market.</p>",,--Choose Location,2019-04-19 14:45:04.070,True,2015-01-01,Leveraging leadership expertise more effectively in organizations,PUBLIC,,True,Human Dynamics,False
evolution-strategies-applied-to-collective-intelligence,dhaval,False,"<p>We build recommender bots that use machine learning and network analytics to create personalized recommendations for users on various social and financial platforms. We show that bots that work not just on the raw user data, but instead build on human intuition, do far better. We are in the process of live testing these bots on various platforms.&nbsp;</p>",,,2017-04-05 18:49:11.444,True,2017-03-10,Social Learning Recommender Bots,PUBLIC,,True,Human Dynamics,False
prediction-markets-leveraging-internal-knowledge-to-beat-industry-prediction-experts,dhaval,False,"<p>Markets are notorious for bubbles and bursts. Other research has found that crowds of lay-people can replace even leading experts to predict everything from product sales to the next big diplomatic event. In this project, we leverage both threads of research to see how prediction markets can be used to predict business and technological innovations, and use them as a model to fix financial bubbles. For example, a prediction market was rolled out inside of Intel and the experiment was very successful, and led to better predictions than the official Intel forecast 75 percent of the time. Prediction markets also led to as much as a 25 percent reduction in mean squared error over the prediction of official experts at Google, Ford, and Koch industries.</p>",,--Choose Location,2019-04-19 14:51:23.617,True,2015-01-01,Prediction Markets: Leveraging internal knowledge to beat industry prediction experts,PUBLIC,,True,Human Dynamics,False
social-ai-and-extended-intelligence,dhaval,False,"<p>There is a deep fear that human jobs will be replaced by AI. Rather than racing against the machines, our aim is to show that a human-AI combination will perform better than humans and AI working alone. Although no man is better than a machine for some tasks, ""no machine is better than a man with a machine"" (Paul Tudor Jones) . Thus, by building ""bots"" that are compatible with human behavior, and specifically leverage the manner in which humans use social information, we have been able to build bots that extend human intelligence capabilities. In a large-scale financial trading experiment, we have shown that groups of humans and ""socially compatible"" AI bots can successfully incorporate human intuition into their decisions and consequently not only do better than humans alone, but also do better than similar AI bots that use only objective information.
                    
                </p>",,,2019-02-12 15:14:05.048,True,2016-07-01,Social AI and Extended Intelligence,PUBLIC,,True,Human Dynamics,False
deep-reinforcement-learning-inspired-by-human-collective-intelligence,dhaval,False,"<p>We know that it's groups, not individuals, that are capable of the most complex and daunting achievements. Why should AI be any different?&nbsp;</p><p>We show that deep reinforcement learning algorithms that use lessons from how humans learn and communicate with each other can provide large improvements over state of the art reinforcement learning methods.</p><p>Researchers have been studying how groups of problem-solvers organize themselves and communicate for years, under the field of ""collective intelligence."" It's been shown that there are some surprisingly simple relationships between a group's communication network structure and how well that group is able to perform on different kinds of tasks.&nbsp;</p><p>Using these simple lessons, we designed a deep reinforcement learning algorithm that, instead of using one massive neural network (NN), leverages a community of many smaller NNs. We enable these neural networks to communicate with one another, and to learn from each others' explorations and successes.&nbsp;</p><p>Using this strategy yields significant improvements over the state of the art. By placing these neural nets on a communication network that is similar in structure to how humans communicate,&nbsp;we see a 33% improvement in how fast the networks are able to learn and in how well they are able to perform at a benchmark reinforcement learning task.</p><p><br></p>",,,2018-05-09 14:23:16.868,True,2017-07-01,Deep Reinforcement Learning Inspired by Human Collective Intelligence,LAB-INSIDERS,,True,Human Dynamics,False
blockchain-a-new-framework-for-swarm-robotic-systems,ecstll,False,"<p>Swarms of robots will revolutionize many applications, from targeted material delivery to farming. However, the characteristics that make them ideal for certain future applications, such as robot autonomy or decentralized control, can also be an obstacle when transferring this technology from academia to real-world problems. Blockchain, an emerging technology, demonstrates that by combining peer-to-peer networks with cryptographic algorithms, a group of agents can reach agreements without the need for a controlling authority. The combination of blockchain with other distributed systems, such as robotic swarm systems, can provide the necessary capabilities to make robotic swarm operations more secure, autonomous, flexible, and even profitable.</p>",,,2019-04-19 14:39:26.259,True,2017-09-01,Blockchain: A new framework for robotic swarm systems,PUBLIC,http://www.eduardocastello.com,True,Human Dynamics,False
robochain-a-secure-data-sharing-framework-for-human-robot-interaction,ecstll,False,"<p> A learning framework for secure, decentralized, computationally efficient data and model sharing among multiple robot units installed at multiple sites.<br></p><p>Robots have potential to revolutionize the way we interact with the world around us. One of their greatest potentials is in the domain of mobile health, where they can be used to facilitate clinical interventions. However, to accomplish this, robots need to have access to our private data in order to learn from these data and improve their interaction capabilities. To enhance this learning process, knowledge sharing among multiple robot units is the natural step forward. However, to date, there is no well-established framework which allows for such data sharing while preserving the privacy of the users, such as hospital patients. To this end, we introduce RoboChain: the first learning framework for secure, decentralized, computationally efficient data and model sharing among multiple robot units installed at multiple sites such as hospitals. RoboChain builds upon and combines the latest advances in open data access, blockchain technologies, and machine learning. We illustrate this framework using the example of a clinical intervention conducted in a private network of hospitals. Specifically, we lay down the system architecture that allows multiple robot units, conducting the interventions at different hospitals, to perform efficient learning without compromising the data privacy.&nbsp;&nbsp;</p>",,,2019-04-19 14:53:57.705,True,2018-03-01,RoboChain: A secure data-sharing framework for human-robot interaction,PUBLIC,http://www.eduardocastello.com,True,Human Dynamics,False
urban-swarms,ecstll,False,"<p>Modern cities have to respond to the growing demands of more efficient and sustainable urban development, as well as an increased quality of life. In this context, the cities of the future will need the ability to gain insight about current urban conditions and react dynamically to them. According to this view, ""smart cities"" can be seen as cybernetic urban environments in which different agents (e.g., citizens) and actuators (e.g., robots) exploit the city-wide infrastructure as a medium to operate synergistically.<b><i> Urban Swarms</i></b> explores the feasibility of swarm robotics systems in urban environments. By using bio-inspired methods, a swarm of robots is able to handle important urban systems and infrastructures, improving their efficiency and autonomy. A diverse set of simulation experiments were designed and conducted using real-world GIS data. Results show that the proposed combination is able to outperform current approaches. <i><b>Urban Swarms</b></i> not only aims to show the efficiency of our proposed solution, but also to give insights about how to design and customize these systems.&nbsp;<a href=""https://www.media.mit.edu/projects/cityscope-volpe/overview/"" style=""font-size: 18px; font-weight: 400;"">CityScope</a><span style=""font-size: 18px; font-weight: 400;"">&nbsp;Volpe ABM model has been customized to integrate Swarm behavior using the </span><a href=""https://gama-platform.github.io/"" style=""font-size: 18px; font-weight: 400;"">Gama Platform</a><span style=""font-size: 18px; font-weight: 400;""> as an </span><a href=""https://github.com/mitmedialab/UrbanSwarms"" style=""font-size: 18px; font-weight: 400;"">open source project</a><span style=""font-size: 18px; font-weight: 400;"">.&nbsp;</span></p>",,,2019-03-12 15:37:36.573,True,2018-10-01,Urban Swarms,PUBLIC,http://www.eduardocastello.com,True,Human Dynamics,False
opal-health,ecstll,False,<h1><b>Open Algorithms (OPAL)</b></h1>,,,2018-10-19 21:07:55.443,True,2017-11-01,OPAL 4 Health,PUBLIC,https://www.shadaalsalamah.com/,True,Human Dynamics,False
healthy-blockchain,ecstll,False,<p>Achieving a safe privacy-preserving information sharing environment for individualized care using blockchain-based technology in multiple use cases in the healthcare space.</p>,,,2019-04-01 17:40:08.489,False,2017-09-01,Healthy Blockchain,PUBLIC,,True,Human Dynamics,False
basic,ecstll,False,"<p>Autonomous vehicles (AVs), drones, and robots will revolutionize our way of traveling and understanding urban space. In order to operate, all of these devices are expected to collect and analyze a lot of sensitive data about our daily activities. However, current operational models for these devices have extensively relied on centralized models of managing these data. The security of these models unveiled significant issues.</p><p>This project&nbsp; proposes BASIC, the Blockchained Agent-based Simulator for Cities. This tool aims to verify the feasibility of the use of blockchain in simulated urban scenarios by considering the communication between agents through&nbsp;<i>smart contracts</i>. In order to test the proposed tool, we implemented a car-sharing model within the city of Cambridge (Massachusetts, USA). In this research, the relevant literature was explored, new methods were developed, and different solutions were designed and tested. Finally, conclusions about the feasibility of the combination between blockchain technology and agent-based simulations were drawn.</p><p>Developed using&nbsp;<a href=""https://gama-platform.github.io/"">Gama Platform</a>.&nbsp;&nbsp;</p><p>Click <a href=""https://github.com/mitmedialab/Basic"">here</a> for the Open Source Repository.</p>",,,2019-04-23 20:08:21.338,True,2018-09-03,BASIC: Blockchained Agent-based Simulator for Cities,PUBLIC,,True,Human Dynamics,False
bandicoot-a-python-toolbox-for-mobile-phone-metadata,yva,False,"<p>bandicoot provides a complete, easy-to-use environment for researchers using mobile phone metadata. It allows them to easily load their data, perform analysis, and export their results with a few lines of code. It computes 100+ standardized metrics in three categories: individual (number of calls, text response rate), spatial (radius of gyration, entropy of places), and social network (clustering coefficient, assortativity). The toolbox is easy to extend and contains extensive documentation with guides and examples.</p>",,--Choose Location,2019-04-19 14:40:08.264,True,2015-09-01,bandicoot: A Python toolbox for mobile phone metadata,PUBLIC,,True,Human Dynamics,False
on-the-reidentifiability-of-credit-card-metadata,yva,False,"<p>Even when real names and other personal information are stripped from metadata datasets, it is often possible to use just a few pieces of information to identify a specific person. Here, we study three months of credit card records for 1.1 million people and show that four spatiotemporal points are enough to uniquely reidentify 90 percent of individuals. We show that knowing the price of a transaction increases the risk of reidentification by 22 percent, on average. Finally, we show that even data sets that provide coarse information at any or all of the dimensions provide little anonymity, and that women are more reidentifiable than men in credit card metadata.</p>",,--Choose Location,2019-04-19 14:48:17.075,True,2015-01-01,On the re-identifiability of credit card metadata,PUBLIC,,True,Human Dynamics,False
yvas-untitled-project-2,yva,False,"<p>OPAL is a project to allow for private data to be used in privacy-conscientious ways for good. Collaborating companies can use OPAL's open platform and algorithms behind their own firewalls to extract key development indicators. OPAL grew out of the recognition that accessing big data sources for research and policy purposes has been a conundrum. To date, data held by private companies, such as large-scale mobile phone data, have been accessed and analyzed externally, either through data challenges, or through bilateral agreements. While these types of engagements offered evidence of big data's promise and demand, these modalities limit the full realization of its potential. By ""sending the code to the data"" rather than the other way around, OPAL seeks to address these challenges and develop data services on the basis of greater trust between all parties involved.</p>",,--Choose Location,2016-10-24 19:44:56.655,False,2016-01-01,OPAL: Privacy-Conscientious Use of Mobile Phone Data,PUBLIC,,True,Human Dynamics,False
improving-official-statistics-in-emerging-markets-using-machine-learning-and-mobile-phone-data,yva,False,"<p><span style=""font-size: 18px; font-weight: 400;"">Mobile phones are one of the fastest growing technologies in the developing world with global penetration rates reaching 90%. Mobile phone data, also called CDR, are generated every time phones are used and recorded by carriers at scale. CDR have generated groundbreaking insights in public health, official statistics, and logistics. However, the fact that most phones in developing countries are prepaid means that the data lacks key information about the user, including gender and other demographic variables. This precludes numerous uses of this data in social science and development economic research. It furthermore severely prevents the development of humanitarian applications such as the use of mobile phone data to target aid towards the most vulnerable groups during crisis.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: 400;"">We developed a framework to extract more than 1,400 features from standard mobile phone data and used them to predict useful individual characteristics and group estimates. We here present a systematic cross-country study of the applicability of machine learning for dataset augmentation at low cost. We validate our framework by showing how it can be used to reliably predict gender and other information for more than half a million people in two countries. We show how standard machine learning algorithms trained on only 10,000 users are sufficient to predict individual’s gender with an accuracy ranging from 74.3 to 88.4% in a developed country and from 74.5 to 79.7% in a developing country using only metadata. This is significantly higher than previous approaches and, once calibrated, gives highly accurate estimates of gender balance in groups. Performance suffers only marginally if we reduce the training size to 5,000, but significantly decreases in a smaller training set. We finally show that our indicators capture a large range of behavioral traits using factor analysis and that the framework can be used to predict other indicators of vulnerability such as age or socio-economic status. Mobile phone data has a great potential for good and our framework allows this data to be augmented with vulnerability and other information at a fraction of the cost.</span></p>",,,2018-10-19 20:56:21.936,True,2016-06-01,Improving official statistics in emerging markets using machine learning and mobile phone data,PUBLIC,,True,Human Dynamics,False
wearable-lab-on-body,pratiks,False,"<p>Wearables are being widely researched for monitoring individual's health and wellbeing. Current generation wearable devices sense an individual's physiological data such as heart rate, respiration, electrodermal activity, and EEG,  but lack in sensing their biological counterparts, which drive the majority of an individual's physiological signals. On the other hand, biosensors for detecting biochemical markers are currently limited to one-time use, are non-continuous, and don't provide flexibility in choosing which biomarker they sense. We present ""wearable lab on body,"" a platform for active continuous monitoring of human biomarkers from the biological fluid.&nbsp;<br></p><p><i>To appear in IEEE Engineering for Biology and Medicine Society (EMBC) - Pataranutaporn et. al., 2019</i></p>",,,2019-05-10 04:27:17.379,True,2018-08-30,Wearable Lab on Body,PUBLIC,,True,Other,False
machine-learning-from-biomarker-signatures-and-correlation-to-systemic-health-conditions,pratiks,False,"<p>Imaging fluorescent disease biomarkers in tissues and skin is a non-invasive method to screen for health conditions. We report an automated process that combines intraoral fluorescent porphyrin biomarker imaging, clinical examinations, and machine learning for correlation of systemic health conditions with periodontal disease. 1,215 intraoral fluorescent images, from 284 consenting adults aged 18-90, were analyzed using a machine learning classifier that can segment periodontal inflammation. The classifier achieved an AUC of 0.677 with precision and recall of 0.271 and 0.429, respectively, indicating a learned association between disease signatures in collected images. Periodontal diseases were more prevalent among males (p=0.0012) and older subjects (p=0.0224) in the screened population. Physicians independently examined the collected images, assigning localized modified gingival indices (MGIs). MGIs and periodontal disease were then cross-correlated with responses to a medical history questionnaire, blood pressure and body mass index measurements, and optic nerve, tympanic membrane, neurological, and cardiac rhythm imaging examinations. Gingivitis and early periodontal disease were associated with subjects diagnosed with optic nerve abnormalities (p &lt;0.0001) in their retinal scans. We also report significant co-occurrences of periodontal disease in subjects reporting swollen joints (p=0.0422) and a family history of eye disease (p=0.0337). These results indicate cross-correlation of poor periodontal health with systemic health outcomes and stress the importance of oral health screenings at the primary care level. Our screening process and analysis method, using images and machine learning, can be generalized for automated diagnoses and systemic health screenings for other diseases.</p><p><strong>Why is this work important?</strong></p><p>Standard practices like visual assessment and diagnosis of oral diseases using bleeding with a probe do not account for patient-to-patient variation or identify disease progression risk. This study uses a machine learning model to segment oral porphyrin biomarker levels from intraoral photographs and find correlations with and prognoses of systemic health conditions.</p><p><strong>What has been done before?</strong></p><p>Current methods to diagnose oral diseases include visual inspection by doctors and probing the gums. Positive correlations have been found between oral health and heart diseases, diabetes, tobacco use, and smoking, but all depend on visual examination by doctors.</p><p><strong>What are our contributions?</strong></p><p>We report a novel process for automated machine learning oral health examinations using images of fluorescent biomarkers and cross-correlations between oral and systemic health. We collect a novel dataset for the study and find correlations between oral health and systemic conditions like swollen joints, optical nerve abnormalities in retinal scans, and a family history of eye disease. Our approach can be generalized for predicting systemic health by analyzing other biomarker images.</p><p><strong>What are the next steps?</strong></p><p>We are actively expanding the work to a larger population to discover novel cross-correlations between other biomarkers and systemic health outcomes.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/"">Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care</a></li><li><a href=""https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/"">Machine Learning and Automated Segmentation of Oral Diseases using Biomarker Images</a></li></ol>",,,2018-11-14 19:32:31.569,True,2017-09-18,Machine Learning from Biomarker Signatures and Correlation to Systemic Health Conditions,PUBLIC,,True,Other,False
porphyrin-imaging,pratiks,False,"<p>We built a low-cost and open source 405 nm imaging device to capture red fluorescence signatures associated with the oral biomarker porphyrin, demonstrating comparable performance to an expensive commercially available device.&nbsp; We also provide a miniaturized mobile-adaptable version of the device. A step-by-step guide for device assembly and the&nbsp;associated computer vision algorithm are shared on the project website&nbsp;to facilitate open-source access to imaging technologies.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/machine-learning-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images/overview/"">Machine Learning for Combined Classification of Fluorescent Biomarkers and Expert Annotations Using White Light Images</a></li></ol>",,,2018-05-04 20:56:15.953,True,2015-06-01,Biomarker Imaging with Mobile Phones,PUBLIC,,True,Other,False
near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging,pratiks,False,"<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Cone beam computed tomography (CBCT) is also widely used for diagnosis but is expensive and relatively cumbersome. Near-infrared imaging (NIR) offers a non-ionizing alternative for dental analysis. We examine and compare features in multiple extracted teeth using conventional radiographic, CBCT, and NIR transillumination imaging modes. NIR imaging can provide unique diagnostic value, primarily in its ability to reveal the extent of surface demineralization. We also provide examples where NIR illumination indicated underlying problem sites in need of further clinical attention and propose the use of NIR imaging to guide targeted and rational use of ionizing radiation in patients.</p><p><strong>Why is this work important?</strong></p><p>Two-dimensional radiographs and cone beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong><br></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. Much previous work has focused on light at 1310 nm, which strikes a balance between enamel and water attenuation, but such a wavelength often requires expensive sensors to image. NIR light at 850 nm has similar dental imaging properties, but it has not been studied as thoroughly as NIR at 1310 nm. It is not well understood what clinical features, if any, are present in NIR dental images, especially at 850 nm. Our previous work has examined the sensitivity of 850 nm NIR images to early caries lesions, but if NIR is to synergistically augment X-rays and CBCT as the standard of care, we must evaluate how well such images represent other clinical features.</p><p><strong>What are our contributions?</strong></p><p>We examine and compare features in multiple extracted teeth using conventional radiographic, CBCT, and NIR transillumination modes. NIR imaging can provide unique diagnostic value, primarily in its ability to reveal the extent of surface demineralization. We also provide examples where NIR illumination indicated underlying problem sites in need of further clinical attention and propose the use of NIR imaging to guide targeted and rational use of ionizing radiation in patients. We also show that NIR imaging identifies clinical features associated with early dimineralization and enamel caries that are not apparent upon expert visual examination.</p><p><strong>What are the next steps?</strong></p><p>Ongoing work is being done to model the interaction of light inside the tooth in order to provide even more diagnostic power.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth/overview/"">Near-Infrared Imaging for Detecting Dental Caries</a></li><li><a href=""https://www.media.mit.edu/projects/replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis/overview/"">Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography</a></li></ol>",,,2018-10-21 18:35:38.823,True,2017-05-08,Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging,PUBLIC,,True,Other,False
technology-enabled-mobile-phone-screenings-augment-routine-primary-care,pratiks,False,"<p>We have developed a new process to screen patients at the point-of-care with FDA-approved technology-enabled mobile health screenings (TES) and compare the results with routine health screenings. A study of nearly 500 patients was conducted to test the effectiveness of this new screening process. This is one of the first studies to investigate using TES to augment routine health examinations. We recommend using TES in synergy with routine health screenings to identify missing sick patients who might otherwise lack comprehensive primary care.<br></p><p><strong>Why is this work important?</strong></p><p>Providing good healthcare in low- and middle-income countries (LMIC) paradoxically requires expensive equipment,&nbsp;<span style=""font-family: &quot;Neue Haas Grotesk Display&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 18px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400;"">which may not be easily available because of resource limitations,</span><span style=""font-size: 18px; font-weight: 400;"">&nbsp;for health monitoring and assessment. There is high variation in the degrees of healthcare access in LMIC, but such access is important because cardiovascular diseases, preventable blindness, oral cancer, and treatable neurological conditions constitute more than half of the disease burden in LMIC. Comprehensive TES may allow for more patients to be screened for more conditions in resource-limited settings, improving their access to primary healthcare. A lack of consensus exists about the usefulness of TES in augmenting primary health screenings in LMIC.</span></p><p><strong>What has been done before?</strong><br></p><p>Devices that allow TES have typically been evaluated in isolated silos, concentrating on individual devices or specific anatomical sites. They have additionally not been comprehensively evaluated alongside routine health screenings.</p><p><strong>What are our contributions?</strong><br></p><p>This is one of the first studies to investigate using multiple TES to augment routine health examinations. To facilitate this large-scale study, we developed and successfully used web examination platforms that enabled multiple physicians to diagnose health conditions remotely. We identified patients who would not have received the care they need in the absence of TES, and link TES to primary health outcomes.</p><p>This study led to significant insights about strategies to develop technologies at MIT that are ready for deployment for effective and scalable primary care in the real world.</p><p><strong>What technology-enabled examinations were performed?</strong></p><p>Single-lead ECG: AliveCor Mobile ECG</p><p>Blood oxygen saturation: Contec Medical Systems 50-DL Pulse Oximeter</p><p>Oral imaging: ACTEON Soprocare</p><p>Retinal scan: D-EYE direct ophthalmoscopy adapter attached to iPhone5s camera</p><p>Tympanic membrane imaging: CellScope Oto with iPhone5 LEDs and camera</p><p>Neurological examinations: Microsoft Kinect</p><p><strong>What are the next steps?</strong></p><p>We are actively working on automated diagnoses, analyses of disease co-occurrence, and patient risk stratification.</p><p>Future studies that build on our technology-enabled screening process can evaluate the process for larger numbers of patients. A future longitudinal study may allow for additional insights into time-varying conditions.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/"">Machine Learning and Automated Segmentation of Oral Diseases using Biomarker Images</a></li></ol>",,,2018-10-21 18:32:18.676,True,2015-05-15,Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care,PUBLIC,,True,Other,False
antimicrobials-that-treat-infectious-diseases-without-leading-to-resistant-bacteria,pratiks,False,"<p>""Without urgent, coordinated action by many stakeholders, the world is headed for a post-antibiotic era, in which common infections and minor injuries which have been treatable for decades can once again kill,"" says Dr. Keiji Fukuda, WHO’s assistant director-general for health security. ""Effective antibiotics have been one of the pillars allowing us to live longer, live healthier, and benefit from modern medicine. Unless we take significant actions to improve efforts to prevent infections and also change how we produce, prescribe, and use antibiotics, the world will lose more and more of these global public health goods and the implications will be devastating.""</p><p>The WHO's ""Antimicrobial resistance: global report on surveillance 2014 report"" notes that resistance is occurring across many different infectious agents, but the report focuses on antibiotic resistance in seven different bacteria responsible for common, serious diseases such as bloodstream infections (sepsis), diarrhea, pneumonia, urinary tract infections, and gonorrhea. The results are cause for high concern, documenting resistance to antibiotics, especially ""last resort"" antibiotics, in all regions of the world. New research by Dr. Pratik Shah&nbsp;at Harvard Medical School identified a cost-effective way to&nbsp;treat bacterial infections without antibiotics. Dr. Shah described a molecular switch, controlled by bacterial&nbsp;diets, that toggles microbial infectivity in humans. Exploiting bacterial diets to train them to be&nbsp;good residents of our bodies&nbsp;shows that unorthodox ways to combat and treat antibiotic resistance may lead to the next generation of antimicrobials.</p>",,,2018-10-22 17:55:29.309,True,2014-11-03,Antimicrobials to Treat Infectious Diseases without Leading to Resistant Bacteria,PUBLIC,,True,Other,False
artificial-intelligence-for-drug-discovery-and-clinical-trials,pratiks,False,"<p>Future of clinical development is on the verge of a major transformation due to convergence of large new digital data sources, the computing power to identify clinically-meaningful patterns in the data using efficient artificial intelligence (AI) and machine-learning (ML) algorithms, and regulators embracing this change through new collaborations. This perspective summarizes insights and recommendations for a new digital paradigm for healthcare from academy, biotechnology industry, non-profit foundations, regulators and technology corporations. Analysis and learning from publically available biomedical and clinical trial datasets, real world evidence from sensors and health records by machine learning architectures are discussed. Strategies for modernizing the clinical development process by integration of AI and ML based digital methods and secure computing technologies through recently announced regulatory pathways at the United States Food and Drug Administration are outlined. We conclude by discussing impact of digital algorithmic evidence to improve medical care for patients.</p>",,,2019-03-11 17:36:08.527,True,2017-01-23,Artificial Intelligence and Machine Learning in Clinical Development: a Translational Perspective,PUBLIC,,True,Other,False
near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth,pratiks,False,"<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Near-infrared imaging offers a non-ionizing alternative for dental analysis. We describe the construction and validation of a near-infrared imaging device to identify dental caries without the use of radiographs. It uses 850 nm light, allowing for a low-cost sensor and device construction.</p><p><strong>Why is this work important?</strong></p><p>While two-dimensional radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. Much previous work has focused on light at 1310 nm, which strikes a balance between enamel and water attenuation, but such a wavelength often requires expensive sensors to image. NIR light at 850 nm has similar dental imaging properties, but it has not been studied as thoroughly as NIR at 1310 nm. Previous studies have similarly neglected the extent to which indicators of dental health, especially early caries associated with the onset of more severe conditions, can be identified in 850 nm NIR images.</p><p><strong>What are our contributions?</strong></p><p>We describe the construction of a near-infrared imaging device to identify dental caries without the use of radiographs. Light-emitting diodes at 850 nm allow for the use of a low-cost imaging sensor. Its camera-wand design allows for multiple imaging configurations: reflectance, transillumination, and occlusal transillumination. We validate the diagnostic uses for the images produced by our device, determining that they provide insight into the location of caries without ionizing radiation. The camera-wand system was also capable of revealing demineralized areas, deep and superficial cracks, and other clinical features of teeth usually only visualized by X-rays.</p><p><strong>What are the next steps?</strong></p><p>Ongoing work is being done to analyze the extent of features made visible by our device and to model the interaction of light inside teeth in order to provide even more diagnostic power.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging/overview/"">Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging</a></li><li><a href=""https://www.media.mit.edu/projects/replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis/overview/"">Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography</a></li></ol>",,,2018-10-21 18:23:04.788,True,2016-06-01,Near-Infrared Imaging for Detecting Dental Caries,PUBLIC,,True,Other,False
computational-histological-staining-and-destaining-of-prostate-core-biopsy-rgb-images-with-generative-adversarial-neural-networks-1,pratiks,False,"<h2><b>General Overview</b></h2><p>Staining of tissues sections using chemical and biological dyes has been used for over a century for visualizing various tissue types and morphologic changes associated with contemporary cancer diagnosis.&nbsp;The staining procedure however is labor intensive, needs trained technicians, costly, and often results in loss of irreplaceable specimen and delays diagnoses. In collaboration with Brigham and Women's Hospital (Boston, MA), we&nbsp; describe a “computational staining” approach to digitally stain photographs of unstained tissue biopsies with Haematoxylin and Eosin (H&amp;E) dyes to diagnose cancer.&nbsp;&nbsp;</p><p>Our method uses neural networks to rapidly stain photographs of non-stained tissues, providing physicians timely information about the anatomy and structure of the tissue.&nbsp; We also report a&nbsp; ""computational destaining"" algorithm that can remove dyes and stains from photographs of previously stained tissues, allowing reuse of patient samples.&nbsp;&nbsp;</p><p>These methods and neural networks assist physicians and patients by novel computational processes&nbsp;at the point-of-care,&nbsp;which can integrate seamlessly into clinical workflows in hospitals all over the world.</p>",,,2018-12-20 15:09:03.246,True,2018-05-01,AI Methods for Rapid Automated Staining and Destaining of Tissue Biopsies in Hospitals,PUBLIC,http://bit.ly/2nMCs3C,True,Other,False
machine-learning-algorithms-for-classification-of-microcirculation-images-from-septic-and-non-septic-patients-1,pratiks,False,"<p><b>General overview:</b></p><p>Sepsis, a life-threatening complication of bacterial infection, leads to millions of worldwide deaths requires significant time and resources to diagnose. This disease is associated with very high mortality rates, making early detection crucial for treatment.&nbsp;</p><p>Researchers have investigated direct clinical evaluation by using dark field imaging of capillary beds under the tongue of septic and healthy subjects for signatures of microcirculatory dysfunction associated with sepsis. Our published results, in collaboration with Beth Israel Deaconess Medical Center, have shown that machine learning and vision can learn higher-order hierarchical diagnostic and prognostic features for rapid and non-invasive diagnosis of sepsis using these dark field microcirculatory images.&nbsp;A neural network capable of distinguishing between images from non-septic and septic patients with more than 90% accuracy is reported for the first time. This approach can help physicians to rapidly stratify patients, facilitate rational use of antibiotics, and reduce disease burden in hospital emergency rooms.</p>",,,2018-12-20 18:14:27.714,True,2018-08-01,Helping Emergency Care Physicians Diagnose Sepsis and Bacterial Infections  with Machine Learning and Vision,PUBLIC,http://bit.ly/2nMCs3C,True,Other,False
self-learning-ai-model-learns-from-patient-data-to-design-novel-clinical-trials,pratiks,False,"<p><b>Technical summary</b></p><p>Unstructured learning problems without well-defined rewards are unsuitable for current reinforcement learning (RL) approaches. Action-derived rewards can allow RL agents to fully explore state and action trade-offs in scenarios that require specific outcomes yet are unstructured by external reward. Clinical trial dosing choice is an example of such a problem. We report the successful formulation of clinical trial dosing choice as an RL problem using action-based rewards and learning of dosing regimens to reduce mean tumor diameters (MTD) in patients undergoing simulated temozolomide (TMZ) and procarbazine, 1-(2-chloroethyl)-3-cyclohexyl-l-nitrosourea, and vincristine (PCV) chemo- and radiotherapy clinical trials. The use of action-derived rewards as partial proxies for outcomes is described for the first time. Novel dosing regimens learned by an RL agent in the presence of action-derived rewards achieve significant reduction in MTD for cohorts and individual patients in simulated TMZ and PCV clinical trials while reducing treatment cycle administrations and dosage concentrations compared to human-expert dosing regimens. Our approach can be easily adapted for other learning tasks where outcome-based learning is not practical.</p><p><b>Glioblastoma (brain tumors) background:&nbsp;</b>Glioblastoma is an aggressive type of cancer that can occur in the brain
or spinal cord. A hard-hitting treatment typically involves combining surgery
with radiation therapy and chemotherapy, which is necessary to combat the
aggressive nature of a glioblastoma. Chemo-and Radiotherapy Treatments (CRT) may
slow the progression of cancer and reduce signs and symptoms, but are unable to
cure the disease. Survival rates are low (about 14-18 months) and only about
10% of patients live five years or longer. CRTs are often given as a
combination of drugs. Procarbazine, 1-(2-chloroethyl)-3-cyclohexyl-l-nitrosourea,
and Vincristine (PCV) is triple drug chemotherapy for glioblastomas and can be
toxic for the patients. Temozolomide (TMZ), is less toxic, has shown greater
efficacy when compared to radiotherapy alone, in the treatment of glioblastoma.
There is significant and urgent need for novel CRT dosing regimens in human subjects
to optimize for maximum therapeutic benefit for patients while reducing
toxicity.</p><p><b>Reinforcement learning background:&nbsp;</b>Reinforcement learning (RL) is an area of machine learning and AI inspired by behaviorist psychology. RL agents can self-learn how to solve complex tasks in a relatively unstructured environment so as to maximize some notion of cumulative&nbsp;rewards and reduce penalties set by human programmers. Reward functions in RL domains are typically derived from a measure that is external to the chosen representation of the states (data) and actions (steps) used for the self-training algorithm. Using RL to solve tasks without readily accessible external scalar outcomes is a relatively unexplored field, as many currently studied domains have well-defined outcomes and associated rewards as part of their definitions.<br></p><p><b>Clinical trials background:&nbsp;</b>Clinical trials to evaluate new drugs, therapies, and vaccines are among the most complex experiments performed in medicine. Nearly half of phase 2 and phase 3 trials fail. For oncology trials, the failure rate rises to two-thirds. A common theme is the difficulty of predicting clinical results in a wide patient base given limited knowledge of key parameters which need to be considered to test candidate molecules, eliminate adverse events, and identify the drugs half maximal inhibitory concentration. Optimal CRT dosing for patients enrolled in glioblastoma clinical trials thus provides one example of an open-ended problem characterized by complex interactions between different drug properties, dosage and timing of administrations (actions), and effects on tumors (state) and where survival (outcomes) may not be available.</p>",,,2018-10-22 14:52:11.990,True,2018-08-10,Self-Learning AI Model Learns from Patient Data to Design Novel Clinical Trials,PUBLIC,https://www.media.mit.edu/groups/health-0-0/overview/,True,Other,False
machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images,pratiks,False,"<p>We report a novel method that processes biomarker images collected at the point of care and uses machine learning algorithms to provide a first level of screening against oral diseases. A machine learning classifier is trained to learn pixel-by-pixel mappings from RGB oral images and output areas with disease. This method can be adapted to&nbsp; process biomarker images from other organs as well.</p><p><strong>Why is this work important?</strong></p><p>Visual inspection and probing techniques have been traditionally used for diagnosis of oral diseases in patients. These traditional methods are subjective and not scalable. We describe the use of RGB color images acquired by low-cost camera devices coupled with machine learning to detect areas with poor oral health.</p><p><strong>What has been done before?</strong></p><p>Currently the gold standard for oral diagnosis is visual inspections by a dentist followed by X-rays. These methods are expensive and invasive.&nbsp;</p><p><strong>What are our contributions?</strong></p><p>We implement a novel technique to combine medical expert knowledge with biomarker signatures.&nbsp; We&nbsp; use RGB color images taken directly at the point-of-care, using low-cost hand-held devices, to provide a first level machine-learning powered screening for patients.</p><p><strong>What are the next steps?</strong></p><p>We are expanding the repertoire of biomarkers that can be detected in RGB color images acquired at the point-of-care and pairing them with automated machine learning exams.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/"">Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care</a></li></ol>",,,2018-08-09 17:24:03.930,True,2017-04-24,Machine Learning and Automated Segmentation of Oral Diseases Using Biomarker Images,PUBLIC,,True,Other,False
detecting-biomarkers-with-printable-paper-diagnostics,pratiks,False,"<p>Paper diagnostics can be used for affordable and scalable biomarker detection. We propose new methods for stabilizing cellulose-based immunoassays, leading to a low-cost paper diagnostic assay for the detection of oral disease biomarkers in human saliva.<br></p><p><strong>Why is this work important?</strong></p><p>Low-cost detection of biomarkers associated with the majority of diseases is not feasible.</p><p><strong>What has been done before?</strong></p><p>Paper-based assays have been used to detect biomarkers in human serum or blood. The paper-based detection of biomarkers in saliva has not been extensively studied.</p><p><strong>What are our contributions?</strong></p><p>We propose methods for stabilizing paper-based immunoassays for robust and low-cost detection of salivary biomarkers. This method is low-cost and can facilitate rapid detection of oral biomarkers. We specifically describe the detection of biomarkers associated with oral diseases, MMP-8 and -9.</p><p><strong>What are the next steps?</strong></p><p>We are expanding this approach to assay biomarkers in serum and blood.</p>",,,2018-05-04 20:38:05.791,True,2016-05-02,Detecting Biomarkers with Printable Paper Diagnostics,PUBLIC,,True,Other,False
at-home-sleep-apnea-screening,pratiks,False,"<p>A large proportion of the American population currently suffers from sleep disorders. Among them are patients with obstructive sleep apnea (OSA), who repeatedly stop breathing while asleep. Current screening methods and devices are impractical for widespread screening. We introduce a new model for low-cost OSA screening consisting of an at-home, wearable sleep mask that can easily track the wearer's sleep patterns. The data collected overnight by this sensory mask provides a determination of a patient's OSA risk.<br></p><p><strong>Why is this work important?</strong></p><p>There are 7-18 million Americans suffering from sleep disorders. Among them are patients with OSA, who stop breathing either completely or partially while asleep. This is a serious condition with few reliable low-cost devices available for primary diagnosis without expert supervision.</p><p><strong>What has been done before?</strong></p><p>The gold standard for OSA diagnosis is overnight polysomnography (PSG). Apart from that there are many home diagnostics devices available. However, many at-home devices offer poor diagnostic quality and some of them also require expert intervention, from installation of the device to analysis of the data.</p><p><strong>What are our contributions?</strong><br></p><p>We report the construction and validation of a design for low-cost OSA screening built around a simplified screening device embedded in an at-home wearable sleep mask. This simplified screening system allows for OSA diagnosis without imposing the costs or time commitment of a full PSG.</p><p><strong>What are the next steps?</strong></p><p>In the next iterations of the device, we aim to improve the mechanical design and ease of use, as well as automate data analysis and screening so that the device can be evaluated in larger studies.</p>",,,2018-05-06 23:20:12.777,True,2016-05-09,At-Home Sleep Apnea Screening,PUBLIC,,True,Other,False
machine-learning-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images,pratiks,False,"<p>Biomarker imaging provides non-invasive indicators of disease and is used by human experts to augment disease diagnosis. It is, however, often expensive and reliant on experts to interpret the resulting images. We have developed a process for learning associations between standard white light images and both biomarkers and expert annotations of disease. Oral imaging is one particular example of biomarker imaging that can supplement expert knowledge; the biomarker porphyrin is associated with poor oral health and oral cancer. We report that our process learns to accurately predict the presence of porphyrin and expert-annotated conditions.</p><p><strong>Why is this work important?</strong></p><p>Biomarker imaging provides non-invasive indicators of disease and is used by human experts to augment disease diagnosis. Capturing biomarker images requires specialized and often expensive hardware, annotations, and analyses by experts, resulting in substantial diagnosis delays.</p><p><strong>What has been done before?</strong></p><p>Even when biomarker imaging is available, experts are often needed to interpret the resulting images. There is a rich literature on medical image segmentation, but many approaches—especially deep learning—require large amounts of images and operate on information from only a single given imaging modality.</p><p><strong>What are our contributions?</strong></p><p>We successfully learn assocations between images and union signatures of biomarker presence and expert disease annotations. By transforming the image-level segmentation problem into a region-based problem, we are able to learn from far fewer images than other approaches. We specifically test our approach on detecting the biomarker porphyrin and associated conditions in millions of image patches. Once trained, the classifiers predict the location of porphyrin in images without requiring specialized biomarker imaging devices or expert intervention.</p><p><strong>What are the next steps?</strong></p><p>We are developing processes incorporating numerous other biomarkers, conditions, and imaging modalities.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/porphyrin-imaging/overview/"">Biomarker Imaging with Mobile Phones</a></li></ol>",,,2018-08-15 19:45:24.252,True,2017-02-01,Machine Learning for Combined Classification of Fluorescent Biomarkers and Expert Annotations Using White Light Images,PUBLIC,,True,Other,False
replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis,pratiks,False,"<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Our goal is to augment ultimately replace the prevalence of ionizing and expensive X-ray imaging and cone-beam computed tomography (CBCT) for dental care with near-infrared (NIR) imaging. Translucency of teeth in the NIR range offers non-ionizing and safe detection of dental features. NIR can be used in conjunction with multiple light sources to create three-dimensional images of teeth. By modeling the scattering of photons in teeth, we can effectively see several millimeters inside, providing additional diagnostic value.</p><p><strong>Why is this work important?</strong></p><p>Two-dimensional radiographs and cone-beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. NIR light at 850 nm and 1310 nm, which strike a balance between enamel and water attenuation, have been shown to provide helpful diagnostics that visual examination alone lacks. Our previous work has demonstrated the sensitivity of 850 nm NIR images to early caries lesions and demineralization. For NIR is to synergistically augment or eventually replace ionizing radiation as the standard of care, we aim to expand its diagnostic potential to clinical features that exist beyond the surface of the tooth.</p><p><strong>What are our contributions?</strong></p><p>This is ongoing research. Preliminary work shows promise for augmenting the diagnostic power of NIR by modeling scattering.</p><p><strong>What are the next steps?</strong></p><p>Large-scale screenings can evaluate the effectiveness of our new imaging process.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth/overview/"">Near-Infrared Imaging for Detecting Dental Caries</a></li><li><a href=""https://www.media.mit.edu/projects/near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging/overview/"">Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging</a></li></ol>",,,2018-10-26 16:23:18.913,True,2017-05-08,Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography,PUBLIC,,True,Other,False
wearable-lab-on-body,patpat,False,"<p>Wearables are being widely researched for monitoring individual's health and wellbeing. Current generation wearable devices sense an individual's physiological data such as heart rate, respiration, electrodermal activity, and EEG,  but lack in sensing their biological counterparts, which drive the majority of an individual's physiological signals. On the other hand, biosensors for detecting biochemical markers are currently limited to one-time use, are non-continuous, and don't provide flexibility in choosing which biomarker they sense. We present ""wearable lab on body,"" a platform for active continuous monitoring of human biomarkers from the biological fluid.&nbsp;<br></p><p><i>To appear in IEEE Engineering for Biology and Medicine Society (EMBC) - Pataranutaporn et. al., 2019</i></p>",,,2019-05-10 04:27:17.379,True,2018-08-30,Wearable Lab on Body,PUBLIC,,True,Fluid Interfaces,False
wearable-wisdom,patpat,False,"<p>Having good mentors and role models is important for personal growth. Knowledge, advice, and inspiration from people a person admires can help motivate people when making life choices. Empirical research has shown that having a role model and insights from a mentor can positively affect the performance and progression of a person's career. However, such advice is not always available from the right people at the right time. Some of our personal heroes have passed away leaving only their writings and other artifacts. We present Wearable Wisdom, a context-aware, audio-based system in a glasses form factor for mediating wisdom from personal mentors to users. Our novel system offers just-in-time knowledge, advice, and inspiration from these mentors, based on the user's inquiry and current context. It does so by performing automated semantic analysis of the mentors' written text and selecting the most relevant quote to the user's inquiry.&nbsp;</p>",,,2019-04-28 03:26:25.734,True,2018-10-18,Wearable Wisdom,PUBLIC,,True,Fluid Interfaces,False
wearable-biotech,patpat,False,"<p>Wearable Biocomputer explores the intersection of wearable computation and biological computation. We designed on-body interfaces for culturing genetically engineered bacteria to sense, process, and actuate.&nbsp;</p>",,,2019-04-29 15:14:59.273,True,2019-04-27,Wearable Biocomputer,LAB,,True,Fluid Interfaces,False
Biological-Enhancement,patpat,False,"<h2><b>Lab on Body, Synthetic Biology, and Bio-Digital Systems for Health and Human Enhancement</b></h2>",,,2019-05-10 15:15:39.601,True,2019-02-03,Theme | Wearable Biotech Enhancement,PUBLIC,,True,Fluid Interfaces,False
human-microbe-interaction,patpat,False,"<p>One of the current foci within the HCI community is to understand and augment human capabilities using physiological and biological data. Microbes living on, inside, and around the human play significant roles in life, from improving health to causing infectious diseases. As the knowledge of human-microbe interaction continues to unfold, we propose a framework for microbial HCI based on a growing body of work aiming to observe, integrate, and modify microorganisms in interactive systems. Our motivation for the framework is to advancing the next generation of biological HCI and exploring novel human-microbe interfaces across contexts, scales, and species.</p>",,,2019-04-29 16:39:12.651,True,2018-11-01,Microbial Augmentation Interfaces,LAB-INSIDERS,,True,Fluid Interfaces,False
SensorySynchrony,patpat,False,"<p>Space and space flight are extreme environments for the human body due to exposure to microgravity and high radiation levels. While the brain is neuroplastic and adapts to different habitats by learning over time, sudden changes in the environment and unpreparedness for it can totally hinder the functioning of the individual.&nbsp;The physiological changes caused by microgravity include vestibular problems causing space motion sickness, bone demineralization, skeletal muscle atrophy, cardiovascular problems, and more.&nbsp;The primary goal of this research project is to investigate vestibular system stimulation techniques to combat motion sickness and create more intuitive experiences when being in non-natural gravity environments.</p><p>Motion sickness is theorized to be either a cause of sensory mismatch between visual and vestibular afferent nerves(inter-sensory) or between semicircular and otolith nerve in the vestibular system (intra-sensory). The magnitude of alteration and the latency between the sensory inputs also contributes to the severeness of the motion sickness.&nbsp;To combat the non-congruent changes in sensory signals while transitioning into space, we propose to investigate vestibular neuromodulation techniques for facilitating adaptation in a more natural way, appeasing the effects of motion sickness and use the altered gravity to create novel experiences in virtual/augmented reality devices.</p><p>We built a prototype for multipole vestibular stimulation for simulating acceleration in roll and pitch axis. The prototype will be tested on the upcoming zero gravity flight for minimizing the effects of alterations between micro and hyper gravity phases.</p>",,,2019-05-31 13:37:03.535,True,2018-09-24,Sensory Synchrony,LAB,,True,Fluid Interfaces,False
micropsi-an-architecture-for-motivated-cognition,joscha,False,"<p>The MicroPsi project explores broad models of cognition, built on a motivational system that gives rise to autonomous social and cognitive behaviors. MicroPsi agents are grounded AI agents, with neuro-symbolic representations, affect, top-down/bottom-up perception, and autonomous decision making. We are interested in finding out how motivation informs social interaction (cooperation and competition, communication and deception), learning, and playing; shapes personality; and influences perception and creative problem-solving.</p>",,--Choose Location,2016-12-05 00:16:37.978,True,2014-01-01,MicroPsi: An Architecture for Motivated Cognition,PUBLIC,,True,Playful Systems,False
cognitive-integration-the-nature-of-the-mind,joscha,False,"<p>While we have learned much about human behavior and neurobiology, there is arguably no field that studies the mind itself. We want to overcome the fragmentation of the cognitive sciences. We aim to create models and concepts that bridge between methodologies, and can support theory-driven research. Among the most interesting questions: How do our minds construct the dynamic simulation environment that we subjectively inhabit, and how can this be realized in a neural substrate? How can neuronal representations be compositional? What determines the experiential qualities of cognitive processes? What makes us human?</p>",,--Choose Location,2016-12-05 00:17:08.864,True,2016-01-01,Cognitive Integration: The Nature of the Mind,PUBLIC,,True,Playful Systems,False
scratch-3-0,tmickel,False,"<p>Since the release of <a href=""https://scratch.mit.edu"">Scratch</a> in 2007, young people around the world have programmed and shared more than 15 million Scratch projects . The <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_1.4"">first generation of Scratch</a> was an application that kids downloaded to local machines. With <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_2.0"">Scratch 2.0</a>, the second and current generation of Scratch, kids create and share their interactive stories, games, and animations directly in web browsers.<br></p><p><span style=""font-size: 18px; font-weight: normal;"">Scratch 3.0 is the next generation of Scratch which takes this experience further by empowering children to create with technology on their mobile devices. In addition, Scratch 3.0 puts a special emphasis on creating with a wide variety of mediums including sound, data, and even the physical world by seamlessly integrating with IoT and digitally enhanced construction kits.</span></p>",,,2016-12-14 20:11:47.650,True,2016-01-01,Scratch 3.0,PUBLIC,,True,Lifelong Kindergarten,False
scratch-3-0,kaschm,False,"<p>Since the release of <a href=""https://scratch.mit.edu"">Scratch</a> in 2007, young people around the world have programmed and shared more than 15 million Scratch projects . The <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_1.4"">first generation of Scratch</a> was an application that kids downloaded to local machines. With <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_2.0"">Scratch 2.0</a>, the second and current generation of Scratch, kids create and share their interactive stories, games, and animations directly in web browsers.<br></p><p><span style=""font-size: 18px; font-weight: normal;"">Scratch 3.0 is the next generation of Scratch which takes this experience further by empowering children to create with technology on their mobile devices. In addition, Scratch 3.0 puts a special emphasis on creating with a wide variety of mediums including sound, data, and even the physical world by seamlessly integrating with IoT and digitally enhanced construction kits.</span></p>",,,2016-12-14 20:11:47.650,True,2016-01-01,Scratch 3.0,PUBLIC,,True,Lifelong Kindergarten,False
scratch-3-0,cwillisf,False,"<p>Since the release of <a href=""https://scratch.mit.edu"">Scratch</a> in 2007, young people around the world have programmed and shared more than 15 million Scratch projects . The <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_1.4"">first generation of Scratch</a> was an application that kids downloaded to local machines. With <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_2.0"">Scratch 2.0</a>, the second and current generation of Scratch, kids create and share their interactive stories, games, and animations directly in web browsers.<br></p><p><span style=""font-size: 18px; font-weight: normal;"">Scratch 3.0 is the next generation of Scratch which takes this experience further by empowering children to create with technology on their mobile devices. In addition, Scratch 3.0 puts a special emphasis on creating with a wide variety of mediums including sound, data, and even the physical world by seamlessly integrating with IoT and digitally enhanced construction kits.</span></p>",,,2016-12-14 20:11:47.650,True,2016-01-01,Scratch 3.0,PUBLIC,,True,Lifelong Kindergarten,False
scratch-3-0,rschamp,False,"<p>Since the release of <a href=""https://scratch.mit.edu"">Scratch</a> in 2007, young people around the world have programmed and shared more than 15 million Scratch projects . The <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_1.4"">first generation of Scratch</a> was an application that kids downloaded to local machines. With <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_2.0"">Scratch 2.0</a>, the second and current generation of Scratch, kids create and share their interactive stories, games, and animations directly in web browsers.<br></p><p><span style=""font-size: 18px; font-weight: normal;"">Scratch 3.0 is the next generation of Scratch which takes this experience further by empowering children to create with technology on their mobile devices. In addition, Scratch 3.0 puts a special emphasis on creating with a wide variety of mediums including sound, data, and even the physical world by seamlessly integrating with IoT and digitally enhanced construction kits.</span></p>",,,2016-12-14 20:11:47.650,True,2016-01-01,Scratch 3.0,PUBLIC,,True,Lifelong Kindergarten,False
scratch-3-0,ericr,False,"<p>Since the release of <a href=""https://scratch.mit.edu"">Scratch</a> in 2007, young people around the world have programmed and shared more than 15 million Scratch projects . The <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_1.4"">first generation of Scratch</a> was an application that kids downloaded to local machines. With <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_2.0"">Scratch 2.0</a>, the second and current generation of Scratch, kids create and share their interactive stories, games, and animations directly in web browsers.<br></p><p><span style=""font-size: 18px; font-weight: normal;"">Scratch 3.0 is the next generation of Scratch which takes this experience further by empowering children to create with technology on their mobile devices. In addition, Scratch 3.0 puts a special emphasis on creating with a wide variety of mediums including sound, data, and even the physical world by seamlessly integrating with IoT and digitally enhanced construction kits.</span></p>",,,2016-12-14 20:11:47.650,True,2016-01-01,Scratch 3.0,PUBLIC,,True,Lifelong Kindergarten,False
scratch-pad,ericr,False,"<p>ScratchBit is an effort to enable children to create more seamlessly in both the physical and digital world by creating a dedicated physical interface for the&nbsp;<a href=""https://scratch.mit.edu"">Scratch</a>&nbsp;programming language and environment. Designed to be rugged, low cost, and highly composable, the ScratchBit allows children to take the materials around them—such as cardboard, clothes, skateboards, and trees—and &nbsp;transform them into inputs to their digital creations on Scratch. Unlike the <a href=""http://makeymakey.com/"">Makey Makey</a> which was designed to make these connections electronically, the ScratchBit is designed to create these connections through motion and mechanism.</p>",,,2018-11-03 16:11:24.635,True,2016-09-01,ScratchBit,PUBLIC,,True,Lifelong Kindergarten,False
scratch-3-0,ascii,False,"<p>Since the release of <a href=""https://scratch.mit.edu"">Scratch</a> in 2007, young people around the world have programmed and shared more than 15 million Scratch projects . The <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_1.4"">first generation of Scratch</a> was an application that kids downloaded to local machines. With <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_2.0"">Scratch 2.0</a>, the second and current generation of Scratch, kids create and share their interactive stories, games, and animations directly in web browsers.<br></p><p><span style=""font-size: 18px; font-weight: normal;"">Scratch 3.0 is the next generation of Scratch which takes this experience further by empowering children to create with technology on their mobile devices. In addition, Scratch 3.0 puts a special emphasis on creating with a wide variety of mediums including sound, data, and even the physical world by seamlessly integrating with IoT and digitally enhanced construction kits.</span></p>",,,2016-12-14 20:11:47.650,True,2016-01-01,Scratch 3.0,PUBLIC,,True,Lifelong Kindergarten,False
scratch-data-blocks,ascii,False,"<p>Scratch Community Blocks is an NSF-funded project that extends the Scratch programming language to enable youth to analyze and visualize their own learning and participation in the Scratch online community. With Scratch Community Blocks, youth in the Scratch community can easily access, analyze, and represent data about the ways they program, share, and discuss Scratch projects.</p>",,--Choose Location,2016-12-05 00:17:22.736,True,2014-01-01,Scratch Community Blocks,PUBLIC,,True,Lifelong Kindergarten,False
scratch-pad,ascii,False,"<p>ScratchBit is an effort to enable children to create more seamlessly in both the physical and digital world by creating a dedicated physical interface for the&nbsp;<a href=""https://scratch.mit.edu"">Scratch</a>&nbsp;programming language and environment. Designed to be rugged, low cost, and highly composable, the ScratchBit allows children to take the materials around them—such as cardboard, clothes, skateboards, and trees—and &nbsp;transform them into inputs to their digital creations on Scratch. Unlike the <a href=""http://makeymakey.com/"">Makey Makey</a> which was designed to make these connections electronically, the ScratchBit is designed to create these connections through motion and mechanism.</p>",,,2018-11-03 16:11:24.635,True,2016-09-01,ScratchBit,PUBLIC,,True,Lifelong Kindergarten,False
making-scratch-accessible,ascii,False,"<p>A conversational, voice-based interface for creating and playing Scratch projects makes Scratch accessible to children regardless of visual ability. Just as Scratch’s visual language lowers the barrier to entry for sighted children, the conversational interface lowers the barrier for children with visual impairments. The screenless interface is inspired by voice assistants and demonstrates the potential for programming through conversation.</p>",,,2019-04-19 18:38:49.254,True,2018-02-13,Agent-based programming interfaces for children,PUBLIC,,True,Lifelong Kindergarten,False
scratch-project-recommendation,ascii,False,"<p>Scratch is built on the principle of “wide walls,” encouraging a wide array of diverse projects, reflecting the diverse interests of Scratchers. In order for Scratch to fulfill this promise, we&nbsp;propose a personalized Scratch project recommendation algorithm that will recommend projects for users to explore, while also taking into account their interests, preferences, and learning pathway. The objective is for such an algorithm to increase the distribution of project attention across the platform as well as broaden the experience of Scratchers. To that end, this project involves using machine learning to classify Scratch projects by type and complexity, making it easier for the project recommendation algorithm to pull specific types of projects to suggest to the user.</p>",,,2019-04-30 15:14:49.042,True,2018-09-05,Scratch Project Recommendation,LAB-INSIDERS,,True,Lifelong Kindergarten,False
scratch-3-0,bowman,False,"<p>Since the release of <a href=""https://scratch.mit.edu"">Scratch</a> in 2007, young people around the world have programmed and shared more than 15 million Scratch projects . The <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_1.4"">first generation of Scratch</a> was an application that kids downloaded to local machines. With <a href=""https://wiki.scratch.mit.edu/wiki/Scratch_2.0"">Scratch 2.0</a>, the second and current generation of Scratch, kids create and share their interactive stories, games, and animations directly in web browsers.<br></p><p><span style=""font-size: 18px; font-weight: normal;"">Scratch 3.0 is the next generation of Scratch which takes this experience further by empowering children to create with technology on their mobile devices. In addition, Scratch 3.0 puts a special emphasis on creating with a wide variety of mediums including sound, data, and even the physical world by seamlessly integrating with IoT and digitally enhanced construction kits.</span></p>",,,2016-12-14 20:11:47.650,True,2016-01-01,Scratch 3.0,PUBLIC,,True,Lifelong Kindergarten,False
dancing-membrane,vsumini,False,"<p>We present an interactive shape-changing display—Dancing Membrane—using the deformation of fabric and airflow control. To explore the new way of rendering an organic and natural experience with the shape-changing display, we have been experimenting with different types of fabric that can give soft textures. </p><p>In order to create and control the local deformation of fabric, we developed a 6-DOF variable diameter nozzle platform which enables us to control the direction and pressure of airflow.&nbsp;By controlling those, we were able to create variable sizes of fabric deformation and vibration of fabric with the computed simulation results. The computational model we created allowed us to predict the responsive dynamic motion of fabric to the airflow. For the next step, we hope to exhibit it as an interactive art installation—as well as a shape-changing display in general for games, projection mapping with organic textures, and create a library of different types of fabric interacting with airflow for a computational simulation model.</p>",,,2019-04-23 21:15:19.625,True,2018-07-01,Dancing Membrane: Controlling the local deformation of fabric for an interactive shape-changing display and its computational simulation,LAB-INSIDERS,,True,Responsive Environments,False
spacehuman,vsumini,False,"<p>SpaceHuman is a soft robotics device designed to facilitate the exploration of environments with reduced gravity in a view of democratization and openness towards access to space and its exploration. &nbsp;It is based on the idea that one day, people who have not received a long preparation and training, as happens today with the astronauts, will be able to have access to the space having a type of conformation and physical configuration that is not adapted to this kind of setting.&nbsp;</p><p>The analysis of the unique seahorse's tail structure became the insight of the overall biomimetic design process. In fact, seahorse tail movement, gripping and protection to the seahorse while floating.&nbsp;Moreover, seahorses do not use their tails to swim; instead, they use them to grasp objects in their environment while they camouflage to hide from predators and hunts for prey. Flexibility and resiliency are key features that enable these behaviours.</p><p>SpaceHuman is an additive prosthesis or otherwise definable as a ""supernumerary robot."" SpaceHuman will facilitate the use of space in zero gravity or reduced gravity restoring the right motion and balance of our body and assigning a new function to a part of our body that until now has not been fully exploited except for the transport of loads, our back. Users will thus be able to build a new poetics of the body and its movements within this radically different space through SpaceHuman, creating new scenarios of its application. Through air chambers specifically designed to be able to change their shape and bend along a reinforcing rib of the material, the people who will use SpaceHuman will be able to cling to useful surfaces inside orbital housing or in Lunar or Martian villages.&nbsp;</p>",,,2019-05-13 21:47:19.873,True,2018-08-01,SpaceHuman,PUBLIC,,True,Responsive Environments,False
tidmarsh-living-observatory-portal,vsumini,False,"<p>The Tidmarsh Living Observatory Portal is a research project that focuses on the realization of a pavilion that will&nbsp;generate an immersive experience about the Tidmarsh Living Observatory. This&nbsp;site has been restored from a former cranberry farm to natural wetland.&nbsp;Through an extensive Responsive Environments research, this networked&nbsp; and&nbsp;outdoor instrumented site streams live&nbsp; data that will be part of the portal&nbsp;experience.</p>",,,2019-04-19 18:16:55.498,False,2019-04-19,Tidmarsh Living Observatory Portal,PUBLIC,,True,Responsive Environments,False
living-materials-library,ssunanda,False,"<p>The control of living systems as part of design interfaces is of interest to both the scientific and design communities due to the ability of living organisms to sense and respond to their environments.  They may, for example, detect and break down harmful environmental agents, or create beneficial products when environmental levels dropped below a certain threshold.  However, it is also important for these systems to be reversible, so that the biological components are only active when their functionality is necessary, and the system can remain dormant otherwise.&nbsp;</p><p>The Living Material Library is an exploration of tunable hybrid systems. Our work in this area demonstrates the means through which intrinsic material properties may be functionally changed through environmental factors and, in turn, serve as dynamic substrates for living systems. Nearly all organisms have highly developed sensing capabilities, and have been shown to behaviorally respond to changes in substrate properties. By creating a tunable and reversible material system, we explore how cell behavior such as adhesion, patterning, and differentiation may be influenced via an active interface.
                    
                </p><p>In this iteration, we propose a reversible material system that allows for control of living interactions (much like a light switch). We are particularly interested in fluid material systems (such as electrorheological fluids) that transition from a liquid-like to a solid-like state when exposed to electric fields and currents.&nbsp;</p><p>This endeavor brings to light the complex relationship between dynamic materials and living systems. While other methods of cell intervention often rely on light, chemicals, or temperature, here we explore substrate material properties as inputs for organisms. &nbsp;Our library may allow for more directed inquiry into processes such as collective cell durotaxis, general mechanotaxis, and active sensing. This marks an initial foray into establishing candidate design methods for responsive applications.<br></p>",,,2017-08-04 19:50:13.221,True,2017-03-01,Living Materials Library,PUBLIC,,True,Mediated Matter,False
vespers-iii,ssunanda,False,"<p>Vespers is a collection of masks exploring what it means to design (with) life. From the relic of the death mask to a contemporary living device, the collection embarks on a journey that begins with an ancient typology and culminates with a novel technology for the design and digital fabrication of adaptive and responsive interfaces. We begin with a conceptual piece and end with a tangible set of tools, techniques and technologies combining programmable matter and programmable life.</p><p>The project points towards an imminent future where wearable interfaces and building skins are customized not only to fit a particular shape, but also a specific material, chemical and even genetic make-up, tailoring the wearable to both the body and the environment which it inhabits.</p><p>Imagine, for example, a wearable interface designed to guide ad-hoc antibiotic formation customized to fit the genetic makeup of its user; or, consider smart packaging or surface coatings devices that can detect contamination; finally, consider environmentally responsive architectural skins that can respond to, and adapt—in real time—to environmental cues. Research at the core of this project offers a new design space for biological augmentation across a wide breadth of application domains, leveraging resolution and scale.</p><p>The collection includes three series. The first series features the death mask as a cultural artefact. The final series features a living mask as an enabling technology. The second series mediates between the two, marking the process of ‘metamorphosis’ between the ancient relic and its contemporaneous interpretation.The living masks in the final series embody habitats that guide, inform and ‘template’ gene expression of living microorganisms. Such microorganisms have been synthetically engineered to produce pigments and/or otherwise useful chemical substances for human augmentation such as vitamins, antibodies or antimicrobial drugs.Combined, the three series of the Vespers collection represent the transition from death to life, or from life to death, depending on one’s reading of the collection.</p>",,,2018-10-23 15:19:03.760,True,2018-04-01,Vespers III,PUBLIC,,True,Mediated Matter,False
aguahoja,ssunanda,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
printing-living-materials,ssunanda,False,"<p>How can biological organisms be incorporated into product, fashion, and architectural design to enable the generation of multi-functional, responsive, and highly adaptable objects? This research pursues the intersection of synthetic biology, digital fabrication, and design. Our goal is to incorporate engineered biological organisms into inorganic and organic materials to vary material properties in space and time. We aim to use synthetic biology to engineer organisms with varied output functionalities and digital fabrication tools to pattern these organisms and induce their specific capabilities with spatiotemporal precision.</p>",,--Choose Location,2017-04-03 20:39:02.693,True,2014-01-01,Printing Living Materials,PUBLIC,,True,Mediated Matter,False
living-mushtari,ssunanda,False,<p>How can we design relationships between the most primitive and the most  sophisticated life forms? Can we design wearables embedded with  synthetic microorganisms that can enhance and augment biological  functionality? Can we design wearables that generate consumable energy  when exposed to the sun?</p>,,--Choose Location,2017-10-13 23:28:11.103,True,2015-01-01,Living Mushtari,PUBLIC,,True,Mediated Matter,False
synthetic-apiary,ssunanda,False,"<p>The Synthetic Apiary proposes a new kind of environment, bridging urban and organismic scales by exploring one of the most important organisms for both the human species and our planet: bees. We explore the cohabitation of humans and other species through the creation of a controlled atmosphere and associated behavioral paradigms. The project facilitates Mediated Matter's ongoing research into biologically augmented digital fabrication with eusocial insect communities in architectural, and possibly urban, scales. Many animal communities in nature present collective behaviors known as ""swarming,"" prioritizing group survival over individuals, and constantly working to achieve a common goal. Often, swarms of organisms are skilled builders; for example, ants can create extremely complex networks by tunneling, and wasps can generate intricate paper nests with materials sourced from local areas.</p>",,--Choose Location,2017-10-13 23:33:55.831,True,2016-01-01,Synthetic Apiary,PUBLIC,,True,Mediated Matter,False
vespers,ssunanda,False,"<p>Novel technologies for additive manufacturing are enabling design and production at nature’s scale. We can seamlessly vary the physical properties of materials at the resolution of a sperm cell, a muscle cell, or a nerve cell. Stiffness, color, hygroscopy, transparency, conductivity, even scent, can be individually tuned for each three-dimensional pixel within a physical object. The generation of products is therefore no longer limited to assemblages of discrete parts with homogeneous properties. Rather like organs, objects can be computationally ""grown"" and 3D printed to form materially heterogeneous and multi-functional products.</p>",,,2018-05-07 19:48:07.209,True,2016-12-12,Vespers II,PUBLIC,,True,Mediated Matter,False
totems,ssunanda,False,"<p>Biodiversity on planet Earth is under momentous threat, with extinction rates estimated between 100 and 1,000 times their pre-human level. The Mediated Matter group has been in search of materials and chemical substances that can sustain and enhance biodiversity across living systems, and that have so far endured the perils of climate change. Melanin is one such substance illustrating biodiversity at the genetic, species, and ecosystem levels.</p>",,,2019-05-07 13:44:31.523,True,2019-02-27,Totems,PUBLIC,,True,Mediated Matter,False
making-data-matter,ssunanda,False,"<p>We present a multimaterial voxel-printing method enabling the physical visualization of data sets commonly associated with scientific imaging. Leveraging voxel-based control of multimaterial 3D printing, our method enables additive manufacturing of discontinuous data types such as point cloud data, curve and graph data, image-based data, and volumetric data. By converting data sets into dithered material deposition descriptions, through modifications to rasterization processes, we demonstrate that data sets frequently visualized on screen can be converted into physical, materially heterogeneous objects.&nbsp;</p><p>Our approach alleviates the need to post-process data sets to boundary representations, preventing alteration of data and loss of information in the produced physicalizations. Therefore, it bridges the gap between digital information representation and physical material composition. We evaluate the visual characteristics and features of our method, assess its relevance and applicability in the production of physical visualizations, and detail the conversion of data sets for multimaterial 3D printing. We conclude with exemplary 3D printed datasets produced by our method pointing towards potential applications across scales, disciplines, and problem domains.</p>",,,2019-04-18 19:51:47.144,True,2018-05-30,Making Data Matter: Voxel-printing for the digital fabrication of data across scales and domains,PUBLIC,,True,Mediated Matter,False
living-materials-library,bdatta,False,"<p>The control of living systems as part of design interfaces is of interest to both the scientific and design communities due to the ability of living organisms to sense and respond to their environments.  They may, for example, detect and break down harmful environmental agents, or create beneficial products when environmental levels dropped below a certain threshold.  However, it is also important for these systems to be reversible, so that the biological components are only active when their functionality is necessary, and the system can remain dormant otherwise.&nbsp;</p><p>The Living Material Library is an exploration of tunable hybrid systems. Our work in this area demonstrates the means through which intrinsic material properties may be functionally changed through environmental factors and, in turn, serve as dynamic substrates for living systems. Nearly all organisms have highly developed sensing capabilities, and have been shown to behaviorally respond to changes in substrate properties. By creating a tunable and reversible material system, we explore how cell behavior such as adhesion, patterning, and differentiation may be influenced via an active interface.
                    
                </p><p>In this iteration, we propose a reversible material system that allows for control of living interactions (much like a light switch). We are particularly interested in fluid material systems (such as electrorheological fluids) that transition from a liquid-like to a solid-like state when exposed to electric fields and currents.&nbsp;</p><p>This endeavor brings to light the complex relationship between dynamic materials and living systems. While other methods of cell intervention often rely on light, chemicals, or temperature, here we explore substrate material properties as inputs for organisms. &nbsp;Our library may allow for more directed inquiry into processes such as collective cell durotaxis, general mechanotaxis, and active sensing. This marks an initial foray into establishing candidate design methods for responsive applications.<br></p>",,,2017-08-04 19:50:13.221,True,2017-03-01,Living Materials Library,PUBLIC,,True,Object Based Media,False
printed-wearable-holographic-display,bdatta,False,"<p>Holographic displays offer many advantages, including comfort and maximum realism. In this project we adapt our guided-wave light-modulator technology to see-through lenses to create a wearable 3D display suitable for augmented or virtual reality applications. As part of this work we also are developing a femtosecond-laser-based process that can fabricate the entire device by ""printing.""</p>",,--Choose Location,2016-12-05 00:16:56.157,True,2015-09-01,Printed Wearable Holographic Display,PUBLIC,,True,Object Based Media,False
emotive-materials,bdatta,False,"<p>The design process is no longer limited to one group of individuals, as number, level, and cost make tools ever more accessible. As we move towards tools that allow us to create our own materials, having a set of rules with which to evaluate, interpret, and design them will become increasingly important. One way of approaching this problem is by unpacking the ways in which materials create meaning. This project explores the more emotive aspects of materials, such as haptic responses to, cognitive evaluation of, and emotive perception of materials to understand how materials communicate meaning.The development of an effective methodology aims to lower the barriers of fabrication of engaging objects. By incorporating qualities that were not previously quantifiable, we aim to encourage a more interactive design process that allows for the production of experiences tailored to individual preference, and a framework for conversations around material issues.</p>",,--Choose Location,2016-12-05 00:17:01.184,True,2015-09-01,Emotive Materials,PUBLIC,,True,Object Based Media,False
dusk,bdatta,False,"<p>DUSK was created as part of the Media Lab's Advancing Wellbeing initiative (supported by the Robert Wood Johnson Foundation) to create private, restful spaces for people in the workplace. DUSK promotes a vision of a new type of ""nap pod,"" where workers are encouraged to use the structure on a daily basis for regular breaks and meditation. The user is provided with the much-needed privacy to take a phone call, focus, or rest inside the pod for short periods during the day. The inside can be silent, or filled with binaural beats audio; pitch black, or illuminated by a sunlamp; whatever works for users to get the rest and relaxation needed to continue to be healthy and productive. DUSK is created with a parametric press-fit design, making it scalable and suitable for fabrication customizable on a per-user basis.</p>",,--Choose Location,2017-04-05 01:19:27.607,True,2014-09-01,DUSK,PUBLIC,,True,Object Based Media,False
bio-inspired-photonic-materials-producing-structurally-colored-surfaces,bdatta,False,"<p>Advances in science and engineering are bringing us closer and closer to systems that respond to human stimuli in real time. Scientists often look to biology for examples of efficient, spatially tailored multifunctional systems, drawing inspiration from photonic structures like multilayer stacks similar to those in the&nbsp;<i>Morpho butterfly</i>. &nbsp;In this project, we develop an understanding of the landscape of responsive, bio-inspired, and active materials, drawing from principles of photonics and bio-inspired material systems. We are exploring various material processing techniques to produce and replicate structurally colored surfaces, while developing simulation and modeling tools (such as inverse design processes) to generate new structures and colors. Such complex biological systems require advanced fabrication techniques. Our designs are realizable through fabrication using direct laser writing techniques such as two photon polymerization. We aim to compare our model system and simulations to fabricated structures using optical microscopy, scanning electron microscopy, and angular spectrometry. This process provides a toolkit with which to examine and build other bio-inspired, tunable, and responsive photonic systems and expand the range of achievable structural colors.</p><p>Unlike with natural structures, producing biomimetic surfaces allows researchers to test beyond tunability that occurs naturally and explore new theory and models to design structures with optimized functions.&nbsp;The benefits of such biomimetic nanostructures are plentiful: they provide brilliant, iridescent color with mechanical stability and light steering capabilities.&nbsp;&nbsp;By producing biomimetic nanostructures, designers and engineers can capitalize on unique properties of optical structural color, and examine these structures based on human perception and response.</p>",,,2019-04-18 17:28:00.132,True,2018-06-01,"MORPHO: Bio-inspired photonic materials, producing structurally colored surfaces",LAB-INSIDERS,http://www.biancadatta.com,True,Object Based Media,False
for-once-in-your-life,mslw,False,"<p>""For Once In Your Life..."" is a site-specific interactive radio play that uses the various sensors in a smartphone to determine specific details, such as where the user walks within a space, to dynamically affect the story. It's a blend of experiential theatre, modern choice-based interactive fiction, and audio walks such as the work of Janet Cardiff.</p>",,--Choose Location,2016-12-05 00:17:12.536,True,2015-09-01,For Once In Your Life...,PUBLIC,,True,Playful Systems,False
hello-operator,mslw,False,"<p>Hello, Operator! is a vintage telephone switchboard from 1927, refurbished and wired up to a modern computer. It currently runs a time-management game; other games being prototyped are exploring the narrative potential of the system. Overall, the project exists to explore what we gain when we are able to physically engage with the antiquated technology that made the past tick.</p>",,--Choose Location,2016-12-05 00:16:37.619,True,2016-09-01,"Hello, Operator!",PUBLIC,,True,Playful Systems,False
studying-the-evolution-of-gene-drive-systems,jmin01,False,"<p>How will gene drive systems evolve once released into the wild? Can they be reliably overwritten and blocked by immunizing reversal drives? Might they spread into related species? These are difficult questions because wild populations are so much larger than laboratory colonies, meaning critical evolutionary events would never be observed in the lab. We seek to develop nematode worms as a model system to help answer these questions. Nematodes are genetically tractable, reproduce twice each week, and are readily grown in populations numbering in the billions. This allows us to study drive systems intended for other organisms in nematodes. Synthetic site targeting, split drives, and ecological confinement will prevent spread into wild nematodes. Because nematodes are easy to culture and count using Foldscope microscopes, we intend to work with educators to enable students, museum-goers, and citizen scientists to participate in gene drive research.</p>",,--Choose Location,2019-04-17 18:56:53.592,True,2016-01-01,Studying the evolution of gene drive systems,PUBLIC,,True,Sculpting Evolution,False
computer-assisted-transgenesis,jmin01,False,"<p>This is a new platform to automate experiments in genetic engineering and bring large-scale moonshot projects within reach. Too often, lab experiments are limited in scale by human fatigue and costs associated with manual labor. In particular, the process of delivering genetic materials via manual microinjection remains a long-standing bottleneck. We are developing a computer-assisted microinjection platform to streamline the production of transgenic organisms. Briefly, organisms are immobilized in a gel and microinjections are performed using precision robotics using computer vision algorithms. This platform demonstrated high-throughput gene editing in an animal model (C. elegans) for the first time. We will use this technology to refine and create safeguards for our gene drive technology.</p>",,--Choose Location,2016-12-05 00:17:09.401,True,2016-01-01,Computer-Assisted Transgenesis,PUBLIC,,True,Sculpting Evolution,False
studying-the-evolution-of-gene-drive-systems,codyg,False,"<p>How will gene drive systems evolve once released into the wild? Can they be reliably overwritten and blocked by immunizing reversal drives? Might they spread into related species? These are difficult questions because wild populations are so much larger than laboratory colonies, meaning critical evolutionary events would never be observed in the lab. We seek to develop nematode worms as a model system to help answer these questions. Nematodes are genetically tractable, reproduce twice each week, and are readily grown in populations numbering in the billions. This allows us to study drive systems intended for other organisms in nematodes. Synthetic site targeting, split drives, and ecological confinement will prevent spread into wild nematodes. Because nematodes are easy to culture and count using Foldscope microscopes, we intend to work with educators to enable students, museum-goers, and citizen scientists to participate in gene drive research.</p>",,--Choose Location,2019-04-17 18:56:53.592,True,2016-01-01,Studying the evolution of gene drive systems,PUBLIC,,True,Sculpting Evolution,False
computer-assisted-transgenesis,codyg,False,"<p>This is a new platform to automate experiments in genetic engineering and bring large-scale moonshot projects within reach. Too often, lab experiments are limited in scale by human fatigue and costs associated with manual labor. In particular, the process of delivering genetic materials via manual microinjection remains a long-standing bottleneck. We are developing a computer-assisted microinjection platform to streamline the production of transgenic organisms. Briefly, organisms are immobilized in a gel and microinjections are performed using precision robotics using computer vision algorithms. This platform demonstrated high-throughput gene editing in an animal model (C. elegans) for the first time. We will use this technology to refine and create safeguards for our gene drive technology.</p>",,--Choose Location,2016-12-05 00:17:09.401,True,2016-01-01,Computer-Assisted Transgenesis,PUBLIC,,True,Sculpting Evolution,False
collective-sensemaking-in-cryptocurrency-community,suhara,False,"<p>Participants in cryptocurrency markets are in constant communication with each other about the latest coins&nbsp;and news releases. Do these conversations build hype through the contagiousness of excitement, help the&nbsp;community process information, or play some other role? Using a novel dataset from a major cryptocurrency&nbsp;forum, we conduct an exploratory study of the characteristics of online discussion around cryptocurrencies. We find that coins with more information available and higher levels of&nbsp;technical innovation are associated with higher quality discussion. People who talk about serious coins tend&nbsp;to participate in discussion displaying signatures of collective intelligence and information processing, while&nbsp;people who talk about less serious coins tend to display signatures of hype and naïvety. Interviews with&nbsp;experienced forum members also confirm these quantitative findings. These results highlight the varied roles&nbsp;of discussion in the cryptocurrency ecosystem and suggest that discussion of serious coins may be oriented&nbsp;towards earnest, perhaps more accurate, attempts at discovering which coins are likely to succeed.&nbsp;</p>",,,2019-04-19 14:41:02.807,True,2016-06-01,Collective sensemaking in cryptocurrency community,PUBLIC,,True,Affiliates,True
deepshop-understanding-purchase-patterns-via-deep-learning,suhara,False,"<p>The recent availability of quantitative behavioral data provides an opportunity to study human behavior at unprecedented scale. Using large-scale financial transaction data, we propose a novel deep learning framework for understanding human purchase patterns and testing the link between them and the existence of individual financial troubles. Our work opens new possibilities in studying human behavioral traits using state-of-the-art machine learning techniques, without the need for hand-engineered features.</p>",,--Choose Location,2019-04-19 14:43:35.517,True,2016-01-01,DeepShop: Understanding purchase patterns via deep learning,PUBLIC,,True,Affiliates,True
social-bridges-in-community-purchase-behavior,suhara,False,"<p>The understanding and modeling of social influence on human economic behavior in city environments can have important implications. In this project, we study human purchase behavior at a community level and argue that people who live in different communities but work at similar locations could act as ""social bridges"" that link their respective communities and make the community purchase behavior similar through the possibility of social learning through face-to-face interactions.</p>",,--Choose Location,2016-12-05 00:17:04.354,True,2015-09-01,Social Bridges in Community Purchase Behavior,PUBLIC,,True,Affiliates,True
collective-sensemaking-in-cryptocurrency-community,pkrafft,False,"<p>Participants in cryptocurrency markets are in constant communication with each other about the latest coins&nbsp;and news releases. Do these conversations build hype through the contagiousness of excitement, help the&nbsp;community process information, or play some other role? Using a novel dataset from a major cryptocurrency&nbsp;forum, we conduct an exploratory study of the characteristics of online discussion around cryptocurrencies. We find that coins with more information available and higher levels of&nbsp;technical innovation are associated with higher quality discussion. People who talk about serious coins tend&nbsp;to participate in discussion displaying signatures of collective intelligence and information processing, while&nbsp;people who talk about less serious coins tend to display signatures of hype and naïvety. Interviews with&nbsp;experienced forum members also confirm these quantitative findings. These results highlight the varied roles&nbsp;of discussion in the cryptocurrency ecosystem and suggest that discussion of serious coins may be oriented&nbsp;towards earnest, perhaps more accurate, attempts at discovering which coins are likely to succeed.&nbsp;</p>",,,2019-04-19 14:41:02.807,True,2016-06-01,Collective sensemaking in cryptocurrency community,PUBLIC,,True,Human Dynamics,False
evolution-strategies-applied-to-collective-intelligence,pkrafft,False,"<p>We build recommender bots that use machine learning and network analytics to create personalized recommendations for users on various social and financial platforms. We show that bots that work not just on the raw user data, but instead build on human intuition, do far better. We are in the process of live testing these bots on various platforms.&nbsp;</p>",,,2017-04-05 18:49:11.444,True,2017-03-10,Social Learning Recommender Bots,PUBLIC,,True,Human Dynamics,False
social-ai-and-extended-intelligence,pkrafft,False,"<p>There is a deep fear that human jobs will be replaced by AI. Rather than racing against the machines, our aim is to show that a human-AI combination will perform better than humans and AI working alone. Although no man is better than a machine for some tasks, ""no machine is better than a man with a machine"" (Paul Tudor Jones) . Thus, by building ""bots"" that are compatible with human behavior, and specifically leverage the manner in which humans use social information, we have been able to build bots that extend human intelligence capabilities. In a large-scale financial trading experiment, we have shown that groups of humans and ""socially compatible"" AI bots can successfully incorporate human intuition into their decisions and consequently not only do better than humans alone, but also do better than similar AI bots that use only objective information.
                    
                </p>",,,2019-02-12 15:14:05.048,True,2016-07-01,Social AI and Extended Intelligence,PUBLIC,,True,Human Dynamics,False
deep-reinforcement-learning-inspired-by-human-collective-intelligence,pkrafft,False,"<p>We know that it's groups, not individuals, that are capable of the most complex and daunting achievements. Why should AI be any different?&nbsp;</p><p>We show that deep reinforcement learning algorithms that use lessons from how humans learn and communicate with each other can provide large improvements over state of the art reinforcement learning methods.</p><p>Researchers have been studying how groups of problem-solvers organize themselves and communicate for years, under the field of ""collective intelligence."" It's been shown that there are some surprisingly simple relationships between a group's communication network structure and how well that group is able to perform on different kinds of tasks.&nbsp;</p><p>Using these simple lessons, we designed a deep reinforcement learning algorithm that, instead of using one massive neural network (NN), leverages a community of many smaller NNs. We enable these neural networks to communicate with one another, and to learn from each others' explorations and successes.&nbsp;</p><p>Using this strategy yields significant improvements over the state of the art. By placing these neural nets on a communication network that is similar in structure to how humans communicate,&nbsp;we see a 33% improvement in how fast the networks are able to learn and in how well they are able to perform at a benchmark reinforcement learning task.</p><p><br></p>",,,2018-05-09 14:23:16.868,True,2017-07-01,Deep Reinforcement Learning Inspired by Human Collective Intelligence,LAB-INSIDERS,,True,Human Dynamics,False
cityscope-hamburg,mdchurch,False,"<p><a href=""https://medium.com/mit-media-lab/shifting-priorities-finding-places-9ad3bdbe38b8"">Read more about this project here</a></p><p>MIT City Science is working with HafenCity University to develop CityScope for the neighborhood of Rothenburgsort in Hamburg, Germany. The goal is to create an interactive stakeholder engagement tool that also serves as the platform for joint research of modules for city simulation. Researchers are developing modules for walkability, neighborhood connectivity, energy efficiency, and economic activity, among others.</p>",,--Choose Location,2019-05-10 19:07:36.178,True,2015-01-01,City Science Lab Hamburg,PUBLIC,,True,City Science,False
city-science-lab-shanghai,mdchurch,False,"<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>",,,2019-05-16 20:43:38.677,True,2017-02-14,City Science Lab Shanghai,PUBLIC,,True,City Science,False
city-science-guadalajara,mdchurch,False,"<p>The University of Guadalajara, referred to as UdeG, is a university network composed of 15 campuses within the state of Jalisco and one online system. The University offers undergraduate and graduate studies to around 130,000 students. UdeG strives to understand urban performance metrics using evidence-based decision making tools, facilitated through a collaboration with the MIT Media Lab City Science group.</p>",,,2019-05-10 19:44:20.937,True,2018-11-01,City Science Collaboration Guadalajara,PUBLIC,,True,City Science,False
cities-without,mdchurch,False,"<p>Cities are becoming increasingly complex. As of&nbsp;2008, more than half of the world's population has been living in cities; this number is expected to be as high as 70 percent by 2050, with the most growth occurring in the southern hemisphere. According to the Word Bank, 85 percent of the global GDP is created in cites.&nbsp;We must find new solutions to meet the complex challenges of the future.&nbsp;</p><p>We pose an opportunity to rethink current models and invent new systems and strategies. We consider&nbsp;<i>cities without</i>, and the opportunities this can pose for a more livable, equitable, and resilient future.&nbsp;</p>",,,2019-04-08 14:52:00.977,True,2019-03-27,Cities Without,GROUP,,True,City Science,False
cityscope-cooper-hewitt,mdchurch,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
cityscope-hamburg,noyman,False,"<p><a href=""https://medium.com/mit-media-lab/shifting-priorities-finding-places-9ad3bdbe38b8"">Read more about this project here</a></p><p>MIT City Science is working with HafenCity University to develop CityScope for the neighborhood of Rothenburgsort in Hamburg, Germany. The goal is to create an interactive stakeholder engagement tool that also serves as the platform for joint research of modules for city simulation. Researchers are developing modules for walkability, neighborhood connectivity, energy efficiency, and economic activity, among others.</p>",,--Choose Location,2019-05-10 19:07:36.178,True,2015-01-01,City Science Lab Hamburg,PUBLIC,,True,City Science,False
andorra-innovation,noyman,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>The MIT Media Lab's City Science research group, the University of Andorra, and national and international companies are collaborating in order to bring an innovative ecosystem into the capital of Andorra. This innovation district aims to engage local citizens, researchers, and R&amp;D from the companies in order to build together an Andorran living lab, an ""innovation district"" where national and international companies can test and deploy their products and ideas and cultivate human capital.</p><p><b>Current Projects</b></p><ul><li>Andorra Innovation Space</li><li>Andorra Cultural Heritage</li><li>Drones patterns and flows, collaboration living lab<br></li><li>Young Future</li></ul>",,,2018-07-09 18:49:41.844,True,2016-09-01,Andorra | Innovation,PUBLIC,,True,City Science,False
andorra-dynamic-urban-planning,noyman,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>",,,2019-02-25 15:17:09.063,True,2016-09-01,Andorra | Dynamic Urban Planning,PUBLIC,,True,City Science,False
city-science-lab-aalto,noyman,False,"<p>Aalto University, Finland, and the MIT Media Lab’s City Science group are co-developing a version of the MIT CityScope platform for urban analysis, efficient resource utilization, and spatial programming for campus development, using Otaniemi as a testbed. Aalto joins a network of City Science collaborators which includes Tongji University (Shanghai), Taipei Tech (Taiwan), HafenCity University (Hamburg), and ActuaTech (Andorra).</p>",,,2019-05-07 19:59:14.315,True,2017-05-01,City Science Lab Aalto,PUBLIC,,True,City Science,False
andorra-living-lab,noyman,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,City Science,False
Reversed-Urbanism,noyman,False,"<h2>Predicting Urban Performance through Behavioral Patterns in Temporal Telecom Data</h2><p>This study explores a novel method to analyze diverse behavioral patterns in large urban populations and to associate them with discrete urban features. This work utilizes machine learning and anonymized telecom data to understand which fragments of the city has greater potential to attract dense and diverse populations over longer periods of time. Finally, this work suggests a road map for building spatial prediction tools in an effort to improve city-design and planning processes.&nbsp;&nbsp;</p><p><b><a href=""https://cityscope.github.io/CS_Andorra_RNC/"">Click here for an interactive visualization of this study</a>&nbsp;<br><br></b></p><p><b>Advisors:</b>&nbsp;Kent Larson&nbsp;and&nbsp;Esteban Moro<br><b>Thanks to</b> Andorra Telecom, ActuaTech,&nbsp;Núria Macià. <br>Data was&nbsp;obtained by Andorra Telecom as part of MIT Media Lab City Science and the State of Andorra collaboration.&nbsp;</p>",,,2019-02-24 23:21:12.068,True,2017-07-01,Reversed Urbanism,PUBLIC,http://ArielNoyman.com,True,City Science,False
cityscope-volpe,noyman,False,"<p>CityScope Volpe is demonstrating most of the urban planning, analysis, and prediction features developed for the CityScope project. The site, a 14-acre parcel on the northern part of MIT/Kendall Square area of Cambridge, has been acquired and is&nbsp; being developed by MIT. City Science researchers designed and built a CityScope urban performance tool that is aiming to predict the outcomes of multiple planning and development scenarios.&nbsp;</p>",,,2019-02-25 15:14:11.403,True,2016-11-01,CityScope Volpe,PUBLIC,,True,City Science,False
CityscopeBostonbrt,noyman,False,"<p>The&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://mfc.mit.edu/"">Mobility Futures Collaborative</a>&nbsp;in the MIT Department of Urban Studies and Planning (DUSP) and the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://cp.media.mit.edu/"">Changing Places group</a>&nbsp;at the MIT Media Lab have developed new interactive tools aimed to better communicate the possible impacts of new transit systems. The Media Lab and DUSP have partnered with the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""https://www.barrfoundation.org/"">Barr Foundation</a>&nbsp;to test these tools in a series of community engagement workshops to examine the impacts of Bus Rapid Transit (BRT) systems in greater Boston. These tools include the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://cp.media.mit.edu/city-simulation/"">CityScope</a>&nbsp;— an interactive platform that utilizes physical models (built from LEGO bricks) and 3-D projection — to enable community members to engage in neighborhood and street-level decisions including alternative bus corridor designs and station-level variations (such as pre-pay boarding). The second tool,&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://mfc.mit.edu/innovations-participatory-design-brt-systems"">CoAXs</a>&nbsp;is a new interactive platform for collaborative transit planning that builds on open-source urban analytics tools such as&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://conveyal.com/projects/analyst/"">Conveyal Transport Analyst</a></p>",,--Choose Location,2017-10-16 18:32:03.412,True,2015-01-01,CityScope Boston BRT,PUBLIC,,True,City Science,False
finding-places,noyman,False,"<h1><b>What is FindingPlaces?</b></h1><p>In reaction to the sudden arrival of tens of thousands of refugees in the city of Hamburg (DE) in 2015, the Lord Mayor requested the CityScienceLab (CSL) at HafenCity University to facilitate a public discussion and decision-making process on locations for refugee accommodation in Hamburg neighborhoods. With highly sensitive socio-political implications, this project demanded a well-designed technological and procedural approach. CSL employed an innovative Human-Computer Interaction tool, CityScope, to facilitate public participation and urban decision-making. A workshop process was also designed to help multiple participants and stakeholders interact effectively. Running from May to July 2016, the FindingPlaces (FP) project enabled about 400 participants to identify 160 locations accepted by Hamburg’s citizens, out of which 44 passed legal confirmation by the authorities. Overall, on a qualitative level, the project facilitated surprisingly constructive and collaborative interaction, raising awareness and a sense of ownership among participants.</p>",,,2019-02-12 19:05:58.708,True,2016-01-01,FindingPlaces,PUBLIC,https://findingplaces.hamburg/,True,City Science,False
mobcho,noyman,False,"<p>MoCho (short for ""Mobility Choices"") is a&nbsp;<a href=""http://cityscope.media.mit.edu/"">CityScope</a>&nbsp;module focused on mobility choices and societal impacts. This tool helps predict the choices of mobility modes made at the individual level throughout the entire Boston Metro area.</p><p><i style=""font-size: 18px; font-weight: 400;""><a href=""https://cityscope.media.mit.edu/CS_choiceModels/index.html"">Check out a live demo of MoCho predictions here</a>.&nbsp;</i></p>",,,2019-04-19 19:11:18.982,True,2018-06-01,MoCho: Mobility choices and societal impacts,PUBLIC,,True,City Science,False
cityscope_playground,noyman,False,"<p>This project depicts the design, deployment and operation of a Tangible Regulation Platform, a physical-technological apparatus made for the distilment of regulations. The platform is set to exemplify the effects of regulations on a designated territory, allowing planners, designers, stakeholders and community members a common ground for discussion and decision making. An accessible and self-explanatory tool, this platform illustrates the relationship between urban form and regulations, offering a seamless and transparent process of regulation-based urban design. Lastly, projecting on the foreseen future of law and urbanism, this project proposes an alternative data and performance-based approach for the making of new regulations. Beyond excelling the processes of design under regulations, this platform and other new tools are offered to help facilitate a discussion on the way future regulations will be devised, improving both the design processes and their final outcome.</p>",,--Choose Location,2019-04-09 14:40:07.692,True,2014-09-01,CityScope PlayGround: MIT East Campus,PUBLIC,http://ArielNoyman.com,True,City Science,False
city-science-lab-shanghai,noyman,False,"<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>",,,2019-05-16 20:43:38.677,True,2017-02-14,City Science Lab Shanghai,PUBLIC,,True,City Science,False
andorra-tourism,noyman,False,"<p><span style=""font-size: 18px; font-weight: 400;""></span><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/""><span style=""font-size: 18px; font-weight: 400;"">View the main City Science Andorra project profile.</span></a><br></p><p><span style=""font-size: 18px; font-weight: 400;"">With more than eight million visitors a year, tourism represents almost 30% of the economy of Andorra. By gathering and analyzing data from social media, call detail records, and wifi, we can understand the country's dynamics of tourism and commerce as well as design interventions that can improve the experience for tourists, encouraging them to visit Andorra more frequently, stay longer, and increase spending.&nbsp;</span><br></p><h2><b>Current Projects</b></h2><ul><li>Event Analysis<br></li><li>Social Network<br></li><li>Location Recommendation system<br></li></ul><p> </p><h2><b>EVENT ANALYSIS</b></h2><p>Based on the analysis of call detail records and social media, the goal of this project is to understand the tourist behaviors in Andorra.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">After mining those anonymized data, we have been able to learn different patterns and behaviors of the tourism in Andorra thanks to an agent-based model developed in order to represent the flow of people. This simulation is also coupled with an interactive table called CityMatrix.</span></p>",,,2019-02-25 15:33:28.936,True,2015-08-01,Andorra | Tourism,PUBLIC,,True,City Science,False
cityio,noyman,False,"<h1>Cloud-Based Urban Data Platform</h1><p><br>cityIO (input/output) is a cloud- and database-driven platform which allows remote participation, database augmentation, and high-end complex visualization. cityIO operates anywhere, on multiple platforms and devices, using client-side apps or web-based interfaces.&nbsp;The cityIO platform is built for scale and to serve large volumes of end-users in real time, in order to augment multi-participant discussions and decision-making processes. Utilizing the mass adaptation of mobile and hand-held devices, cityIO promotes an equal and decentralized discussion for multiparty stakeholders.&nbsp;cityIO offers a suite of augmented reality data-visualization tools that utilize server-side data and analysis. cityIO allows client-side interactions in multiple forms:</p><p><b>AUGMENTED REALITY URBAN SIMULATION</b></p><p>cityIO is intended to reduce complexity in design and planning tools and to support  data-driven environments for planners, designers, and decision makers. cityIO uses modern simulation tools and employs cutting-edge AR applications in order to offer an immersive user experience for both planning professionals and the general public. These simulations can augment indoor and outdoor environments, physical models, and technical drawings. </p><p><b>REMOTE AND DECENTRALIZED PUBLIC PARTICIPATION</b></p><p>Using self-explanatory web and mobile apps with high-end visualization and user interfaces, cityIO offers cities, municipalities, and planning authorities the ability to better communicate complex planning processes and to aggregate the public's opinion in real time. cityIO's scalable server side allows multiple users to collaborate, participate, and voice their opinions on design and planning initiatives.  </p><p><b>CITYI/O HAMBURG<br></b></p><p>cityIO Hamburg augments cityMatrix table. This deployment allows design in the urban context of the Rothenburgsort neighborhood.</p>",,,2018-10-20 17:04:48.161,True,2016-05-01,cityIO,PUBLIC,,True,City Science,False
cityscope-cooper-hewitt,noyman,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
city-science-andorra,noyman,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
cityscope,noyman,False,"<p>City Science researchers are developing a slew of tangible and digital platforms dedicated to solving spatial design and urban planning challenges. The tools range from simulations that quantify the impact of disruptive interventions in cities to communicable collaboration applications. We develop and deploy these tools around the world and maintain open source repositories for the majority of deployments. ""CityScope"" is a concept for shared, interactive computation for urban planning.</p><p>All current CityScope development, tools, and software are open source <a href=""https://cityscope.github.io/"">here</a>.&nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2019-05-16 20:42:57.474,True,2017-08-01,Theme | CityScope,PUBLIC,,True,City Science,False
printing-multi-material-3d-microfluidics,stevenk,False,"<p>Computation and fabrication in biology occur in aqueous environments. Through on-chip mixing, analysis, and fabrication, microfluidic chips have introduced new possibilities in biology for over two decades. Existing construction processes for microfluidics use complex, cumbersome, and expensive lithography methods that produce single-material, multi-layered 2D chips. Multi-material 3D printing presents a promising alternative to existing methods that would allow microfluidics to be fabricated in a single step with functionally graded material properties. We aim to create multi-material microfluidic devices using additive manufacturing to replicate current devices, such as valves and ring mixers, and to explore new possibilities enabled by 3D geometries and functionally graded materials. Applications range from medicine to genetic engineering to product design.</p>",,--Choose Location,2016-12-14 01:49:52.124,True,2014-01-01,Printing Multi-Material 3D Microfluidics,PUBLIC,,True,Mediated Matter,False
3d-printed-hemi-ellipsoidal-dome,stevenk,False,"<p>The Digital Construction Environment is the first architectural-scale structure fabricated with the <a href=""https://www.media.mit.edu/projects/digital-construction-platform-v-2/overview/"">Digital Construction Platform (DCP)</a>.  Using the Mediated Matter group’s Print-In-Place construction technique, an open-domed structure with a diameter of 14.6 m and a height of 3.7 m was manufactured over a print time of 13.5 hours.&nbsp;</p>",,,2019-02-11 20:03:42.274,True,2016-07-17,DCP: Digital Construction Environment,PUBLIC,,True,Mediated Matter,False
digital-construction-platform-v-2,stevenk,False,"<p>The Digital Construction Platform (DCP) is an experimental enabling technology for large-scale digital manufacturing. In contrast to the typical gantry-based approach to digital construction, robotic arm systems offer the promise of greater task flexibility, dynamically expandable workspaces, rapid setup times, and easier implementation with existing construction techniques. Potential applications for this system include fabrication of non-standard architectural forms; incorporation of data gathered on-site in real time into fabrication processes; improvements in construction efficiency, quality, and safety; and exploration of autonomous construction systems for use in disaster relief, hazardous environments, and extraterrestrial exploration.</p>",,,2017-05-05 16:41:04.046,True,2015-08-01,Digital Construction Platform,PUBLIC,,True,Mediated Matter,False
printing-living-materials,stevenk,False,"<p>How can biological organisms be incorporated into product, fashion, and architectural design to enable the generation of multi-functional, responsive, and highly adaptable objects? This research pursues the intersection of synthetic biology, digital fabrication, and design. Our goal is to incorporate engineered biological organisms into inorganic and organic materials to vary material properties in space and time. We aim to use synthetic biology to engineer organisms with varied output functionalities and digital fabrication tools to pattern these organisms and induce their specific capabilities with spatiotemporal precision.</p>",,--Choose Location,2017-04-03 20:39:02.693,True,2014-01-01,Printing Living Materials,PUBLIC,,True,Mediated Matter,False
living-mushtari,stevenk,False,<p>How can we design relationships between the most primitive and the most  sophisticated life forms? Can we design wearables embedded with  synthetic microorganisms that can enhance and augment biological  functionality? Can we design wearables that generate consumable energy  when exposed to the sun?</p>,,--Choose Location,2017-10-13 23:28:11.103,True,2015-01-01,Living Mushtari,PUBLIC,,True,Mediated Matter,False
printing-multi-material-3d-microfluidics,moonshot,False,"<p>Computation and fabrication in biology occur in aqueous environments. Through on-chip mixing, analysis, and fabrication, microfluidic chips have introduced new possibilities in biology for over two decades. Existing construction processes for microfluidics use complex, cumbersome, and expensive lithography methods that produce single-material, multi-layered 2D chips. Multi-material 3D printing presents a promising alternative to existing methods that would allow microfluidics to be fabricated in a single step with functionally graded material properties. We aim to create multi-material microfluidic devices using additive manufacturing to replicate current devices, such as valves and ring mixers, and to explore new possibilities enabled by 3D geometries and functionally graded materials. Applications range from medicine to genetic engineering to product design.</p>",,--Choose Location,2016-12-14 01:49:52.124,True,2014-01-01,Printing Multi-Material 3D Microfluidics,PUBLIC,,True,Mediated Matter,False
printing-living-materials,moonshot,False,"<p>How can biological organisms be incorporated into product, fashion, and architectural design to enable the generation of multi-functional, responsive, and highly adaptable objects? This research pursues the intersection of synthetic biology, digital fabrication, and design. Our goal is to incorporate engineered biological organisms into inorganic and organic materials to vary material properties in space and time. We aim to use synthetic biology to engineer organisms with varied output functionalities and digital fabrication tools to pattern these organisms and induce their specific capabilities with spatiotemporal precision.</p>",,--Choose Location,2017-04-03 20:39:02.693,True,2014-01-01,Printing Living Materials,PUBLIC,,True,Mediated Matter,False
living-mushtari,moonshot,False,<p>How can we design relationships between the most primitive and the most  sophisticated life forms? Can we design wearables embedded with  synthetic microorganisms that can enhance and augment biological  functionality? Can we design wearables that generate consumable energy  when exposed to the sun?</p>,,--Choose Location,2017-10-13 23:28:11.103,True,2015-01-01,Living Mushtari,PUBLIC,,True,Mediated Matter,False
data-for-refugees,singhv,False,"<p>Data for refugees is a big data challenge whereby Turk Telekom opens a large dataset of anonymized mobile phone records to research groups for the purpose of providing better living conditions to Syrian refugees in Turkey.&nbsp;&nbsp;</p><p>We introduce different measures extracted from mobile phone metadata to study the integration of refugees along three dimensions: (1) social integration, (2) spatial integration, and (3) economic integration through signatures of employment activity. We use these measures to compare integration across different regions in Turkey and find striking differences both in the distributions of these dimensions and the relations between them.&nbsp;<br></p><p>The paper is currently under review but will be shared soon.&nbsp;</p>",,,2019-04-19 14:42:49.278,True,2018-06-01,Data for Refugees,PUBLIC,,True,Human Dynamics,False
social-bridges-in-community-purchase-behavior,singhv,False,"<p>The understanding and modeling of social influence on human economic behavior in city environments can have important implications. In this project, we study human purchase behavior at a community level and argue that people who live in different communities but work at similar locations could act as ""social bridges"" that link their respective communities and make the community purchase behavior similar through the possibility of social learning through face-to-face interactions.</p>",,--Choose Location,2016-12-05 00:17:04.354,True,2015-09-01,Social Bridges in Community Purchase Behavior,PUBLIC,,True,Human Dynamics,False
data-for-refugees,bakker,False,"<p>Data for refugees is a big data challenge whereby Turk Telekom opens a large dataset of anonymized mobile phone records to research groups for the purpose of providing better living conditions to Syrian refugees in Turkey.&nbsp;&nbsp;</p><p>We introduce different measures extracted from mobile phone metadata to study the integration of refugees along three dimensions: (1) social integration, (2) spatial integration, and (3) economic integration through signatures of employment activity. We use these measures to compare integration across different regions in Turkey and find striking differences both in the distributions of these dimensions and the relations between them.&nbsp;<br></p><p>The paper is currently under review but will be shared soon.&nbsp;</p>",,,2019-04-19 14:42:49.278,True,2018-06-01,Data for Refugees,PUBLIC,,True,Human Dynamics,False
vizml,bakker,False,"<p>Visualization recommender systems aim to lower the barrier to exploring basic visualizations by automatically generating results for analysts to search and select, rather than manually specify. Here, we demonstrate a novel machine learning-based approach to visualization recommendation that learns visualization design choices from a large corpus of datasets and associated visualizations. First, we identify five key design choices made by analysts while creating visualizations, such as selecting a visualization type and choosing to encode a column along the X- or Y-axis. We train models to predict these design choices using one million dataset-visualization pairs collected from a popular online visualization platform. Neural networks predict these design choices with high accuracy compared to baseline models. We report and interpret feature importances from one of these baseline models. To&nbsp;evaluate the generalizability and uncertainty of our approach, we benchmark with a crowdsourced test set, and show that the performance of our model is comparable to human performance when predicting consensus visualization type, and exceeds that of other visualization recommender systems.&nbsp;</p>",,,2019-05-06 20:23:32.720,True,2017-12-05,VizML: A Machine Learning Approach to Visualization Recommendation,PUBLIC,,True,Human Dynamics,False
active-fairness,bakker,False,"<h2>Algorithmic Fairness</h2><p>Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Substantial work in algorithmic fairness has surged, focusing on either post-processing trained models, constraining learning processes, or pre-processing training data.&nbsp;Recent work has proposed optimal post-processing methods that randomize classification decisions on a fraction of individuals in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concerns due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail.&nbsp;</p><h2>Active Fairness</h2><p>The present work proposes an alternative <b>active framework for fair classification</b>, where, in deployment, a decision maker adaptively acquires information according to the needs of different groups or individuals towards balancing disparities in classification performance. We propose two such methods where information collection is adapted to group- and individual-level needs, respectively. We show on real-world datasets that these can achieve: 1) <b>calibration and single error parity</b> (e.g., equal opportunity) and 2) <b>parity in both false positive and false negative rates</b> (e.g., equal odds). Moreover, we show that, by leveraging their additional degree of freedom, active approaches can <b>outperform randomization-based classifiers previously considered optimal</b>, while also avoiding limitations such as intra-group unfairness.</p>",,,2019-01-02 20:52:50.453,True,2018-01-31,Active Fairness in Algorithmic Decision Making,PUBLIC,,True,Human Dynamics,False
data-for-refugees,jobalbar,False,"<p>Data for refugees is a big data challenge whereby Turk Telekom opens a large dataset of anonymized mobile phone records to research groups for the purpose of providing better living conditions to Syrian refugees in Turkey.&nbsp;&nbsp;</p><p>We introduce different measures extracted from mobile phone metadata to study the integration of refugees along three dimensions: (1) social integration, (2) spatial integration, and (3) economic integration through signatures of employment activity. We use these measures to compare integration across different regions in Turkey and find striking differences both in the distributions of these dimensions and the relations between them.&nbsp;<br></p><p>The paper is currently under review but will be shared soon.&nbsp;</p>",,,2019-04-19 14:42:49.278,True,2018-06-01,Data for Refugees,PUBLIC,,True,Human Dynamics,False
opal-health,jobalbar,False,<h1><b>Open Algorithms (OPAL)</b></h1>,,,2018-10-19 21:07:55.443,True,2017-11-01,OPAL 4 Health,PUBLIC,https://www.shadaalsalamah.com/,True,Human Dynamics,False
smart-2-opal,jobalbar,False,"<p>Privacy-preserving mHealth application using Open Algorithm (OPAL) architecture to address urgent care challenges in Riyadh, Saudi Arabia.</p>",,,2019-04-01 17:39:27.492,False,2017-11-01,SMART^2 OPAL,PUBLIC,,True,Human Dynamics,False
data-for-refugees,yleng,False,"<p>Data for refugees is a big data challenge whereby Turk Telekom opens a large dataset of anonymized mobile phone records to research groups for the purpose of providing better living conditions to Syrian refugees in Turkey.&nbsp;&nbsp;</p><p>We introduce different measures extracted from mobile phone metadata to study the integration of refugees along three dimensions: (1) social integration, (2) spatial integration, and (3) economic integration through signatures of employment activity. We use these measures to compare integration across different regions in Turkey and find striking differences both in the distributions of these dimensions and the relations between them.&nbsp;<br></p><p>The paper is currently under review but will be shared soon.&nbsp;</p>",,,2019-04-19 14:42:49.278,True,2018-06-01,Data for Refugees,PUBLIC,,True,Human Dynamics,False
recurrent-neural-network-in-context-free-next-location-prediction,yleng,False,"<p>Location prediction is a critical building block in many location-based services and transportation management. This project explores the issue of next-location prediction based on the longitudinal movements of the locations individuals have visited, as observed from call detail decords (CDR). In a nutshell, we apply recurrent neural network (RNN) to next-location prediction on CDR. RNN can take in sequential input with no restriction on the dimensions of the input. The method can infer the hidden similarities among locations and interpret the semantic meanings of the locations. We compare the proposed method with Markov and a Naive Model proving that RNN has better accuracy in location prediction. </p>",,--Choose Location,2019-04-19 14:52:06.657,True,2016-01-01,Recurrent neural network in context-free next-location prediction,PUBLIC,,True,Human Dynamics,False
the-ripple-effect-your-are-more-influential-than-you-think,yleng,False,"<p>The well-known ""small-world"" phenomenon indicates that an individual can be connected with any other in the world through a limited number of personal acquaintances. Furthermore, Nicholas and Fowler show that not only are we connected to each other, but we could also shape the behavior of our friends' friends. In this project, we are interested in understanding how social influence propagates and triggers behavioral change in social networks. Specifically, we analyze a large-scale, one-month international event held in the European country of Andorra using country-wide mobile phone data, and investigate the change in the likelihood of attending the event for people that have been&nbsp;<span style=""font-size: 18px; font-weight: normal;"">influenced by and are of different social distances from the attendees.&nbsp;</span></p><p><span style=""font-size: 18px; font-weight: normal;"">Our results suggest that social influence exhibits the ripple effect, decaying across social distances from the source but persisting up to six degrees of separation. We further show that influence decays as communication delay increases and intensity decreases. Such ripple effect in social communication can lead to important policy implications in applications where it is critical to trigger behavior change in the population.</span></p>",,,2019-04-19 14:55:02.488,True,2016-08-01,The Ripple Effect: You are more influential than you think,PUBLIC,,True,Human Dynamics,False
andorra-dynamic-urban-planning,yleng,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>",,,2019-02-25 15:17:09.063,True,2016-09-01,Andorra | Dynamic Urban Planning,PUBLIC,,True,Human Dynamics,False
andorra-living-lab,yleng,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,Human Dynamics,False
deep-reinforcement-learning-inspired-by-human-collective-intelligence,yleng,False,"<p>We know that it's groups, not individuals, that are capable of the most complex and daunting achievements. Why should AI be any different?&nbsp;</p><p>We show that deep reinforcement learning algorithms that use lessons from how humans learn and communicate with each other can provide large improvements over state of the art reinforcement learning methods.</p><p>Researchers have been studying how groups of problem-solvers organize themselves and communicate for years, under the field of ""collective intelligence."" It's been shown that there are some surprisingly simple relationships between a group's communication network structure and how well that group is able to perform on different kinds of tasks.&nbsp;</p><p>Using these simple lessons, we designed a deep reinforcement learning algorithm that, instead of using one massive neural network (NN), leverages a community of many smaller NNs. We enable these neural networks to communicate with one another, and to learn from each others' explorations and successes.&nbsp;</p><p>Using this strategy yields significant improvements over the state of the art. By placing these neural nets on a communication network that is similar in structure to how humans communicate,&nbsp;we see a 33% improvement in how fast the networks are able to learn and in how well they are able to perform at a benchmark reinforcement learning task.</p><p><br></p>",,,2018-05-09 14:23:16.868,True,2017-07-01,Deep Reinforcement Learning Inspired by Human Collective Intelligence,LAB-INSIDERS,,True,Human Dynamics,False
location-recommendations-based-on-large-scale-call-detail-records,yleng,False,"<p>Growth in leisure travel has become increasingly significant economically, socially, and environmentally. However, flexible but uncoordinated travel behaviors exacerbate traffic congestion. Mobile phone records not only reveal human mobility patterns, but also enable us to manage travel demand for system efficiency. We propose a location recommendation system that infers personal preferences while accounting for constraints imposed by road capacity in order to manage travel demand. We first infer unobserved preferences using a machine learning technique from phone records. We then formulate an optimization method to improve system efficiency. Coupling mobile phone data with traffic counts and road network infrastructures collected in Andorra, this study shows that uncoordinated travel behaviors lead to longer average travel delay, implying opportunities in managing travel demand by collective decisions. The interplay between congestion relief and overall satisfied location preferences observed in extensive simulations indicate that moderate sacrifices of individual utility lead to significant travel time savings. Specifically, the results show that under full compliance rate, travel delay fell by 52 percent at a cost of 31 percent less satisfaction. Under 60 percent compliance rate, 41 percent travel delay is saved with a 17 percent reduction in satisfaction.This research highlights the effectiveness of the synergy among collective behaviors in increasing system efficiency. </p>",,--Choose Location,2018-10-19 21:02:37.566,True,2015-12-01,Managing Travel Demand: Location recommendation for system efficiency,PUBLIC,,True,Human Dynamics,False
proximity-networks,saquib,False,"<p>A crucial part of Montessori education is observation of the students, so teachers can assist individuals and structure the environment as needed. Our work aims to assist this observation by measuring proximity of students through Simblee COM sensors. We provide detailed visualizations in a dashboard-style interface to both teachers and parents. This dashboard helps teachers individualize their own methods to facilitate a child's growth in the classroom.</p>",,--Choose Location,2016-12-05 00:16:46.304,True,2016-09-01,Proximity Networks,PUBLIC,,True,Social Machines,False
shapeblocks,saquib,False,"<p>ShapeBlocks is a play analytics observatory that tracks, remembers, and aids players in building traditional LEGO-style structures. As players build a structure using these blocks, an underlying geometry engine analyzes the players' moves and suggests next steps (if a target structure is provided). The players can see real-time updates of what they are building in 3D. Instead of only suggesting, the AI learns from the players' moves and corrects itself through reinforcement learning. This essentially gives an opportunity for children and machines to learn shapes and geometry together.</p><p>Other use cases include urban design, and interactive strategy games and/or storytelling experiences that fuse the physical and virtual world together.</p><p>This is a work in progress. The hardware is complete, and the AI tool and games are currently being built.</p>",,,2016-12-05 00:17:27.676,True,2016-08-01,ShapeBlocks,PUBLIC,,True,Social Machines,False
big-data-for-small-places,saquib,False,"<p>Big Data for Small Places is a quantitative study of the qualities that define our neighborhoods and our collective role in the production of local places over time. We are translating the potentials of big data from the scale of the city to the scale of the urban block, the scale at which we physically experience urban space, to gain a better understanding of the local patterns and social spaces that aggregate to form metropolitan identity. We hope that this study will improve our collective understanding of the urban environments we shape and the stories they generate, that it will allow us to more sensitively test and implement real change in our shared public realm and support the invisible narratives it generates.</p>",,--Choose Location,2016-12-05 00:16:14.672,True,2015-01-01,Big Data for Small Places,PUBLIC,,True,Social Machines,False
storyboards,saquib,False,"<p>Giving opaque technology a glass house, Storyboards present the tinkerers or owners of electronic devices with stories of how their devices work. Just as the circuit board is a story of star-crossed lovers—Anode and Cathode—with its cast of characters (resistor, capacitor, transistor), Storyboards have their own characters driving a parallel visual narrative.</p>",,--Choose Location,2017-04-03 01:11:34.352,True,2014-09-01,Storyboards,PUBLIC,,True,Social Machines,False
playful-words,saquib,False,"<p>While there are a number of literacy technology solutions developed for individuals, the role of social—or networked—literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.</p><p>By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.</p><p>To learn more about this project, please check out:&nbsp;<a href=""http://playfulwords.org/"">http://playfulwords.org/</a></p>",,--Choose Location,2018-04-30 20:28:15.298,True,2014-09-01,Playful Words,PUBLIC,,True,Social Machines,False
human-organized-swarms,saquib,False,"<p>Swarm robotics traditionally have relied on autonomous organization of swarm robots using localization algorithms and self-actuation. In this project, we introduce and explore a new human-machine paradigm where humans (specially children, in the context of this project) organize and ""actuate"" the swarm units to solve specific educational tasks, and the swarms infer their group's spatial configuration and sense individual interactions with the child to provide feedback on learning/educational outcomes. By giving the child autonomy to manipulate the spatial configuration, we explore a shared cognitive paradigm wherein children and swarms work together to learn.</p>",,,2019-04-19 18:39:54.427,True,2016-11-01,Human-organized swarms,PUBLIC,,True,Social Machines,False
instrumentation-of-montessori-learning-materials,saquib,False,"<p>The Montessori Method is an educational approach that emphasizes independence and respect for a child's natural development process. Montessori materials are a hallmark of the Montessori
Method. These self-teaching tools encourage exploration of concepts in the
areas of mathematics, language, sensorial development, and practical life, and
allow children to direct their own learning with the light guidance of teachers
and peers. </p><p>We envision a novel framework of unobtrusive sensor networks to understand and reflect on a child’s learning progress, by instrumenting existing Montessori learning materials using distributed sensing techniques.</p>",,,2018-05-04 11:39:56.455,True,2017-09-04,Instrumentation of Montessori Learning Materials,PUBLIC,,True,Social Machines,False
designing-social-robots-for-older-adults,akostrow,False,"<p>Most countries are projected to see the number of people ages 65 and older surpass the population under the age of 15 by 2050. The limitations of current solutions to assisting older adults, the increased social and emotional toll on caregivers, and the inability of institutions to create structural solutions in a timely manner calls for a paradigm shift in the way we approach aging.</p><p>As these new meanings of age, aged, and aging are re-negotiated at a personal and collective level, the <b>main goal of this research initiative is to&nbsp;study aging adults’ daily living assistance, social and emotional needs, and intergenerational connection</b> while exploring the optimized modalities for embodied agents to successfully deliver these interactions.&nbsp;We see embodied agents as a method to enable older adults to age-in-place, supporting them in ways such as promoting social connectedness, tracking vitals, coaching in emotional wellness, and assisting with medical adherence.</p><p>Our work is rooted in partnering with the community through co-design and participatory design methods to inform robot design by empowering older adults to engage in our research. We prioritize developing robot interactions that can be tested long-term in older adults’ homes to better inform how social robots can shape aging-in-place.<br></p><p>Currently, we are running a long-term codesign study with older adults. Over the course of the year, older adults will engage in interviews, interactive artwork, living with a robot, prototyping on a robot, and design guideline generation.&nbsp;</p><p>If you are 70 years of age or older and interested in participating in future study opportunities, please contact Anastasia Ostrowski (akostrow@media.mit.edu).</p>",,,2019-05-10 19:51:29.310,True,2017-06-01,Designing social robots for older adults,PUBLIC,,True,Personal Robots,False
talking-machines-democratizing-the-design-of-voice-based-agents-for-the-home,akostrow,False,"<p>Embodied voice-based agents, such as Amazon’s Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most, these agents represent their first experience of&nbsp;<i>living</i> with artificial intelligence in such private and personal spaces.&nbsp;</p><p>However, little is known about people’s desires, preferences, and boundaries for these technologies. This projects seeks to answer questions surrounding this space:&nbsp;<b>How do we live with voice-based agents in the home? How do different generations interact with voice-based agents? How should these technologies be designed to incorporate people’s preferences, desires, and boundaries? What tools can be used to understand this space?</b></p><p>This work presents insights from a long-term exploration with over 70 children, adults, and older adults over a one-year period to interact with, discover, experience, reflect upon, and design voice-based agents. In addition, design tools and learnings from the experience have been developed into an open-source design kit to enable designers and researchers to explore these ideas with the broader population.</p><p>For more information, please contact <b>Nikhita Singh (nikhita@media.mit.edu) </b>and <b>Anastasia Ostrowski (akostrow@media.mit.edu)</b>.</p>",,,2018-07-15 03:34:47.660,True,2017-09-01,Talking Machines: Democratizing the design of voice-based agents for the home,PUBLIC,,True,Personal Robots,False
talking-machines-democratizing-the-design-of-voice-controlled-agents-in-the-home,akostrow,False,"<p>Embodied voice-based agents, such as Amazon’s Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most people, these agents represent their first experience <i>living</i> with artificial intelligence in such private and personal spaces. However, little is known about people’s desires, preferences, and boundaries for these technologies. This project seeks to answer questions surrounding this space. How do we live with voice-based agents in the home? How do different generations interact with voice-based agents? How should these technologies be designed to incorporate people’s preferences, desires, and boundaries? What tools can be used to understand this space?</p>",,,2018-07-15 03:35:18.476,False,2017-05-01,Talking Machines: Democratizing the Design of Voice-Based Agents in the Home,PUBLIC,,True,Personal Robots,False
robot-mindset-and-curiosity,akostrow,False,"<h1>Young Learner's Companion&nbsp;</h1><h2>Developing robots' growth mindset and pro-curious behavior and fostering the same in young learners via long-term interaction</h2><p>A growth mindset and curiosity have significant impact on children's academic and social achievements. We are developing and evaluating a novel expressive cognitive-affective architecture that synergistically integrates models of curiosity, understanding of mindsets, and expressive social behaviors to advance the state-of the-art of robot companions. In doing so, we aim to contribute major advancements in the design of AI algorithms for artificial curiosity, artificial mindset, and their verbal and non-verbal expressiveness in a social robot companion for children. In our longitudinal study, we aim to evaluate the robot companion's ability to sustain engagement and promote children's curiosity and growth mindset for improved learning outcomes in an educational play context.<br></p>",,--Choose Location,2018-05-09 04:35:35.760,True,2015-09-01,Robot Mindset and Curiosity,PUBLIC,,True,Personal Robots,False
design-inquiry,akostrow,False,"<p>Social robots are seen as a potential device to promote and enable older adults to age-in-place. The work here is a component of our long-term, co-design process with older adults happening in 2019.&nbsp; Through co-creating artworks of&nbsp; interactions with a robot with older adults, abstract technology concepts such as security and privacy, accountability, and autonomy are translated into icons that older adults can leverage to express their thoughts around various interactions. The interactions focused upon for this study include medical adherence, exercise and physical therapy, body signal monitoring, cognitive health monitoring, emotional wellness, social connectedness, and financial literacy.&nbsp;</p><p>In this component, participants create a self-representation and a representation of the robot using physical models. These models are then scanned into a digital space where older adults can scale the representations and add icons to represent their thoughts and desires around the interactions in the artwork.</p>",,,2019-04-18 14:28:23.440,True,2019-04-01,Design Inquiry: Translating abstract technology concepts to artwork,LAB-INSIDERS,,True,Personal Robots,False
shaping-engagement,akostrow,False,"<p>Voice-user interfaces (VUIs), such as Amazon Echo and&nbsp;Google Home, are increasingly becoming present in domestic&nbsp;environments. Users attribute agency and personality traits to these AI agents. Due to the social attributes of these technologies,&nbsp; users try to understand the agents' characteristics based on social norms. These factors affect user experience quality and overall engagement, which, when considering first experiences, can impact continuous usage and engagement with VUI technology.</p><p>Our work examines users’ first impressions and interactions&nbsp;with&nbsp; VUI agents, such as Google Home, Amazon Echo, and Jibo, with varying brands and modalities. Using personality and experience questionnaires, we seek to understand how VUI modalities, form, and personality affect engagement with VUIs.</p>",,,2018-10-09 20:17:18.440,True,2018-08-13,Engagement with Voice-User Interface Agents,PUBLIC,,True,Personal Robots,False
tools-to-investigate-societal-impacts-of-robot-ai,akostrow,False,"<p>Artificial intelligence (AI) agents in an embodied form, such as Jibo, Amazon Alexa, and Google Home, are increasingly becoming part of our daily lives and our homes. While there have been numerous studies in lab settings documenting short-term individual interactions with intelligent agents, we are at a point where we need to be exploring the larger impact of these technologies in the world, living with real people over longer periods of time.</p><p>From a design research perspective, understanding and developing robots and AI that intersect with society is a “wicked problem,” a problem with many components that cannot be solved without interdisciplinary approaches. Design research within interdisciplinary applications has sought to develop approaches, methods, tools, and techniques to investigate the impact of technologies and inform future development. This work focuses on developing tools for exploring robots’ and AI’s impact on daily lives to better inform the&nbsp;development of these technologies by elucidating academia’s and industry’s requirements of tools for this domain.</p><p>For more information, please contact Anastasia Ostrowski (<a href=""mailto:akostrow@media.mit.edu"">akostrow@media.mit.edu</a>).</p>",,,2019-04-17 18:48:44.577,True,2018-07-01,Tools to investigate societal impacts of robots and AI,PUBLIC,,True,Personal Robots,False
hackable-bike,yasushis,False,"<p>To explore future mobility modes, the City Science group is working with Media Lab member company Panasonic to explore the use and potential adaptations of the popular&nbsp;<i>MamaChari</i>&nbsp;bikes. &nbsp;Like other mobility modes, the&nbsp;<i>MamaChari</i>&nbsp;bikes have developed and adapted over the past decades. &nbsp;Bikes for women first became popular during Japan’s economic boom in the 1980s when many households benefited from one income, and women were encouraged to stay home and take care of their children. &nbsp;Women used bikes to quickly navigate their cities and make frequent trips to shops and schools, kids in tow. Even as women gradually entered the workforce in the 1990s and 2000s, the stereotype of the Japanese biking woman remained. &nbsp;By 2008, electric assist bikes were introduced to the market, and again they targeted women with children as the primary users. Today&nbsp;<i>MamaChari</i>&nbsp;bikes are stable, secure and ubiquitous in Japan, yet they have yet to enter other global markets.&nbsp;&nbsp;<br></p><p>The City Science group strives to understand current uses of the MamaChari and adapt the bike for new and future uses globally. Ideation workshops were completed in February and May 2018.</p><p>Learn more about the first workshop here:&nbsp;&nbsp;<a href=""https://www.media.mit.edu/posts/mamachari/"">https://www.media.mit.edu/posts/mamachari/</a></p>",,,2018-06-19 18:01:03.323,True,2018-02-28,Hackable Bike,PUBLIC,,True,City Science,False
mobcho,yasushis,False,"<p>MoCho (short for ""Mobility Choices"") is a&nbsp;<a href=""http://cityscope.media.mit.edu/"">CityScope</a>&nbsp;module focused on mobility choices and societal impacts. This tool helps predict the choices of mobility modes made at the individual level throughout the entire Boston Metro area.</p><p><i style=""font-size: 18px; font-weight: 400;""><a href=""https://cityscope.media.mit.edu/CS_choiceModels/index.html"">Check out a live demo of MoCho predictions here</a>.&nbsp;</i></p>",,,2019-04-19 19:11:18.982,True,2018-06-01,MoCho: Mobility choices and societal impacts,PUBLIC,,True,City Science,False
city-science-lab-shanghai,yasushis,False,"<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>",,,2019-05-16 20:43:38.677,True,2017-02-14,City Science Lab Shanghai,PUBLIC,,True,City Science,False
diastrophisms,yasushis,False,"<p><br>Diastrophisms is a sound installation with a modular system that sends images through rhythmic patterns. It is built on a set of debris from the Alto Río building that was destroyed by the 27F earthquake in 2010 in Chile. With&nbsp;&nbsp;Diastrophisms we were looking for a poetical, critical and political crossing between technology and matter, in order to raise questions about the relationship between human beings and nature, and to consider the construction of memory in a community by questioning the notion of monument, as well as to imagine new forms of communication in times of crisis.</p><p>Work by:&nbsp;Nicole L’Huillier, Thomas Sanchez Lengeling, and Yasushi Sakai</p><p>Exhibited at Siggraph Art Gallery 2018,&nbsp;curated by Andres Burbano. A&nbsp;paper about this work was published&nbsp;in Leonardo Journal for the special edition of Siggraph 2018 Art Papers and Art Gallery Exhibition. The paper was written by Nicole L’Huillier and Valentina Montero.</p><p>Diastrophisms&nbsp;was also exhibited as<a href=""http://www.bienaldeartesmediales.cl/13/obra/talking-rock/""> ""Diastrofismos""</a> at the <a href=""http://www.bienaldeartesmediales.cl/13/"">Media Arts Bienal,</a> Santiago de Chile, 2017, curated by Valentina Montero.</p>",,,2019-02-14 19:56:31.323,True,2017-10-01,Diastrophisms,PUBLIC,,True,City Science,False
cityscope-cooper-hewitt,yasushis,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
cityscope,yasushis,False,"<p>City Science researchers are developing a slew of tangible and digital platforms dedicated to solving spatial design and urban planning challenges. The tools range from simulations that quantify the impact of disruptive interventions in cities to communicable collaboration applications. We develop and deploy these tools around the world and maintain open source repositories for the majority of deployments. ""CityScope"" is a concept for shared, interactive computation for urban planning.</p><p>All current CityScope development, tools, and software are open source <a href=""https://cityscope.github.io/"">here</a>.&nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2019-05-16 20:42:57.474,True,2017-08-01,Theme | CityScope,PUBLIC,,True,City Science,False
jons-untitled-project-2,jon,False,..,,--Choose Location,2018-10-05 18:02:46.646,True,2014-01-01,jon's untitled project,LAB,,True,Personal Robots,False
jibo-research-platform,jon,False,"<p>The Jibo Research Platform is an in-the-field deployable Social Robotics experimentation and data collection infrastructure. Built upon the world's first commercial social robot for the home, it extends Jibo's design, hardware, and data security for research purposes.</p>",,,2018-10-15 01:36:16.436,True,2018-08-01,Jibo Social Robotic Research Platform,PUBLIC,,True,Personal Robots,False
vespers-iii,bader_ch,False,"<p>Vespers is a collection of masks exploring what it means to design (with) life. From the relic of the death mask to a contemporary living device, the collection embarks on a journey that begins with an ancient typology and culminates with a novel technology for the design and digital fabrication of adaptive and responsive interfaces. We begin with a conceptual piece and end with a tangible set of tools, techniques and technologies combining programmable matter and programmable life.</p><p>The project points towards an imminent future where wearable interfaces and building skins are customized not only to fit a particular shape, but also a specific material, chemical and even genetic make-up, tailoring the wearable to both the body and the environment which it inhabits.</p><p>Imagine, for example, a wearable interface designed to guide ad-hoc antibiotic formation customized to fit the genetic makeup of its user; or, consider smart packaging or surface coatings devices that can detect contamination; finally, consider environmentally responsive architectural skins that can respond to, and adapt—in real time—to environmental cues. Research at the core of this project offers a new design space for biological augmentation across a wide breadth of application domains, leveraging resolution and scale.</p><p>The collection includes three series. The first series features the death mask as a cultural artefact. The final series features a living mask as an enabling technology. The second series mediates between the two, marking the process of ‘metamorphosis’ between the ancient relic and its contemporaneous interpretation.The living masks in the final series embody habitats that guide, inform and ‘template’ gene expression of living microorganisms. Such microorganisms have been synthetically engineered to produce pigments and/or otherwise useful chemical substances for human augmentation such as vitamins, antibodies or antimicrobial drugs.Combined, the three series of the Vespers collection represent the transition from death to life, or from life to death, depending on one’s reading of the collection.</p>",,,2018-10-23 15:19:03.760,True,2018-04-01,Vespers III,PUBLIC,,True,Mediated Matter,False
aguahoja,bader_ch,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
wanderers,bader_ch,False,"<p>The Wanderers&nbsp;were unveiled as part of the exhibition: ‘The  Sixth Element: Exploring the Natural Beauty of 3D Printing' on display  at EuroMold, 25-28 November, Frankfurt, Germany. This work was done in collaboration with <a href=""http://www.deskriptiv.de/"">Christoph Bader and Dominik Kolb</a>.  The wearables were 3D-printed with Stratasys multi-material 3D printing  technology. Members of the Mediated Matter group led by <a href=""http://matter.media.mit.edu/people/bio/william-patrick"">Will Patrick</a>&nbsp;and <a href=""http://matter.media.mit.edu/people/bio/sunanda-sharma"">Sunanda Sharma</a>  are currently working on embedding living matter in the form of  engineered bacteria within the 3D structures in order to augment the  environment.</p>",,,2018-04-27 17:06:00.697,True,2014-07-01,Wanderers,PUBLIC,,True,Mediated Matter,False
water-based-additive-manufacturing,bader_ch,False,"<p>This research presents water-based robotic fabrication as a design approach and enabling technology for additive manufacturing (AM) of biodegradable hydrogel composites. We focus on expanding the dimensions of the fabrication envelope, developing structural materials for additive deposition, incorporating material-property gradients, and manufacturing architectural-scale biodegradable systems. The technology includes a robotically controlled AM system to produce biodegradable composite objects, combining natural hydrogels with other organic aggregates. It demonstrates the approach by designing, building, and evaluating the mechanics and controls of a multi-chamber extrusion system. Finally, it provides evidence of large-scale composite objects fabricated by our technology that display graded properties and feature sizes ranging from micro- to macro-scale. Fabricated objects may be chemically stabilized or dissolved in water and recycled within minutes. Applications include the fabrication of fully recyclable products or temporary architectural components, such as tent structures with graded mechanical and optical properties.</p>",,--Choose Location,2019-06-04 21:35:42.589,True,2014-01-01,Water-Based Additive Manufacturing,PUBLIC,,True,Mediated Matter,False
fiberbots,bader_ch,False,"<p>FIBERBOTS is a digital fabrication platform fusing cooperative robotic manufacturing with abilities to generate highly sophisticated material architectures. The platform can enable design and digital fabrication of large-scale structures with high spatial resolution leveraging mobile fabrication nodes, or robotic ""agents"" designed to <i>tune</i> the material make-up of the structure being constructed on the fly as informed by their environment.<br></p><p>Some of nature’s most successful organisms collaborate in a swarm fashion. Nature’s builders leverage hierarchical structures in order to control and optimize multiple material properties. Spiders, for instance, spin protein fibers to weave silk webs with tunable local and global material properties, adjusting their material composition and fiber placement to create strong yet flexible structures optimized to capture prey. Other organisms, such as bees, ants and termites cooperate to rapidly build structures much larger than themselves. </p>",,,2019-02-13 16:36:22.730,True,2016-01-01,"FIBERBOTS: Design of a multi-agent, fiber composite digital fabrication system",PUBLIC,,True,Mediated Matter,False
living-mushtari,bader_ch,False,<p>How can we design relationships between the most primitive and the most  sophisticated life forms? Can we design wearables embedded with  synthetic microorganisms that can enhance and augment biological  functionality? Can we design wearables that generate consumable energy  when exposed to the sun?</p>,,--Choose Location,2017-10-13 23:28:11.103,True,2015-01-01,Living Mushtari,PUBLIC,,True,Mediated Matter,False
rottlace,bader_ch,False,"<p>Rottlace is a family of masks designed for Icelandic singer-songwriter Björk. Inspired by Björk’s most recent album—Vulnicura—the Mediated Matter Group explored themes associated with self-healing and expressing ""the face without a skin."" The series originates with a mask that emulates Björk’s facial structure and concludes with a mask that reveals a new identity, independent of its origin. What originates as a form of portraiture culminates in reincarnation.</p>",,--Choose Location,2018-05-07 19:47:18.431,True,2016-01-01,Rottlace,PUBLIC,,True,Mediated Matter,False
vespers,bader_ch,False,"<p>Novel technologies for additive manufacturing are enabling design and production at nature’s scale. We can seamlessly vary the physical properties of materials at the resolution of a sperm cell, a muscle cell, or a nerve cell. Stiffness, color, hygroscopy, transparency, conductivity, even scent, can be individually tuned for each three-dimensional pixel within a physical object. The generation of products is therefore no longer limited to assemblages of discrete parts with homogeneous properties. Rather like organs, objects can be computationally ""grown"" and 3D printed to form materially heterogeneous and multi-functional products.</p>",,,2018-05-07 19:48:07.209,True,2016-12-12,Vespers II,PUBLIC,,True,Mediated Matter,False
totems,bader_ch,False,"<p>Biodiversity on planet Earth is under momentous threat, with extinction rates estimated between 100 and 1,000 times their pre-human level. The Mediated Matter group has been in search of materials and chemical substances that can sustain and enhance biodiversity across living systems, and that have so far endured the perils of climate change. Melanin is one such substance illustrating biodiversity at the genetic, species, and ecosystem levels.</p>",,,2019-05-07 13:44:31.523,True,2019-02-27,Totems,PUBLIC,,True,Mediated Matter,False
making-data-matter,bader_ch,False,"<p>We present a multimaterial voxel-printing method enabling the physical visualization of data sets commonly associated with scientific imaging. Leveraging voxel-based control of multimaterial 3D printing, our method enables additive manufacturing of discontinuous data types such as point cloud data, curve and graph data, image-based data, and volumetric data. By converting data sets into dithered material deposition descriptions, through modifications to rasterization processes, we demonstrate that data sets frequently visualized on screen can be converted into physical, materially heterogeneous objects.&nbsp;</p><p>Our approach alleviates the need to post-process data sets to boundary representations, preventing alteration of data and loss of information in the produced physicalizations. Therefore, it bridges the gap between digital information representation and physical material composition. We evaluate the visual characteristics and features of our method, assess its relevance and applicability in the production of physical visualizations, and detail the conversion of data sets for multimaterial 3D printing. We conclude with exemplary 3D printed datasets produced by our method pointing towards potential applications across scales, disciplines, and problem domains.</p>",,,2019-04-18 19:51:47.144,True,2018-05-30,Making Data Matter: Voxel-printing for the digital fabrication of data across scales and domains,PUBLIC,,True,Mediated Matter,False
vespers-iii,jpcosta,False,"<p>Vespers is a collection of masks exploring what it means to design (with) life. From the relic of the death mask to a contemporary living device, the collection embarks on a journey that begins with an ancient typology and culminates with a novel technology for the design and digital fabrication of adaptive and responsive interfaces. We begin with a conceptual piece and end with a tangible set of tools, techniques and technologies combining programmable matter and programmable life.</p><p>The project points towards an imminent future where wearable interfaces and building skins are customized not only to fit a particular shape, but also a specific material, chemical and even genetic make-up, tailoring the wearable to both the body and the environment which it inhabits.</p><p>Imagine, for example, a wearable interface designed to guide ad-hoc antibiotic formation customized to fit the genetic makeup of its user; or, consider smart packaging or surface coatings devices that can detect contamination; finally, consider environmentally responsive architectural skins that can respond to, and adapt—in real time—to environmental cues. Research at the core of this project offers a new design space for biological augmentation across a wide breadth of application domains, leveraging resolution and scale.</p><p>The collection includes three series. The first series features the death mask as a cultural artefact. The final series features a living mask as an enabling technology. The second series mediates between the two, marking the process of ‘metamorphosis’ between the ancient relic and its contemporaneous interpretation.The living masks in the final series embody habitats that guide, inform and ‘template’ gene expression of living microorganisms. Such microorganisms have been synthetically engineered to produce pigments and/or otherwise useful chemical substances for human augmentation such as vitamins, antibodies or antimicrobial drugs.Combined, the three series of the Vespers collection represent the transition from death to life, or from life to death, depending on one’s reading of the collection.</p>",,,2018-10-23 15:19:03.760,True,2018-04-01,Vespers III,PUBLIC,,True,Mediated Matter,False
aguahoja,jpcosta,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
fiberbots,jpcosta,False,"<p>FIBERBOTS is a digital fabrication platform fusing cooperative robotic manufacturing with abilities to generate highly sophisticated material architectures. The platform can enable design and digital fabrication of large-scale structures with high spatial resolution leveraging mobile fabrication nodes, or robotic ""agents"" designed to <i>tune</i> the material make-up of the structure being constructed on the fly as informed by their environment.<br></p><p>Some of nature’s most successful organisms collaborate in a swarm fashion. Nature’s builders leverage hierarchical structures in order to control and optimize multiple material properties. Spiders, for instance, spin protein fibers to weave silk webs with tunable local and global material properties, adjusting their material composition and fiber placement to create strong yet flexible structures optimized to capture prey. Other organisms, such as bees, ants and termites cooperate to rapidly build structures much larger than themselves. </p>",,,2019-02-13 16:36:22.730,True,2016-01-01,"FIBERBOTS: Design of a multi-agent, fiber composite digital fabrication system",PUBLIC,,True,Mediated Matter,False
totems,jpcosta,False,"<p>Biodiversity on planet Earth is under momentous threat, with extinction rates estimated between 100 and 1,000 times their pre-human level. The Mediated Matter group has been in search of materials and chemical substances that can sustain and enhance biodiversity across living systems, and that have so far endured the perils of climate change. Melanin is one such substance illustrating biodiversity at the genetic, species, and ecosystem levels.</p>",,,2019-05-07 13:44:31.523,True,2019-02-27,Totems,PUBLIC,,True,Mediated Matter,False
making-data-matter,jpcosta,False,"<p>We present a multimaterial voxel-printing method enabling the physical visualization of data sets commonly associated with scientific imaging. Leveraging voxel-based control of multimaterial 3D printing, our method enables additive manufacturing of discontinuous data types such as point cloud data, curve and graph data, image-based data, and volumetric data. By converting data sets into dithered material deposition descriptions, through modifications to rasterization processes, we demonstrate that data sets frequently visualized on screen can be converted into physical, materially heterogeneous objects.&nbsp;</p><p>Our approach alleviates the need to post-process data sets to boundary representations, preventing alteration of data and loss of information in the produced physicalizations. Therefore, it bridges the gap between digital information representation and physical material composition. We evaluate the visual characteristics and features of our method, assess its relevance and applicability in the production of physical visualizations, and detail the conversion of data sets for multimaterial 3D printing. We conclude with exemplary 3D printed datasets produced by our method pointing towards potential applications across scales, disciplines, and problem domains.</p>",,,2019-04-18 19:51:47.144,True,2018-05-30,Making Data Matter: Voxel-printing for the digital fabrication of data across scales and domains,PUBLIC,,True,Mediated Matter,False
vespers-iii,limulus,False,"<p>Vespers is a collection of masks exploring what it means to design (with) life. From the relic of the death mask to a contemporary living device, the collection embarks on a journey that begins with an ancient typology and culminates with a novel technology for the design and digital fabrication of adaptive and responsive interfaces. We begin with a conceptual piece and end with a tangible set of tools, techniques and technologies combining programmable matter and programmable life.</p><p>The project points towards an imminent future where wearable interfaces and building skins are customized not only to fit a particular shape, but also a specific material, chemical and even genetic make-up, tailoring the wearable to both the body and the environment which it inhabits.</p><p>Imagine, for example, a wearable interface designed to guide ad-hoc antibiotic formation customized to fit the genetic makeup of its user; or, consider smart packaging or surface coatings devices that can detect contamination; finally, consider environmentally responsive architectural skins that can respond to, and adapt—in real time—to environmental cues. Research at the core of this project offers a new design space for biological augmentation across a wide breadth of application domains, leveraging resolution and scale.</p><p>The collection includes three series. The first series features the death mask as a cultural artefact. The final series features a living mask as an enabling technology. The second series mediates between the two, marking the process of ‘metamorphosis’ between the ancient relic and its contemporaneous interpretation.The living masks in the final series embody habitats that guide, inform and ‘template’ gene expression of living microorganisms. Such microorganisms have been synthetically engineered to produce pigments and/or otherwise useful chemical substances for human augmentation such as vitamins, antibodies or antimicrobial drugs.Combined, the three series of the Vespers collection represent the transition from death to life, or from life to death, depending on one’s reading of the collection.</p>",,,2018-10-23 15:19:03.760,True,2018-04-01,Vespers III,PUBLIC,,True,Mediated Matter,True
aguahoja,limulus,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,True
vespers,limulus,False,"<p>Novel technologies for additive manufacturing are enabling design and production at nature’s scale. We can seamlessly vary the physical properties of materials at the resolution of a sperm cell, a muscle cell, or a nerve cell. Stiffness, color, hygroscopy, transparency, conductivity, even scent, can be individually tuned for each three-dimensional pixel within a physical object. The generation of products is therefore no longer limited to assemblages of discrete parts with homogeneous properties. Rather like organs, objects can be computationally ""grown"" and 3D printed to form materially heterogeneous and multi-functional products.</p>",,,2018-05-07 19:48:07.209,True,2016-12-12,Vespers II,PUBLIC,,True,Mediated Matter,True
making-data-matter,limulus,False,"<p>We present a multimaterial voxel-printing method enabling the physical visualization of data sets commonly associated with scientific imaging. Leveraging voxel-based control of multimaterial 3D printing, our method enables additive manufacturing of discontinuous data types such as point cloud data, curve and graph data, image-based data, and volumetric data. By converting data sets into dithered material deposition descriptions, through modifications to rasterization processes, we demonstrate that data sets frequently visualized on screen can be converted into physical, materially heterogeneous objects.&nbsp;</p><p>Our approach alleviates the need to post-process data sets to boundary representations, preventing alteration of data and loss of information in the produced physicalizations. Therefore, it bridges the gap between digital information representation and physical material composition. We evaluate the visual characteristics and features of our method, assess its relevance and applicability in the production of physical visualizations, and detail the conversion of data sets for multimaterial 3D printing. We conclude with exemplary 3D printed datasets produced by our method pointing towards potential applications across scales, disciplines, and problem domains.</p>",,,2019-04-18 19:51:47.144,True,2018-05-30,Making Data Matter: Voxel-printing for the digital fabrication of data across scales and domains,PUBLIC,,True,Mediated Matter,True
kino-kinetic-wearable,dmajilo,False,"<p><span style=""font-size: 18px; font-weight: 400;"">This work explores a dynamic future in which the accessories we wear are no longer static, but are instead mobile, living objects on the body. Engineered with the functionality of miniaturized robotics, this ""living"" jewelry roams on unmodified clothing, changing location and reconfiguring appearance according to social context and enabling multiple presentations of self. With the addition of sensor devices, they can actively respond to environmental conditions. They can also be paired with existing mobile devices to become personalized on-body assistants to help complete tasks. Attached to garments, they generate shape-changing clothing and kinetic pattern designs—creating a new, dynamic fashion.</span><br></p><p> </p><p><span style=""font-size: 18px; font-weight: 400;"">It is our vision that in the future, these robots will be miniaturized to the extent that they can be seamlessly integrated into existing practices of body ornamentation. With the addition of kinetic capabilities, traditionally static jewelry and accessories will start displaying life-like qualities, learning, shifting, and reconfiguring to the needs and preferences of the wearer, also assisting in fluid presentation of self. With wearables that possess hybrid qualities of the living and the crafted, we explore a new on-body ecology for human-wearable symbiosis.</span><span style=""font-size: 18px; font-weight: 400;"">&nbsp;</span></p>",,--Choose Location,2017-10-17 03:54:26.236,True,2015-01-01,Kino,PUBLIC,,True,Speech + Mobility,False
rovables,dmajilo,False,"<p>We introduce Rovables, a miniature robot that can move freely on unmodified clothing. The robots are held in place by magnetic wheels, and can climb vertically. The robots are untethered and have an onboard battery, microcontroller, and wireless communications. They also contain a low-power localization system that uses wheel encoders and IMU, allowing Rovables to perform limited autonomous navigation on the body. In the technical evaluations, we found that Rovables can operate continuously for 45 minutes and can carry up to 1.5N. We propose an interaction space for mobile on-body devices spanning sensing, actuation, and interfaces, and develop application scenarios in that space. Our applications include on-body sensing, modular displays, tactile feedback and interactive clothing and jewelry.
                    
                </p>",,,2017-03-03 18:38:10.500,True,2016-09-01,Rovables,PUBLIC,,True,Speech + Mobility,False
nashik-smart-citizen-collaboration-with-tcs,ajdas,False,"<p>We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: What billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, Big Data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.</p>",,--Choose Location,2016-12-05 00:17:18.564,True,2016-01-01,Nashik Smart Citizen Collaboration with TCS,PUBLIC,,True,Camera Culture,False
food-sensing-on-smartphones,ajdas,False,"<p>We demonstrate a smartphone based spectrometer design that is standalone and supported on a wireless platform. The device is inherently low-cost and the power consumption is minimal making it portable to carry out a range of studies in the field. All essential components of the device like the light source, spectrometer, filters, microcontroller and wireless circuits have been assembled in a housing of dimensions 88 mm × 37 mm × 22 mm and the entire device weighs 48 g. The resolution of the spectrometer is 15 nm, delivering accurate and repeatable measurements. The device has a dedicated app interface on the smartphone to communicate, receive, plot and analyze spectral data. The performance of the smartphone spectrometer is comparable to existing bench-top spectrometers in terms of stability and wavelength resolution. Validations of the device were carried out by demonstrating non-destructive ripeness testing in fruit samples. Ultra-Violet (UV) fluorescence from Chlorophyll present in the skin was measured across various apple varieties during the ripening process and correlated with destructive firmness tests. A satisfactory agreement was observed between ripeness and fluorescence signals. This demonstration is a step towards possible consumer, bio-sensing and diagnostic applications that can be carried out in a rapid manner.</p>",,,2018-10-22 20:30:10.303,True,2016-11-08,Food sensing on smartphones,PUBLIC,http://www.mit.edu/~ajdas,True,Camera Culture,False
health-tech-innovations-with-tata-trusts-mumbai,ajdas,False,"<p>We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: what billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, big data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.</p>",,--Choose Location,2019-04-19 17:52:03.224,True,2016-01-01,"Health-tech innovations with Tata Trusts, Mumbai",PUBLIC,,True,Camera Culture,False
hyderabad-eye-health-collaboration-with-lvp,ajdas,False,"<p>We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: What billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, big data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.</p>",,--Choose Location,2019-04-19 17:54:21.753,True,2016-01-01,Hyderabad eye health collaboration with LVP,PUBLIC,,True,Camera Culture,False
ajdass-untitled-project,ajdas,False,"<p>A smartphone based spectrometer design that is standalone and supported on a wireless platform. The device is low-cost and the power consumption is minimal making it portable to perform a range of studies in the field. Essential components of the device like the light source, spectrometer, filters, microcontroller and wireless circuits have been assembled in a housing that fits into a pocket and the entire device weighs 48 g. The device has a dedicated app on the smartphone to communicate, receive, plot and analyze spectral data. Validations of the device were carried out by demonstrating non-destructive ripeness testing in fruits. Ultra-Violet fluorescence from Chlorophyll present in the skin was measured across various apple varieties during the ripening process and correlated with destructive firmness tests. This demonstration is a step towards possible consumer, bio-sensing and diagnostic applications that can be carried out in a rapid manner.</p>",,--Choose Location,2017-03-31 23:01:55.075,True,2015-01-01,Smartphone spectrometer for food sensing,PUBLIC,http://www.mit.edu/~ajdas,True,Camera Culture,False
printed-wearable-holographic-display,sjolly,False,"<p>Holographic displays offer many advantages, including comfort and maximum realism. In this project we adapt our guided-wave light-modulator technology to see-through lenses to create a wearable 3D display suitable for augmented or virtual reality applications. As part of this work we also are developing a femtosecond-laser-based process that can fabricate the entire device by ""printing.""</p>",,--Choose Location,2016-12-05 00:16:56.157,True,2015-09-01,Printed Wearable Holographic Display,PUBLIC,,True,Object Based Media,False
bio-inspired-photonic-materials-producing-structurally-colored-surfaces,sjolly,False,"<p>Advances in science and engineering are bringing us closer and closer to systems that respond to human stimuli in real time. Scientists often look to biology for examples of efficient, spatially tailored multifunctional systems, drawing inspiration from photonic structures like multilayer stacks similar to those in the&nbsp;<i>Morpho butterfly</i>. &nbsp;In this project, we develop an understanding of the landscape of responsive, bio-inspired, and active materials, drawing from principles of photonics and bio-inspired material systems. We are exploring various material processing techniques to produce and replicate structurally colored surfaces, while developing simulation and modeling tools (such as inverse design processes) to generate new structures and colors. Such complex biological systems require advanced fabrication techniques. Our designs are realizable through fabrication using direct laser writing techniques such as two photon polymerization. We aim to compare our model system and simulations to fabricated structures using optical microscopy, scanning electron microscopy, and angular spectrometry. This process provides a toolkit with which to examine and build other bio-inspired, tunable, and responsive photonic systems and expand the range of achievable structural colors.</p><p>Unlike with natural structures, producing biomimetic surfaces allows researchers to test beyond tunability that occurs naturally and explore new theory and models to design structures with optimized functions.&nbsp;The benefits of such biomimetic nanostructures are plentiful: they provide brilliant, iridescent color with mechanical stability and light steering capabilities.&nbsp;&nbsp;By producing biomimetic nanostructures, designers and engineers can capitalize on unique properties of optical structural color, and examine these structures based on human perception and response.</p>",,,2019-04-18 17:28:00.132,True,2018-06-01,"MORPHO: Bio-inspired photonic materials, producing structurally colored surfaces",LAB-INSIDERS,http://www.biancadatta.com,True,Object Based Media,False
valinor-mathematical-models-to-understand-and-predict-self-harm,kdinakar,False,"<p>We are developing statistical tools for understanding, modeling, and predicting self-harm by using advanced probabilistic graphical models and fail-soft machine learning in collaboration with Harvard University and Microsoft Research.</p>",,--Choose Location,2019-04-19 17:37:54.819,True,2014-01-01,Valinor: Mathematical models to understand and predict self-harm,PUBLIC,,True,Other,False
lensing-cardiolinguistics-for-atypical-angina,kdinakar,False,"<p>Conversations between two individuals—whether between doctor and patient, mental health therapist and client, or between two people romantically involved with each other—are complex. Each participant contributes to the conversation using her or his own ""lens."" This project involves advanced probabilistic graphical models to statistically extract and model these dual lenses across large datasets of real-world conversations, with applications that can improve crisis and psychotherapy counseling and patient-cardiologist consultations. We're working with top psychologists, cardiologists, and crisis counseling centers in the United States.</p>",,--Choose Location,2019-04-18 03:29:15.605,True,2014-01-01,Lensing: Cardiolinguistics for Atypical Angina,PUBLIC,,True,Other,False
HAL,kdinakar,False,"<p>The&nbsp;&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Humanizing AI in Law (HAL)&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">project aims to build the technical and legal foundations necessary to establish a due-process framework for auditing and improving decisions made by artificial intelligence systems as they evolve over time. This work is directed at the concerning software that has been deployed within the criminal justice system to aid judges in the sentencing of criminal defendants.</span></p>",,,2018-05-07 14:46:00.241,True,2017-06-01,Humanizing AI in Law (HAL),PUBLIC,,True,Other,False
image-sentiment-anlysis,agata,False,<p>&nbsp;In this project we explore how to recognize and localize affect in images.</p>,,,2018-10-22 19:54:06.978,True,2017-01-02,Image Sentiment Analysis,PUBLIC,,True,Affective Computing,False
emotion-recognition-in-context,agata,False,"<p>The goal of this project is providing machines with the ability of understanding what a person is experiencing from her frame of reference, taking into account the scene context: where is this person, what is this person doing, how does this person look, etc.&nbsp;</p><p>You can find more information about this project on this <a href=""http://sunai.uoc.edu/emotic/"">website</a>.</p>",,,2018-10-20 16:59:10.142,True,2017-01-02,Emotion Recognition in Scene Context,PUBLIC,,True,Affective Computing,False
place-recognition-and-categorization-1,agata,False,"<p>The <b>Places</b> dataset&nbsp;(<a href=""http://places2.csail.mit.edu/"">website</a>)&nbsp;is designed following principles of human visual cognition. Our goal is to build a core of visual knowledge that can be used to train artificial systems for high-level visual understanding tasks, such as <b>scene context</b>, object recognition, action and event prediction, <b>emotion recognition, or theory-of-mind inference</b>.&nbsp;</p><p>You can try our <a href=""http://places2.csail.mit.edu/demo.html"">online demo</a>.</p>",,,2018-10-22 19:53:47.491,True,2014-10-01,Place Recognition and Categorization,PUBLIC,,True,Affective Computing,False
personalized-emotional-wellness-coach,agata,False,"<p>The diagnosis and tracking of mood disorders still rely on clinical assessments, originating more than 50 years ago, of self-reported depressive symptoms via surveys and interviews. Such methods are known to provide limited accuracy and reliability in addition to being costly to track and scale. Once a problem is detected, providing personalized daily intervention and support is also too costly and does not scale. The goal of this pilot project is to develop a proof of concept of personalized emotional wellness coach focusing on key technology modules to create an emotionally intelligent social robot for this targeted domain. We shall also conduct a pilot evaluation with a five-week user study to evaluate the robot coach with respect to its ability to successfully sustain a user long-term adherence (i.e., daily self-report and efficacious advice)—with the expected result that it is more effective than state-of-the-art, gamified mobile apps currently used.&nbsp;</p>",2019-09-30,,2019-04-22 15:26:39.440,True,2018-09-01,Personalized Emotional Wellness Coach,PUBLIC,,True,Affective Computing,False
elsa,agata,False,"<h2><b><i>What is ELSA?</i></b></h2><p>ELSA is an AI-powered chatbot that acts as an empathetic companion, encouraging users to talk about their day through a form of interactive journaling.</p><p>You can try some of the current ELSA bots in this online&nbsp;<a href=""http://elsaneural.net"">demo</a>.&nbsp;</p><h2><b><i>How does ELSA work?</i></b></h2><p>Our project goal is to build a more empathetic neural network conversational AI by incorporating a deeper understanding of both the affective content of the conversation and the topic.&nbsp; More specifically, we build hierarchical recurrent neural network models that can converse like people &nbsp;and use transfer learning of topic and emotional tone recognition models to improve our final model.</p><h2><b><i>What are the applications of ELSA?</i></b></h2><p>Beyond the development of chatbots that act as an empathetic companion, we have a more ambitious and longer term goal: deploy the empathetic companion bots to support mental health.&nbsp; In particular, &nbsp;we aim to make ELSA useful for:</p><ul><li>Eliciting journaling</li><li>Suggesting behavioura interventions</li><li>Using Cognition Behavioral Therapy</li><li>Detecting individuals at risk of depression or suicide</li></ul><h2><b><i>Work in progress</i></b></h2><p>ELSA is a recently started project in the Affective Computing group. You can see an example of ELSA bot conversations below. You can also try our online <a href=""http://elsaneural.net"">demo</a>. &nbsp;&nbsp;</p>",,,2019-04-22 19:06:31.702,True,2019-03-03,"ELSA: Empathy learning, socially-aware agents",PUBLIC,,True,Affective Computing,False
footwear-x-mentalhealth,britneyj,False,"<p>In honor of May being Mental Health Awareness month, the collaboration between Footwear x Mental Health needs YOUR participation in a call to action.</p><p>According to the National Alliance on Mental Illness (NAMI), approximately 46.6 million adults in the US live with a mental illness and half of all lifetime mental health conditions begin by age 14 and 75 percent begin by age 24 [<a href=""https://www.nami.org/Learn-More/Mental-Health-By-the-Numbers"">1</a>]. Additionally, African Americans are “20 percent more likely to experience serious mental health problems than the general population” [<a href=""https://www.nami.org/find-support/diverse-communities/african-americans"">2</a>].</p><p>Barriers to treatment and engagement for African Americans include lack of cultural competence by health professionals, and shame and stigma within the community surrounding the topic. Other barriers affecting engagement with mental health treatment include an inability or unwillingness to use creative and innovative approaches to engagement, and an inability to work effectively within and across diverse cultures [<a href=""https://www.nami.org/About-NAMI/Publications-Reports/Public-Policy-Reports/Engagement-A-New-Standard-for-Mental-Health-Care/NAMI_Engagement_Web.pdf"">3</a>].</p><p>This is why we need your help in unleashing the power of footwear to prove it’s more than just a shoe or a product. Sneakers impact communities.</p><p><i>[Embed video]</i></p><p>To participate:</p><ul><li>Post a sneaker (designed, already owned, or one you admire) on Twitter, along with one to two sentences related to how it links to mental health and wellness. You may answer one of the three prompts below or share thoughts of your own:</li></ul><ol><li>How does this sneaker inspire you or a friend in overcoming a difficult situation?</li><li>Describe how this sneaker makes you feel.</li><li>How can footwear provide an innovative approach to destigmatizing mental health?</li></ol><ul><li>Include in your post the hashtags: #FootwearxMentalHealth and #BlackMentalHealthMatters (or #MentalHealthAwareness ).</li></ul><ul><li>Tag two people in your post, and encourage them to also share how their kicks collaborate with mental health and wellness.</li></ul><p>The content you share will be reviewed to see how footwear may assist in destigmatizing the conversation surrounding mental health.</p><p>For more mental health resources, please visit:</p><ul><li><a href=""http://www.nami.org/"">The National Alliance on Mental Illness</a></li><li><a href=""https://www.nimh.nih.gov/index.shtml"">National Institute of Mental Health</a></li><li><a href=""https://silencetheshame.com/"">Silence the Shame</a></li><li><a href=""https://www.instagram.com/askdrjess/"">Ask Dr. Jess</a></li></ul>",,,2019-05-22 14:21:12.337,False,2019-05-21,Footwear x Mental Health: A Collaboration,PUBLIC,,True,Viral Communications,False
civic-link,britneyj,False,"<p>CivicLink is an online organization in a box. A leader or moderator plugs it in, invites members, and the tools needed to build community action are in place. It is grassroots mobilization recursed to the lower level: we envision extremely large networks of extremely local, single-issue orgs. Core elements are an events calendar and forum for each event. The link is a private server that retains all communications and personal information within it; there is no contribution to an online cloud. The architecture is extensible to add features such as mapping, canvassing, etc. It is designed to be used where groups physically meet and for access via a smartphone app.</p><p>Related work tests whether privacy is important to users, whether games can be used to promote actions, how web sites can be distributed offline via QR codes, and how this link can merge culturally unique resonances.</p>",,,2019-04-18 01:23:05.324,True,2018-09-04,CivicLink,PUBLIC,,True,Viral Communications,False
fiftynifty,britneyj,False,"<p>This is a grassroots challenge to get friends to participate in democracy by making calls to congresspeople in all 50 states. Live phone calls are the best way to directly express your opinion on an issue to your elected officials. Your mission is to pass message this along to friends who will make calls and also pass the message/link along to others who will do the same. It's a social chain letter and a call to action for a better participatory democracy. &nbsp;<span style=""font-size: 18px; font-weight: normal;"">We help you make your call and you pass on an invitation for your friends to do the same. Your invite can stress your opinion on a given issue.&nbsp;</span></p><p>The winners are the first ten chains to reach 50 states and accumulate the most challenge points. You get 250 points for making a call, 125 points for a call that your friend makes, 65 points for the call their friend makes, on and on. Everyone on the chain earns points. Points count for your first call to each of your two senators and your representative. You get a bonus for a ""grand slam""—a network that reaches all 435 representatives and 100 senators.</p><p>There is a leaderboard and a network view so you can track how you are doing. You can also see how much of the country your chain is covering.</p>",,,2019-06-04 20:46:21.258,True,2017-02-13,FiftyNifty,PUBLIC,https://fiftynifty.org,True,Viral Communications,False
footwear-in-action,britneyj,False,"<p><b>Leveraging sneaker culture to influence civic engagement.&nbsp;</b></p><p>Marginalized groups have influence on digital platforms but are often unheard in other forums. We show how culturally resonant physical artifacts extend community reach and impact. &nbsp;We leverage the sneaker, a growing political symbol and cultural influence, to impact community participation.</p>",,,2019-05-30 18:58:37.122,True,2018-10-11,Kicks x Cliques,PUBLIC,,True,Viral Communications,False
attentivu,nkosmyna,False,"<p>It is increasingly hard for adults and children alike to be attentive given the increasing amounts of information and distractions surrounding us. </p><p>We have developed AttentivU: a device, in a socially acceptable form factor of a pair of glasses, that a person can put on in moments when he/she wants/needs to be attentive. </p><p>The AttentivU glasses use brain activity (electroencephalography - EEG) as well as eye movements (electrooculography - EOG) sensors to measure engagement of a person in real-time and provide either audio or haptic feedback to the user when their engagement is low, thereby nudging them to become engaged again. </p><p>We have tested the first generation prototype of the device in workplace and class- room settings with over 100 subjects. We have performed experiments with people studying or working by themselves, viewing online lectures as well as listening to classroom lectures. The obtained results show that our device makes a person more attentive and produces improved learning and work performance outcomes.&nbsp;</p><p>We have now finished the first tests of the glasses (second prototype) with more than 30 subjects who were performing driving task in the simulator or using the glasses during everyday activities like reading, watching videos or writing.&nbsp;</p><p>The novelty of our system is that it is meant to be used in the moment, that is, in the context where sustained attention is necessary. In order to make real-world use possible, we have developed a socially acceptable, inconspicuous form factor: a pair of glasses that contain EEG and EOG electrodes as well as a an amplifier, Bluetooth LTE module, and a speaker for bone-conduction auditory feedback. The user can optionally receive the feedback or nudges through a wireless vibration brooch that can be attached where desired and remains invisible.&nbsp;</p><p>We envision a future in which people can decide when they want to be more attentive and can in those moments put on their AttentivU glasses to help them be focused.&nbsp;</p><p>Project Lead: Nataliya Kosmyna, Ph.D</p><p>Project Team: Caitlin Morris,&nbsp;Thanh Nguyen and Pattie Maes.</p>",,,2019-05-24 18:43:31.915,True,2018-01-01,AttentivU,PUBLIC,,True,Fluid Interfaces,False
thinking-cap,nkosmyna,False,"<p><span style=""font-size: 18px; font-weight: 400;"">Peoples' mindsets, meaning their beliefs about their own intellectual abilities, affect their effort and thereby their performance on tasks. The goal of this project is to investigate if we can change peoples' mindsets using a technological intervention.&nbsp;</span></p><p>The Thinking Cap is a wearable system that communicates praise for effort and ability in order to improve the resilience&nbsp;and self-esteem of the student wearing it and thus&nbsp; positively influence their motivation and academic achievements.&nbsp;</p><p><span style=""font-size: 18px; font-weight: 400;"">The Thinking Cap is built into a ""Sorting Hat"" from the Harry Potter franchise, which we equipped with an embedded electroencephalography (EEG) headset and a Bluetooth speaker. We chose this “magical” object from the well-known film/book franchise because popular press articles&nbsp;have&nbsp;suggested that people are likely to believe they possess the traits the Sorting Hat tells them they have, and consequently behave in related ways. One goal of this study is to investigate these findings in more depth. In our study we measure the self-esteem of children before and after the “intervention of the hat” to determine whether we observe any changes in their self-perception. The Sorting Hat could be replaced by any other object that a child may believe has ""magical"" powers. The hat uses established state-of-the-art Brain-Computer Interface (BCI) algorithms to recognize several mental processes like motor, auditory, or visual imagery as well as cognitive load and engagement level of the child (see also a related&nbsp; project from our group called </span><a href=""https://www.media.mit.edu/projects/attentivu/overview/"" style=""font-size: 18px; font-weight: 400;"">AttentivU</a><span style=""font-size: 18px; font-weight: 400;"">). In an initial phase, the hat is used to recognize and report on the brain patterns of the child. We use supervised and unsupervised ML algorithms to train the system by asking the user explicitly to imagine/visualize either a simple movement or an object in their head (binary classification in most of the cases). The hat ""tells"" the child, via the Bluetooth speaker embedded in the hat, which of the two things he/she is thinking about.&nbsp;We hypothesize that, by demonstrating this basic capability of the hat to recognize their brain activity, the child will develop trust in the hat’s abilities to know him or her. Thus, when the hat in a later phase praises the child for their ability or effort on a task (e.g., a math test), the child is likely to be affected by its suggestions in their future performance (""You are doing well on this test now, let's do one more!""). We hypothesize that using the hat can thus lead to improved academic performance.&nbsp;</span><br></p><p>If you are interested in participating in this study (your kid should be at least eight years old), please contact us at nkosmyna@media.mit.edu.&nbsp; &nbsp; &nbsp;</p>",,,2019-04-16 17:38:17.218,True,2017-12-01,Thinking Cap,PUBLIC,,True,Fluid Interfaces,False
bias-by-us,orestis,False,"<p><b>Bias by us</b>&nbsp;envisions a future of media diversity by understanding the bias of today.</p><p>Our work seeks to understand how the US media ecosystem reports on underrepresented minorities. By using natural language processing algorithms and data-intensive models, we aim to uncover underlying stereotypes, associations, and modes of narration that media produces and reproduces when covering minority related events. We perform a multi- and cross-platform analysis, capturing media dynamics on different social media platforms and traditional media outlets across the political spectrum. Besides understanding how media language portrays underrepresented minorities, we locate effects on and associations to political saliency and bias-motivated crime.<br></p><p>By understanding media bias and its effects on underrepresented minorities, we reflect on the conditions that can ensure a diverse and inclusive US media ecosystem.</p>",2019-06-15,,2019-04-21 19:06:53.374,True,2019-02-18,Bias by us,PUBLIC,,True,Civic Media,False
zap-pore,dkong,False,"<p>An electroporator is&nbsp;&nbsp;a fundamental tool for biotechnology,&nbsp;used to transform bacteria with higher efficiency when compared to heat shock transformation. However, electroporators are expensive devices.&nbsp;This project involves a novel approach to develop a low-cost electroporator.&nbsp;</p>",,,2019-05-06 14:38:29.578,True,2017-10-01,Zap-Pore,LAB,,True,Other,False
metafluidics-openrepository,dkong,False,"<p>Metafluidics is an open source, community-driven repository that hosts digital design files, assembly specifications, and the bill of materials necessary for users to make and operate a fluidic device. The site enables the global microfluidics community, from trained scientists and engineers to hobbyists, students, and amateur makers alike, the ability to submit designs and reproduce and remix devices with the ultimate goal of democratizing microfluidics.</p><p>This repository is the hardware portal for the National Science Foundation-supported&nbsp;<strong><a href=""https://www.programmingbiology.org/"">Living Computing Project</a></strong>. Share your fluidic devices with the global community today!</p>",,,2019-05-06 14:41:03.684,True,2017-01-01,Metafluidics: Open repository for fluidic systems,PUBLIC,,True,Other,False
metafluidics,dkong,False,"<p>Fluidics are promising foundational tools for synthetic biology. Unfortunately, fluidics are not broadly used, because they are difficult to manufacture and operate, and designs are currently not shared in a systematic fashion. <a href=""https://metafluidics.org/""><b>Metafluidics</b></a> is an open repository of both device and hardware designs to enable communities of users from around the world to share and remix bio-hardware. This repository is the hardware portal for the National Science Foundation-supported&nbsp;<strong><a href=""https://www.programmingbiology.org/"">Living Computing Project</a></strong>. Share your fluidic devices with the global community today!</p>",,,2019-05-06 14:41:38.233,True,2017-01-01,Metafluidics,PUBLIC,,True,Other,False
zappore,dkong,False,"<p>Zappore is a low-cost ($10), portable electroporator that can be fabricated in any community or academic setting with readily available and common materials. It is a tool to explore and experiment the possibility of transformation for known and unknown strains of microbes.</p>",,,2019-04-24 20:16:00.368,True,2018-01-01,Zappore: A low-cost electroporator,LAB-INSIDERS,,True,Other,False
co-culture,dkong,False,"<p>We aim to create an easy-to-use, scalable, widely accessible method to allow for the co-culturing of multiple organisms through the use of water droplets stabilized by surfactants in oil. This methodology will be used to explore and study the effects of various strains of biota living in the human microbiome on each other.&nbsp;</p>",,,2019-04-24 18:45:39.803,True,2019-02-04,Co-Culture: Open source technique to study dynamics of the microbiome,LAB-INSIDERS,,True,Other,False
Biological-Enhancement,dkong,False,"<h2><b>Lab on Body, Synthetic Biology, and Bio-Digital Systems for Health and Human Enhancement</b></h2>",,,2019-05-10 15:15:39.601,True,2019-02-03,Theme | Wearable Biotech Enhancement,PUBLIC,,True,Other,False
participatory-biotechnology,dkong,False,"<p>The development of biotechnologies since the era of recombinant DNA in the 1970s has occurred largely via the interaction of academic, industrial, and governmental institutions.&nbsp;Largely absent from this ecosystem are the informed inputs of grassroots communities at any point in the technology developmental cycle.</p><p>The parallel rise of high-throughput, next generation DNA sequencing and advanced DNA synthesis technologies (reading and writing DNA), along with invention of precise genome editing technologies (e.g., CRISPR) has humanity at the brink of a new era, one where living technologies rule. Given the vital importance of living technologies, not only to human health, manufacturing, the economy, and environment, but to our social fabric and culture, we ask:</p><ul><li>How should living technologies be developed?<br></li><li>How can we ensure there is broad, diverse participation in biotechnology?<br></li><li>How can marginalized, under-represented and indigenous communities be agents of change in this era?<br></li><li>What types of institutions and design practices can be employed to ensure just outcomes?<br></li><li>How can humanity work harmoniously, in concert with nature, to co-evolve and flourish?</li></ul><p><br></p><p>We will host a <a href=""https://www.media.mit.edu/events/participatory-design-conference/"">workshop</a>&nbsp;on this topic during the biennial Participatory Design Conference in Genk and Hasselt, Belgium.&nbsp;</p>",,,2018-05-02 14:59:36.627,True,2018-05-27,Participatory Biotechnology,PUBLIC,,True,Other,False
human-microbe-interaction,dkong,False,"<p>One of the current foci within the HCI community is to understand and augment human capabilities using physiological and biological data. Microbes living on, inside, and around the human play significant roles in life, from improving health to causing infectious diseases. As the knowledge of human-microbe interaction continues to unfold, we propose a framework for microbial HCI based on a growing body of work aiming to observe, integrate, and modify microorganisms in interactive systems. Our motivation for the framework is to advancing the next generation of biological HCI and exploring novel human-microbe interfaces across contexts, scales, and species.</p>",,,2019-04-29 16:39:12.651,True,2018-11-01,Microbial Augmentation Interfaces,LAB-INSIDERS,,True,Other,False
measuring-and-reducing-social-segregation-in-cities,dcalacci,False,"<p>We use high-resolution geospatial data collected from mobile phones to measure social segregation at an unprecedented resolution in cities across the United States. Social segregation happens when people of varying socioeconomic groups in a city have little opportunity to be exposed to people different than them.</p><p>To construct this measure, we aggregate high-resolution data from&nbsp;over 4.5 million users in the principal metro areas in the US to characterize places in the city by how mixed their visitors are by income. Using this measure, rather than traditional residential metrics, reveals that social exposure in third places is crucial to understanding economic segregation patterns in cities. In fact, the social segregation of different economic groups is dependent on an extremely small proportion of overall venues in a city.&nbsp;</p><p>We also look at how much individual citizens would need to change their behavior in order to make their patterns of exposure more integrated. Surprisingly, small changes in the amount of time people spend in different categories of places—changes as low as 2-5%—can reduce their social segregation by half.&nbsp;</p><p>We're currently working on finalizing these results and exploring how we might translate these findings into policy.</p>",,,2019-04-19 14:46:25.026,True,2017-10-01,Measuring and reducing social segregation in cities,PUBLIC,,True,Human Dynamics,False
rhythm,dcalacci,False,"<p>Rhythm is a collection of open-source tools to make it easier for researchers to examine, analyze, and augment human interaction.&nbsp;Rhythm includes hardware to measure face to face interaction, software platforms to quantify social dynamics from online videoconferencing, and analysis and visualization tools to craft interventions that affect social behavior. For more information, visit &nbsp;<a href=""http://rhythm.mit.edu"">rhythm.mit.edu</a>, or our main <a href=""https://github.com/HumanDynamics/openbadge"">github repository</a>.</p>",,,2019-01-21 17:59:43.190,True,2016-09-01,Rhythm: Open measurement and feedback tools for human interaction,PUBLIC,http://rhythm.mit.edu/,True,Human Dynamics,False
deep-reinforcement-learning-inspired-by-human-collective-intelligence,dcalacci,False,"<p>We know that it's groups, not individuals, that are capable of the most complex and daunting achievements. Why should AI be any different?&nbsp;</p><p>We show that deep reinforcement learning algorithms that use lessons from how humans learn and communicate with each other can provide large improvements over state of the art reinforcement learning methods.</p><p>Researchers have been studying how groups of problem-solvers organize themselves and communicate for years, under the field of ""collective intelligence."" It's been shown that there are some surprisingly simple relationships between a group's communication network structure and how well that group is able to perform on different kinds of tasks.&nbsp;</p><p>Using these simple lessons, we designed a deep reinforcement learning algorithm that, instead of using one massive neural network (NN), leverages a community of many smaller NNs. We enable these neural networks to communicate with one another, and to learn from each others' explorations and successes.&nbsp;</p><p>Using this strategy yields significant improvements over the state of the art. By placing these neural nets on a communication network that is similar in structure to how humans communicate,&nbsp;we see a 33% improvement in how fast the networks are able to learn and in how well they are able to perform at a benchmark reinforcement learning task.</p><p><br></p>",,,2018-05-09 14:23:16.868,True,2017-07-01,Deep Reinforcement Learning Inspired by Human Collective Intelligence,LAB-INSIDERS,,True,Human Dynamics,False
the-atlas-of-inequality,dcalacci,False,"<h2><b>Segregation is hurting our societies and especially our cities. But economic inequality isn't just limited to neighborhoods. The restaurants, stores, and other places we visit in cities are all unequal in their own way.&nbsp;</b></h2><p>The Atlas of Inequality &nbsp;shows the income inequality of people who visit different places in the Boston metro area. It uses aggregated anonymous location data from digital devices to estimate people's incomes and where they spend their time.&nbsp;Using that data, we've made our own <b>place inequality </b><b>metric</b> to capture how unequal the incomes of visitors to each place are. Economic inequality isn't just limited to neighborhoods; it's part of the places you visit every day.</p><p>Try it yourself here:</p><h2><a href=""http://inequality.media.mit.edu""><b>The Atlas of Inequality</b></a></h2><p>The Atlas of Inequality is a project from the Human Dynamics group at the <a href=""https://www.media.mit.edu/"">MIT Media Lab</a> and the Department of Mathematics at <a href=""http://www.uc3m.es/"">Universidad Carlos III de Madrid</a>.</p><p>It is part of a broader initiative to understand human behavior in our cities and how large-scale problems like transportation, housing, segregation, or inequality depend in part on the emergent patterns of people’s individual opportunities and choices.</p>",,,2019-03-20 17:07:49.005,True,2019-03-01,The Atlas of Inequality,PUBLIC,https://inequality.media.mit.edu,True,Human Dynamics,False
the-trade-off-between-the-utility-and-privacy-risks-of-location-data-and-implications-for-data-as-a-public-good,dcalacci,False,"<p><i>Paper presented at the ""Connected Life 2019: Data &amp; Disorder"" conference at the Oxford Internet Institute.</i></p><p>High-resolution individual geolocation data passively collected from mobile phones is increasingly sold in private markets and shared with researchers.</p><p>This data poses significant security, privacy, and ethical risks: it’s been shown that users can be re-identified in such datasets, and its collection rarely involves their full consent or knowledge. This data is valuable to private firms (e.g. targeted marketing) but also presents clear value as a public good. Recent public interest research has demonstrated that high-resolution location data can more accurately measure segregation in cities and provide inexpensive transit modeling. But as data is aggregated to mitigate its re-identifiability risk, its value as a good diminishes. How do we rectify the clear security and safety risks of this data, its high market value, and its potential as a resource for public good? We extend the recently proposed concept of a tradeoff curve that illustrates the relationship between dataset utility and privacy. We then hypothesize how this tradeoff differs between private market use and its potential use for public good. We further provide real-world examples of how high resolution location data, aggregated to varying degrees of privacy protection, can be used in the public sphere and how it is currently used by private firms.</p>",,,2019-05-24 15:26:51.045,False,2019-03-01,The Tradeoff Between the Utility and Risk of Location Data and Implications for Public Good,LAB,,True,Human Dynamics,False
g3p-II,ked03,False,"<h2><p><span style=""font-size: 18px; font-weight: normal;"">Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods—such as blowing, pressing, and forming—have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.</span><br></p></h2>",,--Choose Location,2018-10-23 15:19:17.532,True,2015-09-01,Glass II,PUBLIC,,True,Mediated Matter,False
g3p-II,dlizardo,False,"<h2><p><span style=""font-size: 18px; font-weight: normal;"">Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods—such as blowing, pressing, and forming—have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.</span><br></p></h2>",,--Choose Location,2018-10-23 15:19:17.532,True,2015-09-01,Glass II,PUBLIC,,True,Mediated Matter,False
aguahoja,dlizardo,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
g3p,dlizardo,False,"<p>Ancient yet modern, enclosing yet invisible, glass was first created in  Mesopotamia and Ancient Egypt 4,500 years ago. Precise recipes for its  production - the chemistry and techniques - often remain closely guarded  secrets. Glass can be molded, formed, blown, plated or sintered; its  formal qualities are closely tied to techniques used for its  formation.&nbsp;From the discovery of core-forming process for bead-making in  ancient Egypt, through the invention of the metal blow pipe during  Roman times, to the modern industrial Pilkington process for making  large-scale flat glass; each new breakthrough in glass technology  occurred as a result of prolonged experimentation and ingenuity, and has  given rise to a new universe of possibilities for uses of the material.  <br></p>",,--Choose Location,2017-10-13 19:43:29.330,True,2014-01-01,Glass I,PUBLIC,,True,Mediated Matter,False
g3p-II,giorgiaf,False,"<h2><p><span style=""font-size: 18px; font-weight: normal;"">Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods—such as blowing, pressing, and forming—have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.</span><br></p></h2>",,--Choose Location,2018-10-23 15:19:17.532,True,2015-09-01,Glass II,PUBLIC,,True,Mediated Matter,False
g3p-II,mlstern,False,"<h2><p><span style=""font-size: 18px; font-weight: normal;"">Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods—such as blowing, pressing, and forming—have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.</span><br></p></h2>",,--Choose Location,2018-10-23 15:19:17.532,True,2015-09-01,Glass II,PUBLIC,,True,Mediated Matter,False
g3p,mlstern,False,"<p>Ancient yet modern, enclosing yet invisible, glass was first created in  Mesopotamia and Ancient Egypt 4,500 years ago. Precise recipes for its  production - the chemistry and techniques - often remain closely guarded  secrets. Glass can be molded, formed, blown, plated or sintered; its  formal qualities are closely tied to techniques used for its  formation.&nbsp;From the discovery of core-forming process for bead-making in  ancient Egypt, through the invention of the metal blow pipe during  Roman times, to the modern industrial Pilkington process for making  large-scale flat glass; each new breakthrough in glass technology  occurred as a result of prolonged experimentation and ingenuity, and has  given rise to a new universe of possibilities for uses of the material.  <br></p>",,--Choose Location,2017-10-13 19:43:29.330,True,2014-01-01,Glass I,PUBLIC,,True,Mediated Matter,False
g3p-II,nassia,False,"<h2><p><span style=""font-size: 18px; font-weight: normal;"">Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods—such as blowing, pressing, and forming—have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.</span><br></p></h2>",,--Choose Location,2018-10-23 15:19:17.532,True,2015-09-01,Glass II,PUBLIC,,True,Mediated Matter,False
fiberbots,nassia,False,"<p>FIBERBOTS is a digital fabrication platform fusing cooperative robotic manufacturing with abilities to generate highly sophisticated material architectures. The platform can enable design and digital fabrication of large-scale structures with high spatial resolution leveraging mobile fabrication nodes, or robotic ""agents"" designed to <i>tune</i> the material make-up of the structure being constructed on the fly as informed by their environment.<br></p><p>Some of nature’s most successful organisms collaborate in a swarm fashion. Nature’s builders leverage hierarchical structures in order to control and optimize multiple material properties. Spiders, for instance, spin protein fibers to weave silk webs with tunable local and global material properties, adjusting their material composition and fiber placement to create strong yet flexible structures optimized to capture prey. Other organisms, such as bees, ants and termites cooperate to rapidly build structures much larger than themselves. </p>",,,2019-02-13 16:36:22.730,True,2016-01-01,"FIBERBOTS: Design of a multi-agent, fiber composite digital fabrication system",PUBLIC,,True,Mediated Matter,False
skin-perfusion-photography,guysatat,False,"<p>Skin and tissue perfusion measurements are important parameters for diagnosis of wounds and burns, and for monitoring plastic and reconstructive surgeries. In this project, we use a standard camera and a laser source in order to image blood-flow speed in skin tissue. We show results of blood-flow maps of hands, arms, and fingers. We combine the complex scattering of laser light from blood with computational techniques found in computer science.</p>",,--Choose Location,2016-12-14 20:46:59.692,True,2014-09-01,Skin Perfusion Photography,PUBLIC,,True,Camera Culture,False
imaging-with-all-photons,guysatat,False,"<h2><span style=""font-weight: normal;"">How to see through tissue</span></h2><p>We demonstrate a new method to image through scattering materials like tissue and fog. The demonstration includes imaging an object hidden behind 1.5cm of tissue; it's like imaging through the palm of a hand. Our optical method is based on measuring and using all photons in the signal (as opposed to traditional methods, which use only part of the signal). Specifically, we use a time-resolved method that allows us to distinguish between photons that travel different paths in the tissue. Combining this unique measurement process with novel algorithms allows us to recover the hidden objects. This technique can be used in biomedical imaging, as well as imaging through fog and clouds.</p>",,--Choose Location,2017-04-05 01:49:57.109,True,2015-09-01,Imaging with All Photons,PUBLIC,,True,Camera Culture,False
imaging-without-a-lens-and-only-a-few-pixels,guysatat,False,"<h2>Lensless imaging with compressive ultrafast sensing</h2><p>Traditional cameras require a lens and a mega-pixel sensor to capture images. The lens focuses light from the scene onto the sensor. We demonstrate a new imaging method that is lensless and requires only a single pixel for imaging. Compared to previous single pixel cameras our system allows significantly faster and more efficient acquisition. This is achieved by using ultrafast time-resolved measurement with compressive sensing. The time-resolved sensing adds information to the measurement, thus fewer measurements are needed and the acquisition is faster. Lensless and single pixel imaging computationally resolves major constraints in imaging systems design. Notable applications include imaging in challenging parts of the spectrum (like infrared and THz), and in challenging environments where using a lens is problematic.
                    
                </p>",,,2019-04-19 17:50:37.233,True,2015-01-01,Efficient lensless imaging with a femto-pixel,PUBLIC,http://web.media.mit.edu/~guysatat/,True,Camera Culture,False
seeing-through-fog,guysatat,False,"<h2>Seeing through dense, dynamic, and heterogeneous fog conditions. The technique, based on visible light, uses hardware that is similar to LIDAR to recover the target depth and reflectance.</h2><p>The system relies on ultrafast measurements, used to computationally remove inclement weather conditions such as fog, and produce a photo and depth map as if the fog weren’t there (with contrast improved by 6.5x in dense fog conditions).&nbsp;&nbsp;<br></p><h2>Applications</h2><ul><li>Autonomous and augmented driving in challenging weather.</li><li>Airplanes and helicopters take off, landing and low level flight in dense fog conditions.</li><li>Trains traveling at normal speeds during inclement weather conditions.</li></ul>",,,2018-08-22 17:18:20.368,True,2017-03-01,Seeing Through Realistic Fog,PUBLIC,http://web.media.mit.edu/~guysatat/,True,Camera Culture,False
identi-wheez-a-device-for-in-home-diagnosis-of-asthma,guysatat,False,"<p>Asthma is the most common chronic illness among children. The skills required to diagnose it make it an even greater concern. Our solution is a child-friendly wearable device that allows in-home diagnosis of asthma. The device acquires simultaneous measurements from multiple stethoscopes. The recordings are then sent to a specialist who uses assistive diagnosis algorithms that enable auscultation (listening to lung sounds with a stethoscope). Sound refocusing algorithms enable the specialist to listen to any location in the lungs. The specialist also has access to a sound ""heat map"" that shows the location of sound sources in the lungs.</p>",,--Choose Location,2019-04-19 18:25:32.160,True,2016-01-01,Identi-Wheez: A device for in-home diagnosis of asthma,PUBLIC,,True,Camera Culture,False
towards-in-vivo-biopsy,guysatat,False,"<p>A new method to detect and distinguish between different types of fluorescent materials. The suggested technique has provided a dramatically larger depth range compared to previous methods; thus it enables medical diagnosis of body tissues without removing the tissue from the body, which is the current medical standard. It uses fluorescent probes, which are commonly used in medical diagnosis. One of these parameters is the fluorescence lifetime, that is the average time the fluorescence emission lasts. The new method can distinguish between different fluorescence lifetimes, which allows diagnosis of deep tissues. Locating fluorescence probes in the body using this method can, for example, indicate the location of a tumor in deep tissue, and classify it as malignant or benign according to the fluorescence lifetime, thus eliminating the need for X-ray or biopsy.</p>",,--Choose Location,2018-03-29 20:34:54.336,True,2014-09-01,Towards In-Vivo Biopsy,PUBLIC,,True,Camera Culture,False
time-folded-optics,guysatat,False,<h2>Rethinking photography optics in the time dimension</h2><p><i>What if we could design optics in time instead of space?</i></p>,,,2018-09-24 18:15:42.266,True,2018-08-01,Time-folded optics,PUBLIC,http://web.media.mit.edu/~barmak/Time-folded.html,True,Camera Culture,False
calibration-invariant-i,guysatat,False,"<p><b>Object Classification through Scattering Media&nbsp;with Deep Learning</b></p><p>A method for classifying objects hidden behind a scattering layer with a neural network. Training on synthetic data with variations in calibration parameters allows the network to learn a model that doesn't require calibration during lab experiments.<br></p><p>Traditional techniques to see through scattering media rely on a physical model that needs to be precisely calibrated. Computationally overcoming the scattering relies heavily on accurately calibrated physical models. Thus, such systems are extremely sensitive to a precise and lengthy calibration process. </p><p>In this work we overcome this bottleneck by utilizing neural networks and their ability to learn models that are invariant to data transformation. In our case, the transformations are variations in the imaging system calibration parameters. To that end, we create a synthetic dataset that contains variations in all calibration parameters (we use a Monte Carlo forward model to render the measurements). The system is then tested on actual lab experiments without specific calibration or tuning.</p>",,,2018-10-20 00:40:19.319,True,2016-09-01,Calibration Invariant Imaging,PUBLIC,,True,Camera Culture,False
at-home-sleep-apnea-screening,guysatat,False,"<p>A large proportion of the American population currently suffers from sleep disorders. Among them are patients with obstructive sleep apnea (OSA), who repeatedly stop breathing while asleep. Current screening methods and devices are impractical for widespread screening. We introduce a new model for low-cost OSA screening consisting of an at-home, wearable sleep mask that can easily track the wearer's sleep patterns. The data collected overnight by this sensory mask provides a determination of a patient's OSA risk.<br></p><p><strong>Why is this work important?</strong></p><p>There are 7-18 million Americans suffering from sleep disorders. Among them are patients with OSA, who stop breathing either completely or partially while asleep. This is a serious condition with few reliable low-cost devices available for primary diagnosis without expert supervision.</p><p><strong>What has been done before?</strong></p><p>The gold standard for OSA diagnosis is overnight polysomnography (PSG). Apart from that there are many home diagnostics devices available. However, many at-home devices offer poor diagnostic quality and some of them also require expert intervention, from installation of the device to analysis of the data.</p><p><strong>What are our contributions?</strong><br></p><p>We report the construction and validation of a design for low-cost OSA screening built around a simplified screening device embedded in an at-home wearable sleep mask. This simplified screening system allows for OSA diagnosis without imposing the costs or time commitment of a full PSG.</p><p><strong>What are the next steps?</strong></p><p>In the next iterations of the device, we aim to improve the mechanical design and ease of use, as well as automate data analysis and screening so that the device can be evaluated in larger studies.</p>",,,2018-05-06 23:20:12.777,True,2016-05-09,At-Home Sleep Apnea Screening,PUBLIC,,True,Camera Culture,False
radio_o,geppetto,False,"<p>radiO_o is a battery-powered speaker worn by hundreds of party guests, turning each person into a local mobile sound system. The radiO_o broadcast system allows the DJ to transmit sounds over several pirate radio channels to mix sounds between hundreds of speakers roaming around the space and the venue's existing sound system.</p>",,--Choose Location,2016-12-05 00:17:21.437,True,2014-01-01,radiO_o,PUBLIC,,True,Responsive Environments,True
conjugate,dvlevine,False,"<p>A recent focus of our lab has been making use of Tangible Displays and Body Object Space to develop new assistive technologies. As a test case, we prototyped the Mario side-scrolling game for visually impaired users, using body movement analogies to control Mario in the game. Mario and 2D side scrollers present a particularly interesting case, as they keep the main character location in the center of the display and move the world around the character. The shape display itself provides spatial audio of enemy positions. We make use of the AUFLIP sensor platform to pick up body movements—walking and jumping, causing Mario to do the same in-game. This enables users to keep their hands engaged to understand the game landscape, while using their body to control Mario at the same time.&nbsp;</p>",,,2019-02-08 17:48:57.541,True,2017-03-01,CONJURE,PUBLIC,,True,Biomechatronics,False
in-force,dvlevine,False,"<p>We propose a novel tangible interaction with pin-based shape display that can reproduce haptic perception of shape, material stiffness, and heterogeneous internal structures of volumetric shape. This is enabled by newly developed pin display, inFORCE, that can detect the force that is applied to each pin, and exert arbitrary force to contact body and objects at the same time. Our proposed interaction methods enabled people to ""press through"" computationally rendered shapes to understand the internal structure of 3D volumetric information. Our design space explores a range of interaction capability enabled by the Force Shape Display system including capturing physical material properties.</p>",,,2019-03-25 13:24:48.264,True,2017-10-01,inFORCE,LAB-INSIDERS,http://tangible.media.mit.edu/project/inforce/,True,Biomechatronics,False
transformative-appetite,dvlevine,False,"<p>We developed a concept of transformative appetite, where edible 2D films made of common food materials (protein, cellulose or starch) can transform into 3D food during cooking. This transformation process is triggered by water adsorption, and it is strongly compatible with the ‘flat packaging’ concept for substantially reducing shipping costs and storage space. To develop these transformable foods, we performed material-based design, established a hybrid fabrication strategy, and conducted performance simulation. Users can customize food shape transformations through a pre-defined simulation platform, and then fabricate these designed patterns using additive manufacturing. Three application techniques are provided: &nbsp;2D-to-3D folding, hydration-induced wrapping, and temperature-induced self-fragmentation, to present the shape, texture, and interaction with food materials. Based on this concept, several dishes were created in the kitchen, to demonstrate the futuristic dining experience through materials-based interaction design.</p>",,,2019-02-14 19:43:51.271,True,2017-05-07,Transformative Appetite,PUBLIC,,True,Biomechatronics,False
AUFLIP_Feedback,dvlevine,False,"<p>How can people learn advanced motor skills such as front flips and tennis swings without starting from a young age? The answer, following the work of Masters et. al., we believe, is implicitly. Implicit learning is associated with higher retention and knowledge transfer, but that is unable to be explicitly articulated as a set of rules. To achieve implicit learning is difficult, but may be taught using obscured feedback—that is, feedback that does not directly describe the result of an action.<br></p><p>With AUFLIP , we sought to provide auditory feedback to help newcomers learn front flips. We created a wearable system with a simplified  model of a front flip that compares a user’s time to peak rotation  against their ideal time. As the user approaches their ideal  performance, the system begins playing a chord, only completing the  chord if the user manages to rotate at their ideal peak time. We tested  this system by integrating it into an environment where professional  coaches teach novices how to perform front flips; we found preliminary  results suggesting that users wearing the device exhibited implicit  learning. </p>",,,2019-01-29 19:46:29.141,True,2016-10-14,AUFLIP,PUBLIC,,True,Biomechatronics,False
stem-accessibility-tool,namdev,False,"<p>We are developing a very intuitive and interactive platform to make complex information--especially science, technology, engineering, and mathematics (STEM) material--truly accessible to blind and visually impaired students by using a tactile device with no loss of information compared with printed materials. A key goal of this project is to develop tactile information-mapping protocols through which the tactile interface can best convey educational and other graphical materials.</p>",,--Choose Location,2016-12-05 00:16:52.685,True,2014-01-01,STEM Accessibility Tool,PUBLIC,,True,Fluid Interfaces,False
medium-shapes-message,crisjf,False,"<p>Communication technologies, from printing to social media, affect our historical records by changing the way ideas are spread and recorded. Yet, finding statistical evidence of this fact has been challenging. Here we combine a common causal inference technique (instrumental variable estimation) with a dataset on nearly forty thousand biographies from Wikipedia (Pantheon 2.0) to study the effect of the introduction of printing in European cities on Wikipedia’s digital biographical records. By using a city’s distance to Mainz as an instrument for the adoption of the movable type press, we show that European cities that adopted printing earlier were more likely to become the birthplace of a famous scientist or artist during the years following the invention of printing. We bring these findings to recent communication technologies by showing that the number of radios and televisions in a country correlates with the number of globally famous performing artists and sports players born in that country, even after controlling for GDP, population, and including country and year fixed effects. These findings support the hypothesis that the introduction of communication technologies can bias historical records in the direction of the content that is best suited for each technology.&nbsp;</p>",,,2019-03-08 15:11:50.010,True,2015-12-01,How the medium shapes the message: Printing and the rise of the arts and sciences,PUBLIC,,True,Collective Learning,False
temporal-scales-in-human-collective-forgetting,crisjf,False,"<p>Collective memory and attention are sustained by two channels: oral communication (communicative memory) and the physical recording of information (cultural memory). Here, we use data on the citation of academic articles and patents, and on the online attention received by songs, movies, and biographies, to describe the temporal decay of the attention received by cultural products. We show that, once we isolate the temporal dimension of the decay, the attention received by cultural products decays following a universal biexponential function. We explain this universality by proposing a mathematical model based on communicative and cultural memory, which fits the data better than previously proposed log-normal and exponential models. Our results reveal that biographies remain in our communicative memory the longest (20–30 years) and music the shortest (about 5.6 years). These findings show that the average attention received by cultural products decays following a universal biexponential function.</p>",,,2019-04-17 19:27:00.707,True,2017-02-01,The universal decay of collective memory and attention,PUBLIC,,True,Collective Learning,False
economic-complexity-and-income-inequality,crisjf,False,"<p>Decades ago development scholars argued that the productive structure of a country (i. e. the mix of industries operating in the country) constrains its ability to generate and distribute income. They were correct! It was recently shown that the mix of products that a country exports is predictive of its future pattern of diversification and economic growth. But what is the link between a country's productive structure and its ability to distribute income?&nbsp;Here, we combine methods from econometrics, network science, and economic complexity, together with data on income inequality and world trade, to show that countries exporting complex products have lower levels of income inequality than countries exporting simpler products. Using multivariate regression analysis, we show that economic complexity is a significant and negative predictor of income inequality and that this relationship is robust to controlling for aggregate measures of income, institutions, export concentration, and human capital. Moreover, we introduce a measure that associates a product to a level of income inequality equal to the average GINI of the countries exporting that product (weighted by the share the product represents in that country’s export basket). The Product-GINI index, or PGI,&nbsp;can provide important insights on the constraints to inequality imposed by a country's productive structure. Finally, we integrate our results to the Observatory of Economic Complexity, an online resource that allows its users to visualize the structural transformation of over 150 countries&nbsp;and their associated changes in income inequality during 1963–2008.</p>",,--Choose Location,2017-10-10 16:03:06.641,True,2014-09-01,Inequality and the impact of industrial structures,PUBLIC,,True,Collective Learning,False
which-industries-follow-relatedness,crisjf,False,"<p>Industries are more likely to enter and less likely to exit regions that are densely populated by related industries. Unfortunately, the measures used to estimate the relatedness of industries often combine information about multiple forms of relatedness. Here, we use data on the entire formal sector economy of a large country to construct five different measures of relatedness and compare their ability to predict diversification events. We interpret differences in the ability of these metrics to predict entry events as evidence of the relative importance of each relatedness channel for specific industries. These findings advance our understanding of the forms of relatedness that are more likely to predict regional diversification events for specific industries.</p>",,,2018-05-03 20:55:25.540,True,2017-02-01,Untangling Relatedness: What forms of relatedness predict diversification?,LAB-INSIDERS,,True,Collective Learning,False
trains-of-thought-railroad-access-and-knowledge-diffusion-in-sweden,crisjf,False,"<p>Industrial diversification is a path-dependent process that leverages knowledge, skills, and new technologies. Because such resources are difficult to move, geography plays a crucial role in determining the future economic activities of countries, regions, and cities. Yet most of the evidence on the geographic diffusion of economic activities is restricted to the last 60 years and relies on correlations.&nbsp;</p><p>This paper analyzes the geographic diffusion of economic activities for Swedish towns between 1850 and 1950, using the evolution of the railroad network as a way to address endogeneity. We use the straight line between Sweden's 10 largest towns as an instrument for train adoption. Our instrumental variable estimates show that regions are more likely to diversify into sectors that are present in their train neighbors, suggesting that the impact of connectivity goes beyond access to markets: connectivity also promotes diffusion of economic activities, even at early stages of development.&nbsp;<br></p>",,,2018-05-07 00:25:12.390,True,2017-11-01,Railroad Access and Diffusion of Industries: Evidence from Sweden during the Second Industrial Revolution,PUBLIC,,True,Collective Learning,False
the-laws-of-forgetting-ii,crisjf,False,"<p>In order to understand how exogenous shocks, like death, impact memorability by remembering, we use a data-set of biographies from Wikipedia for all individuals who have more than 15 different language editions. Here, we focus on different external shocks that are able to trigger remembering, such as Death, Nobel Prize, Academy Awards (Oscars), Ballon d'Or, Golden Globes, and Grammy's. All of these events show an exogenous-critical non-trivial herd behavior, as described by <a href=""http://www.pnas.org/content/105/41/15649"">Crane and Sornette 2008</a>.</p>",,,2018-10-20 16:47:24.281,True,2017-10-16,The laws of forgetting II: How death and exogenous events shape our collective memory,PUBLIC,,True,Collective Learning,False
industry-knowledge,crisjf,False,"<p>How do regions acquire the knowledge they need to diversify their economic activities? How does the migration of workers among firms and industries contribute to the diffusion of that knowledge? Here we measure the industry-, occupation-, and location-specific knowledge carried by workers from one establishment to the next, using a dataset summarizing the individual work history for an entire country. We study pioneer firms—firms operating in an industry that was not present in a region—because the success of pioneers is the basic unit of regional economic diversification. We find that the growth and survival of pioneers increase significantly when their first hires are workers with experience in a related industry and with work experience in the same location, but not with past experience in a related occupation. We compare these results with new firms that are not pioneers and find that industry-specific knowledge is significantly more important for pioneer than for non-pioneer firms. To address endogeneity we use Bartik instruments, which leverage national fluctuations in the demand for an activity as shocks for local labor supply. The instrumental variable estimates support the finding that industry-specific knowledge is a predictor of the survival and growth of pioneer firms. These findings expand our understanding of the micromechanisms underlying regional economic diversification.</p>",,,2018-12-14 20:31:53.678,True,2017-03-01,"The role of industry, occupation, and location-specific knowledge in the survival of new firms",PUBLIC,,True,Collective Learning,False
medium-shapes-message,amy_yu,False,"<p>Communication technologies, from printing to social media, affect our historical records by changing the way ideas are spread and recorded. Yet, finding statistical evidence of this fact has been challenging. Here we combine a common causal inference technique (instrumental variable estimation) with a dataset on nearly forty thousand biographies from Wikipedia (Pantheon 2.0) to study the effect of the introduction of printing in European cities on Wikipedia’s digital biographical records. By using a city’s distance to Mainz as an instrument for the adoption of the movable type press, we show that European cities that adopted printing earlier were more likely to become the birthplace of a famous scientist or artist during the years following the invention of printing. We bring these findings to recent communication technologies by showing that the number of radios and televisions in a country correlates with the number of globally famous performing artists and sports players born in that country, even after controlling for GDP, population, and including country and year fixed effects. These findings support the hypothesis that the introduction of communication technologies can bias historical records in the direction of the content that is best suited for each technology.&nbsp;</p>",,,2019-03-08 15:11:50.010,True,2015-12-01,How the medium shapes the message: Printing and the rise of the arts and sciences,PUBLIC,,True,Other,False
inflated-appetite,wwen,False,"<p>As part of human evolution and revolution, food is among the earliest forms of human interaction, but it has remained essentially unchanged from ancient to modern times. What if we introduced engineered and programmable food materials? With that change, food can change its role from passive to active. Food can ""communicate"" using its inherent behaviors combined with engineering accuracy. Food becomes media and interface. During an MIT winter course we initiated and taught, we encouraged students to design pneumatic food. Students successfully implemented inflatable sugar and cheese products. To inflate food, we use both an engineering approach and a biological approach; to solidify the inflated food, we introduce both heat via the oven, and coldness with liquid nitrogen.</p>",,--Choose Location,2016-12-24 22:05:38.254,True,2016-01-01,Inflated Appetite,PUBLIC,,True,Tangible Media,False
biologic,wwen,False,"<p>Cells’ biomechanical responses to external stimuli have been intensively studied but rarely implemented into devices&nbsp;<span style=""font-size: 18px; font-weight: 400;"">that interact with the human body. We demonstrate that the hygroscopic and biofluorescent behaviors of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">living cells can be engineered to design biohybrid wearables, which give multifunctional responsiveness to hu</span><span style=""font-size: 18px; font-weight: 400;"">man sweat. By depositing genetically tractable microbes on a humidity-inert material to form a heterogeneous&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">multilayered structure, we obtained biohybrid films that can reversibly change shape and biofluorescence intensity&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">within a few seconds in response to environmental humidity gradients. Experimental characterization and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">mechanical modeling of the film were performed to guide the design of a wearable running suit and a fluorescent&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">shoe prototype with bio-flaps that dynamically modulates ventilation in synergy with the body’s need for cooling.</span></p>",,--Choose Location,2018-05-04 15:32:39.880,True,2014-01-01,bioLogic—Science Advances,PUBLIC,,True,Tangible Media,False
transformative-appetite,wwen,False,"<p>We developed a concept of transformative appetite, where edible 2D films made of common food materials (protein, cellulose or starch) can transform into 3D food during cooking. This transformation process is triggered by water adsorption, and it is strongly compatible with the ‘flat packaging’ concept for substantially reducing shipping costs and storage space. To develop these transformable foods, we performed material-based design, established a hybrid fabrication strategy, and conducted performance simulation. Users can customize food shape transformations through a pre-defined simulation platform, and then fabricate these designed patterns using additive manufacturing. Three application techniques are provided: &nbsp;2D-to-3D folding, hydration-induced wrapping, and temperature-induced self-fragmentation, to present the shape, texture, and interaction with food materials. Based on this concept, several dishes were created in the kitchen, to demonstrate the futuristic dining experience through materials-based interaction design.</p>",,,2019-02-14 19:43:51.271,True,2017-05-07,Transformative Appetite,PUBLIC,,True,Tangible Media,False
inflated-appetite,liningy,False,"<p>As part of human evolution and revolution, food is among the earliest forms of human interaction, but it has remained essentially unchanged from ancient to modern times. What if we introduced engineered and programmable food materials? With that change, food can change its role from passive to active. Food can ""communicate"" using its inherent behaviors combined with engineering accuracy. Food becomes media and interface. During an MIT winter course we initiated and taught, we encouraged students to design pneumatic food. Students successfully implemented inflatable sugar and cheese products. To inflate food, we use both an engineering approach and a biological approach; to solidify the inflated food, we introduce both heat via the oven, and coldness with liquid nitrogen.</p>",,--Choose Location,2016-12-24 22:05:38.254,True,2016-01-01,Inflated Appetite,PUBLIC,,True,Tangible Media,False
biologic,liningy,False,"<p>Cells’ biomechanical responses to external stimuli have been intensively studied but rarely implemented into devices&nbsp;<span style=""font-size: 18px; font-weight: 400;"">that interact with the human body. We demonstrate that the hygroscopic and biofluorescent behaviors of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">living cells can be engineered to design biohybrid wearables, which give multifunctional responsiveness to hu</span><span style=""font-size: 18px; font-weight: 400;"">man sweat. By depositing genetically tractable microbes on a humidity-inert material to form a heterogeneous&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">multilayered structure, we obtained biohybrid films that can reversibly change shape and biofluorescence intensity&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">within a few seconds in response to environmental humidity gradients. Experimental characterization and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">mechanical modeling of the film were performed to guide the design of a wearable running suit and a fluorescent&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">shoe prototype with bio-flaps that dynamically modulates ventilation in synergy with the body’s need for cooling.</span></p>",,--Choose Location,2018-05-04 15:32:39.880,True,2014-01-01,bioLogic—Science Advances,PUBLIC,,True,Tangible Media,False
pneuduino,liningy,False,"<p>Pneuduino is a hardware platform for kids, students, artists, designers, and researchers who are interested in controlling air flow and pressure for their projects. The Pneuduino toolkit is currently used in workshops with high school or college students. While each workshop has a different focus, they all introduce concepts of air as actuator and sensor as well as different fabrication methods to create transforming artifacts. Air is one the most abundant resources on earth. By adding computation ability to air, we can create new types of materials that enable us to design robots that are soft, furniture that is adaptive, clothing that is intelligent, and art pieces that are breathing.</p>",,--Choose Location,2016-12-05 00:17:03.712,True,2015-01-01,Pneuduino,PUBLIC,,True,Tangible Media,False
transformative-appetite,liningy,False,"<p>We developed a concept of transformative appetite, where edible 2D films made of common food materials (protein, cellulose or starch) can transform into 3D food during cooking. This transformation process is triggered by water adsorption, and it is strongly compatible with the ‘flat packaging’ concept for substantially reducing shipping costs and storage space. To develop these transformable foods, we performed material-based design, established a hybrid fabrication strategy, and conducted performance simulation. Users can customize food shape transformations through a pre-defined simulation platform, and then fabricate these designed patterns using additive manufacturing. Three application techniques are provided: &nbsp;2D-to-3D folding, hydration-induced wrapping, and temperature-induced self-fragmentation, to present the shape, texture, and interaction with food materials. Based on this concept, several dishes were created in the kitchen, to demonstrate the futuristic dining experience through materials-based interaction design.</p>",,,2019-02-14 19:43:51.271,True,2017-05-07,Transformative Appetite,PUBLIC,,True,Tangible Media,False
inflated-appetite,jifei,False,"<p>As part of human evolution and revolution, food is among the earliest forms of human interaction, but it has remained essentially unchanged from ancient to modern times. What if we introduced engineered and programmable food materials? With that change, food can change its role from passive to active. Food can ""communicate"" using its inherent behaviors combined with engineering accuracy. Food becomes media and interface. During an MIT winter course we initiated and taught, we encouraged students to design pneumatic food. Students successfully implemented inflatable sugar and cheese products. To inflate food, we use both an engineering approach and a biological approach; to solidify the inflated food, we introduce both heat via the oven, and coldness with liquid nitrogen.</p>",,--Choose Location,2016-12-24 22:05:38.254,True,2016-01-01,Inflated Appetite,PUBLIC,,True,Tangible Media,False
circuit-robots,jifei,False,"<h2>Integrating sensors and actuators using flexible electronics</h2><p>Currently, the manufacturing of self-actuating and self-sensing robots requires non-standard manufacturing techniques and assembly steps to integrate electrical and mechanical systems. In this work, we developed a novel manufacturing technique, where such robots can be produced at a flexible electronics factory. We developed the technique using standard industrial machines, processes, and materials. Using a lamination process, we were able to integrate air pouches or shape memory alloy (SMA) inside a polyamide-based flexible circuit to produce bending actuators. The bend angle of the actuators is sensed with a chain of inertial measurement units integrated on the actuator. Air-pouch actuators can produce a force of a 2.24N, and a maximum bend angle of 74 degrees. To demonstrate, we manufactured a five-legged robot with the developed actuators and bend sensors, with all the supporting electronics (e.g., microcontrollers, radio) directly integrated into the flexible printed circuit. Such robots are flat and lightweight (15 grams) and thus conveniently compact for transportation and storage. We believe that our technique can allow inexpensive and fast prototyping and deployment of self-actuating and self-sensing robots.<br></p>",,,2019-04-17 19:28:42.580,True,2017-08-01,Circuit Robots: Mass manufacturing of self-actuating robots,PUBLIC,,True,Tangible Media,False
inflatables,jifei,False,"<p>Printflatables is a design and fabrication system for human-scale, functional and dynamic inflatable objects. The user begins with specifying an intended 3D model which is decomposed to two dimensional fabrication geometry. This forms the input for a numerically controlled contact iron that seals layers of thermoplastic fabric. </p><p>In this project, we showcase the system design in detail, the pneumatic primitives that this technique enables and merits of being able to make large, functional and dynamic pneumatic artifacts. We demonstrate the design output through multiple objects which could motivate fabrication of inflatable media and pressure-based interfaces.</p><p><a href=""http://tangible.media.mit.edu/project/printflatables/"">Project Website</a></p>",,,2019-04-17 19:31:11.620,True,2017-05-10,"Printflatables: Printing human-scale, functional, and dynamic inflatable objects",PUBLIC,http://tangible.media.mit.edu/project/printflatables/,True,Tangible Media,False
trans-dock,jifei,False,"<p>We introduce TRANS-DOCK, a passive docking system for pin-based shape displays that enhances the interaction capability for both the output and input. By simply switching the ""transducer"" module to be docked on a single shape display, users can selectively switch between different display sizes and resolutions, movement modalities, as well as pin alignments enabled by the transducers. We introduce a design space consisting of mechanical elements and enabled interaction capabilities. We utilized several mechanical elements to develop the docking hardware for customizable interactions. With this idea, we present potential application spaces, which include digital 3D model explorations, active tangible interface prototyping, gaming, and dynamic house models. TRANS-DOCK intends to expand what a single shape display can do for dynamic physical interactions, by converting arrays of linear motion to several types of dynamic motion in an adaptable and flexible manner.</p>",,,2018-10-05 18:57:23.901,True,2018-08-01,TRANS-DOCK,LAB-INSIDERS,,True,Tangible Media,False
biologic,jifei,False,"<p>Cells’ biomechanical responses to external stimuli have been intensively studied but rarely implemented into devices&nbsp;<span style=""font-size: 18px; font-weight: 400;"">that interact with the human body. We demonstrate that the hygroscopic and biofluorescent behaviors of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">living cells can be engineered to design biohybrid wearables, which give multifunctional responsiveness to hu</span><span style=""font-size: 18px; font-weight: 400;"">man sweat. By depositing genetically tractable microbes on a humidity-inert material to form a heterogeneous&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">multilayered structure, we obtained biohybrid films that can reversibly change shape and biofluorescence intensity&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">within a few seconds in response to environmental humidity gradients. Experimental characterization and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">mechanical modeling of the film were performed to guide the design of a wearable running suit and a fluorescent&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">shoe prototype with bio-flaps that dynamically modulates ventilation in synergy with the body’s need for cooling.</span></p>",,--Choose Location,2018-05-04 15:32:39.880,True,2014-01-01,bioLogic—Science Advances,PUBLIC,,True,Tangible Media,False
pneuduino,jifei,False,"<p>Pneuduino is a hardware platform for kids, students, artists, designers, and researchers who are interested in controlling air flow and pressure for their projects. The Pneuduino toolkit is currently used in workshops with high school or college students. While each workshop has a different focus, they all introduce concepts of air as actuator and sensor as well as different fabrication methods to create transforming artifacts. Air is one the most abundant resources on earth. By adding computation ability to air, we can create new types of materials that enable us to design robots that are soft, furniture that is adaptive, clothing that is intelligent, and art pieces that are breathing.</p>",,--Choose Location,2016-12-05 00:17:03.712,True,2015-01-01,Pneuduino,PUBLIC,,True,Tangible Media,False
sensorknits,jifei,False,"<p>Digital machine knitting is a highly programmable manufacturing process that has been utilized to produce apparel, accessories, and footwear.&nbsp;Our research presents three classes of textile sensors exploiting the resistive, piezoresistive, and capacitive&nbsp;properties of various textile structures enabled by machine knitting with conductive yarn.&nbsp;</p>",,,2019-04-09 13:53:02.936,True,2017-09-01,SensorKnits: Architecting textile sensors with machine knitting,PUBLIC,,True,Tangible Media,False
aeromorph,jifei,False,"<p>The project investigates how to make origami structure with inflatables with various materials. We introduce a universal bending mechanism that creates programmable shape-changing behaviors with paper, plastics, and fabrics. We developed a software tool that generates this bending mechanism for a given geometry, simulates its transformation, and exports the compound geometry as digital fabrication files. A custom heat-sealing head that can be mounted on usual three-axis CNC machines to precisely fabricate the designed transforming material is presented. We envision this technology could be used for designing interactive wearables and toys, and for the packaging industry.
                    
                </p><p>Visit&nbsp;<a href=""http://tangible.media.mit.edu/project/aeromorph/"">http://tangible.media.mit.edu/project/aeromorph/</a>.<br></p><p>Honorable Mention Paper Award, UIST 2016</p>",,,2017-04-24 18:56:38.724,True,2016-11-01,aeroMorph,PUBLIC,http://tangible.media.mit.edu/project/aeromorph/,True,Tangible Media,False
auto-inflatables,jifei,False,,,,2017-11-25 17:16:39.275,True,2017-01-01,Auto-Inflatables,PUBLIC,,True,Tangible Media,False
cilllia-3d-printed-micro-pillar-structures-for-surface-texture-actuation-and-sensing,jifei,False,"<p>In nature, hair has numerous functions such as providing warmth, adhesion, locomotion, sensing, and a sense of touch, as well as its well-known aesthetic qualities. This work presents a computational method of 3D printing hair structures. It allows us to design and generate hair geometry at 50 micrometer resolution and assign various functionalities to the hair. The ability to fabricate customized hair structures enables us to create superfine surface texture, mechanical adhesion properties, new passive actuators, and touch sensors on a 3D-printed artifact. We also present several applications to show how the 3D-printed hair can be used for designing everyday interactive objects.</p>",,--Choose Location,2019-05-23 19:18:17.084,True,2015-01-01,"Cilllia: 3D-printed micro pillar structures for surface texture, actuation, and sensing",PUBLIC,,True,Tangible Media,False
kinetix,jifei,False,"<p>kinetiX is a transformable material featuring a design that resembles a cellular structure. It consists of rigid plates or rods and elastic hinges. These modular elements can be combined in a wide variety of ways and assembled into multifarious forms.</p><p>This project describes a group of auxetic-inspired material structures that can transform into various shapes upon compression. While the majority of the studies of auxetic materials focus on their mechanical properties and topological variations, our work proposes a parametric design approach that gives auxetic structures the ability to deform beyond shrinking or expanding. To do so, we see the auxetic structure as a parametric four-bar linkage. We developed four cellular-based material structure units composed of rigid plates and elastic/rotary hinges. Different compositions of these units lead to a variety of tunable shape-changing possibilities, such as uniform scaling, shearing, bending and rotating. By tessellating those transformations together, we can create various higher level transformations for design. The simulation is validated by the 3D printed structures.&nbsp;</p><p>&nbsp;We hope this work will inspire research in metamaterials design, shape-changing materials, and transformable architecture.</p>",,,2019-01-24 14:26:13.216,True,2017-03-01,kinetiX,PUBLIC,,True,Tangible Media,False
family-learning-coach,anneli,False,"<p>Developed as part of the Playful Words research project at the MIT Media Lab's Laboratory for Social Machines, Learning Loops aims to make literacy learning—both on and off the screen—a family experience. We create small-scale coach-family networks centered around children’s play on custom-built, open-ended literacy learning apps. Building Coach-family networks helps to empower children as authors and facilitate their narrative development. </p><p>StoryBlocks, a Learning Loops app, aims to promote literacy and social-emotional development through storytelling for children ages 6-10. StoryBlocks allows children to create and customize their own comic-style stories. These stories are analyzed using tools developed by the Learning Loops team to document children’s narrative development, and support Coaches as they provide personalized scaffolding for children’s narratives.&nbsp;</p><p><b><i>How does the Learning Loop work?</i></b></p><p>Data captured from a child’s use of StoryBlocks is streamed via the internet to cloud servers, and can immediately be accessed from a remote location by the child’s Coach. We have developed a Coach’s dashboard, called the Coach Console, powered by play analytics which enables a Coach to rapidly inspect play traces collected from a child’s activity and pull out their salient achievements, or meaningful moments. The Coaches then translate these moments into short personalized messages for the caregiver to inform them on their child’s narrative progress and provide suggestions for how to encourage new activities using StoryBlocks, together with background knowledge about their child’s path to literacy. Caregivers communicate with Coaches via text messages. Coaches can also help the children expand their sphere of learning and exploration by providing feedback on children's stories and suggesting new story starters&nbsp; directly to the child’s device that are based on trends in the child’s play data.</p><p><a href=""http://learningloops.org"">More info is available on the Learning Loops website.</a></p>",,,2019-04-11 16:00:08.728,True,2017-11-01,Learning Loops,PUBLIC,,True,Social Machines,False
playful-words,anneli,False,"<p>While there are a number of literacy technology solutions developed for individuals, the role of social—or networked—literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.</p><p>By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.</p><p>To learn more about this project, please check out:&nbsp;<a href=""http://playfulwords.org/"">http://playfulwords.org/</a></p>",,--Choose Location,2018-04-30 20:28:15.298,True,2014-09-01,Playful Words,PUBLIC,,True,Social Machines,False
play-analytics,anneli,False,"<p>Analyzing detailed data from SpeechBlocks&nbsp;to understand how kids engage with constructionist literacy learning technologies, with the goal of empowering caregivers (e.g. parents, older siblings, tutors) with these insights.</p>",,,2018-04-30 20:56:57.445,True,2016-02-01,Play Analytics,PUBLIC,,True,Social Machines,False
storyblocks,anneli,False,"<p><span style=""font-size: 18px; font-weight: 400;"">StoryBlocks aims to promote creative expression, literacy development, and social-emotional development through storytelling for children ages six to ten. In this app, children create personally generated, comic-style stories by inserting characters, setting emotions, typing dialogue, using words to insert images that customize scenes, and recording their voices to narrate their unique stories. With StoryBlocks, we can collect a corpus of children’s stories in order to build analysis tools that&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">can document children’s narrative development over time, and support coaches in providing personalized scaffolding for children’s narratives.&nbsp;</span></p>",,,2019-04-11 15:39:53.269,True,2016-07-01,StoryBlocks,PUBLIC,,True,Social Machines,False
family-learning-coach,bonnerd,False,"<p>Developed as part of the Playful Words research project at the MIT Media Lab's Laboratory for Social Machines, Learning Loops aims to make literacy learning—both on and off the screen—a family experience. We create small-scale coach-family networks centered around children’s play on custom-built, open-ended literacy learning apps. Building Coach-family networks helps to empower children as authors and facilitate their narrative development. </p><p>StoryBlocks, a Learning Loops app, aims to promote literacy and social-emotional development through storytelling for children ages 6-10. StoryBlocks allows children to create and customize their own comic-style stories. These stories are analyzed using tools developed by the Learning Loops team to document children’s narrative development, and support Coaches as they provide personalized scaffolding for children’s narratives.&nbsp;</p><p><b><i>How does the Learning Loop work?</i></b></p><p>Data captured from a child’s use of StoryBlocks is streamed via the internet to cloud servers, and can immediately be accessed from a remote location by the child’s Coach. We have developed a Coach’s dashboard, called the Coach Console, powered by play analytics which enables a Coach to rapidly inspect play traces collected from a child’s activity and pull out their salient achievements, or meaningful moments. The Coaches then translate these moments into short personalized messages for the caregiver to inform them on their child’s narrative progress and provide suggestions for how to encourage new activities using StoryBlocks, together with background knowledge about their child’s path to literacy. Caregivers communicate with Coaches via text messages. Coaches can also help the children expand their sphere of learning and exploration by providing feedback on children's stories and suggesting new story starters&nbsp; directly to the child’s device that are based on trends in the child’s play data.</p><p><a href=""http://learningloops.org"">More info is available on the Learning Loops website.</a></p>",,,2019-04-11 16:00:08.728,True,2017-11-01,Learning Loops,PUBLIC,,True,Other,False
family-learning-coach,snehapm,False,"<p>Developed as part of the Playful Words research project at the MIT Media Lab's Laboratory for Social Machines, Learning Loops aims to make literacy learning—both on and off the screen—a family experience. We create small-scale coach-family networks centered around children’s play on custom-built, open-ended literacy learning apps. Building Coach-family networks helps to empower children as authors and facilitate their narrative development. </p><p>StoryBlocks, a Learning Loops app, aims to promote literacy and social-emotional development through storytelling for children ages 6-10. StoryBlocks allows children to create and customize their own comic-style stories. These stories are analyzed using tools developed by the Learning Loops team to document children’s narrative development, and support Coaches as they provide personalized scaffolding for children’s narratives.&nbsp;</p><p><b><i>How does the Learning Loop work?</i></b></p><p>Data captured from a child’s use of StoryBlocks is streamed via the internet to cloud servers, and can immediately be accessed from a remote location by the child’s Coach. We have developed a Coach’s dashboard, called the Coach Console, powered by play analytics which enables a Coach to rapidly inspect play traces collected from a child’s activity and pull out their salient achievements, or meaningful moments. The Coaches then translate these moments into short personalized messages for the caregiver to inform them on their child’s narrative progress and provide suggestions for how to encourage new activities using StoryBlocks, together with background knowledge about their child’s path to literacy. Caregivers communicate with Coaches via text messages. Coaches can also help the children expand their sphere of learning and exploration by providing feedback on children's stories and suggesting new story starters&nbsp; directly to the child’s device that are based on trends in the child’s play data.</p><p><a href=""http://learningloops.org"">More info is available on the Learning Loops website.</a></p>",,,2019-04-11 16:00:08.728,True,2017-11-01,Learning Loops,PUBLIC,,True,Social Machines,False
playful-words,snehapm,False,"<p>While there are a number of literacy technology solutions developed for individuals, the role of social—or networked—literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.</p><p>By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.</p><p>To learn more about this project, please check out:&nbsp;<a href=""http://playfulwords.org/"">http://playfulwords.org/</a></p>",,--Choose Location,2018-04-30 20:28:15.298,True,2014-09-01,Playful Words,PUBLIC,,True,Social Machines,False
picture-blocks,snehapm,False,,,,2018-05-06 22:19:25.415,True,2017-02-01,Picture Blocks,LAB-INSIDERS,,True,Social Machines,False
speech-blocks,snehapm,False,"<p>SpeechBlocks is a medium that allows children (4-5 years old) to engage in open-ended play with writing. They can build arbitrary compositions out of words and associated images, which can become cards, signs, stories, and ""books."" We hypothesize that such creative, self-expressive play can foster development of basic literacy skills, like phonological awareness. However, because users of SpeechBlocks are not yet in command of writing, it is necessary for the system to scaffold and guide them. We study a variety of ways to accomplish this.<br></p>",,,2019-05-21 17:11:02.880,True,2015-03-01,SpeechBlocks,PUBLIC,,True,Social Machines,False
storyblocks,snehapm,False,"<p><span style=""font-size: 18px; font-weight: 400;"">StoryBlocks aims to promote creative expression, literacy development, and social-emotional development through storytelling for children ages six to ten. In this app, children create personally generated, comic-style stories by inserting characters, setting emotions, typing dialogue, using words to insert images that customize scenes, and recording their voices to narrate their unique stories. With StoryBlocks, we can collect a corpus of children’s stories in order to build analysis tools that&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">can document children’s narrative development over time, and support coaches in providing personalized scaffolding for children’s narratives.&nbsp;</span></p>",,,2019-04-11 15:39:53.269,True,2016-07-01,StoryBlocks,PUBLIC,,True,Social Machines,False
family-learning-coach,sballing,False,"<p>Developed as part of the Playful Words research project at the MIT Media Lab's Laboratory for Social Machines, Learning Loops aims to make literacy learning—both on and off the screen—a family experience. We create small-scale coach-family networks centered around children’s play on custom-built, open-ended literacy learning apps. Building Coach-family networks helps to empower children as authors and facilitate their narrative development. </p><p>StoryBlocks, a Learning Loops app, aims to promote literacy and social-emotional development through storytelling for children ages 6-10. StoryBlocks allows children to create and customize their own comic-style stories. These stories are analyzed using tools developed by the Learning Loops team to document children’s narrative development, and support Coaches as they provide personalized scaffolding for children’s narratives.&nbsp;</p><p><b><i>How does the Learning Loop work?</i></b></p><p>Data captured from a child’s use of StoryBlocks is streamed via the internet to cloud servers, and can immediately be accessed from a remote location by the child’s Coach. We have developed a Coach’s dashboard, called the Coach Console, powered by play analytics which enables a Coach to rapidly inspect play traces collected from a child’s activity and pull out their salient achievements, or meaningful moments. The Coaches then translate these moments into short personalized messages for the caregiver to inform them on their child’s narrative progress and provide suggestions for how to encourage new activities using StoryBlocks, together with background knowledge about their child’s path to literacy. Caregivers communicate with Coaches via text messages. Coaches can also help the children expand their sphere of learning and exploration by providing feedback on children's stories and suggesting new story starters&nbsp; directly to the child’s device that are based on trends in the child’s play data.</p><p><a href=""http://learningloops.org"">More info is available on the Learning Loops website.</a></p>",,,2019-04-11 16:00:08.728,True,2017-11-01,Learning Loops,PUBLIC,,True,Social Machines,False
storyblocks,sballing,False,"<p><span style=""font-size: 18px; font-weight: 400;"">StoryBlocks aims to promote creative expression, literacy development, and social-emotional development through storytelling for children ages six to ten. In this app, children create personally generated, comic-style stories by inserting characters, setting emotions, typing dialogue, using words to insert images that customize scenes, and recording their voices to narrate their unique stories. With StoryBlocks, we can collect a corpus of children’s stories in order to build analysis tools that&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">can document children’s narrative development over time, and support coaches in providing personalized scaffolding for children’s narratives.&nbsp;</span></p>",,,2019-04-11 15:39:53.269,True,2016-07-01,StoryBlocks,PUBLIC,,True,Social Machines,False
experiential-lighting-new-user-interfaces-for-lighting-control,maldrich,False,"<p>The vision of pervasive computing is now mainstream. These connected devices permeate every aspect of our lives. Yet, we remain tethered to arcane user interfaces. Unlike consumer devices, building appliances and utilities perpetuate this outdated vision. Lighting control is a prime example. Here, we show how a data-driven methodology—using people and sensors—enables an entirely new method of lighting control.<br></p><p>We are evaluating new methods of interacting and controlling solid-state lighting based on our findings of how participants experience and perceive architectural lighting in our new lighting laboratory (E14-548S). This work, aptly named ""Experiential Lighting,"" reduces the complexity of modern lighting controls (intensity/color/space) into a simple mapping, aided by both human input and sensor measurement. We believe our approach extends beyond general lighting control and is applicable in situations where human-based rankings and preference are critical requirements for control and actuation. We expect our foundational studies to guide future camera-based systems that will inevitably incorporate context in their operation (e.g., Google Glass).</p>",,--Choose Location,2019-04-19 14:25:05.542,True,2014-01-01,Experiential Lighting: New user interfaces for lighting control,PUBLIC,,True,Responsive Environments,False
experiential-lighting-new-user-interfaces-for-lighting-control,nanzhao,False,"<p>The vision of pervasive computing is now mainstream. These connected devices permeate every aspect of our lives. Yet, we remain tethered to arcane user interfaces. Unlike consumer devices, building appliances and utilities perpetuate this outdated vision. Lighting control is a prime example. Here, we show how a data-driven methodology—using people and sensors—enables an entirely new method of lighting control.<br></p><p>We are evaluating new methods of interacting and controlling solid-state lighting based on our findings of how participants experience and perceive architectural lighting in our new lighting laboratory (E14-548S). This work, aptly named ""Experiential Lighting,"" reduces the complexity of modern lighting controls (intensity/color/space) into a simple mapping, aided by both human input and sensor measurement. We believe our approach extends beyond general lighting control and is applicable in situations where human-based rankings and preference are critical requirements for control and actuation. We expect our foundational studies to guide future camera-based systems that will inevitably incorporate context in their operation (e.g., Google Glass).</p>",,--Choose Location,2019-04-19 14:25:05.542,True,2014-01-01,Experiential Lighting: New user interfaces for lighting control,PUBLIC,,True,Responsive Environments,False
halo-wearable-lighting,nanzhao,False,"<p>Imagine a future where lights are not fixed to the ceiling, but follow us wherever we are. In this colorful world we enjoy lighting that is designed to go along with the moment, the activity, our feelings, and our outfits. Halo is a wearable lighting device created to explore this scenario. Different from architectural lighting, this personal lighting device aims to illuminate and present its user. Halo changes the wearer's appearance with the ease of a button click, similar to adding a filter to a photograph. It can also change the user's view of the world, brightening up a rainy day or coloring a gray landscape. Halo can react to activities and adapt based on context. It is a responsive window between the wearer and his or her surroundings.</p>",,--Choose Location,2016-12-05 00:16:27.619,True,2014-01-01,Halo: Wearable Lighting,PUBLIC,,True,Responsive Environments,False
halo-flex,nanzhao,False,"<p>Halo Flex is the latest version of a series of wearable lighting devices that illuminates the face. It explores how light manipulates our facial qualities and visual perception. It brings together flexible circuits with wearable sensing to define a new form of dynamically controlled E-makeup.
                    
                </p><p>A wire bend circlet functions as the base. A flexible circuit board curves around the frame. Translucent solder mask, copper, and translucent dielectric material assemble a decorative pattern. Ten RGB LEDs are positioned on the bottom layer to achieve a variety of lighting effects. Lighting compositions can be set manually using a Bluetooth-enabled device, such as a smart phone, or automatically using the onboard motion sensor. Halo explores opportunities for wearable lighting.<br></p>",,,2017-04-05 18:41:05.488,True,2015-09-01,Halo Flex,LAB-INSIDERS,,True,Responsive Environments,False
mediated-atmospheres,nanzhao,False,"<h2>The Mediated Atmosphere project envisions a smart office that is capable of dynamically transforming itself to enhance occupants' work experience.</h2><p>In the knowledge economy, worker satisfaction is paramount to retention and productivity. Recent studies have identified a decline in workplace satisfaction. Our research demonstrates how Mediated Atmosphere address this growing need.&nbsp;We created a workspace prototype equipped with a modular real-time control infrastructure, integrating biosignal sensors, controllable lighting, projection, and sound.</p>",,,2018-07-16 12:52:08.873,True,2015-09-01,Mediated Atmosphere,PUBLIC,,True,Responsive Environments,False
computer-assisted-transgenesis,erikad,False,"<p>This is a new platform to automate experiments in genetic engineering and bring large-scale moonshot projects within reach. Too often, lab experiments are limited in scale by human fatigue and costs associated with manual labor. In particular, the process of delivering genetic materials via manual microinjection remains a long-standing bottleneck. We are developing a computer-assisted microinjection platform to streamline the production of transgenic organisms. Briefly, organisms are immobilized in a gel and microinjections are performed using precision robotics using computer vision algorithms. This platform demonstrated high-throughput gene editing in an animal model (C. elegans) for the first time. We will use this technology to refine and create safeguards for our gene drive technology.</p>",,--Choose Location,2016-12-05 00:17:09.401,True,2016-01-01,Computer-Assisted Transgenesis,PUBLIC,,True,Sculpting Evolution,False
engineering-microbial-ecosystems,erikad,False,"<p>We are developing methods of controlling the genetic and cellular composition of microbial communities in the gut. Stably colonized microbes could be engineered to sense disease, resist pathogen invasion, and release appropriate therapeutics in situ.</p><p><br></p>",,--Choose Location,2017-07-11 00:30:42.637,True,2016-01-01,Engineering Microbial Ecosystems,PUBLIC,,True,Sculpting Evolution,False
understanding-molecular-evolution,erikad,False,"<p>Humanity has harnessed evolution to sculpt domesticated animals, crops, and molecules, but the process remains a black box. Which combinations of evolutionary parameters will enable us to discover the best solutions? We plan to answer this question by performing massively parallel directed evolution experiments. Our system will use phage-assisted continuous evolution (PACE), a method of building synthetic ecosystems in which billions of fast-replicating viruses compete to optimize a molecular function of our choice. We are developing methods of running many experiments in parallel, each with real-time fitness monitoring and customized evolutionary conditions such as mutation rate, selection stringency, and evolutionary goal-switching. We will use these methods to systematically characterize the relationship between evolutionary parameters and outcomes. </p>",,--Choose Location,2017-03-23 17:11:16.348,True,2016-01-01,Understanding Molecular Evolution,PUBLIC,,True,Sculpting Evolution,False
spaceTouch,cherston,False,"<p>An astronaut's ability to leverage the sense of touch is substantially reduced when wearing a protective, pressurized spacesuit. We imagine reinstating the biological skin's sense of touch by mapping inputs on the protective suit's exterior layer to a haptic feedback system on the biological skin.&nbsp;</p><p>This concept is an illustrative application area for the <a href=""https://www.media.mit.edu/projects/SpaceSkin/overview/""><b>SpaceSkin</b></a>&nbsp;project, showcasing how aerospace-grade electronic textiles&nbsp; might improve astronaut situational awareness, as well as strengthen a sense of connection to fellow explorers.&nbsp;</p>",,,2019-05-28 18:04:56.501,True,2019-04-01,SpaceTouch,PUBLIC,,True,Responsive Environments,False
Grappler,cherston,False,"<p>Can a modified snap bracelet be used to land infrastructure on an asteroid?&nbsp;</p><p>It is notoriously difficult to stick a landing on a low gravity body, particularly if locomotion across the body is desired.&nbsp;We have been studying the use of arrays of bistable pinching elements for grappling onto the unpredictable contours of asteroids and other distant low gravity bodies. Each pinching element is mechanically actuated via an impact force, much like a snap bracelet. By coupling together arrays of such elements, we seek to demonstrate that the chain&nbsp; can conform with added precision to the topological structure of the body, as well as grapple more effectively.&nbsp;</p><p>This mechanism can ultimately be used to land large-scale structures like nets and tethers across the body, which then serve as infrastructure for crawling distributed sensors or sensory membranes, among other possibilities.&nbsp;</p><p>Additionally, we completed a study on a candidate low cost spectral imager payload for determining iron content in rock samples.&nbsp;</p><p>A&nbsp;<a href=""https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=3738&amp;context=smallsat"">concept paper</a>&nbsp;on the broader work was published by the 31st Annual AIAA/USU Conference on Small Satellites.&nbsp; A prototype was deployed on a microgravity flight and results summarized at <a href=""https://arc.aiaa.org/doi/pdf/10.2514/6.2019-0871"">AIAA Scitech 2019</a>.&nbsp;</p><p>There may also be compelling uses for the technology on Earth for adhering sensors to&nbsp; terrain that is erratic and difficult to access, like the roof of a cave, or structures at the bottom of the sea floor.&nbsp;</p><p>Two prototypes—one equipped with sensors—were tested on a microgravity flight by throwing them at a rocky target object. Data from the flight will be used to characterize the behavior of&nbsp; chains of one vs three bistable elements in order to inform future design decisions.&nbsp;</p>",,,2019-06-05 16:39:54.329,True,2017-09-01,Grappler:  Arrays of bistable elements for landing distributed sensor networks on low gravity bodies,PUBLIC,,True,Responsive Environments,False
quantizer-sonification-platform-for-high-energy-physics-data,cherston,False,"<p>Inspired by previous work in the field of data sonification, we built a data-driven composition platform that enables users to map collision event information from experiments in high-energy physics to audio properties, and thus make music from real-time data. The tool is used for outreach purposes, allowing physicists and composers to interact with collision data through novel interfaces. Three real-time compositions were streamed from May 2016–July 2016. Two additional compositions are streamed in fall 2018. This project can inspire the development of strategic mappings that facilitate the auditory perception of hidden regularities in high-dimensional datasets, and one day evolve into a useful analysis tool for physicists as well, possibly for the purpose of monitoring slow control data in experiment control rooms. The project is accessible at <a href=""http://quantizer.media.mit.edu"">Quantizer.media.mit.edu.</a></p>",,--Choose Location,2018-10-09 20:36:43.077,True,2015-01-01,Quantizer: Sonification Platform for High-Energy Physics Data,PUBLIC,http://www.quantizer.media.mit.edu,True,Responsive Environments,False
SpaceSkin,cherston,False,"<p>The outermost skin of a space-based structure is designed using materials known to protect against the harsh elements of space. Simultaneously, the skin provides a unique opportunity to characterize the environment proximate to a spacecraft and to perform real-time damage detection. Thus, we propose developing an aerospace-grade fabric that simultaneously senses and protects, emulating the dual protective and sensory capabilities of biological skin.&nbsp;Aerospace-grade sensory skins will serve a key role in&nbsp;next generation haptic feedback systems for spacesuits (see<a href=""https://www.media.mit.edu/projects/spaceTouch/overview/""> SpaceTouch application area</a>), as well as next generation thermal blankets for distributed detection of high velocity debris impact.&nbsp;</p><p>For example, Beta Cloth—the outermost layer of the International Space Station—is particularly resilient to atomic oxygen erosion and extended UV radiation exposure. It is also regularly exposed to high velocity debris impact. We draw from recent advances in functional fibers and electronic textiles in order to weave and coat sensors directly into the teflon-coated fiberglass that comprises Beta Cloth, enabling the skin to detect and characterize impact events. We seek to demonstrate that the well-characterized, protective properties of aerospace-grade woven materials can be preserved even when modified to include sensory functionality. </p><p>Our work begins by examining integrated piezoelectric yarns and piezoelectric ink coatings for vibration sensing. We will also consider state-of-the-art manufacturing methods (e.g., device-in-fiber technology [1]) and intriguing high velocity impact sensing modalities; e.g., detection of impact plasma RF emission [2]). Other possibilities include the use of free-flying external optical sensors that collaborate with the skin to assess damage (""Skin and Eyes""), as well as work towards making the sensory skin robust to cutting, sewing, and wrapping.&nbsp; &nbsp;&nbsp;</p><p>[1] Tao, Guangming, et al. ""Multimaterial fibers."" Lab-on-Fiber Technology. Springer, Cham, 2015. 1-26.</p><p>[2] Lee, N., et al. ""Measurements of freely-expanding plasma from hypervelocity impacts."" International Journal of Impact Engineering 44 (2012): 40-49.</p>",,,2019-04-18 14:52:40.026,True,2018-05-01,SpaceSkin: Aerospace-grade electronic textiles for distributed sensing on persistent orbital structures,PUBLIC,,True,Responsive Environments,False
time-of-flight-microwave-camera,michaf,False,"<p>Our architecture takes a hybrid approach to microwaves and treats them like waves of light. Most other work places antennas in a 2D arrangement to directly sample the RF reflections that return. Instead of placing antennas in a 2D arrangment, we use a single, passive, parabolic reflector (dish) as a lens. Think of every point on that dish as an antenna with a fixed phase-offset. This means that the lens acts as a fixed set of 2D antennas which are very dense and spaced across a large aperture. We then sample the focal-plane of that lens. This architecture makes it possible for us to capture higher resolution images at a lower cost.</p>",,--Choose Location,2016-12-05 00:16:55.816,True,2014-09-01,Time-of-Flight Microwave Camera,PUBLIC,,True,Camera Culture,False
ultrasound-tomography,michaf,False,"<p>Traditional medical ultrasound assumes that we are imaging ideal liquids. We are interested in imaging muscle and bone as well as measuring elastic properties of tissues, all of which are places where this assumption fails quite miserably. Interested in cancer detections, Duchenne muscular dystrophy, and prosthetic fitting, we use tomographic techniques as well as ideas from seismic imaging to deal with these issues.</p>",,--Choose Location,2016-12-05 00:17:24.963,True,2014-01-01,Ultrasound Tomography,PUBLIC,,True,Camera Culture,False
robochain-a-secure-data-sharing-framework-for-human-robot-interaction,orudovic,False,"<p> A learning framework for secure, decentralized, computationally efficient data and model sharing among multiple robot units installed at multiple sites.<br></p><p>Robots have potential to revolutionize the way we interact with the world around us. One of their greatest potentials is in the domain of mobile health, where they can be used to facilitate clinical interventions. However, to accomplish this, robots need to have access to our private data in order to learn from these data and improve their interaction capabilities. To enhance this learning process, knowledge sharing among multiple robot units is the natural step forward. However, to date, there is no well-established framework which allows for such data sharing while preserving the privacy of the users, such as hospital patients. To this end, we introduce RoboChain: the first learning framework for secure, decentralized, computationally efficient data and model sharing among multiple robot units installed at multiple sites such as hospitals. RoboChain builds upon and combines the latest advances in open data access, blockchain technologies, and machine learning. We illustrate this framework using the example of a clinical intervention conducted in a private network of hospitals. Specifically, we lay down the system architecture that allows multiple robot units, conducting the interventions at different hospitals, to perform efficient learning without compromising the data privacy.&nbsp;&nbsp;</p>",,,2019-04-19 14:53:57.705,True,2018-03-01,RoboChain: A secure data-sharing framework for human-robot interaction,PUBLIC,http://www.eduardocastello.com,True,Affective Computing,False
personalized-machine-learning-for-future-health,orudovic,False,"<p>The view on Alzheimer’s Disease (AD) diagnosis has shifted towards a more dynamic process in which clinical and pathological markers evolve gradually before diagnostic criteria are met. Given the&nbsp;wide variability in data available per subject, inherent per-person differences, and the slowly changing nature of the disease, accurate prediction of AD progression is a significant, difficult challenge. The goal of this project is to devise novel Personalized Machine Learning Models that can accurately capture future changes in the key biomarkers and cognitive scores related to AD and other neurological conditions. As the basis for our framework, we use the&nbsp;Alzheimer’s Disease Neuroimaging Initiative (ADNI)&nbsp;dataset–the largest publicly available dataset&nbsp; for AD research.&nbsp;&nbsp;These data are highly heterogeneous and multi-modal, and include imaging (MRI, PET), cognitive scores, CSF biomarkers, genetics, and demographics (e.g. age, gender, race). The developed models are the break-through in machine learning for health-care as they allow&nbsp; personalized forecasting of the diseases' progression - in contrast to the traditional ""one-size-fits-all"" approaches. This capability is of&nbsp; great importance to both clinicians and those at risk of AD since it is critical to early identification of at-risk subjects, construction of informative clinical trials, and timely detection of AD.</p>",,,2019-02-14 19:49:51.362,True,2017-11-01,Personalized Machine Learning for Future Health,PUBLIC,,True,Affective Computing,False
engageme,orudovic,False,"<h2>EngageME: Personalized machine learning and humanoid robots for measuring affect and engagement of children with autism</h2><p>EngageME is a project aimed at building a new technology to enable automatic monitoring of affect and engagement of children with ASC (Autism Spectrum Conditions) in communication-centered activities.</p><p>This <a href=""https://www.media.mit.edu/publications/personalized-machine-learning-for-robot-perception-of-affect-and-engagement-in-autism-therapy/"">work</a> has been published in&nbsp;<a href=""http://robotics.sciencemag.org/content/3/19/eaao6760"">Science Robotics</a>, June 2018.</p>",,,2019-02-14 19:50:51.284,True,2016-10-01,Personalized Machine Learning for Autism Therapy,PUBLIC,,True,Affective Computing,False
committee-of-n,jhaas,False,<p>A game-based activity for developing an understanding of the history of American schooling.</p>,,--Choose Location,2018-06-20 19:48:21.060,True,2015-01-01,Committee of N,LAB,,True,Other,False
zero-g-ames,jhaas,False,"<p>Games are a uniquely human endeavour, reducing stress and supporting mental well-being. Astronauts aboard the ISS have created their own games using materials at hand and pure creativity.  What if we could create games for them that took particular advantage of aspects of space, such as micro-gravity, and would help keep astronauts mentally engaged, socially connected, and physically relaxed?&nbsp; </p><p>Zero-G-ames is an ongoing series of workshops&nbsp;to explore, discuss, and design the history and future of games in constrained spaces and microgravity environments.</p><p>Let’s play!</p>",,,2018-06-25 17:52:03.983,True,2018-03-10,Zero-G-ames,PUBLIC,,True,Other,False
gobo,ahope,False,"<h1>Your social media. Your rules.</h1><p><a href=""http://gobo.social"">Gobo</a> is an experiment, not a startup. We’re building it to change the conversation on social media and imagine a better version of it. This is a technology-to-think-with—a tool we want you to play with and push against. Gobo is being built by a small team at <a href=""https://www.media.mit.edu/groups/civic-media/overview/"">MIT Media Lab's Center for Civic Media</a>, where we work on technologies for social change.</p><p>For questions, feedback, and musings, you can reach the Gobo team at <a href=""mailto:gobo@media.mit.edu"">gobo@media.mit.edu</a>.</p><h2>Control your own feed</h2><p>Social media companies use algorithms to control what we see on our feeds, but we don’t know how these algorithms work. &nbsp;As a result, we’re often unaware why certain posts show up in our feed while others don’t. Gobo allows you to control the algorithms, or a set of “rules,” so you can decide what gets shown on your feed and know why.</p><h2>Connect multiple platforms</h2><p>We believe that multiple social media platforms should exist to serve different purposes. However, it’s not easy to keep up with all these platforms, especially when your data can’t be easily shared between them. Gobo allows you to connect up to three platforms, so you can view all of your feeds in one place. </p><h2>See what gets hidden</h2><p>We believe that transparency can help you better understand what you see on social media and keep platforms accountable for algorithmic bias. Gobo tells you why certain posts are hidden based on the rules you set. It also shows you how many posts are hidden, so you can understand the overall impact of the rules you set.</p><h2>Expand your perspective</h2><p>Social media companies make assumptions about what we want to see based on what we read and click on. They tend to show us content we’re already engaging with, reinforcing our echo chambers. Instead of assuming what you want to see, Gobo allows you to add unfamiliar perspectives into your feed, so you can better understand the range of opinions that are shared online.</p>",,,2019-04-10 14:21:45.965,True,2017-09-01,Gobo,PUBLIC,https://gobo.social,True,Civic Media,False
fold,ahope,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Some readers require greater context to understand complex stories.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">FOLD (</span><a href=""http://fold.cm"" style=""font-size: 18px; font-weight: normal;"">fold.cm</a><span style=""font-size: 18px; font-weight: normal;"">) is an open publishing platform with a unique structure that lets writers link media cards to the text of their stories.&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">Media cards can contain videos, maps, tweets, music, interactive visualizations, and more.&nbsp;</span></p><p>FOLD is used by journalists, educators, and storytellers around the world.&nbsp;<br></p>",,--Choose Location,2016-12-15 02:27:31.751,True,2014-01-01,FOLD,PUBLIC,,True,Civic Media,False
my-deepsea-mybackyard,ahope,False,"<p>Seventy percent of nations have deep-sea environments within their maritime Exclusive Economic Zones (EEZs), yet only 16 percent of them are able to explore those environments. This is especially true for less economically developed countries. The dearth of technological capability and knowledge leads to a lack of exploration, inappropriate or inadequate management decisions, and unaware populations. Our goal is to empower countries to explore their own deep-sea backyards using low-cost technology, while building lasting in-country capacity. </p><p>Our project takes place in two small island developing states—the Republic of Kiribati, and Trinidad and Tobago. It utilizes Deep-Sea Drop Cameras developed by National Geographic’s Exploration Technology team (ExTech) and OpenROV’s Trident Remotely Operated Vehicles. Both technologies collect compelling imagery, but require minimal resources and expertise. In our pilot study during summer 2018, an engineer from ExTech and another team member traveled to each country to train a group of scientists, students, and communicators in the use of these technologies, which are to be left in-country until a scientist, a student, and a communicator from each country travel to the USA for further training in data analysis and creating outreach materials.</p>",,,2018-10-22 17:59:56.835,True,2018-04-02,"My Deep Sea, My Backyard",PUBLIC,,True,Civic Media,False
make-the-breast-pump-not-suck-hackathon-2018,ahope,False,"<h1><strong>A Case for Breastfeeding Innovation</strong></h1><h2><strong>Breastfeeding saves lives.</strong></h2><p>If women globally were able to meet the WHO's public health goal to exclusively breastfeed for the first six months, we would prevent 823,000 infant deaths. For every 597 women who optimally breastfeed, one maternal or child death is prevented.</p><h2><strong>Breastfeeding promotes long-term wellness for mother and baby.</strong></h2><p>Breastfeeding protects against child infections and malocclusion, increases intelligence, and reduces the risk of obesity and diabetes for children¹. Breastfeeding decreases mothers' risk of breast cancer and optimal breastfeeding would lead to 20,000 fewer cases every year¹. It may also protect against ovarian cancer and diabetes¹. Women who are supported to successfully establish breastfeeding in early months have a lower risk for postpartum depression.</p><p><strong>Breastfeeding saves healthcare costs.</strong></p><p>If women in the US were able to meet the WHO's public health goal to exclusively breastfeed for the first six months, we would save $17.2 billion dollars in annual costs treating preventable events, including infant and maternal deaths, SIDS, ear infections and necrotizing enterocolitis in babies, and heart attacks, diabetes and breast cancer in mothers.</p><h2><strong>Work environments and policies in the US are hostile to breastfeeding.</strong></h2><p>The US is one of only three nations worldwide without paid parental leave. The other countries in this club are Papua New Guinea and Lesotho⁴. Women's return to work outside the home is the leading factor for early weaning⁵. Most US work environments do not provide material or policy-based support for breastfeeding women, including parental leave, flexible schedules, on-site daycare, breaks and spaces for nursing and pumping.</p><h2>The Hackathon</h2><p><a href=""https://www.makethebreastpumpnotsuck.com/team"">Our team</a>&nbsp;is thrilled to produce a weekend with the leading innovators in breastfeeding and postpartum health, along with many mamas, papas, babies, students, and newcomers. This time around we have a focus on equity and inclusive innovation in breastfeeding. We want to catalyze the development of tech, products, spaces, clothing, programs and services that have an eye on affordability and access as well as cultural diversity.</p><p>REFERENCES:</p><ol><li>Bartick, M. C., Schwarz, E. B., Green, B. D., Jegier, B. J., Reinhold, A. G., Colaizy, T. T., Stuebe, A. M. (2017). Suboptimal breastfeeding in the United States: Maternal and pediatric health outcomes and costs. Maternal &amp; Child Nutrition, 13(1), e12366.&nbsp;<a href=""http://doi.org/10.1111/mcn.12366"">http://doi.org/10.1111/mcn.12366</a></li><li>Watkins, S., Meltzer-Brody, S., Zolnoun, D., &amp; Stuebe, A. (2011). Early Breastfeeding Experiences and Postpartum Depression. Obstetrics &amp; Gynecology, 118(2, Part 1), 214–221.&nbsp;<a href=""http://doi.org/10.1097/AOG.0b013e3182260a2d"">http://doi.org/10.1097/AOG.0b013e3182260a2d</a></li><li>Breastfeeding in the 21st century: epidemiology, mechanisms, and lifelong effect. Victora, Cesar G et al. The Lancet, Volume 387, Issue 10017, 475 - 490.</li><li><a href=""https://en.wikipedia.org/wiki/Parental_leave"">https://en.wikipedia.org/wiki/Parental_leave</a></li><li>Why invest, and what it will take to improve breastfeeding practices? Rollins, Nigel C et al. The Lancet , Volume 387, Issue 10017, 491 - 504.</li></ol>",,,2019-04-19 19:02:17.039,True,2017-04-02,Make the Breast Pump Not Suck Hackathon 2018,PUBLIC,,True,Civic Media,False
gobo,dsjen,False,"<h1>Your social media. Your rules.</h1><p><a href=""http://gobo.social"">Gobo</a> is an experiment, not a startup. We’re building it to change the conversation on social media and imagine a better version of it. This is a technology-to-think-with—a tool we want you to play with and push against. Gobo is being built by a small team at <a href=""https://www.media.mit.edu/groups/civic-media/overview/"">MIT Media Lab's Center for Civic Media</a>, where we work on technologies for social change.</p><p>For questions, feedback, and musings, you can reach the Gobo team at <a href=""mailto:gobo@media.mit.edu"">gobo@media.mit.edu</a>.</p><h2>Control your own feed</h2><p>Social media companies use algorithms to control what we see on our feeds, but we don’t know how these algorithms work. &nbsp;As a result, we’re often unaware why certain posts show up in our feed while others don’t. Gobo allows you to control the algorithms, or a set of “rules,” so you can decide what gets shown on your feed and know why.</p><h2>Connect multiple platforms</h2><p>We believe that multiple social media platforms should exist to serve different purposes. However, it’s not easy to keep up with all these platforms, especially when your data can’t be easily shared between them. Gobo allows you to connect up to three platforms, so you can view all of your feeds in one place. </p><h2>See what gets hidden</h2><p>We believe that transparency can help you better understand what you see on social media and keep platforms accountable for algorithmic bias. Gobo tells you why certain posts are hidden based on the rules you set. It also shows you how many posts are hidden, so you can understand the overall impact of the rules you set.</p><h2>Expand your perspective</h2><p>Social media companies make assumptions about what we want to see based on what we read and click on. They tend to show us content we’re already engaging with, reinforcing our echo chambers. Instead of assuming what you want to see, Gobo allows you to add unfamiliar perspectives into your feed, so you can better understand the range of opinions that are shared online.</p>",,,2019-04-10 14:21:45.965,True,2017-09-01,Gobo,PUBLIC,https://gobo.social,True,Civic Media,False
ultrasound-prosthetic-interface-design,branger,False,"​<p>In the United States, there are an estimated 1.7 million people living with amputation, with that number expected to double by 2050. Complications of prosthetic leg use in persons with lower extremity amputation (LEA) include delayed wound healing, recurrent skin ulcerations, and pressure damage to soft tissues. This can result in limited mobility, which further contributes to conditions such as obesity, musculoskeletal pathologies (e.g., osteoarthritis, osteopenia, and osteoporosis), as well as cardiovascular disease. Traditionally, fabrication of prosthetic sockets remains a fundamentally artisanal process with limited input of quantitative data. Even with advances in computer-aided design and manufacturing (CAD/CAM), prosthetists often modify sockets using non-quantitative craft processes requiring substantial human hours and financial cost. The goal of this research is to develop and validate musculoskeletal ultrasound imaging techniques for creating predictive biomechanical models of residual limbs that will reduce the barrier for and cost of computer aided design (CAD)-driven prosthetic socket design in the US and in low-and middle-income countries.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>​",,,2019-04-17 19:24:01.508,True,2016-10-18,Ultrasound imaging for transtibial prosthetic interface design,PUBLIC,,True,Biomechatronics,False
mechanical-interfaces,branger,False,"<p>This research track focuses on the use of computational (and experimental) techniques to understand the biomechanical behavior of human tissue as well as the musculoskeletal system.&nbsp; This knowledge feeds into novel methods for computational modeling based design of biomechatronic devices which in turn aim to restore or improve the human body. These devices include prosthetic and orthotic devices, and exoskeletons. <br></p>",,,2019-04-26 19:03:38.901,True,2015-11-01,Computational Biomechanics,PUBLIC,,True,Biomechatronics,False
emotionally-intelligent-playback,hanelee,False,<p>Emotionally Intelligent Music Playback opens possibilities to various emotional trajectories through a piece of music. The listener can navigate through emotional territories via a touchscreen interface. The system transitions seamlessly to corresponding emotional interpretations extracted from various existing renditions of the same composition.</p>,,,2018-04-27 21:28:46.680,True,2017-09-01,Emotionally Intelligent Playback,PUBLIC,,True,Opera of the Future,False
freedom-simulator,hanelee,False,"<p><i>""The sense of freedom…entails not simply the absence of frustration but the absence of obstacles to possible choices and activities—absence of obstructions on roads along which a man can decide to walk.""&nbsp;</i>—Isaiah Berlin</p><p>Can we understand freedom as a subjective sensation?</p><p>The Freedom Simulator&nbsp;is a set of three experiences that aim to induce a feeling of freedom. These experiences question the manifestation and significance of freedom in our everyday lives.</p><p>Each experience is based on a modern political philosophy perspective on freedom:&nbsp;&nbsp;positive freedom, negative freedom, and freedom in light of ethical individualism. Various techniques such as spatial audio, motion tracking, and real-time video projection are utilized.</p>",,,2019-04-23 16:53:35.179,True,2018-10-01,Freedom Simulator,PUBLIC,,True,Opera of the Future,False
gamma-instrument,arieger,False,<p>The Gamma Instrument is a small-format interactive device hovering between the realms of musical instrument and medical instrument. A capacitive hand-following interface allows one to create&nbsp;abstract&nbsp;gamma sounds while surrounded by an orbit of gamma-frequency lights. Creating musical tones on the device could heighten gamma entrainment as it mimics higher-level cognition and gamma-band processing noted in musicians. This device is part of a larger exploration of 40 Hz frequencies and Alzheimer’s prevention/reduction within the context of the Aging Brain Initiative at MIT. The tabletop multi-sensory experience brings aspects of the Cognitarium to an interactive and portable platform.</p>,,,2019-05-13 02:44:22.899,True,2018-01-18,Gamma Instrument,PUBLIC,,True,Opera of the Future,False
chronosonogy-sonic-sensory-time-shifting,arieger,False,"<p><span style=""font-size: 18px; font-weight: normal;"">Chronosonogy was born through extending the neuroscience research of Teki Et Al, which reveals ""<i>Distinct Neural Substrates of Duration-Based and Beat-Based Auditory Timing</i>” and Fassnidge Et Al’s work examining ""</span><i style=""font-size: 18px; font-weight: normal;"">Visual Interference of Auditory Signal Detection.""&nbsp;</i><br></p><p><span style=""font-size: 18px; font-weight: normal;"">Our perception of time is impacted by combining factors of visual-auditory override and imaginary notes sensations. Chronosonogy is both an experience and a newly discovered time-shifting phenomenon that activates a neurological quirk situated in&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">fronto-temporal-parietal regions of our brains. &nbsp;</span></p>",,,2019-04-17 19:57:53.118,True,2017-04-04,Chronosonogy: Sonic sensory time shifting,PUBLIC,,True,Opera of the Future,False
cognitarium,arieger,False,"<p>Cognitarium 2.0&nbsp; is a planetarium for the mind. Enter a world of 40 Hz sounds, multicolored lights and mind-entraining frequencies. Cognitarium 2.0 combines early groundbreaking research in gamma frequencies, multi sensory music and cognition&nbsp;research, to leap into the future of cognitive health and cross-modal experiences. This edition of the&nbsp;Cognitarium is a collaboration&nbsp;combining&nbsp;creative cognition research by Alexandra Rieger, lighting physics&nbsp;and control&nbsp;from Ben Bloomberg, the gamma drones of David Su,&nbsp;and 40&nbsp;Hz sculpted sound as defined by Tod Machover.&nbsp;</p>",,,2018-05-02 16:58:17.658,True,2017-06-10,Cognitarium 2.0,PUBLIC,,True,Opera of the Future,False
sonic-murals,arieger,False,"<h2>Giving voice and information to objects and spaces around us</h2><p>Objects in our lives are usually either digital or not; mostly a wall is just a wall. The Sonic Murals project explores what happens when we blur those lines. Implementing touch capacitance and conductive pigments in an innovative way, any surface can become a sensor, a&nbsp;tool for data collection, or a musical instrument, as exhibited in this project. When interacting with touch or proximity sensors on a sonic mural, one can experience spacial exploration and sound creation on a multi-sensory level.</p>",,,2017-10-14 20:32:15.057,True,2017-01-01,Sonic Murals,PUBLIC,,True,Opera of the Future,False
gamma-sense,arieger,False,"<p>The Gamma SENSE (Sensory Engaging Nebulised Scent Experience) is a pioneering instrumental addition to the gamma instrument series. The device delivers multisensory gamma stimulation through auditory, visual, and olfactory channels.&nbsp;</p><blockquote>This medical/musical instrument is based upon our groundbreaking pilot studies revealing (even non-synesthetic) humans link scent to sonic pitches. Due to the steep fall-off rate once gamma stimulation concludes, this olfactory mechanism probes the possibility of slowing 40 Hz frequency attenuation in patients.&nbsp;</blockquote><p>As research continues, the Gamma SENSE device was developed to aid in the testing and identification of the olfactory perceptive triggers which recruit and sustain identified cognitive frequencies. In collaboration with the Aging Brain Alzheimer’s Initiative at MIT, the Gamma SENSE pilots a novel, testing form-factor with the goal of deployment in large-scale clinical trials. The Gamma SENSE instrument also features one of the first touchpad arrangements designed to support EEG testing via the low-motion/high output format. As motion creates unwanted noise artifacts during electroencephalograms, this instrument requires the subtlest of finger-motions to trigger dynamic musical expression. The Gamma SENSE propels myriad frontiers through expression, function, and design.<br></p>",,,2019-04-18 14:12:24.978,True,2019-01-02,Gamma SENSE: Sensory Engaging Nebulised Scent Experience,PUBLIC,,True,Opera of the Future,False
the-gamma-moon-musical-omnisensory-orbital-neuroinstrument,arieger,False,"<p>Introducing the&nbsp;newest edition of the gamma musical/medical instruments - the&nbsp;Gamma MOON (<b>M</b>usical&nbsp;<b>O</b>mnisensory&nbsp;<b>O</b>rbital&nbsp;<b>N</b>euroinstrument).&nbsp;This instrument features a capacitive interface which delivers multisensory gamma stimulation through audio, visual, haptic and tactile feedback. In collaboration with the Aging Brain Alzheimer’s Initiative at MIT, the Gamma MOON pilots a novel treatment form-factor with the goal of device deployment in large-scale clinical trials. Research reveals that gamma instrument interaction can strengthen cognitive function and sensory perception while increasing focus even in neurocognitively healthy individuals.&nbsp;Contact arrangement allows both patients and performers to create high-level musical abstractions as well as follow traditional notational melodies. Gamma MOON's heightened sensorial engagement recruits increased cognitive entrainment, multimodal expression and creative freedom.&nbsp;</p>",,,2018-10-20 15:22:00.051,True,2018-09-01,Gamma MOON: Musical Omnisensory Orbital Neuroinstrument,PUBLIC,,True,Opera of the Future,False
enlight,kubat,False,"<p>In physics education, virtual simulations have given us the ability to show and explain phenomena that are otherwise invisible to the naked eye. However, experiments with analog devices still play an important role. They allow us to verify theories and discover ideas through experiments that are not constrained by software. What if we could combine the best of both worlds? We achieve that by building our applications on a projected augmented reality system. By projecting onto physical objects, we can paint the phenomena that are invisible. With our system, we have built ""physical playgrounds"": simulations that are projected onto the physical world and that respond to detected objects in the space. Thus, we can draw virtual field lines on real magnets, track and provide history on the location of a pendulum, or even build circuits with both physical and virtual components.</p>",,--Choose Location,2016-12-05 00:16:23.642,True,2014-01-01,Enlight,PUBLIC,,True,Fluid Interfaces,False
enlight,linder,False,"<p>In physics education, virtual simulations have given us the ability to show and explain phenomena that are otherwise invisible to the naked eye. However, experiments with analog devices still play an important role. They allow us to verify theories and discover ideas through experiments that are not constrained by software. What if we could combine the best of both worlds? We achieve that by building our applications on a projected augmented reality system. By projecting onto physical objects, we can paint the phenomena that are invisible. With our system, we have built ""physical playgrounds"": simulations that are projected onto the physical world and that respond to detected objects in the space. Thus, we can draw virtual field lines on real magnets, track and provide history on the location of a pendulum, or even build circuits with both physical and virtual components.</p>",,--Choose Location,2016-12-05 00:16:23.642,True,2014-01-01,Enlight,PUBLIC,,True,Fluid Interfaces,False
an-eeg-and-motion-capture-based-expressive-music-interface-for-affective-neurofeedback,gleslie,False,"<p>This project examines how the expression granted by new musical interfaces can be harnessed to create positive changes in health and wellbeing. We are conducting experiments to measure EEG dynamics and physical movements performed by participants who are using software designed to invite physical and musical expression of the basic emotions. The present demonstration of this system incorporates an expressive gesture sonification system using a Leap Motion device, paired with an ambient music engine controlled by EEG-based affective indices. Our intention is to better understand affective engagement, by creating both a new musical interface to invite it, and a method to measure and monitor it. We are exploring the use of this device and protocol in therapeutic settings in which mood recognition and regulation are a primary goal.</p>",,--Choose Location,2019-04-19 14:56:44.273,True,2014-09-01,An EEG and motion-capture based expressive music interface for affective neurofeedback,PUBLIC,,True,Affective Computing,False
modulating-peripheral-and-cortical-arousal-using-a-musical-motor-response-task,gleslie,False,"<p>We are conducting EEG studies to identify the musical features and musical interaction patterns that universally impact measures of arousal. We hypothesize that we can induce states of high and low arousal using electrodermal activity (EDA) biofeedback, and that these states will produce correlated differences in concurrently recorded skin conductance and EEG data, establishing a connection between peripherally recorded physiological arousal and cortical arousal as revealed in EEG. We also hypothesize that manipulation of musical features of a computer-generated musical stimulus track will produce changes in peripheral and cortical arousal. These musical stimuli and programmed interactions may be incorporated into music technology therapy, designed to reduce arousal or increase learning capability by increasing attention. We aim to provide a framework for the neural basis of emotion-cognition integration of learning that may shed light on education and possible applications to improve learning by emotion regulation.</p>",,--Choose Location,2019-04-19 17:19:56.174,True,2015-09-01,Modulating peripheral and cortical arousal using a musical motor response task,PUBLIC,,True,Affective Computing,False
affective-response-to-haptic-signals,gleslie,False,"<p>This study attempts to examine humans' affective responses to superimposed sinusoidal signals. These signals can be perceived either through sound, in the case of electronically synthesized musical notes, or through vibro-tactile stimulation, in the case of vibrations produced by vibrotactile actuators. This study is concerned with the perception of superimposed vibrations, whereby two or more sinusoisal signals are perceived simultaneously, producing a perceptual impression that is substantially different than of each signal alone, owing to the interactions between perceived sinusoidal vibrations that give rise to a unified percept of a sinusoidal chord. The theory of interval affect was derived from systematic analyses of Indian, Chinese, Greek, and Arabic music theory and tradition, and proposes a universal organization of affective response to intervals organized using a multidimensional system. We hypothesize that this interval affect system is multi-modal and will transfer to the vibrotactile domain.</p>",,--Choose Location,2016-12-05 00:17:06.039,True,2015-09-01,Affective Response to Haptic Signals,PUBLIC,,True,Affective Computing,False
brainbeat,gleslie,False,"<p>Can we sonfiy calming breathing and passively influence a state of calm?</p><p>Deep breathing has been scientifically proven to affect the heart, brain, digestive system, and the immune system. We believe designing a technology to promote deep breathing can facilitate transition into a calm state.&nbsp;</p><p>Nowadays, many people are spending a significant amount of time listening to music while working or studying. This makes music a good means for providing auditory breathing cues. While it has been shown that liminal auditory cues can be effective in encouraging a healthy breathing pattern, we are examining the use of subliminal encouragement of &nbsp;breathing modulation using music. Auditory ambient feedback has long been studied and is proved to be effective. It has been explored in concert settings, interactive installations, and smartphone applications. However, our aim is to design an intervention that is unobtrusive and doesn't keep people from doing their primary work. </p><p>In order to find the best auditory feedback design, we have designed a controlled study comparing an interactive rhythmic ambient music track that responds to a user's current breathing patterns to a fixed rate music track whose speed of playback is pegged to a rate slightly below the user's natural resting breathing rate. A control condition with no music is also included. We will compare the resulting breathing patterns, heart rate, EEG signals, and self-reported measures to determine if the ambient music feedback has any effect on the user's state of mind and body. If successful, a musical system to subliminally encourage calming breathing patterns may be integrated into workplace environments, hospitals, and other places where it is necessary to promote less stressful and healthy environments.</p><p>Our preliminary results significant shift in multiple physiological measures that indicate a state of calmness.</p>",,,2019-04-23 04:44:31.353,True,2016-12-01,BrainBeat: Breath-based music therapy,PUBLIC,,True,Affective Computing,False
whose-lives-matter-in-the-news,ngyenes,False,"<p>Since the killing of Michael Brown, the Black Lives Matter movement has organized on social media to draw attention to the deaths of unarmed black people killed by US police. Have news organizations responded to this demand, and have we seen a significant change over time in reporting about those deaths?</p><p>In this analysis of deaths from January 2013 through June 2016, we show that an unarmed black person killed by US police received 10.5x the incidence rate of news articles after Michael Brown’s death than those killed before, but that the predicted number of articles is no longer significantly different from 2013 levels.</p>",,,2016-12-13 16:23:36.317,True,2015-07-01,Whose Lives Matter in the News?,PUBLIC,,True,Civic Media,True
cocoverse,erinhong,False,<h2>Real-time collaborative self-expression in virtual reality&nbsp;</h2>,,,2019-04-17 20:08:49.312,True,2016-10-03,CocoVerse: A playground for co-creation and communication in virtual reality,PUBLIC,,True,Fluid Interfaces,False
vr-physics-lab,erinhong,False,"<p>Room-scale virtual reality opens up exciting new possibilities for exploratory learning. Phenomena that otherwise cannot be experienced directly (e.g. subjects that are microscopic, remote, or dangerous) can be transformed into environments that are immersive, interactive and social. Electrostatic Playground is a VR physics lab where multiple users can explore and discover principles of electrostatics through experimentation.&nbsp;<span style=""font-size: 18px;"">It also concretizes abstract notions of electrostatics in the form of tangible, interactive objects. Users can learn by directly manipulating physics objects while receiving real-time feedback from the environment. We've incorporated the ability to record these interactions in order to provide a means of authoring content, reviewing one's notes, and teaching others. Electrostatic Playground is a multi-user lab where users can explore and discover principles in electrostatics.</span><br></p>",,,2019-04-17 20:09:56.460,True,2016-06-01,Electrostatic Playground: A multi-user virtual reality physics learning experience,PUBLIC,,True,Fluid Interfaces,False
pal,utkarshs,False,"<p>PAL (Personalized Active Learner) is a <i>wearable</i> system&nbsp;with on-device machine learning&nbsp;to help users with&nbsp;<i>real-time, personalized,</i> and <i>context-aware</i> <b>memory augmentation</b>,<b> language learning, </b>and<b> self-awareness</b>.</p>",,,2019-01-31 17:17:43.024,True,2018-06-01,PAL,PUBLIC,,True,Fluid Interfaces,False
microculture,yonatanc,False,"<p>Microculture gardens are a network of small-scale permaculture gardens that are aimed at reimagining our urban food systems, remediating our air supply, and making our streets more amenable to human-scale mobility. Microculture combines micro-gardening with the principles of permaculture, creatively occupying viable space throughout our communities for small-scale self-sustaining food forests. Micro-gardens have proven to be successful for the production of a broad range of species, including leafy vegetables, fruit, root vegetables, herbs, and more. Traditionally, container-based micro-gardens occupy approximately one meter of space or less and are made from found, up-cycled materials. Our innovations involve the combining of permaculture and micro-gardening principles, developing materials and designs that allow for modularity, mobility, easy replicability, placement in parking spots, and software that supports the placement, creation, and maintenance of these gardens.</p>",,--Choose Location,2016-12-05 00:16:36.992,True,2014-01-01,Microculture,PUBLIC,,True,Social Computing,False
computational-scope-and-sequence-for-a-montessori-learning-environment,yonatanc,False,"<p>As part of our motivation to expand the classic Montessori curriculum and to address contemporary proficiencies, we are working closely with Montessori experts and computer scientists to develop a scope and sequence for computational thinking that will contribute to the Montessori classroom. This curriculum outlines the key concepts behind computer science, along with the corresponding materials and their lessons.&nbsp;</p>",,--Choose Location,2016-12-05 00:16:46.370,True,2015-09-01,Computational Scope and Sequence for a Montessori Learning Environment,PUBLIC,,True,Social Computing,False
data-of-children-storytelling,jinjoo,False,<p>Collecting real world data to understand social interactions!</p>,,,2018-01-22 06:31:46.892,True,2017-05-01,Data of Children Storytelling,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,True
tega-a-new-robot-platform-for-long-term-interaction,jinjoo,False,"<p>Tega is a new robot platform designed to support long-term, in-home interactions with children, with applications in early-literacy education from vocabulary to storytelling.<br></p>",,--Choose Location,2018-02-02 12:45:47.861,True,2015-01-01,Tega: A New Social Robot Platform,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,True
artificial-listener-with-social-intelligence,jinjoo,False,<p>A social robot modifies its behavior to change what you think about it!</p>,,,2019-04-17 18:38:39.381,True,2017-05-01,Artificial listener with social intelligence,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,True
realtime-detection-of-social-cues,jinjoo,False,"<h2>Realtime detection of social cues in children’s voices</h2><p>In everyday conversation, people use what are known as backchannels to signal to someone that they are still listening, paying attention, and engaged. As listeners, we smile, nod, and say “uh-huh” to convey attentiveness, and we do this naturally with little thought. We give this feedback not randomly but at certain moments in the conversation because speakers give off social cues that signal upcoming backchanneling opportunities.</p>",,,2018-05-07 17:34:51.716,True,2017-03-01,Realtime Detection of Social Cues,PUBLIC,http://www.haewonpark.com/,True,Personal Robots,True
intentional-inference-of-emotions,jinjoo,False,<p>Emotion recognition modeled as a goal-directed process!</p>,,,2017-10-16 14:31:54.022,True,2016-10-18,Intentional Inference of Emotions,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,True
p2pstory,jinjoo,False,"<p>Understanding social-emotional behaviors in storytelling interactions plays a critical role in the development of interactive and educational technologies for children. A challenge when designing for such interactions using technologies like social robots, virtual agents, and tablets is understanding the social-emotional behaviors pertinent to the storytelling context—especially when emulating a natural peer-to-peer relationship between the child and the technology.&nbsp;&nbsp;We present P2PSTORY, a dataset of young children (5-6 years old) engaging in natural peer-to-peer storytelling interactions with fellow classmates.&nbsp;The dataset contains 58 recorded storytelling sessions along with a diverse set of behavioral annotations as well as developmental and demographic profiles of each child participant.&nbsp;</p><p>The CHI 2018 paper presenting this dataset can be found here:&nbsp;<br><b><a href=""https://www.media.mit.edu/publications/p2pstory-dataset-of-children-storytelling-and-listening-in-peer-to-peer-interactions/"">Nikhita Singh, Jin Joo Lee, Ishaan Grover, and Cynthia Breazeal (2018). P2PSTORY: Dataset of Children Storytelling and Listening in Peer-to-Peer Interactions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems.</a></b></p><br><p>See below for instructions on how to access the dataset.</p>",,,2019-06-10 09:19:51.212,True,2018-01-22,P2PSTORY: Dataset of children as storytellers and listeners in peer-to-peer interactions,PUBLIC,,True,Personal Robots,True
haptic-footprint,paul_str,False,"<p>We only perceive a tiny sliver of the world around us. We are constrained by what our senses can process. These senses evolved to react to what is immediately important for our survival. Our technological development has outstripped the pace at which our physical senses evolve. We do not have access to things such as the electromagnetic spectrum at 2.5 GHz, even though it is relevant to the day to day life of most of us. We are poor at perceiving things such as changes in the chemical composition of the air we breathe, even though it is critical for our long term survival as a species. &nbsp;</p><p>Can we augment a stroll through nature with sensory experiences usually outside the range of our perception? Haptic Footprints explore using vibrotactile rendering for this purpose.</p>",,,2018-01-08 22:47:19.225,True,2017-10-12,Haptic Footprint,PUBLIC,,True,Responsive Environments,True
Grappler,paul_str,False,"<p>Can a modified snap bracelet be used to land infrastructure on an asteroid?&nbsp;</p><p>It is notoriously difficult to stick a landing on a low gravity body, particularly if locomotion across the body is desired.&nbsp;We have been studying the use of arrays of bistable pinching elements for grappling onto the unpredictable contours of asteroids and other distant low gravity bodies. Each pinching element is mechanically actuated via an impact force, much like a snap bracelet. By coupling together arrays of such elements, we seek to demonstrate that the chain&nbsp; can conform with added precision to the topological structure of the body, as well as grapple more effectively.&nbsp;</p><p>This mechanism can ultimately be used to land large-scale structures like nets and tethers across the body, which then serve as infrastructure for crawling distributed sensors or sensory membranes, among other possibilities.&nbsp;</p><p>Additionally, we completed a study on a candidate low cost spectral imager payload for determining iron content in rock samples.&nbsp;</p><p>A&nbsp;<a href=""https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=3738&amp;context=smallsat"">concept paper</a>&nbsp;on the broader work was published by the 31st Annual AIAA/USU Conference on Small Satellites.&nbsp; A prototype was deployed on a microgravity flight and results summarized at <a href=""https://arc.aiaa.org/doi/pdf/10.2514/6.2019-0871"">AIAA Scitech 2019</a>.&nbsp;</p><p>There may also be compelling uses for the technology on Earth for adhering sensors to&nbsp; terrain that is erratic and difficult to access, like the roof of a cave, or structures at the bottom of the sea floor.&nbsp;</p><p>Two prototypes—one equipped with sensors—were tested on a microgravity flight by throwing them at a rocky target object. Data from the flight will be used to characterize the behavior of&nbsp; chains of one vs three bistable elements in order to inform future design decisions.&nbsp;</p>",,,2019-06-05 16:39:54.329,True,2017-09-01,Grappler:  Arrays of bistable elements for landing distributed sensor networks on low gravity bodies,PUBLIC,,True,Responsive Environments,True
narrative-visualization-for,zhangjia,False,"<p>The use of data-driven methods to examine dynamic spaces, relationships, and mechanisms within an urban environment frames the city as a complex system. This framing in turn casts residents as contributors and actors rather than passive subjects. This project uses narrative interactive visualization to integrate public data with urban planning concepts for communication to and by residents of a city.&nbsp;
                    
                </p>",,,2016-12-14 13:52:42.196,True,2016-09-01,Narrative Visualization for Distributed Urban Interventions,PUBLIC,,True,Civic Media,False
big-data-for-small-places,zhangjia,False,"<p>Big Data for Small Places is a quantitative study of the qualities that define our neighborhoods and our collective role in the production of local places over time. We are translating the potentials of big data from the scale of the city to the scale of the urban block, the scale at which we physically experience urban space, to gain a better understanding of the local patterns and social spaces that aggregate to form metropolitan identity. We hope that this study will improve our collective understanding of the urban environments we shape and the stories they generate, that it will allow us to more sensitively test and implement real change in our shared public realm and support the invisible narratives it generates.</p>",,--Choose Location,2016-12-05 00:16:14.672,True,2015-01-01,Big Data for Small Places,PUBLIC,,True,Civic Media,False
the-constant-atlas,zhangjia,False,<p>An interactive atlas of census data for direct consumption by individual citizens.</p>,,,2018-10-23 14:32:44.183,True,2017-01-01,The Constant Atlas,PUBLIC,https://constantatlas.github.io/,True,Civic Media,False
computational-scope-and-sequence-for-a-montessori-learning-environment,smithkim,False,"<p>As part of our motivation to expand the classic Montessori curriculum and to address contemporary proficiencies, we are working closely with Montessori experts and computer scientists to develop a scope and sequence for computational thinking that will contribute to the Montessori classroom. This curriculum outlines the key concepts behind computer science, along with the corresponding materials and their lessons.&nbsp;</p>",,--Choose Location,2016-12-05 00:16:46.370,True,2015-09-01,Computational Scope and Sequence for a Montessori Learning Environment,PUBLIC,,True,Social Computing,False
live-objects,vpanzica,False,"<p>A Live Object is a small device that can stream media content wirelessly to nearby mobile devices without an Internet connection. Live Objects are associated with real objects in the environment, such as an art piece in a museum, a statue in a public space, or a product in a store. Users exploring a space can discover nearby Live Objects and view content associated with them, as well as leave comments for future visitors. The mobile device retains a record of the media viewed (and links to additional content), while the objects can retain a record of who viewed them. Future extensions will look into making the system more social, exploring game applications such as media ""scavenger hunts"" built on top of the platform, and incorporating other types of media such as live and historical data from sensors associated with the objects.</p>",,--Choose Location,2016-12-14 13:53:33.699,True,2015-01-01,Live Objects,PUBLIC,,True,Object Based Media,False
live-objects,arata,False,"<p>A Live Object is a small device that can stream media content wirelessly to nearby mobile devices without an Internet connection. Live Objects are associated with real objects in the environment, such as an art piece in a museum, a statue in a public space, or a product in a store. Users exploring a space can discover nearby Live Objects and view content associated with them, as well as leave comments for future visitors. The mobile device retains a record of the media viewed (and links to additional content), while the objects can retain a record of who viewed them. Future extensions will look into making the system more social, exploring game applications such as media ""scavenger hunts"" built on top of the platform, and incorporating other types of media such as live and historical data from sensors associated with the objects.</p>",,--Choose Location,2016-12-14 13:53:33.699,True,2015-01-01,Live Objects,PUBLIC,,True,Object Based Media,True
inflatables,kakehi,False,"<p>Printflatables is a design and fabrication system for human-scale, functional and dynamic inflatable objects. The user begins with specifying an intended 3D model which is decomposed to two dimensional fabrication geometry. This forms the input for a numerically controlled contact iron that seals layers of thermoplastic fabric. </p><p>In this project, we showcase the system design in detail, the pneumatic primitives that this technique enables and merits of being able to make large, functional and dynamic pneumatic artifacts. We demonstrate the design output through multiple objects which could motivate fabrication of inflatable media and pressure-based interfaces.</p><p><a href=""http://tangible.media.mit.edu/project/printflatables/"">Project Website</a></p>",,,2019-04-17 19:31:11.620,True,2017-05-10,"Printflatables: Printing human-scale, functional, and dynamic inflatable objects",PUBLIC,http://tangible.media.mit.edu/project/printflatables/,True,Fluid Interfaces,False
organic-primitives,kakehi,False,"<p>A large portion of the chemical and biological processes underlying our everyday experience remains imperceptible to us. Be it the contents of rain, the ocean, or human tears, chemical codes mediate interactions between organic systems from the environment to our bodies and food. <br></p><p>As humans, we understand information mediated by our senses—through textures, symbols, odors, and tastes. In order to design for a wider array of sensory modalities in representing fluid-based information and enable user interaction with these systems, we have developed Organic Primitives. It is a new medium for transforming objects into information displays. Chemical input is converted into human senses through a set of color-, odor-, and form-changing materials. <br></p>",,,2017-10-26 00:15:41.352,True,2014-11-23,Organic Primitives,PUBLIC,http://www.OrganicPrimitives.com,True,Fluid Interfaces,False
jibo-research-platform,samuelsp,False,"<p>The Jibo Research Platform is an in-the-field deployable Social Robotics experimentation and data collection infrastructure. Built upon the world's first commercial social robot for the home, it extends Jibo's design, hardware, and data security for research purposes.</p>",,,2018-10-15 01:36:16.436,True,2018-08-01,Jibo Social Robotic Research Platform,PUBLIC,,True,Personal Robots,False
personalized-interaction-for-language-learning,samuelsp,False,"<p>The process by which children learn native languages is markedly different from the process of learning a second, or non-native, language. Children are typically immersed in their native languages. They receive input from the adults and other children surrounding them, based on immediate need and interaction, during every waking hour. &nbsp;</p><p>Second language learners are exposed to input from the new language in very different ways, most commonly in a classroom setting.&nbsp;The second language learner relies heavily on memory skills with sparse interaction, in contrast to the first language learner that can rely on environmental reinforcement and social interaction to learn words.&nbsp;</p><p>Social robots have the potential to drastically improve on this paradigm, making the second-language learning experience more like the experience of learning a native language by engaging the child in a rich, interactive exposure to the target language, especially aspects not typically covered by traditional technological solutions, such as prosody, fundamental phonetics, common linguistic structures, etc.</p><p>Our project explores how to design child-robot &nbsp;interactions that encourage child-driven language learning, that adapt and personalize each child’s learning experience. We incorporate game design and machine learning into the child-robot interaction design. The child and robot play through a suite of educational games together. Using real-time sensor data and gameplay features, the robot constructs a model of each child's learning and emotional trajectory, then uses these models to inform its own decision making during the game. Thus, the robot's behaviors become personalized to individual children based on their learning style, personality and knowledge/emotional states during gameplay.&nbsp;</p><p></p><p></p><p></p><p></p><p></p><p></p>",,,2018-05-03 20:56:50.720,True,2017-01-01,Personalized  Interaction for Language Learning,PUBLIC,,True,Personal Robots,False
collaborative-robot-storyteller,samuelsp,False,"<p>Could a social robot collaboratively exchange stories with children as a peer and help improve their linguistic and storytelling skills? Tega uses machine learning algorithms to learn actions that improve children's storytelling and keep them engaged. &nbsp;We are also interested in how Tega can personalize its interaction with each child over multiple encounters, because every child learns and engages differently.&nbsp;</p><p>In Spring 2017, Tega went to twelve preschool classrooms in the Greater Boston area for&nbsp;three months, pioneering the field of long-term human-robot interaction.&nbsp;Using Q-learning, a policy was trained to tell stories optimized for each child’s engagement and linguistic skill progression. Tega monitored children's affect signals and asked dialogic questions during storytelling to gauge their engagement. Tega also invited children to&nbsp;tell&nbsp;it stories, which Tega used to assess each child's linguistic skill development. Our results show robot's interaction policy indeed personalized to each child. At the end of the sessions, the policy significantly differed from one child to the other. Children who interacted and built relationships with a personalized robot showed higher engagement, learned and retained more vocabularies, and used more complex syntax structure in their speech compared to where they had started.</p>",,--Choose Location,2019-01-22 17:42:46.144,True,2015-09-01,Personalized Robot Storytelling Companion,PUBLIC,,True,Personal Robots,False
zappore,suryaj,False,"<p>Zappore is a low-cost ($10), portable electroporator that can be fabricated in any community or academic setting with readily available and common materials. It is a tool to explore and experiment the possibility of transformation for known and unknown strains of microbes.</p>",,,2019-04-24 20:16:00.368,True,2018-01-01,Zappore: A low-cost electroporator,LAB-INSIDERS,,True,Molecular Machines,False
co-culture,suryaj,False,"<p>We aim to create an easy-to-use, scalable, widely accessible method to allow for the co-culturing of multiple organisms through the use of water droplets stabilized by surfactants in oil. This methodology will be used to explore and study the effects of various strains of biota living in the human microbiome on each other.&nbsp;</p>",,,2019-04-24 18:45:39.803,True,2019-02-04,Co-Culture: Open source technique to study dynamics of the microbiome,LAB-INSIDERS,,True,Molecular Machines,False
nccu-the-creation-of-new-black-musical-instruments-and-new-black-musical-idioms,bdunning,False,"<p>NCCU: music is a strong and universally accepted aspect of black culture. It has given birth to the genres of blues, jazz, gospel, rap, house, disco, funk, soul, trap, ragga, ska, dub, grime, reggae, calypso, hip hop, r&amp;b, dubstep, soul, and neo-soul. We believe that there are new instruments and ideas to be discovered. We are beginning this paradigmatic journey at an innovation center we launched at North Carolina Central University.&nbsp;</p>",,,2018-05-08 12:02:11.010,True,2015-03-01,NCCU: The Design and Fabrication of New Black Musical Instruments,PUBLIC,,True,Other,False
icenters-the-design-of-a-methodology-to-encourage-a-new-generation-of-computer-scientists-inventors-and-innovators,bdunning,False,"<p>iCenters: We work with a select group of Historically Black Colleges and Universities (HBCUs) to encourage and support the development of iCenters (Innovation Centers). The model for the iCenters is greatly influenced by the South End Technology Center (SETC). SETC was the first community located Maker Space to spin out of the Media Lab’s Bits and Atoms Lab. It is helmed by MIT Professor Emeritus Mel King. iCenters emphasize experiential learning by making and doing. By learning to code and learning to make and do with fabrication tools, we capture and retain interest in technology and facilitate closure of the Technology gap.</p>",,,2017-04-04 21:48:44.601,True,2014-01-01,"iCenters : The Design of a Methodology to Encourage  a New Generation of Computer Scientists, Inventors, and Innovators",PUBLIC,,True,Other,False
l2d-d2l-high-school-students-develop-coding-and-fabrication-learning-activities-for-their-peers,bdunning,False,"<p><b>L2D:D2L:</b> Learn to Develop:Develop to Learn. Much of our work is with young people from underserved communities. To better understand the age group as end users, we added a youth cohort to our Content Development staff. Their responsibility is to learn the nuances of Content Development, recommend Best Practices, create Prototypes, and develop Peer Appropriate Learning Activities.</p>",,,2017-04-04 21:27:36.398,True,2016-10-17,L2D:D2L : High School Students Develop Coding and Fabrication Learning Activities For Their Peers,PUBLIC,,True,Other,False
spelman-the-design-of-a-two-way-television-studio-in-a-suitcase-broadcast-system-to-teach-fabrication-and-prototyping-remotely,bdunning,False,"<p><b style=""font-size: 18px;"">Spelman</b>: Spelman College is a distinguished Black Women’s college in Atlanta. Nationally, twenty-three percent (23%) of the Black Women in STEM graduate from Spelman College. With their Innovation Center at Spelman, we piloted a two-way Long Distance Learning course in coding, microcontrollers (MaKey MaKey and Arduino), and digital fabrication.&nbsp;Students received credit for the course. For the course, we authored and designed &nbsp;Experiential Learning Tutorials which&nbsp;we first taught to our Spelman teaching counterpart, Dr. Jerry Volcy. He heads Spelman’s Innovation Center. The final products of the course were fabricated interactive lamps, and &nbsp;fabricated digital musical instruments.<br></p><p>The course allowed us to test our Pilot design for a “Television Studio in a Suitcase” (TSS) broadcast system. TSS is an affordable, miniaturized and portable interactive broadcast system. It enables us to teach technology online from anywhere. It works. In the next series at Spelman, we will teach Robotics and Mechatronics.</p>",,,2017-10-11 17:49:30.834,True,2016-09-15,Spelman: The Design of a two-way ‘Television Studio in a Suitcase’ Broadcast System to Teach Fabrication and Prototyping Remotely,PUBLIC,,True,Other,False
co-culture,desireed,False,"<p>We aim to create an easy-to-use, scalable, widely accessible method to allow for the co-culturing of multiple organisms through the use of water droplets stabilized by surfactants in oil. This methodology will be used to explore and study the effects of various strains of biota living in the human microbiome on each other.&nbsp;</p>",,,2019-04-24 18:45:39.803,True,2019-02-04,Co-Culture: Open source technique to study dynamics of the microbiome,LAB-INSIDERS,,True,Other,False
atmopragmascope,elawson,False,"<p>We are surrounded by displays and technologies whose mechanisms are hidden from view. This work is an exploration of revealing the underlying mechanisms of not only how we generate and parse visual information but how it can be delivered.&nbsp; The design process and its outcomes are derivatives, in part, of the mechanics of the eye which guide our estimation of a “fair curve.”&nbsp; Working within the parameters of 19th-century tools and techniques, I adopted the perspective of research modalities relevant for a time in which the eye and low-level visual mechanisms for discerning thresholds, edges, and shapes were the dominant tools in the creation of experimentation. Modern design and engineering tools–while enabling increasingly complex and sophisticated research–can also obfuscate the fundaments of form and function.&nbsp; By minimizing the influence of technological aides, we give our vision in the act of creation, the articulation of this machine intended to give form to the fundamental algorithms inherent in early biological visual processing.</p><p>Process album:<br><a href=""https://photos.app.goo.gl/ybEyqeIWt0ugMXd73"">https://photos.app.goo.gl/ybEyqeIWt0ugMXd73</a><br></p>",,,2019-04-18 01:33:53.490,True,2018-01-27,Atmopragmascope,PUBLIC,,True,Object Based Media,False
programming-perception,elawson,False,"<p>Our sensory mechanisms can be thought of as networked input controls responsible for encoding the natural world before us.  Acting as go-between from environment to our ultimate perception of it, sensory organs themselves exhibit deep learning potentials that, if acted upon, could radically challenge the way we develop and experience future technologies and extant realities. If we consider neural plasticity as a form of temporary memory which can be written and rewritten, consciously or unconsciously, externally or internally, there is potential to biologically encode new sensing parameters directly onto receptive fields, re-scripting and filtering input data from natural scenes into new and unique outcomes in our perception.  By generating tailored stimuli specific to adaptation mechanisms, there is an exciting opportunity to create novel environments of controlled perceptual phenomenon, programming inherent biological functions to facilitate more dynamic, immersive environments.<br></p>",,,2016-12-05 00:16:27.941,True,2015-10-14,Programming Perception: Emergent Potentials in Perceptual Mechanisms,LAB-INSIDERS,,True,Object Based Media,False
synthetic-shape-motion,elawson,False,<p>Our visual realities are flooded with complex and robust natural scenes. New findings in curvature estimation illustrate where our expectation of reality can break down under very simple mechanisms.</p>,,,2018-10-23 19:51:45.892,True,2018-05-05,Synthetic Shape Motion,LAB-INSIDERS,,True,Object Based Media,False
biologically-encoded-augmented-reality,elawson,False,"<p>We drive faster than we can react, watch screens that are sharper than we can resolve, and are surrounded by architectural facades with greater depth and scale than we can perceive. When our focus is saturated by a demanding task, we can be blind to what is directly in front of us. What happens if we high-jack the biologically hard-coded mechanisms in the visual processing pipeline and present them with discrete, coded instructions to heighten and augment perceptual awareness toward a richer engagement in our everyday lives? Our visual mechanisms constitute a set of powerful, networked input controls responsible for parsing and encoding the natural world before us. If we can<br>interject parallel streams of information tailored to the subtle and sometimes preconscious mechanisms of vision, there is a potential to multiplex within our spatial and attentional bandwidth, to deliver more dynamic, content-rich environments.<br></p><p>The function and structure of early visual processes are well established, and technologies capable of delivering dynamic, tailored information exist; in this dissertation I propose to build systems capable of bridging the gap by translating functional building blocks of vision into real-world environments. These systems will serve as platforms to quantify and parameterize the potentials in new perceptual realities. The experiments will target visual mechanisms responsible for image motion, curve estimation, and perceived position shift to challenge the observer’s proprioception, sensations of shape and depth, and to create digital media environments which appear to expand beyond the physical limitations of the display.<br></p><p>From how we navigate visually unpredictable environments with multifaceted restriction on visual attention, to perceiving the made environment through perceptual skins, to arguing for a deeper look into the way we generate media and display technologies, this dissertation will explore real world application for unattended visual potentials.</p>",,,2019-04-23 01:11:54.681,True,2019-01-01,Biologically Encoded Augmented Reality,LAB-INSIDERS,,True,Object Based Media,False
evolutron,kfirs,False,"<p>Technological advances in the past decade have allowed us to take a close look at the proteomes of living organisms. As a result, more than 120,000 solved protein structures are readily available, and we are still on an exponential growth curve. By looking at the proteomes of current living organisms, we are essentially taking snapshots of the successful results in this evolutionary process of continuous adaptation to the environment. Could we process the information available to us from nature to design new proteins, without the need for millions of years of Darwinian evolution?</p><p>To answer this question, we are developing an integrated Deep Learning framework for the evolutionary analysis, search, and design of proteins, which we call Evolutron. Evolutron is based on a hierarchical decomposition of proteins into a set of functional motif embeddings. Two of our strongest motivations for this work are gene therapy and drug discovery. In both cases, protein analysis and design play a fundamental role in the implementation of safe and effective therapeutics.</p>",,--Choose Location,2019-04-17 19:37:56.781,True,2016-01-01,Evolutron: Deep Learning for Protein Design,PUBLIC,,True,Molecular Machines,False
deepppi,kfirs,False,"<p>Protein-protein interactions (PPIs) are an essential part of many biological pathways in living organisms. With use cases such as regulation of gene expression, enzymatic catalyzation, and muscle contraction, understanding PPIs is a critical step toward a better understanding of life itself. Moreover, aberrant human PPIs may lead to multiple diseases, such as Alzheimer's, Creutzfeldt–Jakob, and cancer. Despite the undisputed importance of PPIs, only a small portion of the human interactome is known. </p><p>The PPI mapping problem is composed of two subproblems: the Interaction Problem—identifying the two or more proteins involved in a particular interaction; and the Position Problem—recognizing the residues within the interacting proteins that are crucial for the interaction (also known as hot spots or interacting residues). Current experimental techniques for PPI mapping, like Yeast 2 Hybrid or Alanine scans, are limited in scale, tedious, and expensive, therefore establishing the need for a fast, efficient, and accurate computational system.</p><p>DeepPPI is a Deep Learning algorithm that uses known PPIs to identify reoccurring patterns in the human interactome. These underlying patterns can be used, in turn, to predict both the existence of a new interaction and the interacting residues within the relevant proteins. Through this project, we hope to answer a fundamental biological question: How does nature, via evolution, create new protein-protein interactions? Additionally, we believe that DeepPPI will serve as a large-scale computational alternative to Alanine Scans and other experimental methods, contributing to the study of diseases and development of new therapeutics.&nbsp;</p>",,,2017-09-05 00:43:44.403,True,2017-08-01,DeepPPI,PUBLIC,,True,Molecular Machines,False
affinity-tensorflow,kfirs,False,"<p>Affinity is a high-level machine learning API (Application Programming Interface) dedicated exclusively to molecular geometry. Affinity is written in TensorFlow; a small proportion of high-performance code is in low-level C++.  Depending on the application it can be configured as multi-CPU, multi-CPU single GPU, or multi-GPU system. Affinity has  its own web page at <a href=""http://affinity.mit.edu"">affinity.mit.edu </a><br></p>",,,2019-04-17 19:38:38.863,True,2017-08-01,Affinity: Deep Learning API for Molecular Geometry,PUBLIC,https://affinity.mit.edu,True,Molecular Machines,False
evolutron,karydis,False,"<p>Technological advances in the past decade have allowed us to take a close look at the proteomes of living organisms. As a result, more than 120,000 solved protein structures are readily available, and we are still on an exponential growth curve. By looking at the proteomes of current living organisms, we are essentially taking snapshots of the successful results in this evolutionary process of continuous adaptation to the environment. Could we process the information available to us from nature to design new proteins, without the need for millions of years of Darwinian evolution?</p><p>To answer this question, we are developing an integrated Deep Learning framework for the evolutionary analysis, search, and design of proteins, which we call Evolutron. Evolutron is based on a hierarchical decomposition of proteins into a set of functional motif embeddings. Two of our strongest motivations for this work are gene therapy and drug discovery. In both cases, protein analysis and design play a fundamental role in the implementation of safe and effective therapeutics.</p>",,--Choose Location,2019-04-17 19:37:56.781,True,2016-01-01,Evolutron: Deep Learning for Protein Design,PUBLIC,,True,Molecular Machines,False
global-cooperation,mrfrank,False,<h1>Measuring Cooperation at Scale</h1>,,,2018-01-10 16:18:38.348,True,2015-09-01,Global Cooperation,PUBLIC,,True,Scalable Cooperation,False
honest-crowds,mrfrank,False,"<p>The Honest Crowds project addresses shortcomings of traditional survey techniques in the modern information and big data age. Web survey platforms, such as Amazon's Mechanical Turk and CrowdFlower, bring together millions of surveys and millions of survey participants, which means paying a flat rate for each completed survey may lead to survey responses that lack desirable care and forethought. Rather than allowing survey takers to maximize their reward by completing as many surveys as possible, we demonstrate how strategic incentives can be used to actually reward information and honesty rather than just participation. The incentive structures that we propose provide scalable solutions for the new paradigm of survey and active data collection.</p>",,--Choose Location,2018-01-10 16:32:52.208,True,2015-09-01,Honest Crowds,PUBLIC,,True,Scalable Cooperation,False
towards-understanding-the-impact-of-ai-on-labor,mrfrank,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-03-22 16:27:19.712,False,2018-03-01,Towards Understanding the Impact of AI on Labor,PUBLIC,http://www.media.mit.edu/~mrfrank,True,Scalable Cooperation,False
the-science-of-ai-research,mrfrank,False,"<p>We must proactively tackle the economic, social, and societal implications that accompany the widespread deployment of AI technology. In service to this goal, examining the evolution of AI research itself could provide a valuable input into models of AI's impact (e.g., models of the future of work).&nbsp;</p>",,,2019-03-13 17:23:51.009,True,2018-05-01,The Science of AI Research,PUBLIC,,True,Scalable Cooperation,False
future-of-work-ai-automation-labor,mrfrank,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-04-03 14:42:46.705,True,2017-06-06,"AI, Automation, Labor, and Cities: How to map the future of work",PUBLIC,,True,Scalable Cooperation,False
honest-crowds,lorenzoc,False,"<p>The Honest Crowds project addresses shortcomings of traditional survey techniques in the modern information and big data age. Web survey platforms, such as Amazon's Mechanical Turk and CrowdFlower, bring together millions of surveys and millions of survey participants, which means paying a flat rate for each completed survey may lead to survey responses that lack desirable care and forethought. Rather than allowing survey takers to maximize their reward by completing as many surveys as possible, we demonstrate how strategic incentives can be used to actually reward information and honesty rather than just participation. The incentive structures that we propose provide scalable solutions for the new paradigm of survey and active data collection.</p>",,--Choose Location,2018-01-10 16:32:52.208,True,2015-09-01,Honest Crowds,PUBLIC,,True,Scalable Cooperation,False
cognitive-limits-of-social-networks,lorenzoc,False,"<p>There is a wide cultural belief in the power of the Internet and social media as enablers of collective intelligence. They help us spread information rapidly, and learn useful information from each other. But there are fundamental limits to the capabilities of those networks. Understanding these limits is essential to improving social media and allowing society to make the most of it.</p>",,--Choose Location,2017-03-22 21:10:42.720,True,2015-01-01,Cognitive Limits of Social Networks,PUBLIC,,True,Scalable Cooperation,False
traffic-andorra,doorleyr,False,"<h2>Data Fusion&nbsp;for Dynamic Traffic Prediction</h2><p>Traffic congestion has huge negative impacts on the productivity, health and personal lives of city dwellers. To manage this problem&nbsp;effectively, transportation engineers need to predict traffic congestion throughout the road network at all hours of the day.&nbsp;Prediction of traffic typically involves travel surveys that are expensive, time consuming and do not capture temporal variation in travel demand.&nbsp;However,&nbsp;anonymised&nbsp;location data from mobile phones present an alternative source of data which is passively collected, widely available and naturally captures temporal trends.&nbsp;On the other hand, these data contain other biases and so if we use these data for transportation models, we must take care to correct for these biases using more reliable data. As part of the City Science collaboration with Andorra, we used&nbsp;a&nbsp;Bayesian network to build a calibrated transportation model for the country based on&nbsp;geolocated telecoms data and validated using a small sample of traffic counts.</p>",,,2019-04-17 19:43:06.032,True,2017-01-01,Dynamic Traffic Prediction in Andorra: a Bayesian network approach,PUBLIC,,True,City Science,False
andorra-innovation,doorleyr,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>The MIT Media Lab's City Science research group, the University of Andorra, and national and international companies are collaborating in order to bring an innovative ecosystem into the capital of Andorra. This innovation district aims to engage local citizens, researchers, and R&amp;D from the companies in order to build together an Andorran living lab, an ""innovation district"" where national and international companies can test and deploy their products and ideas and cultivate human capital.</p><p><b>Current Projects</b></p><ul><li>Andorra Innovation Space</li><li>Andorra Cultural Heritage</li><li>Drones patterns and flows, collaboration living lab<br></li><li>Young Future</li></ul>",,,2018-07-09 18:49:41.844,True,2016-09-01,Andorra | Innovation,PUBLIC,,True,City Science,False
andorra-dynamic-urban-planning,doorleyr,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>",,,2019-02-25 15:17:09.063,True,2016-09-01,Andorra | Dynamic Urban Planning,PUBLIC,,True,City Science,False
Reversed-Urbanism,doorleyr,False,"<h2>Predicting Urban Performance through Behavioral Patterns in Temporal Telecom Data</h2><p>This study explores a novel method to analyze diverse behavioral patterns in large urban populations and to associate them with discrete urban features. This work utilizes machine learning and anonymized telecom data to understand which fragments of the city has greater potential to attract dense and diverse populations over longer periods of time. Finally, this work suggests a road map for building spatial prediction tools in an effort to improve city-design and planning processes.&nbsp;&nbsp;</p><p><b><a href=""https://cityscope.github.io/CS_Andorra_RNC/"">Click here for an interactive visualization of this study</a>&nbsp;<br><br></b></p><p><b>Advisors:</b>&nbsp;Kent Larson&nbsp;and&nbsp;Esteban Moro<br><b>Thanks to</b> Andorra Telecom, ActuaTech,&nbsp;Núria Macià. <br>Data was&nbsp;obtained by Andorra Telecom as part of MIT Media Lab City Science and the State of Andorra collaboration.&nbsp;</p>",,,2019-02-24 23:21:12.068,True,2017-07-01,Reversed Urbanism,PUBLIC,http://ArielNoyman.com,True,City Science,False
andorra-mobility,doorleyr,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile</a></p><p>With no airport or train service, most of the 8 million tourists who visit Andorra each year arrive by car, making traffic management and parking some of the country's most important challenges. We are currently developing different projects spanning from data science to the deployment of autonomous vehicles to help address these issues.<br></p>",,,2017-10-25 05:56:26.309,True,2016-09-01,Andorra | Mobility,PUBLIC,,True,City Science,False
mobcho,doorleyr,False,"<p>MoCho (short for ""Mobility Choices"") is a&nbsp;<a href=""http://cityscope.media.mit.edu/"">CityScope</a>&nbsp;module focused on mobility choices and societal impacts. This tool helps predict the choices of mobility modes made at the individual level throughout the entire Boston Metro area.</p><p><i style=""font-size: 18px; font-weight: 400;""><a href=""https://cityscope.media.mit.edu/CS_choiceModels/index.html"">Check out a live demo of MoCho predictions here</a>.&nbsp;</i></p>",,,2019-04-19 19:11:18.982,True,2018-06-01,MoCho: Mobility choices and societal impacts,PUBLIC,,True,City Science,False
city-science-lab-shanghai,doorleyr,False,"<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>",,,2019-05-16 20:43:38.677,True,2017-02-14,City Science Lab Shanghai,PUBLIC,,True,City Science,False
andorra-tourism,doorleyr,False,"<p><span style=""font-size: 18px; font-weight: 400;""></span><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/""><span style=""font-size: 18px; font-weight: 400;"">View the main City Science Andorra project profile.</span></a><br></p><p><span style=""font-size: 18px; font-weight: 400;"">With more than eight million visitors a year, tourism represents almost 30% of the economy of Andorra. By gathering and analyzing data from social media, call detail records, and wifi, we can understand the country's dynamics of tourism and commerce as well as design interventions that can improve the experience for tourists, encouraging them to visit Andorra more frequently, stay longer, and increase spending.&nbsp;</span><br></p><h2><b>Current Projects</b></h2><ul><li>Event Analysis<br></li><li>Social Network<br></li><li>Location Recommendation system<br></li></ul><p> </p><h2><b>EVENT ANALYSIS</b></h2><p>Based on the analysis of call detail records and social media, the goal of this project is to understand the tourist behaviors in Andorra.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">After mining those anonymized data, we have been able to learn different patterns and behaviors of the tourism in Andorra thanks to an agent-based model developed in order to represent the flow of people. This simulation is also coupled with an interactive table called CityMatrix.</span></p>",,,2019-02-25 15:33:28.936,True,2015-08-01,Andorra | Tourism,PUBLIC,,True,City Science,False
aalto-saas,doorleyr,False,"<h2>How can we get more value from the same buildings?&nbsp;</h2><p>Cities contain many different resources and spaces and typically, these resources operate as products with&nbsp; a single function and a single owner and/or renter. However, the owner's demand&nbsp;for space often varies daily or seasonally, meaning that many buildings tend to be underutilized and are often vacant or partially vacant for large portions of each day.&nbsp;</p><p>Meanwhile, the ""sharing economy"" has been one of the most significant economic shifts in the last 10 years, with companies like Uber and Airbnb experiencing explosive growth. Along these lines, Aalto University—a member of the MIT Media Lab City&nbsp;Science network—has developed the concept&nbsp;of City-as-a-Service, where building space and other resources are shared among institutions, businesses, and citizens in a community. Aalto has already begun experimenting with School-as-a-Service, as a prototype of City-as-a-Service on their campus in Espoo.</p>",,,2019-05-07 20:00:40.076,True,2017-07-01,Aalto Campus-as-a-Service Simulations,PUBLIC,,True,City Science,False
city-science-andorra,doorleyr,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
cityscope,doorleyr,False,"<p>City Science researchers are developing a slew of tangible and digital platforms dedicated to solving spatial design and urban planning challenges. The tools range from simulations that quantify the impact of disruptive interventions in cities to communicable collaboration applications. We develop and deploy these tools around the world and maintain open source repositories for the majority of deployments. ""CityScope"" is a concept for shared, interactive computation for urban planning.</p><p>All current CityScope development, tools, and software are open source <a href=""https://cityscope.github.io/"">here</a>.&nbsp; &nbsp; &nbsp;&nbsp;</p>",,,2019-05-16 20:42:57.474,True,2017-08-01,Theme | CityScope,PUBLIC,,True,City Science,False
cognitive-integration-the-nature-of-the-mind,esb,False,"<p>While we have learned much about human behavior and neurobiology, there is arguably no field that studies the mind itself. We want to overcome the fragmentation of the cognitive sciences. We aim to create models and concepts that bridge between methodologies, and can support theory-driven research. Among the most interesting questions: How do our minds construct the dynamic simulation environment that we subjectively inhabit, and how can this be realized in a neural substrate? How can neuronal representations be compositional? What determines the experiential qualities of cognitive processes? What makes us human?</p>",,--Choose Location,2016-12-05 00:17:08.864,True,2016-01-01,Cognitive Integration: The Nature of the Mind,PUBLIC,,True,Synthetic Neurobiology,False
transdermal-optogenetic-peripheral-nerve-stimulation,esb,False,"<p>
                    Optogenetic techniques have recently been applied to peripheral nerves as a scientific tool with the translatable goal of alleviating a variety of disorders, including chronic pain, muscle fatigue, glucose-related pathologies, and others.  When compared to the electrical stimulation of peripheral nerves, there are numerous advantages: the ability to target molecularly defined subtypes, access to opsins engendering neural inhibition, and optical recruitment of motor axons in a fashion that mimics natural recruitment, which eliminates the fatigue roadblock inherent to functional electrical stimulation. The ability to control peripheral nerves situated under deep tissue structures with transdermal, optical signals would be of enormous benefit, integrating all of the advantages conferred by optogenetics while averting the drawbacks associated with implantable devices, such as mechanical failure, device tissue heating, and a chronic foreign body response.&nbsp;</p><p>We work to develop novel molecular and optical methods in an effort to enable this transdermal optogenetic peripheral nerve control. A further example of a potential clinical application involves optogenetically targeting the vagus nerve, a peripheral cranial nerve implicated in numerous ailments, including epilepsy, migraines, obesity, hypertension, fibromyalgia, Crohn’s disease, asthma, depression, and obsessive-compulsive disorder.  An efficient method of stimulating the vagus nerve with minimal side-effects and high target specificity, such as described here, may have profound implications to the study of various illnesses and disabilities.</p>",,,2019-04-26 19:05:38.186,True,2015-01-01,Transdermal Optogenetic Peripheral Nerve Stimulation,PUBLIC,,True,Synthetic Neurobiology,False
8k-brain-tour,esb,False,"<p>We present an 8K (7680 x 4320 pixels) visualization system for terabyte-scale, three-dimensional microscopy images of a brain slice that can facilitate neuroscience research. High resolution, large format (85” or 188 cm x 106 cm) rendering allows the viewer to dive into the massive dataset of 700 billion voxels capturing thousands of neurons and to investigate nanoscale and macroscale structures of the neurons simultaneously.</p>",,,2019-04-17 18:31:38.208,True,2017-02-23,8K Brain Tour,PUBLIC,,True,Synthetic Neurobiology,False
implosion-fabrication-1,esb,False,"<h2>Shrinking problems in 3D printing</h2><p>Although a range of materials can now be fabricated using additive manufacturing techniques, these usually involve assembly of a series of stacked layers, which restricts three-dimensional (3D) geometry. Oran&nbsp;et al.&nbsp;developed a method to print a range of materials, including metals and semiconductors, inside a gel scaffold (see the Perspective by Long and Williams). When the hydrogels were dehydrated, they shrunk 10-fold, which pushed the feature sizes down to the nanoscale.</p><p>Lithographic nanofabrication is often limited to successive fabrication of two-dimensional (2D) layers. We present a strategy for the direct assembly of 3D nanomaterials consisting of metals, semiconductors, and biomolecules arranged in virtually any 3D geometry. We used hydrogels as scaffolds for volumetric deposition of materials at defined points in space. We then optically patterned these scaffolds in three dimensions, attached one or more functional materials, and then shrank and dehydrated them in a controlled way to achieve nanoscale feature sizes in a solid substrate. We demonstrate that our process, Implosion Fabrication (ImpFab), can directly write highly conductive, 3D silver nanostructures within an acrylic scaffold via volumetric silver deposition. Using ImpFab, we achieve resolutions in the tens of nanometers and complex, non–self-supporting 3D geometries of interest for optical metamaterials.</p>",,,2018-12-13 20:17:08.530,True,2018-01-02,Implosion Fabrication,PUBLIC,,True,Synthetic Neurobiology,False
cognitive-integration-the-nature-of-the-mind,amarbles,False,"<p>While we have learned much about human behavior and neurobiology, there is arguably no field that studies the mind itself. We want to overcome the fragmentation of the cognitive sciences. We aim to create models and concepts that bridge between methodologies, and can support theory-driven research. Among the most interesting questions: How do our minds construct the dynamic simulation environment that we subjectively inhabit, and how can this be realized in a neural substrate? How can neuronal representations be compositional? What determines the experiential qualities of cognitive processes? What makes us human?</p>",,--Choose Location,2016-12-05 00:17:08.864,True,2016-01-01,Cognitive Integration: The Nature of the Mind,PUBLIC,,True,Synthetic Neurobiology,True
scratch-data-blocks,sdg1,False,"<p>Scratch Community Blocks is an NSF-funded project that extends the Scratch programming language to enable youth to analyze and visualize their own learning and participation in the Scratch online community. With Scratch Community Blocks, youth in the Scratch community can easily access, analyze, and represent data about the ways they program, share, and discuss Scratch projects.</p>",,--Choose Location,2016-12-05 00:17:22.736,True,2014-01-01,Scratch Community Blocks,PUBLIC,,True,Lifelong Kindergarten,True
gamr,tekofsky,False,"<p>Does how you play reflect who you really are? The Media Lab and Tilburg University are bringing science into the game to figure out the connections between our play style and our cognitive traits. To do that, we are gathering data from League of Legends, World of Warcraft, and Battlefield 4, and Battlefield: Hardline players to gain insights across all the major online game genres (MOBA, MMORPG, and FPS). In return, every participant will get an in-depth GAMR profile that shows their personality, brain type, and gamer type.</p>",,--Choose Location,2016-12-05 00:16:25.916,True,2015-01-01,GAMR,PUBLIC,,True,Playful Systems,False
microworlds,otts,False,"<p>The MIT Scratch Team is exploring ways to make it easier for newcomers to get started creating with coding. We are designing ""microworlds""— customized versions of the Scratch editor that contain a small set of blocks for making projects based on a theme. </p><p>Microworlds offer a more creative entry point to coding. While many introductory coding experiences focus on engaging children in puzzles with one right answer, microworlds provide an open-ended experience, enabling children to explore, experiment, and create, while still providing a more simplified and scaffolded entry point into coding.</p><p>Each microworld includes subset of the Scratch programming blocks that are most relevant and useful for the particular interest area, along with specialized graphical assets related to the interest area. In addition to aligning with a particular interest area, each microworld highlights how coding can enable young people to create projects and express ideas with code. For example, by tinkering with the music microworld, young people can see how they can use code to make musical melodies and beats; by tinkering with the soccer microworld, young people can see how they can use coding to make objects move and start building their own game. </p><p>The project is part of the <a href=""http://scratch.mit.edu/info/codingforall"">Coding for All project</a>. The Coding for All project brings together an interdisciplinary research team from the MIT Media Lab, the Digital Media and Learning Hub at University of California Irvine, and Harvard University’s Berkman Center for Internet and Society to develop new online tools and activities to engage more young people in developing computational fluency, particularly youth from groups currently underrepresented in computing.&nbsp;</p>",,,2017-04-05 01:53:28.281,True,2015-09-01,Microworlds,PUBLIC,,True,Lifelong Kindergarten,False
microworlds,morant,False,"<p>The MIT Scratch Team is exploring ways to make it easier for newcomers to get started creating with coding. We are designing ""microworlds""— customized versions of the Scratch editor that contain a small set of blocks for making projects based on a theme. </p><p>Microworlds offer a more creative entry point to coding. While many introductory coding experiences focus on engaging children in puzzles with one right answer, microworlds provide an open-ended experience, enabling children to explore, experiment, and create, while still providing a more simplified and scaffolded entry point into coding.</p><p>Each microworld includes subset of the Scratch programming blocks that are most relevant and useful for the particular interest area, along with specialized graphical assets related to the interest area. In addition to aligning with a particular interest area, each microworld highlights how coding can enable young people to create projects and express ideas with code. For example, by tinkering with the music microworld, young people can see how they can use code to make musical melodies and beats; by tinkering with the soccer microworld, young people can see how they can use coding to make objects move and start building their own game. </p><p>The project is part of the <a href=""http://scratch.mit.edu/info/codingforall"">Coding for All project</a>. The Coding for All project brings together an interdisciplinary research team from the MIT Media Lab, the Digital Media and Learning Hub at University of California Irvine, and Harvard University’s Berkman Center for Internet and Society to develop new online tools and activities to engage more young people in developing computational fluency, particularly youth from groups currently underrepresented in computing.&nbsp;</p>",,,2017-04-05 01:53:28.281,True,2015-09-01,Microworlds,PUBLIC,,True,Lifelong Kindergarten,False
predicting-students-wellbeing-from-physiology-phone-mobility-and-behavioral-data,ehinosa,False,"<p>The goal of this project is to apply machine learning methods to model the wellbeing of MIT undergraduate students. Extensive data is obtained from the SNAPSHOT study, which monitors participating students on a 24/7 basis, collecting data on their location, sleep schedule, phone and SMS communications, academics, social networks, and even physiological markers like skin conductance, skin temperature, and acceleration.&nbsp;</p><p>We extract features from this data and apply a variety of machine learning algorithms, including Gaussian mixture models and Multi-task Multi-Kernel Learning; we are currently working to apply Bayesian hierarchical multi-task learning and Deep Learning as well.</p><p>Interesting findings include: when participants visit novel locations they tend to be happier; when they use their phones or stay indoors for long periods they tend to be unhappy; and when several dimensions of wellbeing (including stress, happiness, health, and energy) are learned together, classification accuracy improves. The biggest classification accuracy improvements come when we use multi-tasking algorithms to leverage group data while customizing a model for each participant.</p>",,--Choose Location,2019-04-19 17:23:56.302,True,2015-01-01,"Predicting students' wellbeing from physiology, phone, mobility, and behavioral data",PUBLIC,,True,Affective Computing,False
predicting-students-wellbeing-from-physiology-phone-mobility-and-behavioral-data,asma_gh,False,"<p>The goal of this project is to apply machine learning methods to model the wellbeing of MIT undergraduate students. Extensive data is obtained from the SNAPSHOT study, which monitors participating students on a 24/7 basis, collecting data on their location, sleep schedule, phone and SMS communications, academics, social networks, and even physiological markers like skin conductance, skin temperature, and acceleration.&nbsp;</p><p>We extract features from this data and apply a variety of machine learning algorithms, including Gaussian mixture models and Multi-task Multi-Kernel Learning; we are currently working to apply Bayesian hierarchical multi-task learning and Deep Learning as well.</p><p>Interesting findings include: when participants visit novel locations they tend to be happier; when they use their phones or stay indoors for long periods they tend to be unhappy; and when several dimensions of wellbeing (including stress, happiness, health, and energy) are learned together, classification accuracy improves. The biggest classification accuracy improvements come when we use multi-tasking algorithms to leverage group data while customizing a model for each participant.</p>",,--Choose Location,2019-04-19 17:23:56.302,True,2015-01-01,"Predicting students' wellbeing from physiology, phone, mobility, and behavioral data",PUBLIC,,True,Affective Computing,False
worldbeat,asma_gh,False,"<p>Can we modulate the way we hear the world around us to make it more calming or to induce focus? </p><p>While technology is usually associated with causing stress, technology also&nbsp;has the potential to bring about calm. In particular, breathing usually&nbsp;speeds up with higher stress, but it can be slowed through a manipulation,&nbsp;and in so doing, it can help the person calm down. We are exploring a range&nbsp;of interventions to influence breathing without requiring any focused&nbsp;attention in order to be effective. In multiple projects, we have looked at&nbsp;dynamic composition of music, modulation of screen brightness, and headphone volume to create a seamless pulsating behavior, similar to breathing biofeedback, to indirectly influence breathing. Our preliminary analyses show promising results that such seamless modulation indeed have an influence on breathing rate and pattern.</p><p>In this project, we explore modulating insertion gain on a headphone in harmony with affective signals, particularly breathing rate. We study the influence of this dynamic change between “inside” and “outside” sources of sound to induce a sense of calmness. We experiment in simulated environments that resemble different situations such as a library, a busy street, and a fireplace.</p><p>We would like to thank Dan Gauger for giving us equipment and his thoughtful suggestions, including the project name. We would also like to thank Bose for making this project happen.&nbsp;</p>",,,2019-04-19 17:40:48.922,True,2017-12-01,WorldBeat: Hearing the world differently,PUBLIC,http://www.ghandeharioun.com,True,Affective Computing,False
brightbeat,asma_gh,False,"<p>The relationship between breathing and self-reported stress is bidirectional. Respiration pattern is an indicator of stress, but it can also be manipulated to induce calmness.&nbsp;</p><p>In this project we explore this relationship via novel means of interaction. BrightBeat is a set of seamless visual, auditory, and tactile interventions that mimic a calming breathing oscillation, with the aim of influencing physiological syncing and consequently bringing a sense of focus and calmness.&nbsp;</p><p>The animation above shows an exaggerated version of BrightBeat. These interventions are designed to run easily on commonplace personal electronic devices, respect the user's privacy, and not to require constant focus or attention in order to be effective.&nbsp;<br></p>",,--Choose Location,2019-04-19 14:59:31.588,True,2015-09-01,BrightBeat: Effortlessly influencing breathing for cultivating calmness and focus,PUBLIC,,True,Affective Computing,False
behavioral-indications-of-depression-severity,asma_gh,False,"<p>In collaboration with Massachusetts General Hospital, we are conducting a clinical trial exploring objective methods for assessing depression and its severity.&nbsp;</p><p>We are challenging the assessment methods that were created decades ago and which rely mostly on self-reported measures. We are including information from wearable sensors and regular sensors in mobile phones to collect information about sleep, social interaction, and location changes to find behavioral patterns that are associated with depressive symptoms. 
                    
                </p>",,,2018-04-09 01:37:18.568,True,2015-01-01,Behavioral Indications of Depression Severity,PUBLIC,,True,Affective Computing,False
kind-and-grateful-promoting-kindness-and-gratitude-with-pervasive-technology,asma_gh,False,"<p>We have designed a novel system to promote kindness and gratitude. We leverage pervasive technologies to naturally embed gratitude inspiration in everyday life. Mobile sensor data is utilized to infer optimal moments for stimulating contextually relevant thankfulness and appreciation. We analyze the interplay between mood, contextual cues, and gratitude expressions.</p>",,--Choose Location,2018-10-22 19:48:44.018,True,2015-01-01,"""Kind and Grateful"": Promoting kindness and gratitude with pervasive technology",PUBLIC,,True,Affective Computing,False
elsa,asma_gh,False,"<h2><b><i>What is ELSA?</i></b></h2><p>ELSA is an AI-powered chatbot that acts as an empathetic companion, encouraging users to talk about their day through a form of interactive journaling.</p><p>You can try some of the current ELSA bots in this online&nbsp;<a href=""http://elsaneural.net"">demo</a>.&nbsp;</p><h2><b><i>How does ELSA work?</i></b></h2><p>Our project goal is to build a more empathetic neural network conversational AI by incorporating a deeper understanding of both the affective content of the conversation and the topic.&nbsp; More specifically, we build hierarchical recurrent neural network models that can converse like people &nbsp;and use transfer learning of topic and emotional tone recognition models to improve our final model.</p><h2><b><i>What are the applications of ELSA?</i></b></h2><p>Beyond the development of chatbots that act as an empathetic companion, we have a more ambitious and longer term goal: deploy the empathetic companion bots to support mental health.&nbsp; In particular, &nbsp;we aim to make ELSA useful for:</p><ul><li>Eliciting journaling</li><li>Suggesting behavioura interventions</li><li>Using Cognition Behavioral Therapy</li><li>Detecting individuals at risk of depression or suicide</li></ul><h2><b><i>Work in progress</i></b></h2><p>ELSA is a recently started project in the Affective Computing group. You can see an example of ELSA bot conversations below. You can also try our online <a href=""http://elsaneural.net"">demo</a>. &nbsp;&nbsp;</p>",,,2019-04-22 19:06:31.702,True,2019-03-03,"ELSA: Empathy learning, socially-aware agents",PUBLIC,,True,Affective Computing,False
uncnet,asma_gh,False,"<p>Automatic emotion recognition has become a well-established machine learning task in recent years. The sensitive and subjective nature of emotions may give rise to societal challenges manifesting from incorrect or misinterpreted predictions. In this work, we make the argument that emotion recognition models have an obligation to quantify their uncertainty (or similarly, provide confidence bounds). We provide demonstrations of how classical network architectures can be altered to give measures of epistemic and aleatoric uncertainty using established probabilistic inference techniques. We also explore what these uncertainties explain about the data and predictions and how it can reveal a lack of diversity in training data. We demonstrate how difficult and subjective training samples can be identified using these learned uncertainty measures.</p>",,,2019-04-23 13:45:34.012,True,2019-02-01,UncNet: Modeling uncertainty in deep learning for inherently subjective tasks,LAB-INSIDERS,,True,Affective Computing,False
emma-an-emotionally-intelligent-personal-assistant-for-improving-wellbeing,asma_gh,False,"<p>The delivery of mental health interventions via ubiquitous devices has shown a lot of promise. A natural conversational interface that allows longitudinal symptom tracking and appropriate just-in-time interventions would be extremely valuable. However, the task of designing emotionally aware agents is still poorly understood. Furthermore, the feasibility of automating the delivery of just-in-time mHealth interventions via such an agent has not been fully studied. In this project, we explore the design and evaluation of EMMA (EMotion-Aware mHealth Agent).</p><p>EMMA conducts experience sampling in an empathetic manner and provides emotionally appropriate micro-activities. We show the system can be extended to detect a user's mood purely from smartphone sensor data.&nbsp;</p><p>We have conducted a three-week user study (N=58).&nbsp;Our results show that extroverts preferred EMMA significantly more, and that our personalized machine learning model was effective, as was relying on ground-truth emotion samples from users.</p>",,,2019-04-23 13:43:42.840,True,2017-05-26,EMMA: An emotionally intelligent personal assistant for improving wellbeing,PUBLIC,,True,Affective Computing,False
brainbeat,asma_gh,False,"<p>Can we sonfiy calming breathing and passively influence a state of calm?</p><p>Deep breathing has been scientifically proven to affect the heart, brain, digestive system, and the immune system. We believe designing a technology to promote deep breathing can facilitate transition into a calm state.&nbsp;</p><p>Nowadays, many people are spending a significant amount of time listening to music while working or studying. This makes music a good means for providing auditory breathing cues. While it has been shown that liminal auditory cues can be effective in encouraging a healthy breathing pattern, we are examining the use of subliminal encouragement of &nbsp;breathing modulation using music. Auditory ambient feedback has long been studied and is proved to be effective. It has been explored in concert settings, interactive installations, and smartphone applications. However, our aim is to design an intervention that is unobtrusive and doesn't keep people from doing their primary work. </p><p>In order to find the best auditory feedback design, we have designed a controlled study comparing an interactive rhythmic ambient music track that responds to a user's current breathing patterns to a fixed rate music track whose speed of playback is pegged to a rate slightly below the user's natural resting breathing rate. A control condition with no music is also included. We will compare the resulting breathing patterns, heart rate, EEG signals, and self-reported measures to determine if the ambient music feedback has any effect on the user's state of mind and body. If successful, a musical system to subliminally encourage calming breathing patterns may be integrated into workplace environments, hospitals, and other places where it is necessary to promote less stressful and healthy environments.</p><p>Our preliminary results significant shift in multiple physiological measures that indicate a state of calmness.</p>",,,2019-04-23 04:44:31.353,True,2016-12-01,BrainBeat: Breath-based music therapy,PUBLIC,,True,Affective Computing,False
predicting-students-wellbeing-from-physiology-phone-mobility-and-behavioral-data,azaria,False,"<p>The goal of this project is to apply machine learning methods to model the wellbeing of MIT undergraduate students. Extensive data is obtained from the SNAPSHOT study, which monitors participating students on a 24/7 basis, collecting data on their location, sleep schedule, phone and SMS communications, academics, social networks, and even physiological markers like skin conductance, skin temperature, and acceleration.&nbsp;</p><p>We extract features from this data and apply a variety of machine learning algorithms, including Gaussian mixture models and Multi-task Multi-Kernel Learning; we are currently working to apply Bayesian hierarchical multi-task learning and Deep Learning as well.</p><p>Interesting findings include: when participants visit novel locations they tend to be happier; when they use their phones or stay indoors for long periods they tend to be unhappy; and when several dimensions of wellbeing (including stress, happiness, health, and energy) are learned together, classification accuracy improves. The biggest classification accuracy improvements come when we use multi-tasking algorithms to leverage group data while customizing a model for each participant.</p>",,--Choose Location,2019-04-19 17:23:56.302,True,2015-01-01,"Predicting students' wellbeing from physiology, phone, mobility, and behavioral data",PUBLIC,,True,Responsive Environments,False
kind-and-grateful-promoting-kindness-and-gratitude-with-pervasive-technology,azaria,False,"<p>We have designed a novel system to promote kindness and gratitude. We leverage pervasive technologies to naturally embed gratitude inspiration in everyday life. Mobile sensor data is utilized to infer optimal moments for stimulating contextually relevant thankfulness and appreciation. We analyze the interplay between mood, contextual cues, and gratitude expressions.</p>",,--Choose Location,2018-10-22 19:48:44.018,True,2015-01-01,"""Kind and Grateful"": Promoting kindness and gratitude with pervasive technology",PUBLIC,,True,Responsive Environments,False
mediated-atmospheres,azaria,False,"<h2>The Mediated Atmosphere project envisions a smart office that is capable of dynamically transforming itself to enhance occupants' work experience.</h2><p>In the knowledge economy, worker satisfaction is paramount to retention and productivity. Recent studies have identified a decline in workplace satisfaction. Our research demonstrates how Mediated Atmosphere address this growing need.&nbsp;We created a workspace prototype equipped with a modular real-time control infrastructure, integrating biosignal sensors, controllable lighting, projection, and sound.</p>",,,2018-07-16 12:52:08.873,True,2015-09-01,Mediated Atmosphere,PUBLIC,,True,Responsive Environments,False
holobiont-urbanism-revealing-the-microbiological-world-of-cities,mperez4,False,"<p>This project investigates urban metagenomics to reveal the invisible microbiological worlds within our cities. Using honeybees to gather samples and hives modified to capture ""bee debris,"" the project employs genetic sequencing to discern and visualize urban microbiological neighborhoods and render microbiological landscapes of the city. The Holobiont project was first displayed at the Palazzo Mora in the 2016 Venice Architecture Biennale, with an installation that includes a ""metagenomic beehive."" Creative, scientific, development and production collaboration with: Ben Berman, Dr. Elizabeth Henaff, Regina Flores Mir, Dr. Chris Mason, Devora Najjar, Tri-Lox, and Chris Woebken, with contributions from Timo Arnall and Jack Schulze and local beekeepers in Brooklyn, Sydney, and Venice.</p>",,--Choose Location,2016-12-05 00:17:13.934,True,2016-01-01,Holobiont Urbanism: Revealing the Microbiological World of Cities,PUBLIC,,True,Playful Systems,False
3d-neuromuscular-model-of-the-human-ankle-foot-complex,dhill24,False,"<p>During the gait cycle, the human ankle complex serves as a primary power generator while simultaneously stabilizing the entire limb. These actions are controlled by an intricate interplay of several lower leg muscles that cannot be fully uncovered using experimental methods alone. A combination of experiments and mathematical modeling may be used to estimate aspects of neuromusculoskeletal functions that control human gait. In this research, a three-dimensional neuromuscular model of the human ankle-foot complex based on biplanar fluoroscopy gait analysis is presented.&nbsp;Driven by kinematics, kinetics, and electromyography (EMG), the model seeks to solve the redundancy problem, individual muscle-tendon contributions to net joint torque, in ankle and subtalar joint actuation during overground gait. An optimization approach was employed to calculate sets of morphological parameters that simultaneously maximize the neuromuscular model’s metabolic efficiency and fit to experimental joint torques. Optimal morphological parameter sets produce estimates of force contributions and states for individual muscles.</p>",,,2019-04-26 18:59:57.781,True,2018-08-01,3D neuromuscular model of the human ankle-foot complex,PUBLIC,,True,Biomechatronics,False
machine-learning-from-biomarker-signatures-and-correlation-to-systemic-health-conditions,gyauney,False,"<p>Imaging fluorescent disease biomarkers in tissues and skin is a non-invasive method to screen for health conditions. We report an automated process that combines intraoral fluorescent porphyrin biomarker imaging, clinical examinations, and machine learning for correlation of systemic health conditions with periodontal disease. 1,215 intraoral fluorescent images, from 284 consenting adults aged 18-90, were analyzed using a machine learning classifier that can segment periodontal inflammation. The classifier achieved an AUC of 0.677 with precision and recall of 0.271 and 0.429, respectively, indicating a learned association between disease signatures in collected images. Periodontal diseases were more prevalent among males (p=0.0012) and older subjects (p=0.0224) in the screened population. Physicians independently examined the collected images, assigning localized modified gingival indices (MGIs). MGIs and periodontal disease were then cross-correlated with responses to a medical history questionnaire, blood pressure and body mass index measurements, and optic nerve, tympanic membrane, neurological, and cardiac rhythm imaging examinations. Gingivitis and early periodontal disease were associated with subjects diagnosed with optic nerve abnormalities (p &lt;0.0001) in their retinal scans. We also report significant co-occurrences of periodontal disease in subjects reporting swollen joints (p=0.0422) and a family history of eye disease (p=0.0337). These results indicate cross-correlation of poor periodontal health with systemic health outcomes and stress the importance of oral health screenings at the primary care level. Our screening process and analysis method, using images and machine learning, can be generalized for automated diagnoses and systemic health screenings for other diseases.</p><p><strong>Why is this work important?</strong></p><p>Standard practices like visual assessment and diagnosis of oral diseases using bleeding with a probe do not account for patient-to-patient variation or identify disease progression risk. This study uses a machine learning model to segment oral porphyrin biomarker levels from intraoral photographs and find correlations with and prognoses of systemic health conditions.</p><p><strong>What has been done before?</strong></p><p>Current methods to diagnose oral diseases include visual inspection by doctors and probing the gums. Positive correlations have been found between oral health and heart diseases, diabetes, tobacco use, and smoking, but all depend on visual examination by doctors.</p><p><strong>What are our contributions?</strong></p><p>We report a novel process for automated machine learning oral health examinations using images of fluorescent biomarkers and cross-correlations between oral and systemic health. We collect a novel dataset for the study and find correlations between oral health and systemic conditions like swollen joints, optical nerve abnormalities in retinal scans, and a family history of eye disease. Our approach can be generalized for predicting systemic health by analyzing other biomarker images.</p><p><strong>What are the next steps?</strong></p><p>We are actively expanding the work to a larger population to discover novel cross-correlations between other biomarkers and systemic health outcomes.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/"">Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care</a></li><li><a href=""https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/"">Machine Learning and Automated Segmentation of Oral Diseases using Biomarker Images</a></li></ol>",,,2018-11-14 19:32:31.569,True,2017-09-18,Machine Learning from Biomarker Signatures and Correlation to Systemic Health Conditions,PUBLIC,,True,Other,False
porphyrin-imaging,gyauney,False,"<p>We built a low-cost and open source 405 nm imaging device to capture red fluorescence signatures associated with the oral biomarker porphyrin, demonstrating comparable performance to an expensive commercially available device.&nbsp; We also provide a miniaturized mobile-adaptable version of the device. A step-by-step guide for device assembly and the&nbsp;associated computer vision algorithm are shared on the project website&nbsp;to facilitate open-source access to imaging technologies.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/machine-learning-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images/overview/"">Machine Learning for Combined Classification of Fluorescent Biomarkers and Expert Annotations Using White Light Images</a></li></ol>",,,2018-05-04 20:56:15.953,True,2015-06-01,Biomarker Imaging with Mobile Phones,PUBLIC,,True,Other,False
technology-enabled-mobile-phone-screenings-augment-routine-primary-care,gyauney,False,"<p>We have developed a new process to screen patients at the point-of-care with FDA-approved technology-enabled mobile health screenings (TES) and compare the results with routine health screenings. A study of nearly 500 patients was conducted to test the effectiveness of this new screening process. This is one of the first studies to investigate using TES to augment routine health examinations. We recommend using TES in synergy with routine health screenings to identify missing sick patients who might otherwise lack comprehensive primary care.<br></p><p><strong>Why is this work important?</strong></p><p>Providing good healthcare in low- and middle-income countries (LMIC) paradoxically requires expensive equipment,&nbsp;<span style=""font-family: &quot;Neue Haas Grotesk Display&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 18px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400;"">which may not be easily available because of resource limitations,</span><span style=""font-size: 18px; font-weight: 400;"">&nbsp;for health monitoring and assessment. There is high variation in the degrees of healthcare access in LMIC, but such access is important because cardiovascular diseases, preventable blindness, oral cancer, and treatable neurological conditions constitute more than half of the disease burden in LMIC. Comprehensive TES may allow for more patients to be screened for more conditions in resource-limited settings, improving their access to primary healthcare. A lack of consensus exists about the usefulness of TES in augmenting primary health screenings in LMIC.</span></p><p><strong>What has been done before?</strong><br></p><p>Devices that allow TES have typically been evaluated in isolated silos, concentrating on individual devices or specific anatomical sites. They have additionally not been comprehensively evaluated alongside routine health screenings.</p><p><strong>What are our contributions?</strong><br></p><p>This is one of the first studies to investigate using multiple TES to augment routine health examinations. To facilitate this large-scale study, we developed and successfully used web examination platforms that enabled multiple physicians to diagnose health conditions remotely. We identified patients who would not have received the care they need in the absence of TES, and link TES to primary health outcomes.</p><p>This study led to significant insights about strategies to develop technologies at MIT that are ready for deployment for effective and scalable primary care in the real world.</p><p><strong>What technology-enabled examinations were performed?</strong></p><p>Single-lead ECG: AliveCor Mobile ECG</p><p>Blood oxygen saturation: Contec Medical Systems 50-DL Pulse Oximeter</p><p>Oral imaging: ACTEON Soprocare</p><p>Retinal scan: D-EYE direct ophthalmoscopy adapter attached to iPhone5s camera</p><p>Tympanic membrane imaging: CellScope Oto with iPhone5 LEDs and camera</p><p>Neurological examinations: Microsoft Kinect</p><p><strong>What are the next steps?</strong></p><p>We are actively working on automated diagnoses, analyses of disease co-occurrence, and patient risk stratification.</p><p>Future studies that build on our technology-enabled screening process can evaluate the process for larger numbers of patients. A future longitudinal study may allow for additional insights into time-varying conditions.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/"">Machine Learning and Automated Segmentation of Oral Diseases using Biomarker Images</a></li></ol>",,,2018-10-21 18:32:18.676,True,2015-05-15,Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care,PUBLIC,,True,Other,False
computational-histological-staining-and-destaining-of-prostate-core-biopsy-rgb-images-with-generative-adversarial-neural-networks-1,gyauney,False,"<h2><b>General Overview</b></h2><p>Staining of tissues sections using chemical and biological dyes has been used for over a century for visualizing various tissue types and morphologic changes associated with contemporary cancer diagnosis.&nbsp;The staining procedure however is labor intensive, needs trained technicians, costly, and often results in loss of irreplaceable specimen and delays diagnoses. In collaboration with Brigham and Women's Hospital (Boston, MA), we&nbsp; describe a “computational staining” approach to digitally stain photographs of unstained tissue biopsies with Haematoxylin and Eosin (H&amp;E) dyes to diagnose cancer.&nbsp;&nbsp;</p><p>Our method uses neural networks to rapidly stain photographs of non-stained tissues, providing physicians timely information about the anatomy and structure of the tissue.&nbsp; We also report a&nbsp; ""computational destaining"" algorithm that can remove dyes and stains from photographs of previously stained tissues, allowing reuse of patient samples.&nbsp;&nbsp;</p><p>These methods and neural networks assist physicians and patients by novel computational processes&nbsp;at the point-of-care,&nbsp;which can integrate seamlessly into clinical workflows in hospitals all over the world.</p>",,,2018-12-20 15:09:03.246,True,2018-05-01,AI Methods for Rapid Automated Staining and Destaining of Tissue Biopsies in Hospitals,PUBLIC,http://bit.ly/2nMCs3C,True,Other,False
self-learning-ai-model-learns-from-patient-data-to-design-novel-clinical-trials,gyauney,False,"<p><b>Technical summary</b></p><p>Unstructured learning problems without well-defined rewards are unsuitable for current reinforcement learning (RL) approaches. Action-derived rewards can allow RL agents to fully explore state and action trade-offs in scenarios that require specific outcomes yet are unstructured by external reward. Clinical trial dosing choice is an example of such a problem. We report the successful formulation of clinical trial dosing choice as an RL problem using action-based rewards and learning of dosing regimens to reduce mean tumor diameters (MTD) in patients undergoing simulated temozolomide (TMZ) and procarbazine, 1-(2-chloroethyl)-3-cyclohexyl-l-nitrosourea, and vincristine (PCV) chemo- and radiotherapy clinical trials. The use of action-derived rewards as partial proxies for outcomes is described for the first time. Novel dosing regimens learned by an RL agent in the presence of action-derived rewards achieve significant reduction in MTD for cohorts and individual patients in simulated TMZ and PCV clinical trials while reducing treatment cycle administrations and dosage concentrations compared to human-expert dosing regimens. Our approach can be easily adapted for other learning tasks where outcome-based learning is not practical.</p><p><b>Glioblastoma (brain tumors) background:&nbsp;</b>Glioblastoma is an aggressive type of cancer that can occur in the brain
or spinal cord. A hard-hitting treatment typically involves combining surgery
with radiation therapy and chemotherapy, which is necessary to combat the
aggressive nature of a glioblastoma. Chemo-and Radiotherapy Treatments (CRT) may
slow the progression of cancer and reduce signs and symptoms, but are unable to
cure the disease. Survival rates are low (about 14-18 months) and only about
10% of patients live five years or longer. CRTs are often given as a
combination of drugs. Procarbazine, 1-(2-chloroethyl)-3-cyclohexyl-l-nitrosourea,
and Vincristine (PCV) is triple drug chemotherapy for glioblastomas and can be
toxic for the patients. Temozolomide (TMZ), is less toxic, has shown greater
efficacy when compared to radiotherapy alone, in the treatment of glioblastoma.
There is significant and urgent need for novel CRT dosing regimens in human subjects
to optimize for maximum therapeutic benefit for patients while reducing
toxicity.</p><p><b>Reinforcement learning background:&nbsp;</b>Reinforcement learning (RL) is an area of machine learning and AI inspired by behaviorist psychology. RL agents can self-learn how to solve complex tasks in a relatively unstructured environment so as to maximize some notion of cumulative&nbsp;rewards and reduce penalties set by human programmers. Reward functions in RL domains are typically derived from a measure that is external to the chosen representation of the states (data) and actions (steps) used for the self-training algorithm. Using RL to solve tasks without readily accessible external scalar outcomes is a relatively unexplored field, as many currently studied domains have well-defined outcomes and associated rewards as part of their definitions.<br></p><p><b>Clinical trials background:&nbsp;</b>Clinical trials to evaluate new drugs, therapies, and vaccines are among the most complex experiments performed in medicine. Nearly half of phase 2 and phase 3 trials fail. For oncology trials, the failure rate rises to two-thirds. A common theme is the difficulty of predicting clinical results in a wide patient base given limited knowledge of key parameters which need to be considered to test candidate molecules, eliminate adverse events, and identify the drugs half maximal inhibitory concentration. Optimal CRT dosing for patients enrolled in glioblastoma clinical trials thus provides one example of an open-ended problem characterized by complex interactions between different drug properties, dosage and timing of administrations (actions), and effects on tumors (state) and where survival (outcomes) may not be available.</p>",,,2018-10-22 14:52:11.990,True,2018-08-10,Self-Learning AI Model Learns from Patient Data to Design Novel Clinical Trials,PUBLIC,https://www.media.mit.edu/groups/health-0-0/overview/,True,Other,False
machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images,gyauney,False,"<p>We report a novel method that processes biomarker images collected at the point of care and uses machine learning algorithms to provide a first level of screening against oral diseases. A machine learning classifier is trained to learn pixel-by-pixel mappings from RGB oral images and output areas with disease. This method can be adapted to&nbsp; process biomarker images from other organs as well.</p><p><strong>Why is this work important?</strong></p><p>Visual inspection and probing techniques have been traditionally used for diagnosis of oral diseases in patients. These traditional methods are subjective and not scalable. We describe the use of RGB color images acquired by low-cost camera devices coupled with machine learning to detect areas with poor oral health.</p><p><strong>What has been done before?</strong></p><p>Currently the gold standard for oral diagnosis is visual inspections by a dentist followed by X-rays. These methods are expensive and invasive.&nbsp;</p><p><strong>What are our contributions?</strong></p><p>We implement a novel technique to combine medical expert knowledge with biomarker signatures.&nbsp; We&nbsp; use RGB color images taken directly at the point-of-care, using low-cost hand-held devices, to provide a first level machine-learning powered screening for patients.</p><p><strong>What are the next steps?</strong></p><p>We are expanding the repertoire of biomarkers that can be detected in RGB color images acquired at the point-of-care and pairing them with automated machine learning exams.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/"">Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care</a></li></ol>",,,2018-08-09 17:24:03.930,True,2017-04-24,Machine Learning and Automated Segmentation of Oral Diseases Using Biomarker Images,PUBLIC,,True,Other,False
machine-learning-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images,gyauney,False,"<p>Biomarker imaging provides non-invasive indicators of disease and is used by human experts to augment disease diagnosis. It is, however, often expensive and reliant on experts to interpret the resulting images. We have developed a process for learning associations between standard white light images and both biomarkers and expert annotations of disease. Oral imaging is one particular example of biomarker imaging that can supplement expert knowledge; the biomarker porphyrin is associated with poor oral health and oral cancer. We report that our process learns to accurately predict the presence of porphyrin and expert-annotated conditions.</p><p><strong>Why is this work important?</strong></p><p>Biomarker imaging provides non-invasive indicators of disease and is used by human experts to augment disease diagnosis. Capturing biomarker images requires specialized and often expensive hardware, annotations, and analyses by experts, resulting in substantial diagnosis delays.</p><p><strong>What has been done before?</strong></p><p>Even when biomarker imaging is available, experts are often needed to interpret the resulting images. There is a rich literature on medical image segmentation, but many approaches—especially deep learning—require large amounts of images and operate on information from only a single given imaging modality.</p><p><strong>What are our contributions?</strong></p><p>We successfully learn assocations between images and union signatures of biomarker presence and expert disease annotations. By transforming the image-level segmentation problem into a region-based problem, we are able to learn from far fewer images than other approaches. We specifically test our approach on detecting the biomarker porphyrin and associated conditions in millions of image patches. Once trained, the classifiers predict the location of porphyrin in images without requiring specialized biomarker imaging devices or expert intervention.</p><p><strong>What are the next steps?</strong></p><p>We are developing processes incorporating numerous other biomarkers, conditions, and imaging modalities.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/porphyrin-imaging/overview/"">Biomarker Imaging with Mobile Phones</a></li></ol>",,,2018-08-15 19:45:24.252,True,2017-02-01,Machine Learning for Combined Classification of Fluorescent Biomarkers and Expert Annotations Using White Light Images,PUBLIC,,True,Other,False
replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis,gyauney,False,"<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Our goal is to augment ultimately replace the prevalence of ionizing and expensive X-ray imaging and cone-beam computed tomography (CBCT) for dental care with near-infrared (NIR) imaging. Translucency of teeth in the NIR range offers non-ionizing and safe detection of dental features. NIR can be used in conjunction with multiple light sources to create three-dimensional images of teeth. By modeling the scattering of photons in teeth, we can effectively see several millimeters inside, providing additional diagnostic value.</p><p><strong>Why is this work important?</strong></p><p>Two-dimensional radiographs and cone-beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. NIR light at 850 nm and 1310 nm, which strike a balance between enamel and water attenuation, have been shown to provide helpful diagnostics that visual examination alone lacks. Our previous work has demonstrated the sensitivity of 850 nm NIR images to early caries lesions and demineralization. For NIR is to synergistically augment or eventually replace ionizing radiation as the standard of care, we aim to expand its diagnostic potential to clinical features that exist beyond the surface of the tooth.</p><p><strong>What are our contributions?</strong></p><p>This is ongoing research. Preliminary work shows promise for augmenting the diagnostic power of NIR by modeling scattering.</p><p><strong>What are the next steps?</strong></p><p>Large-scale screenings can evaluate the effectiveness of our new imaging process.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth/overview/"">Near-Infrared Imaging for Detecting Dental Caries</a></li><li><a href=""https://www.media.mit.edu/projects/near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging/overview/"">Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging</a></li></ol>",,,2018-10-26 16:23:18.913,True,2017-05-08,Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography,PUBLIC,,True,Other,False
machine-learning-from-biomarker-signatures-and-correlation-to-systemic-health-conditions,pjavia,False,"<p>Imaging fluorescent disease biomarkers in tissues and skin is a non-invasive method to screen for health conditions. We report an automated process that combines intraoral fluorescent porphyrin biomarker imaging, clinical examinations, and machine learning for correlation of systemic health conditions with periodontal disease. 1,215 intraoral fluorescent images, from 284 consenting adults aged 18-90, were analyzed using a machine learning classifier that can segment periodontal inflammation. The classifier achieved an AUC of 0.677 with precision and recall of 0.271 and 0.429, respectively, indicating a learned association between disease signatures in collected images. Periodontal diseases were more prevalent among males (p=0.0012) and older subjects (p=0.0224) in the screened population. Physicians independently examined the collected images, assigning localized modified gingival indices (MGIs). MGIs and periodontal disease were then cross-correlated with responses to a medical history questionnaire, blood pressure and body mass index measurements, and optic nerve, tympanic membrane, neurological, and cardiac rhythm imaging examinations. Gingivitis and early periodontal disease were associated with subjects diagnosed with optic nerve abnormalities (p &lt;0.0001) in their retinal scans. We also report significant co-occurrences of periodontal disease in subjects reporting swollen joints (p=0.0422) and a family history of eye disease (p=0.0337). These results indicate cross-correlation of poor periodontal health with systemic health outcomes and stress the importance of oral health screenings at the primary care level. Our screening process and analysis method, using images and machine learning, can be generalized for automated diagnoses and systemic health screenings for other diseases.</p><p><strong>Why is this work important?</strong></p><p>Standard practices like visual assessment and diagnosis of oral diseases using bleeding with a probe do not account for patient-to-patient variation or identify disease progression risk. This study uses a machine learning model to segment oral porphyrin biomarker levels from intraoral photographs and find correlations with and prognoses of systemic health conditions.</p><p><strong>What has been done before?</strong></p><p>Current methods to diagnose oral diseases include visual inspection by doctors and probing the gums. Positive correlations have been found between oral health and heart diseases, diabetes, tobacco use, and smoking, but all depend on visual examination by doctors.</p><p><strong>What are our contributions?</strong></p><p>We report a novel process for automated machine learning oral health examinations using images of fluorescent biomarkers and cross-correlations between oral and systemic health. We collect a novel dataset for the study and find correlations between oral health and systemic conditions like swollen joints, optical nerve abnormalities in retinal scans, and a family history of eye disease. Our approach can be generalized for predicting systemic health by analyzing other biomarker images.</p><p><strong>What are the next steps?</strong></p><p>We are actively expanding the work to a larger population to discover novel cross-correlations between other biomarkers and systemic health outcomes.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/"">Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care</a></li><li><a href=""https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/"">Machine Learning and Automated Segmentation of Oral Diseases using Biomarker Images</a></li></ol>",,,2018-11-14 19:32:31.569,True,2017-09-18,Machine Learning from Biomarker Signatures and Correlation to Systemic Health Conditions,PUBLIC,,True,Other,False
machine-learning-algorithms-for-classification-of-microcirculation-images-from-septic-and-non-septic-patients-1,pjavia,False,"<p><b>General overview:</b></p><p>Sepsis, a life-threatening complication of bacterial infection, leads to millions of worldwide deaths requires significant time and resources to diagnose. This disease is associated with very high mortality rates, making early detection crucial for treatment.&nbsp;</p><p>Researchers have investigated direct clinical evaluation by using dark field imaging of capillary beds under the tongue of septic and healthy subjects for signatures of microcirculatory dysfunction associated with sepsis. Our published results, in collaboration with Beth Israel Deaconess Medical Center, have shown that machine learning and vision can learn higher-order hierarchical diagnostic and prognostic features for rapid and non-invasive diagnosis of sepsis using these dark field microcirculatory images.&nbsp;A neural network capable of distinguishing between images from non-septic and septic patients with more than 90% accuracy is reported for the first time. This approach can help physicians to rapidly stratify patients, facilitate rational use of antibiotics, and reduce disease burden in hospital emergency rooms.</p>",,,2018-12-20 18:14:27.714,True,2018-08-01,Helping Emergency Care Physicians Diagnose Sepsis and Bacterial Infections  with Machine Learning and Vision,PUBLIC,http://bit.ly/2nMCs3C,True,Other,False
machine-learning-from-biomarker-signatures-and-correlation-to-systemic-health-conditions,arana,False,"<p>Imaging fluorescent disease biomarkers in tissues and skin is a non-invasive method to screen for health conditions. We report an automated process that combines intraoral fluorescent porphyrin biomarker imaging, clinical examinations, and machine learning for correlation of systemic health conditions with periodontal disease. 1,215 intraoral fluorescent images, from 284 consenting adults aged 18-90, were analyzed using a machine learning classifier that can segment periodontal inflammation. The classifier achieved an AUC of 0.677 with precision and recall of 0.271 and 0.429, respectively, indicating a learned association between disease signatures in collected images. Periodontal diseases were more prevalent among males (p=0.0012) and older subjects (p=0.0224) in the screened population. Physicians independently examined the collected images, assigning localized modified gingival indices (MGIs). MGIs and periodontal disease were then cross-correlated with responses to a medical history questionnaire, blood pressure and body mass index measurements, and optic nerve, tympanic membrane, neurological, and cardiac rhythm imaging examinations. Gingivitis and early periodontal disease were associated with subjects diagnosed with optic nerve abnormalities (p &lt;0.0001) in their retinal scans. We also report significant co-occurrences of periodontal disease in subjects reporting swollen joints (p=0.0422) and a family history of eye disease (p=0.0337). These results indicate cross-correlation of poor periodontal health with systemic health outcomes and stress the importance of oral health screenings at the primary care level. Our screening process and analysis method, using images and machine learning, can be generalized for automated diagnoses and systemic health screenings for other diseases.</p><p><strong>Why is this work important?</strong></p><p>Standard practices like visual assessment and diagnosis of oral diseases using bleeding with a probe do not account for patient-to-patient variation or identify disease progression risk. This study uses a machine learning model to segment oral porphyrin biomarker levels from intraoral photographs and find correlations with and prognoses of systemic health conditions.</p><p><strong>What has been done before?</strong></p><p>Current methods to diagnose oral diseases include visual inspection by doctors and probing the gums. Positive correlations have been found between oral health and heart diseases, diabetes, tobacco use, and smoking, but all depend on visual examination by doctors.</p><p><strong>What are our contributions?</strong></p><p>We report a novel process for automated machine learning oral health examinations using images of fluorescent biomarkers and cross-correlations between oral and systemic health. We collect a novel dataset for the study and find correlations between oral health and systemic conditions like swollen joints, optical nerve abnormalities in retinal scans, and a family history of eye disease. Our approach can be generalized for predicting systemic health by analyzing other biomarker images.</p><p><strong>What are the next steps?</strong></p><p>We are actively expanding the work to a larger population to discover novel cross-correlations between other biomarkers and systemic health outcomes.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/"">Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care</a></li><li><a href=""https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/"">Machine Learning and Automated Segmentation of Oral Diseases using Biomarker Images</a></li></ol>",,,2018-11-14 19:32:31.569,True,2017-09-18,Machine Learning from Biomarker Signatures and Correlation to Systemic Health Conditions,PUBLIC,,True,Other,False
computational-histological-staining-and-destaining-of-prostate-core-biopsy-rgb-images-with-generative-adversarial-neural-networks-1,arana,False,"<h2><b>General Overview</b></h2><p>Staining of tissues sections using chemical and biological dyes has been used for over a century for visualizing various tissue types and morphologic changes associated with contemporary cancer diagnosis.&nbsp;The staining procedure however is labor intensive, needs trained technicians, costly, and often results in loss of irreplaceable specimen and delays diagnoses. In collaboration with Brigham and Women's Hospital (Boston, MA), we&nbsp; describe a “computational staining” approach to digitally stain photographs of unstained tissue biopsies with Haematoxylin and Eosin (H&amp;E) dyes to diagnose cancer.&nbsp;&nbsp;</p><p>Our method uses neural networks to rapidly stain photographs of non-stained tissues, providing physicians timely information about the anatomy and structure of the tissue.&nbsp; We also report a&nbsp; ""computational destaining"" algorithm that can remove dyes and stains from photographs of previously stained tissues, allowing reuse of patient samples.&nbsp;&nbsp;</p><p>These methods and neural networks assist physicians and patients by novel computational processes&nbsp;at the point-of-care,&nbsp;which can integrate seamlessly into clinical workflows in hospitals all over the world.</p>",,,2018-12-20 15:09:03.246,True,2018-05-01,AI Methods for Rapid Automated Staining and Destaining of Tissue Biopsies in Hospitals,PUBLIC,http://bit.ly/2nMCs3C,True,Other,False
machine-learning-algorithms-for-classification-of-microcirculation-images-from-septic-and-non-septic-patients-1,arana,False,"<p><b>General overview:</b></p><p>Sepsis, a life-threatening complication of bacterial infection, leads to millions of worldwide deaths requires significant time and resources to diagnose. This disease is associated with very high mortality rates, making early detection crucial for treatment.&nbsp;</p><p>Researchers have investigated direct clinical evaluation by using dark field imaging of capillary beds under the tongue of septic and healthy subjects for signatures of microcirculatory dysfunction associated with sepsis. Our published results, in collaboration with Beth Israel Deaconess Medical Center, have shown that machine learning and vision can learn higher-order hierarchical diagnostic and prognostic features for rapid and non-invasive diagnosis of sepsis using these dark field microcirculatory images.&nbsp;A neural network capable of distinguishing between images from non-septic and septic patients with more than 90% accuracy is reported for the first time. This approach can help physicians to rapidly stratify patients, facilitate rational use of antibiotics, and reduce disease burden in hospital emergency rooms.</p>",,,2018-12-20 18:14:27.714,True,2018-08-01,Helping Emergency Care Physicians Diagnose Sepsis and Bacterial Infections  with Machine Learning and Vision,PUBLIC,http://bit.ly/2nMCs3C,True,Other,False
machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images,arana,False,"<p>We report a novel method that processes biomarker images collected at the point of care and uses machine learning algorithms to provide a first level of screening against oral diseases. A machine learning classifier is trained to learn pixel-by-pixel mappings from RGB oral images and output areas with disease. This method can be adapted to&nbsp; process biomarker images from other organs as well.</p><p><strong>Why is this work important?</strong></p><p>Visual inspection and probing techniques have been traditionally used for diagnosis of oral diseases in patients. These traditional methods are subjective and not scalable. We describe the use of RGB color images acquired by low-cost camera devices coupled with machine learning to detect areas with poor oral health.</p><p><strong>What has been done before?</strong></p><p>Currently the gold standard for oral diagnosis is visual inspections by a dentist followed by X-rays. These methods are expensive and invasive.&nbsp;</p><p><strong>What are our contributions?</strong></p><p>We implement a novel technique to combine medical expert knowledge with biomarker signatures.&nbsp; We&nbsp; use RGB color images taken directly at the point-of-care, using low-cost hand-held devices, to provide a first level machine-learning powered screening for patients.</p><p><strong>What are the next steps?</strong></p><p>We are expanding the repertoire of biomarkers that can be detected in RGB color images acquired at the point-of-care and pairing them with automated machine learning exams.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/"">Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care</a></li></ol>",,,2018-08-09 17:24:03.930,True,2017-04-24,Machine Learning and Automated Segmentation of Oral Diseases Using Biomarker Images,PUBLIC,,True,Other,False
replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis,arana,False,"<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Our goal is to augment ultimately replace the prevalence of ionizing and expensive X-ray imaging and cone-beam computed tomography (CBCT) for dental care with near-infrared (NIR) imaging. Translucency of teeth in the NIR range offers non-ionizing and safe detection of dental features. NIR can be used in conjunction with multiple light sources to create three-dimensional images of teeth. By modeling the scattering of photons in teeth, we can effectively see several millimeters inside, providing additional diagnostic value.</p><p><strong>Why is this work important?</strong></p><p>Two-dimensional radiographs and cone-beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. NIR light at 850 nm and 1310 nm, which strike a balance between enamel and water attenuation, have been shown to provide helpful diagnostics that visual examination alone lacks. Our previous work has demonstrated the sensitivity of 850 nm NIR images to early caries lesions and demineralization. For NIR is to synergistically augment or eventually replace ionizing radiation as the standard of care, we aim to expand its diagnostic potential to clinical features that exist beyond the surface of the tooth.</p><p><strong>What are our contributions?</strong></p><p>This is ongoing research. Preliminary work shows promise for augmenting the diagnostic power of NIR by modeling scattering.</p><p><strong>What are the next steps?</strong></p><p>Large-scale screenings can evaluate the effectiveness of our new imaging process.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth/overview/"">Near-Infrared Imaging for Detecting Dental Caries</a></li><li><a href=""https://www.media.mit.edu/projects/near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging/overview/"">Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging</a></li></ol>",,,2018-10-26 16:23:18.913,True,2017-05-08,Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography,PUBLIC,,True,Other,False
big-data-for-small-places,cjaffe,False,"<p>Big Data for Small Places is a quantitative study of the qualities that define our neighborhoods and our collective role in the production of local places over time. We are translating the potentials of big data from the scale of the city to the scale of the urban block, the scale at which we physically experience urban space, to gain a better understanding of the local patterns and social spaces that aggregate to form metropolitan identity. We hope that this study will improve our collective understanding of the urban environments we shape and the stories they generate, that it will allow us to more sensitively test and implement real change in our shared public realm and support the invisible narratives it generates.</p>",,--Choose Location,2016-12-05 00:16:14.672,True,2015-01-01,Big Data for Small Places,PUBLIC,,True,Responsive Environments,False
bicycle-study,cjaffe,False,"<p>We are interested in investigating the motivations at play when an individual makes choices about mobility. How do factors like money, time, comfort, and habit impact how people choose to get around cities? And is it possible to disrupt these practices by nudging people in new directions?</p><p>In the first part of this study, we targeted individuals who live within five miles of their main workplace, but primarily use a car to get around. We lent these people a bicycle with the stipulation that they actually use it several times a week. This study is ongoing; we are investigating whether the act of giving somebody a bicycle will prompt them to change established commuting and transportation habits. We are assessing travel behavior through surveys and mobile location tracking app. </p><p>In the second part of this study,  we are developing a Blockchain-based marketplace that allows cyclists to anonymously share their location data and receive financial compensation from organizations that would like to sponsor cycling activity. For example, an insurance company may want to reward its customers with lower premiums for partaking in healthy commuting behavior. An advertising company may wish to understand cycling activity in order to improve their advertisement targeting. A local business may sponsor bicycling activity in its vicinity to increase sales.</p><p>This marketplace uses GPS data from sensors embedded in bicycles frames and powered by the cyclists themselves. The use of Blockchain technology makes transactions in the marketplace secure, seamless, trustworthy, and transparent. Users are able to reveal “just enough” information about themselves to participate in the decentralized marketplace, instead of exposing their entire profile to a central entity. This market-driven system facilitates better incentive-matching, and in turn produces a scalable and stable solution for increasing the use of sustainable transportation in cities.</p><p>Through both these studies, we hope to develop a better understanding of individuals' travel behavior, the psychology of urban cyclists, and the benefits and challenges of city cycling.</p>",,--Choose Location,2017-04-03 18:16:29.852,True,2015-09-01,Motivation and Mobility: Bicycles and Sensors in the City,LAB-INSIDERS,,True,Responsive Environments,False
big-data-for-small-places,echristo,False,"<p>Big Data for Small Places is a quantitative study of the qualities that define our neighborhoods and our collective role in the production of local places over time. We are translating the potentials of big data from the scale of the city to the scale of the urban block, the scale at which we physically experience urban space, to gain a better understanding of the local patterns and social spaces that aggregate to form metropolitan identity. We hope that this study will improve our collective understanding of the urban environments we shape and the stories they generate, that it will allow us to more sensitively test and implement real change in our shared public realm and support the invisible narratives it generates.</p>",,--Choose Location,2016-12-05 00:16:14.672,True,2015-01-01,Big Data for Small Places,PUBLIC,,True,Social Computing,False
big-data-for-small-places,srife,False,"<p>Big Data for Small Places is a quantitative study of the qualities that define our neighborhoods and our collective role in the production of local places over time. We are translating the potentials of big data from the scale of the city to the scale of the urban block, the scale at which we physically experience urban space, to gain a better understanding of the local patterns and social spaces that aggregate to form metropolitan identity. We hope that this study will improve our collective understanding of the urban environments we shape and the stories they generate, that it will allow us to more sensitively test and implement real change in our shared public realm and support the invisible narratives it generates.</p>",,--Choose Location,2016-12-05 00:16:14.672,True,2015-01-01,Big Data for Small Places,PUBLIC,,True,Civic Media,False
embedded-neural-interface-electronics-for-advanced-prosthesis-research,thhsieh,False,"<p>This project aims to develop embedded neural interface electronics for integration on mobile, external prosthetic devices. By pushing the technological limits of every layer within the stack, from semiconductor components to high-level signal processing and classification, we hope to provide practical benefit to people's daily lives in the near future.</p><p>As a first step, we are actively developing a high-fidelity electromyography (EMG) device used to observe human muscle activity. Several prosthetic ankles have already been demonstrated using this portable EMG device as a control system input.&nbsp;</p>",,,2019-04-26 19:01:33.105,True,2017-09-01,Embedded Systems for Neural Interfaces,LAB-INSIDERS,,True,Biomechatronics,False
osseo,thhsieh,False,"<p>Recent advancements in orthopedic implants have made way for a new generation of bionic limbs that attach directly to the skeleton. Leveraging these ""osseointegrated"" implants to pass wires out of the body enables robust, long-term communication with residual muscles and the nervous system. We are exploring the ways in which the improved neural communication afforded by osseointegration can impact the experience of controlling a limb prosthesis.
                    
                </p>",,,2019-04-26 19:06:19.611,True,2016-10-19,An osseointegrated prosthesis with bi-directional neural communication,PUBLIC,,True,Biomechatronics,False
moment-coupled-cantilever-beam-series-elastic-actuator,thhsieh,False,"<p>The design of next-generation bionic ankles and knees aims to improve bionic actuators on all metrics: range of motion, power density, bandwidth, mass, while adopting a futuristic aesthetic. We are pushing the limits of materials and magnetics, combined with new control topologies to enforce a new paradigm in both autonomous and volitional controlled powered prostheses. <br></p>",,,2019-04-26 19:02:31.522,True,2017-03-15,High power bionic joints for dynamic gait actions,PUBLIC,http://matthematic.com,True,Biomechatronics,False
embedded-neural-interface-electronics-for-advanced-prosthesis-research,emrogers,False,"<p>This project aims to develop embedded neural interface electronics for integration on mobile, external prosthetic devices. By pushing the technological limits of every layer within the stack, from semiconductor components to high-level signal processing and classification, we hope to provide practical benefit to people's daily lives in the near future.</p><p>As a first step, we are actively developing a high-fidelity electromyography (EMG) device used to observe human muscle activity. Several prosthetic ankles have already been demonstrated using this portable EMG device as a control system input.&nbsp;</p>",,,2019-04-26 19:01:33.105,True,2017-09-01,Embedded Systems for Neural Interfaces,LAB-INSIDERS,,True,Biomechatronics,False
ankle-foot-prosthesis-for-rock-climbing,emrogers,False,"<p>Lower extremity amputation leads to limitations of biological function of individuals, which leads to challenges remaining physically active and participating in athletic activities. Physical activity is very important for cardiovascular health, weight management, and mental health. Designing devices aimed at increasing accessibility to sports will encourage individuals with amputations to continue or begin participating in athletic pursuits such as rock climbing.&nbsp;</p><p>This research presents the design and evaluation of a 2-degree-of-freedom powered ankle-foot prosthesis for rock climbing. The aim of this device is to restore function of the ankle and subtalar joints for trans-tibial amputees during rock climbing, providing the user with myoelectric position control of the foot. Precise positional control of the foot is especially important while climbing, as the climber’s ability to successfully scale a route requires them to reliably reorient the foot to various shapes and orientations of holds. Passive prostheses do not allow the user to reposition the foot, and current powered prostheses are too bulky and heavy to provide benefit during rock climbing.&nbsp;</p><p>The design requirements for this device are that it must be lightweight (&lt; 1.5 kg), low profile, robust, with 2 degrees of freedom of electromyographically controlled movement. The custom designed device consists of 2 non-backdrivable linear actuators in a differential pair, allowing for powered motion in plantarflexion/dorsiflexion and inversion/eversion. Load cells aligned axially with each actuator are used to provide force feedback to the device, allowing for position control during free-space motion, and powering off the actuators when the device is loaded, relying on the non-backdrivable transmission to maintain ankle and foot position while loaded. This control scheme reduces the power requirements of the device, allowing for lighter batteries as well as smaller motors and transmission.&nbsp;</p>",,,2019-04-26 19:02:57.902,True,2017-09-15,Design of a 2-Degree-of-Freedom Powered Ankle-Foot Prosthesis for Rock Climbing,LAB-INSIDERS,,True,Biomechatronics,False
dynamic-interfaces,emrogers,False,"<p>Mechanical, electrical, and dynamic control systems recreate biological behavior with synthetic hardware. <br></p>",,,2019-04-26 19:04:10.662,True,2016-01-01,Mechatronic Systems,PUBLIC,http://biomech.media.mit.edu,True,Biomechatronics,False
embedded-neural-interface-electronics-for-advanced-prosthesis-research,tonyshu,False,"<p>This project aims to develop embedded neural interface electronics for integration on mobile, external prosthetic devices. By pushing the technological limits of every layer within the stack, from semiconductor components to high-level signal processing and classification, we hope to provide practical benefit to people's daily lives in the near future.</p><p>As a first step, we are actively developing a high-fidelity electromyography (EMG) device used to observe human muscle activity. Several prosthetic ankles have already been demonstrated using this portable EMG device as a control system input.&nbsp;</p>",,,2019-04-26 19:01:33.105,True,2017-09-01,Embedded Systems for Neural Interfaces,LAB-INSIDERS,,True,Biomechatronics,False
dynamic-interfaces,tonyshu,False,"<p>Mechanical, electrical, and dynamic control systems recreate biological behavior with synthetic hardware. <br></p>",,,2019-04-26 19:04:10.662,True,2016-01-01,Mechatronic Systems,PUBLIC,http://biomech.media.mit.edu,True,Biomechatronics,False
osseo,tonyshu,False,"<p>Recent advancements in orthopedic implants have made way for a new generation of bionic limbs that attach directly to the skeleton. Leveraging these ""osseointegrated"" implants to pass wires out of the body enables robust, long-term communication with residual muscles and the nervous system. We are exploring the ways in which the improved neural communication afforded by osseointegration can impact the experience of controlling a limb prosthesis.
                    
                </p>",,,2019-04-26 19:06:19.611,True,2016-10-19,An osseointegrated prosthesis with bi-directional neural communication,PUBLIC,,True,Biomechatronics,False
moment-coupled-cantilever-beam-series-elastic-actuator,tonyshu,False,"<p>The design of next-generation bionic ankles and knees aims to improve bionic actuators on all metrics: range of motion, power density, bandwidth, mass, while adopting a futuristic aesthetic. We are pushing the limits of materials and magnetics, combined with new control topologies to enforce a new paradigm in both autonomous and volitional controlled powered prostheses. <br></p>",,,2019-04-26 19:02:31.522,True,2017-03-15,High power bionic joints for dynamic gait actions,PUBLIC,http://matthematic.com,True,Biomechatronics,False
embedded-neural-interface-electronics-for-advanced-prosthesis-research,jfduval,False,"<p>This project aims to develop embedded neural interface electronics for integration on mobile, external prosthetic devices. By pushing the technological limits of every layer within the stack, from semiconductor components to high-level signal processing and classification, we hope to provide practical benefit to people's daily lives in the near future.</p><p>As a first step, we are actively developing a high-fidelity electromyography (EMG) device used to observe human muscle activity. Several prosthetic ankles have already been demonstrated using this portable EMG device as a control system input.&nbsp;</p>",,,2019-04-26 19:01:33.105,True,2017-09-01,Embedded Systems for Neural Interfaces,LAB-INSIDERS,,True,Biomechatronics,False
moment-coupled-cantilever-beam-series-elastic-actuator,jfduval,False,"<p>The design of next-generation bionic ankles and knees aims to improve bionic actuators on all metrics: range of motion, power density, bandwidth, mass, while adopting a futuristic aesthetic. We are pushing the limits of materials and magnetics, combined with new control topologies to enforce a new paradigm in both autonomous and volitional controlled powered prostheses. <br></p>",,,2019-04-26 19:02:31.522,True,2017-03-15,High power bionic joints for dynamic gait actions,PUBLIC,http://matthematic.com,True,Biomechatronics,False
embedded-neural-interface-electronics-for-advanced-prosthesis-research,mcarney,False,"<p>This project aims to develop embedded neural interface electronics for integration on mobile, external prosthetic devices. By pushing the technological limits of every layer within the stack, from semiconductor components to high-level signal processing and classification, we hope to provide practical benefit to people's daily lives in the near future.</p><p>As a first step, we are actively developing a high-fidelity electromyography (EMG) device used to observe human muscle activity. Several prosthetic ankles have already been demonstrated using this portable EMG device as a control system input.&nbsp;</p>",,,2019-04-26 19:01:33.105,True,2017-09-01,Embedded Systems for Neural Interfaces,LAB-INSIDERS,,True,Biomechatronics,False
ankle-foot-prosthesis-for-rock-climbing,mcarney,False,"<p>Lower extremity amputation leads to limitations of biological function of individuals, which leads to challenges remaining physically active and participating in athletic activities. Physical activity is very important for cardiovascular health, weight management, and mental health. Designing devices aimed at increasing accessibility to sports will encourage individuals with amputations to continue or begin participating in athletic pursuits such as rock climbing.&nbsp;</p><p>This research presents the design and evaluation of a 2-degree-of-freedom powered ankle-foot prosthesis for rock climbing. The aim of this device is to restore function of the ankle and subtalar joints for trans-tibial amputees during rock climbing, providing the user with myoelectric position control of the foot. Precise positional control of the foot is especially important while climbing, as the climber’s ability to successfully scale a route requires them to reliably reorient the foot to various shapes and orientations of holds. Passive prostheses do not allow the user to reposition the foot, and current powered prostheses are too bulky and heavy to provide benefit during rock climbing.&nbsp;</p><p>The design requirements for this device are that it must be lightweight (&lt; 1.5 kg), low profile, robust, with 2 degrees of freedom of electromyographically controlled movement. The custom designed device consists of 2 non-backdrivable linear actuators in a differential pair, allowing for powered motion in plantarflexion/dorsiflexion and inversion/eversion. Load cells aligned axially with each actuator are used to provide force feedback to the device, allowing for position control during free-space motion, and powering off the actuators when the device is loaded, relying on the non-backdrivable transmission to maintain ankle and foot position while loaded. This control scheme reduces the power requirements of the device, allowing for lighter batteries as well as smaller motors and transmission.&nbsp;</p>",,,2019-04-26 19:02:57.902,True,2017-09-15,Design of a 2-Degree-of-Freedom Powered Ankle-Foot Prosthesis for Rock Climbing,LAB-INSIDERS,,True,Biomechatronics,False
dynamic-interfaces,mcarney,False,"<p>Mechanical, electrical, and dynamic control systems recreate biological behavior with synthetic hardware. <br></p>",,,2019-04-26 19:04:10.662,True,2016-01-01,Mechatronic Systems,PUBLIC,http://biomech.media.mit.edu,True,Biomechatronics,False
osseo,mcarney,False,"<p>Recent advancements in orthopedic implants have made way for a new generation of bionic limbs that attach directly to the skeleton. Leveraging these ""osseointegrated"" implants to pass wires out of the body enables robust, long-term communication with residual muscles and the nervous system. We are exploring the ways in which the improved neural communication afforded by osseointegration can impact the experience of controlling a limb prosthesis.
                    
                </p>",,,2019-04-26 19:06:19.611,True,2016-10-19,An osseointegrated prosthesis with bi-directional neural communication,PUBLIC,,True,Biomechatronics,False
moment-coupled-cantilever-beam-series-elastic-actuator,mcarney,False,"<p>The design of next-generation bionic ankles and knees aims to improve bionic actuators on all metrics: range of motion, power density, bandwidth, mass, while adopting a futuristic aesthetic. We are pushing the limits of materials and magnetics, combined with new control topologies to enforce a new paradigm in both autonomous and volitional controlled powered prostheses. <br></p>",,,2019-04-26 19:02:31.522,True,2017-03-15,High power bionic joints for dynamic gait actions,PUBLIC,http://matthematic.com,True,Biomechatronics,False
embedded-neural-interface-electronics-for-advanced-prosthesis-research,clites,False,"<p>This project aims to develop embedded neural interface electronics for integration on mobile, external prosthetic devices. By pushing the technological limits of every layer within the stack, from semiconductor components to high-level signal processing and classification, we hope to provide practical benefit to people's daily lives in the near future.</p><p>As a first step, we are actively developing a high-fidelity electromyography (EMG) device used to observe human muscle activity. Several prosthetic ankles have already been demonstrated using this portable EMG device as a control system input.&nbsp;</p>",,,2019-04-26 19:01:33.105,True,2017-09-01,Embedded Systems for Neural Interfaces,LAB-INSIDERS,,True,Biomechatronics,False
electrical-interfaces,clites,False,"<h2>Nerve-Muscle Graft Chamber and micro-channel arrays tor interface to peripheral nerves for prosthesis control.&nbsp;</h2><p>This research effort consists of two sub-projects with the goal to develop a small implantable device for achieving bi-directional communication with the amputated nerves in a prosthesis user’s residuum. The nerve-muscle graft chamber (NMGC) is a small implanted device which contains one or more electrically isolated chambers (ca. 20mm&nbsp;<i>l&nbsp;</i>x 4mm&nbsp;<i>h&nbsp;</i>x 4mm&nbsp;<i>w )</i>&nbsp;that can be filled with muscle or cutaneous tissue. The electrical activities of the components of a compound peripheral nerve that in the intact limb sub-served different motor functions can be separated by mechanically dividing the nerve and placing each isolated nerve segment into apposition with a small piece of muscle tissue in each of the separate chambers of the NMGC.&nbsp; For example, the muscle filled chambers can be ganged together in a modular design so that a single implanted device containing three chambers would interface to motor nerve fascicles that provide prosthesis command signals for three different motor functions. For a mixed peripheral nerve that is known to contain cutaneous fascicles as well as motor fascicles, an additional compartment could be added that contains cutaneous tissue. This would be done to provide an appropriate target for regenerating cutaneous nerve fibers to prevent the cutaneous axons from competing with regenerating motor nerve fibers and errantly taking up residence in the muscle tissues. Also, by provide cutaneous&nbsp; target tissue, regenerating sensory afferent nerve fiber are less likely to result in the formation of potentially painful &nbsp;neuromas.</p><p>The second sub-project aims to develop a micro-channel array into which peripheral nerve fibers will grow into. Because the micro-channels are on the order of 100 to 200 um I.D., only a small number of nerve fibers will be present in an individual micro-channel. This can potentially provide greater separation of axons by their functionality. Such separation by function is important when seeking to provide cutaneous and proprioceptive feedback by means of direct electrical activation of the sensory components of the interfaced peripheral nerves.&nbsp;&nbsp;</p>",,,2019-04-26 19:04:39.951,True,2016-01-04,Neural Interfaces,PUBLIC,,True,Biomechatronics,False
amputation,clites,False,"<p>Lower-extremity amputation surgery has not seen significant change since the Civil War. This research is focused on the development of novel amputation paradigms that leverage native biological end organs to interpret efferent motor commands and to provide meaningful neural feedback from an artificial limb. Surgical replication of natural agonist-antagonist muscle pairings within the residuum allow us to use biomimetic constructs to communicate joint state and torque from the prosthesis directly to the peripheral nervous system. We hypothesize that these architectures will facilitate control of advanced prosthetic systems to improve gait and reduce metabolic cost of transport.
                    
                </p>",,,2019-04-26 19:05:58.968,True,2016-08-15,Revolutionizing amputation surgery for the restoration of natural neural sensation and mobility,PUBLIC,,True,Biomechatronics,False
osseo,clites,False,"<p>Recent advancements in orthopedic implants have made way for a new generation of bionic limbs that attach directly to the skeleton. Leveraging these ""osseointegrated"" implants to pass wires out of the body enables robust, long-term communication with residual muscles and the nervous system. We are exploring the ways in which the improved neural communication afforded by osseointegration can impact the experience of controlling a limb prosthesis.
                    
                </p>",,,2019-04-26 19:06:19.611,True,2016-10-19,An osseointegrated prosthesis with bi-directional neural communication,PUBLIC,,True,Biomechatronics,False
agonist-antagonist-myoneural-interface-ami,clites,False,"<h2><b>Humans can accurately sense the position, speed, and torque of their limbs, even with their eyes shut. This sense, known as proprioception, allows humans to precisely control their body movements. </b></h2><p>Today’s conventional prosthetic limbs do not provide feedback to the nervous system. Because of this, people with amputated limbs cannot feel the position, speed, and torque of their prosthetic joints without looking at them, making it difficult to control their movement. In order to create a more complete prosthetic control experience, researchers at the Center for Extreme Bionics at the MIT Media Lab invented the&nbsp;<b>agonist-antagonist myoneural interface (AMI)</b>. The AMI is a method to restore proprioception to persons with amputation.</p>",,,2018-08-17 16:20:19.891,True,2014-06-01,Agonist-antagonist Myoneural Interface (AMI),PUBLIC,,True,Biomechatronics,False
embedded-neural-interface-electronics-for-advanced-prosthesis-research,syeon,False,"<p>This project aims to develop embedded neural interface electronics for integration on mobile, external prosthetic devices. By pushing the technological limits of every layer within the stack, from semiconductor components to high-level signal processing and classification, we hope to provide practical benefit to people's daily lives in the near future.</p><p>As a first step, we are actively developing a high-fidelity electromyography (EMG) device used to observe human muscle activity. Several prosthetic ankles have already been demonstrated using this portable EMG device as a control system input.&nbsp;</p>",,,2019-04-26 19:01:33.105,True,2017-09-01,Embedded Systems for Neural Interfaces,LAB-INSIDERS,,True,Biomechatronics,False
ankle-foot-prosthesis-for-rock-climbing,syeon,False,"<p>Lower extremity amputation leads to limitations of biological function of individuals, which leads to challenges remaining physically active and participating in athletic activities. Physical activity is very important for cardiovascular health, weight management, and mental health. Designing devices aimed at increasing accessibility to sports will encourage individuals with amputations to continue or begin participating in athletic pursuits such as rock climbing.&nbsp;</p><p>This research presents the design and evaluation of a 2-degree-of-freedom powered ankle-foot prosthesis for rock climbing. The aim of this device is to restore function of the ankle and subtalar joints for trans-tibial amputees during rock climbing, providing the user with myoelectric position control of the foot. Precise positional control of the foot is especially important while climbing, as the climber’s ability to successfully scale a route requires them to reliably reorient the foot to various shapes and orientations of holds. Passive prostheses do not allow the user to reposition the foot, and current powered prostheses are too bulky and heavy to provide benefit during rock climbing.&nbsp;</p><p>The design requirements for this device are that it must be lightweight (&lt; 1.5 kg), low profile, robust, with 2 degrees of freedom of electromyographically controlled movement. The custom designed device consists of 2 non-backdrivable linear actuators in a differential pair, allowing for powered motion in plantarflexion/dorsiflexion and inversion/eversion. Load cells aligned axially with each actuator are used to provide force feedback to the device, allowing for position control during free-space motion, and powering off the actuators when the device is loaded, relying on the non-backdrivable transmission to maintain ankle and foot position while loaded. This control scheme reduces the power requirements of the device, allowing for lighter batteries as well as smaller motors and transmission.&nbsp;</p>",,,2019-04-26 19:02:57.902,True,2017-09-15,Design of a 2-Degree-of-Freedom Powered Ankle-Foot Prosthesis for Rock Climbing,LAB-INSIDERS,,True,Biomechatronics,False
dynamic-interfaces,syeon,False,"<p>Mechanical, electrical, and dynamic control systems recreate biological behavior with synthetic hardware. <br></p>",,,2019-04-26 19:04:10.662,True,2016-01-01,Mechatronic Systems,PUBLIC,http://biomech.media.mit.edu,True,Biomechatronics,False
electrical-interfaces,syeon,False,"<h2>Nerve-Muscle Graft Chamber and micro-channel arrays tor interface to peripheral nerves for prosthesis control.&nbsp;</h2><p>This research effort consists of two sub-projects with the goal to develop a small implantable device for achieving bi-directional communication with the amputated nerves in a prosthesis user’s residuum. The nerve-muscle graft chamber (NMGC) is a small implanted device which contains one or more electrically isolated chambers (ca. 20mm&nbsp;<i>l&nbsp;</i>x 4mm&nbsp;<i>h&nbsp;</i>x 4mm&nbsp;<i>w )</i>&nbsp;that can be filled with muscle or cutaneous tissue. The electrical activities of the components of a compound peripheral nerve that in the intact limb sub-served different motor functions can be separated by mechanically dividing the nerve and placing each isolated nerve segment into apposition with a small piece of muscle tissue in each of the separate chambers of the NMGC.&nbsp; For example, the muscle filled chambers can be ganged together in a modular design so that a single implanted device containing three chambers would interface to motor nerve fascicles that provide prosthesis command signals for three different motor functions. For a mixed peripheral nerve that is known to contain cutaneous fascicles as well as motor fascicles, an additional compartment could be added that contains cutaneous tissue. This would be done to provide an appropriate target for regenerating cutaneous nerve fibers to prevent the cutaneous axons from competing with regenerating motor nerve fibers and errantly taking up residence in the muscle tissues. Also, by provide cutaneous&nbsp; target tissue, regenerating sensory afferent nerve fiber are less likely to result in the formation of potentially painful &nbsp;neuromas.</p><p>The second sub-project aims to develop a micro-channel array into which peripheral nerve fibers will grow into. Because the micro-channels are on the order of 100 to 200 um I.D., only a small number of nerve fibers will be present in an individual micro-channel. This can potentially provide greater separation of axons by their functionality. Such separation by function is important when seeking to provide cutaneous and proprioceptive feedback by means of direct electrical activation of the sensory components of the interfaced peripheral nerves.&nbsp;&nbsp;</p>",,,2019-04-26 19:04:39.951,True,2016-01-04,Neural Interfaces,PUBLIC,,True,Biomechatronics,False
osseo,syeon,False,"<p>Recent advancements in orthopedic implants have made way for a new generation of bionic limbs that attach directly to the skeleton. Leveraging these ""osseointegrated"" implants to pass wires out of the body enables robust, long-term communication with residual muscles and the nervous system. We are exploring the ways in which the improved neural communication afforded by osseointegration can impact the experience of controlling a limb prosthesis.
                    
                </p>",,,2019-04-26 19:06:19.611,True,2016-10-19,An osseointegrated prosthesis with bi-directional neural communication,PUBLIC,,True,Biomechatronics,False
moment-coupled-cantilever-beam-series-elastic-actuator,syeon,False,"<p>The design of next-generation bionic ankles and knees aims to improve bionic actuators on all metrics: range of motion, power density, bandwidth, mass, while adopting a futuristic aesthetic. We are pushing the limits of materials and magnetics, combined with new control topologies to enforce a new paradigm in both autonomous and volitional controlled powered prostheses. <br></p>",,,2019-04-26 19:02:31.522,True,2017-03-15,High power bionic joints for dynamic gait actions,PUBLIC,http://matthematic.com,True,Biomechatronics,False
embedded-neural-interface-electronics-for-advanced-prosthesis-research,lfreed,False,"<p>This project aims to develop embedded neural interface electronics for integration on mobile, external prosthetic devices. By pushing the technological limits of every layer within the stack, from semiconductor components to high-level signal processing and classification, we hope to provide practical benefit to people's daily lives in the near future.</p><p>As a first step, we are actively developing a high-fidelity electromyography (EMG) device used to observe human muscle activity. Several prosthetic ankles have already been demonstrated using this portable EMG device as a control system input.&nbsp;</p>",,,2019-04-26 19:01:33.105,True,2017-09-01,Embedded Systems for Neural Interfaces,LAB-INSIDERS,,True,Biomechatronics,False
electrical-interfaces,lfreed,False,"<h2>Nerve-Muscle Graft Chamber and micro-channel arrays tor interface to peripheral nerves for prosthesis control.&nbsp;</h2><p>This research effort consists of two sub-projects with the goal to develop a small implantable device for achieving bi-directional communication with the amputated nerves in a prosthesis user’s residuum. The nerve-muscle graft chamber (NMGC) is a small implanted device which contains one or more electrically isolated chambers (ca. 20mm&nbsp;<i>l&nbsp;</i>x 4mm&nbsp;<i>h&nbsp;</i>x 4mm&nbsp;<i>w )</i>&nbsp;that can be filled with muscle or cutaneous tissue. The electrical activities of the components of a compound peripheral nerve that in the intact limb sub-served different motor functions can be separated by mechanically dividing the nerve and placing each isolated nerve segment into apposition with a small piece of muscle tissue in each of the separate chambers of the NMGC.&nbsp; For example, the muscle filled chambers can be ganged together in a modular design so that a single implanted device containing three chambers would interface to motor nerve fascicles that provide prosthesis command signals for three different motor functions. For a mixed peripheral nerve that is known to contain cutaneous fascicles as well as motor fascicles, an additional compartment could be added that contains cutaneous tissue. This would be done to provide an appropriate target for regenerating cutaneous nerve fibers to prevent the cutaneous axons from competing with regenerating motor nerve fibers and errantly taking up residence in the muscle tissues. Also, by provide cutaneous&nbsp; target tissue, regenerating sensory afferent nerve fiber are less likely to result in the formation of potentially painful &nbsp;neuromas.</p><p>The second sub-project aims to develop a micro-channel array into which peripheral nerve fibers will grow into. Because the micro-channels are on the order of 100 to 200 um I.D., only a small number of nerve fibers will be present in an individual micro-channel. This can potentially provide greater separation of axons by their functionality. Such separation by function is important when seeking to provide cutaneous and proprioceptive feedback by means of direct electrical activation of the sensory components of the interfaced peripheral nerves.&nbsp;&nbsp;</p>",,,2019-04-26 19:04:39.951,True,2016-01-04,Neural Interfaces,PUBLIC,,True,Biomechatronics,False
transdermal-optogenetic-peripheral-nerve-stimulation,lfreed,False,"<p>
                    Optogenetic techniques have recently been applied to peripheral nerves as a scientific tool with the translatable goal of alleviating a variety of disorders, including chronic pain, muscle fatigue, glucose-related pathologies, and others.  When compared to the electrical stimulation of peripheral nerves, there are numerous advantages: the ability to target molecularly defined subtypes, access to opsins engendering neural inhibition, and optical recruitment of motor axons in a fashion that mimics natural recruitment, which eliminates the fatigue roadblock inherent to functional electrical stimulation. The ability to control peripheral nerves situated under deep tissue structures with transdermal, optical signals would be of enormous benefit, integrating all of the advantages conferred by optogenetics while averting the drawbacks associated with implantable devices, such as mechanical failure, device tissue heating, and a chronic foreign body response.&nbsp;</p><p>We work to develop novel molecular and optical methods in an effort to enable this transdermal optogenetic peripheral nerve control. A further example of a potential clinical application involves optogenetically targeting the vagus nerve, a peripheral cranial nerve implicated in numerous ailments, including epilepsy, migraines, obesity, hypertension, fibromyalgia, Crohn’s disease, asthma, depression, and obsessive-compulsive disorder.  An efficient method of stimulating the vagus nerve with minimal side-effects and high target specificity, such as described here, may have profound implications to the study of various illnesses and disabilities.</p>",,,2019-04-26 19:05:38.186,True,2015-01-01,Transdermal Optogenetic Peripheral Nerve Stimulation,PUBLIC,,True,Biomechatronics,False
amputation,lfreed,False,"<p>Lower-extremity amputation surgery has not seen significant change since the Civil War. This research is focused on the development of novel amputation paradigms that leverage native biological end organs to interpret efferent motor commands and to provide meaningful neural feedback from an artificial limb. Surgical replication of natural agonist-antagonist muscle pairings within the residuum allow us to use biomimetic constructs to communicate joint state and torque from the prosthesis directly to the peripheral nervous system. We hypothesize that these architectures will facilitate control of advanced prosthetic systems to improve gait and reduce metabolic cost of transport.
                    
                </p>",,,2019-04-26 19:05:58.968,True,2016-08-15,Revolutionizing amputation surgery for the restoration of natural neural sensation and mobility,PUBLIC,,True,Biomechatronics,False
osseo,lfreed,False,"<p>Recent advancements in orthopedic implants have made way for a new generation of bionic limbs that attach directly to the skeleton. Leveraging these ""osseointegrated"" implants to pass wires out of the body enables robust, long-term communication with residual muscles and the nervous system. We are exploring the ways in which the improved neural communication afforded by osseointegration can impact the experience of controlling a limb prosthesis.
                    
                </p>",,,2019-04-26 19:06:19.611,True,2016-10-19,An osseointegrated prosthesis with bi-directional neural communication,PUBLIC,,True,Biomechatronics,False
agonist-antagonist-myoneural-interface-ami,lfreed,False,"<h2><b>Humans can accurately sense the position, speed, and torque of their limbs, even with their eyes shut. This sense, known as proprioception, allows humans to precisely control their body movements. </b></h2><p>Today’s conventional prosthetic limbs do not provide feedback to the nervous system. Because of this, people with amputated limbs cannot feel the position, speed, and torque of their prosthetic joints without looking at them, making it difficult to control their movement. In order to create a more complete prosthetic control experience, researchers at the Center for Extreme Bionics at the MIT Media Lab invented the&nbsp;<b>agonist-antagonist myoneural interface (AMI)</b>. The AMI is a method to restore proprioception to persons with amputation.</p>",,,2018-08-17 16:20:19.891,True,2014-06-01,Agonist-antagonist Myoneural Interface (AMI),PUBLIC,,True,Biomechatronics,False
earthrise-a-50-year-contemplation,drwood,False,"<p>In December of 1968, the first human voyage to the moon catapulted the population of Earth into a new era of space exploration and self-reflection. It was during this voyage that astronauts Bill Anders and Jim Lovell recognized a familiar pale blue dot in the distance and snapped a photo, providing us with the first view of Earth from this distant vantage point. Since its release, this image has been the subject of various works of art and literature. After having seen the Earth from space, some astronauts reported a cognitive shift in awareness about the planet. This shift helped them recognize the fragility of Earth and has inspired feelings of global citizenship. Thanks to the writings of author Frank White, we now call this shift, “The Overview Effect.”<br></p><p>The Media Lab’s Space Enabled research group asks if it is possible to create a similar cognitive shift in Earthlings through an experiential installation piece meant to inspire global citizenship as well as universal citizenship. “Earthrise: A 50 Year Contemplation” will celebrate the original Earthrise photo by creating a meditative space of reflection where participants are transported to the surface of Earth’s moon to reflect on themselves, Earth, and the solar system. The viewer will be immersed in the sensory experience that surrounds them. Through artificially creating “The Overview Effect,” and altering our perspective, might we also inspire a more sustainable approach in our exploration of the solar system?</p><p>Frank White, author of <i>The Overview Effect: Space Exploration and Human Evolution</i>, is launching “The Human Space Program"" through his new book, <i>The Cosma Hypothesis: Implications of the Overview Effect</i> (Emergent Media; February 2019). The goal of the Human Space Program is to create a comprehensive, sustainable, and inclusive plan for exploring and developing the solar system. It is built around White’s “Cosma Hypothesis,” which addresses the question, “What is the purpose of human space exploration? &nbsp;Why has the evolutionary process brought humanity to the brink of becoming a spacefaring species?” White’s surprising conclusion: <i>Homo sapiens</i> have a very significant role to play in the evolution of the universe (Cosma). Space Enabled appreciates the opportunity to dialogue with Frank about these fundamental questions shaping the moral compass of human exploration beyond earth.</p><p><i>By Lizbeth B. De La Torre,&nbsp; Rachael Petersen, Frank White&nbsp; and&nbsp; Danielle Wood</i></p>",,,2019-01-08 17:44:59.020,True,2018-12-23,Earthrise | A 50 Year Contemplation,PUBLIC,,True,Space Enabled,False
accessibility-of-the-microgravity-research-ecosystem,drwood,False,"<p>For decades, the International Space Station (ISS) has operated as a bastion of international cooperation and a unique testbed for microgravity research. Beyond enabling insights into human physiology in space, the ISS has served as a microgravity platform for numerous science experiments. In recent years, private industry has also been affiliating with NASA and international partners to offer transportation, logistics management, and payload demands. As the costs of flying projects to the ISS decrease, the barriers limiting non-traditional partners, like emerging space nations and startups, from accessing the ISS as a platform also decrease.&nbsp; </p><p>However, the ISS in its current form cannot be sustained forever. As NASA looks towards commercialization of the low Earth orbit (LEO) space and the development of a cislunar station, concrete plans for shifting the public-private relationship of the ISS are unclear. With the consistent need to continue microgravity research—from governments and private industry—<b>understanding the socio-technical and policy issues that affect the marketplace for future microgravity platforms is essential to maintaining an accessible and sustainable space economy.&nbsp;</b></p><p>How will the US and other governments design public-private partnerships to pursue economic and social goals in the LEO microgravity ecosystem? What governance structures will influence who is eligible to operate platforms for activities including tourism, research, manufacturing, and outreach? How will international collaboration occur in the future LEO microgravity ecosystem?&nbsp;</p><p>This project contributes to progress on these questions by offering technology policy insight using methods from Systems Engineering. Through case study research and numerous expert interviews, this project examines the stakeholders, needs, objectives, system functions and forms for the ISS and microgravity research platforms now and in the future. Particular attention is paid towards explaining the market dynamics that affect the administrative and economic barriers to entry for emerging space nations and non-traditional spaceflight participants.</p>",,,2019-03-13 17:40:41.842,True,2018-04-01,Accessibility of the microgravity research ecosystem,PUBLIC,,True,Space Enabled,False
technology-design-and-assessment-for-coastal-water-ecosystem-management-a-case-study-of-benin,drwood,False,"<p>All over Africa, experts use satellite Earth Observation (EO) data for applications such as monitoring crop health or assessing the risk of disease vectors. These applications are often done at a national scale meaning there is a challenge to ensure that end users such as small companies, rural communities or otherwise marginalized groups benefit from EO systems. This project explores an EO application with the enterprise Green Keeper Africa (GKA) based in Cotonou, Benin, that addresses the management of an invasive plant species that is threatening local economic activities such as fishing. GKA helps control the infestation of the water hyacinth on Lake Nokoue by repurposing the plant into a product that absorbs oil-based waste. The EO application is an online Environmental Observatory that utilizes satellite, aerial and ground data to map the location of the water hyacinth over time, providing valuable information for government, private and public users. The research outcomes presented in this project address processes that (i) outline the steps for a small company in Benin to setup and operate a new EO technological capability, and (ii) enable low cost data collection of parameters describing the coastal water ecosystem.&nbsp;</p><p>In the observatory, the technique Normalized Difference Vegetation Index (NDVI) is applied to free satellite data to identify likely locations of the hyacinth in the target region of Lake Nokoue.&nbsp;</p>",,,2019-04-22 18:27:41.358,True,2018-08-01,Low-cost invasive species management in coastal ecosystems: A case study in Benin,LAB-INSIDERS,,True,Space Enabled,False
dynamic-interfaces,kenpasch,False,"<p>Mechanical, electrical, and dynamic control systems recreate biological behavior with synthetic hardware. <br></p>",,,2019-04-26 19:04:10.662,True,2016-01-01,Mechatronic Systems,PUBLIC,http://biomech.media.mit.edu,True,Biomechatronics,False
dynamic-interfaces,mbweber,False,"<p>Mechanical, electrical, and dynamic control systems recreate biological behavior with synthetic hardware. <br></p>",,,2019-04-26 19:04:10.662,True,2016-01-01,Mechatronic Systems,PUBLIC,http://biomech.media.mit.edu,True,Biomechatronics,False
metabolic-and-biomechanical-evaluation-of-walking-with-an-autonomous-exoskeleton-on-sloped-terrain,mbweber,False,"<p>We have developed an autonomous powered exoskeleton capable of providing a significant metabolic benefit to the user when walking on level ground. To date, this device has demonstrated the greatest published metabolic benefit for an exoskeleton, even outperforming some exoskeletons that rely on a tethered power supply. To gain a better sense of the practicality and versatility of this system, an assessment of metabolic and biomechanical performance on uneven ground is warranted. To accomplish this, the performance of the Biomechatronics exoskeleton will be evaluated through measurement of human respiratory rate while walking on an instrumented treadmill. Furthermore, EMG and motion-capture data will be collected to provide insight as to the biomechanical basis of observed metabolic changes. This study aims to further demonstrate the practicality of an autonomous powered exoskeleton for military, recreational, rehabilitative, or other use.</p>",,--Choose Location,2019-04-26 19:06:47.182,True,2016-02-01,Metabolic and Biomechanical Evaluation of Walking with an Autonomous Exoskeleton on Sloped Terrain,LAB-INSIDERS,,True,Biomechatronics,False
dynamic-interfaces,kuan525,False,"<p>Mechanical, electrical, and dynamic control systems recreate biological behavior with synthetic hardware. <br></p>",,,2019-04-26 19:04:10.662,True,2016-01-01,Mechatronic Systems,PUBLIC,http://biomech.media.mit.edu,True,Biomechatronics,False
dynamic-interfaces,romka,False,"<p>Mechanical, electrical, and dynamic control systems recreate biological behavior with synthetic hardware. <br></p>",,,2019-04-26 19:04:10.662,True,2016-01-01,Mechatronic Systems,PUBLIC,http://biomech.media.mit.edu,True,Biomechatronics,False
terrain-adaptive-lower-limb-prosthesis,romka,False,"<p>Although there have been great advances in the control of lower extremity prostheses, transitioning between terrains such as ramps or stairs remains a major challenge for the field. The mobility of leg amputees is thus limited, impacting their quality of life and independence. This projects aims to solve this problem by designing, implementing, and integrating a combined terrain-adaptive and volitional controller for powered lower limb prostheses. The controller will be able to predict terrain changes using data from both intrinsic sensors and electromyography (EMG) signals from the user; adapt the ankle position before footfall in a biologically accurate manner; and provide a torque profile consistent with biological ankle kinetics during stance. The result will allow amputees to traverse and transition among flat ground, stairs, and slopes of varying grade with lower energy and pain, greater balance, and without manually changing the walking mode of their prosthesis.</p>",,--Choose Location,2019-04-26 19:07:36.576,True,2014-09-01,Terrain-Adaptive Lower Limb Prosthesis,PUBLIC,,True,Biomechatronics,False
biologic,guanyun,False,"<p>Cells’ biomechanical responses to external stimuli have been intensively studied but rarely implemented into devices&nbsp;<span style=""font-size: 18px; font-weight: 400;"">that interact with the human body. We demonstrate that the hygroscopic and biofluorescent behaviors of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">living cells can be engineered to design biohybrid wearables, which give multifunctional responsiveness to hu</span><span style=""font-size: 18px; font-weight: 400;"">man sweat. By depositing genetically tractable microbes on a humidity-inert material to form a heterogeneous&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">multilayered structure, we obtained biohybrid films that can reversibly change shape and biofluorescence intensity&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">within a few seconds in response to environmental humidity gradients. Experimental characterization and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">mechanical modeling of the film were performed to guide the design of a wearable running suit and a fluorescent&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">shoe prototype with bio-flaps that dynamically modulates ventilation in synergy with the body’s need for cooling.</span></p>",,--Choose Location,2018-05-04 15:32:39.880,True,2014-01-01,bioLogic—Science Advances,PUBLIC,,True,Tangible Media,False
biologic,oksana,False,"<p>Cells’ biomechanical responses to external stimuli have been intensively studied but rarely implemented into devices&nbsp;<span style=""font-size: 18px; font-weight: 400;"">that interact with the human body. We demonstrate that the hygroscopic and biofluorescent behaviors of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">living cells can be engineered to design biohybrid wearables, which give multifunctional responsiveness to hu</span><span style=""font-size: 18px; font-weight: 400;"">man sweat. By depositing genetically tractable microbes on a humidity-inert material to form a heterogeneous&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">multilayered structure, we obtained biohybrid films that can reversibly change shape and biofluorescence intensity&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">within a few seconds in response to environmental humidity gradients. Experimental characterization and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">mechanical modeling of the film were performed to guide the design of a wearable running suit and a fluorescent&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">shoe prototype with bio-flaps that dynamically modulates ventilation in synergy with the body’s need for cooling.</span></p>",,--Choose Location,2018-05-04 15:32:39.880,True,2014-01-01,bioLogic—Science Advances,PUBLIC,,True,Tangible Media,False
biologic,steinerh,False,"<p>Cells’ biomechanical responses to external stimuli have been intensively studied but rarely implemented into devices&nbsp;<span style=""font-size: 18px; font-weight: 400;"">that interact with the human body. We demonstrate that the hygroscopic and biofluorescent behaviors of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">living cells can be engineered to design biohybrid wearables, which give multifunctional responsiveness to hu</span><span style=""font-size: 18px; font-weight: 400;"">man sweat. By depositing genetically tractable microbes on a humidity-inert material to form a heterogeneous&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">multilayered structure, we obtained biohybrid films that can reversibly change shape and biofluorescence intensity&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">within a few seconds in response to environmental humidity gradients. Experimental characterization and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">mechanical modeling of the film were performed to guide the design of a wearable running suit and a fluorescent&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">shoe prototype with bio-flaps that dynamically modulates ventilation in synergy with the body’s need for cooling.</span></p>",,--Choose Location,2018-05-04 15:32:39.880,True,2014-01-01,bioLogic—Science Advances,PUBLIC,,True,Tangible Media,False
biologic,chinyich,False,"<p>Cells’ biomechanical responses to external stimuli have been intensively studied but rarely implemented into devices&nbsp;<span style=""font-size: 18px; font-weight: 400;"">that interact with the human body. We demonstrate that the hygroscopic and biofluorescent behaviors of&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">living cells can be engineered to design biohybrid wearables, which give multifunctional responsiveness to hu</span><span style=""font-size: 18px; font-weight: 400;"">man sweat. By depositing genetically tractable microbes on a humidity-inert material to form a heterogeneous&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">multilayered structure, we obtained biohybrid films that can reversibly change shape and biofluorescence intensity&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">within a few seconds in response to environmental humidity gradients. Experimental characterization and&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">mechanical modeling of the film were performed to guide the design of a wearable running suit and a fluorescent&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">shoe prototype with bio-flaps that dynamically modulates ventilation in synergy with the body’s need for cooling.</span></p>",,--Choose Location,2018-05-04 15:32:39.880,True,2014-01-01,bioLogic—Science Advances,PUBLIC,,True,Tangible Media,False
aeromorph,chinyich,False,"<p>The project investigates how to make origami structure with inflatables with various materials. We introduce a universal bending mechanism that creates programmable shape-changing behaviors with paper, plastics, and fabrics. We developed a software tool that generates this bending mechanism for a given geometry, simulates its transformation, and exports the compound geometry as digital fabrication files. A custom heat-sealing head that can be mounted on usual three-axis CNC machines to precisely fabricate the designed transforming material is presented. We envision this technology could be used for designing interactive wearables and toys, and for the packaging industry.
                    
                </p><p>Visit&nbsp;<a href=""http://tangible.media.mit.edu/project/aeromorph/"">http://tangible.media.mit.edu/project/aeromorph/</a>.<br></p><p>Honorable Mention Paper Award, UIST 2016</p>",,,2017-04-24 18:56:38.724,True,2016-11-01,aeroMorph,PUBLIC,http://tangible.media.mit.edu/project/aeromorph/,True,Tangible Media,False
electrical-interfaces,crtaylor,False,"<h2>Nerve-Muscle Graft Chamber and micro-channel arrays tor interface to peripheral nerves for prosthesis control.&nbsp;</h2><p>This research effort consists of two sub-projects with the goal to develop a small implantable device for achieving bi-directional communication with the amputated nerves in a prosthesis user’s residuum. The nerve-muscle graft chamber (NMGC) is a small implanted device which contains one or more electrically isolated chambers (ca. 20mm&nbsp;<i>l&nbsp;</i>x 4mm&nbsp;<i>h&nbsp;</i>x 4mm&nbsp;<i>w )</i>&nbsp;that can be filled with muscle or cutaneous tissue. The electrical activities of the components of a compound peripheral nerve that in the intact limb sub-served different motor functions can be separated by mechanically dividing the nerve and placing each isolated nerve segment into apposition with a small piece of muscle tissue in each of the separate chambers of the NMGC.&nbsp; For example, the muscle filled chambers can be ganged together in a modular design so that a single implanted device containing three chambers would interface to motor nerve fascicles that provide prosthesis command signals for three different motor functions. For a mixed peripheral nerve that is known to contain cutaneous fascicles as well as motor fascicles, an additional compartment could be added that contains cutaneous tissue. This would be done to provide an appropriate target for regenerating cutaneous nerve fibers to prevent the cutaneous axons from competing with regenerating motor nerve fibers and errantly taking up residence in the muscle tissues. Also, by provide cutaneous&nbsp; target tissue, regenerating sensory afferent nerve fiber are less likely to result in the formation of potentially painful &nbsp;neuromas.</p><p>The second sub-project aims to develop a micro-channel array into which peripheral nerve fibers will grow into. Because the micro-channels are on the order of 100 to 200 um I.D., only a small number of nerve fibers will be present in an individual micro-channel. This can potentially provide greater separation of axons by their functionality. Such separation by function is important when seeking to provide cutaneous and proprioceptive feedback by means of direct electrical activation of the sensory components of the interfaced peripheral nerves.&nbsp;&nbsp;</p>",,,2019-04-26 19:04:39.951,True,2016-01-04,Neural Interfaces,PUBLIC,,True,Biomechatronics,False
electrical-interfaces,rriso,False,"<h2>Nerve-Muscle Graft Chamber and micro-channel arrays tor interface to peripheral nerves for prosthesis control.&nbsp;</h2><p>This research effort consists of two sub-projects with the goal to develop a small implantable device for achieving bi-directional communication with the amputated nerves in a prosthesis user’s residuum. The nerve-muscle graft chamber (NMGC) is a small implanted device which contains one or more electrically isolated chambers (ca. 20mm&nbsp;<i>l&nbsp;</i>x 4mm&nbsp;<i>h&nbsp;</i>x 4mm&nbsp;<i>w )</i>&nbsp;that can be filled with muscle or cutaneous tissue. The electrical activities of the components of a compound peripheral nerve that in the intact limb sub-served different motor functions can be separated by mechanically dividing the nerve and placing each isolated nerve segment into apposition with a small piece of muscle tissue in each of the separate chambers of the NMGC.&nbsp; For example, the muscle filled chambers can be ganged together in a modular design so that a single implanted device containing three chambers would interface to motor nerve fascicles that provide prosthesis command signals for three different motor functions. For a mixed peripheral nerve that is known to contain cutaneous fascicles as well as motor fascicles, an additional compartment could be added that contains cutaneous tissue. This would be done to provide an appropriate target for regenerating cutaneous nerve fibers to prevent the cutaneous axons from competing with regenerating motor nerve fibers and errantly taking up residence in the muscle tissues. Also, by provide cutaneous&nbsp; target tissue, regenerating sensory afferent nerve fiber are less likely to result in the formation of potentially painful &nbsp;neuromas.</p><p>The second sub-project aims to develop a micro-channel array into which peripheral nerve fibers will grow into. Because the micro-channels are on the order of 100 to 200 um I.D., only a small number of nerve fibers will be present in an individual micro-channel. This can potentially provide greater separation of axons by their functionality. Such separation by function is important when seeking to provide cutaneous and proprioceptive feedback by means of direct electrical activation of the sensory components of the interfaced peripheral nerves.&nbsp;&nbsp;</p>",,,2019-04-26 19:04:39.951,True,2016-01-04,Neural Interfaces,PUBLIC,,True,Biomechatronics,True
sample-project,slotnick,False,"<p>This is where you would type or paste in a project description. Please remember that a lay audience will be reading this, so consider rephrasing language from your abstract to be more accessible.</p>",,,2017-06-28 18:48:50.435,True,2017-06-28,Sample Project,LAB,,True,Other,False
test-project-for-people-page,slotnick,False,,,,2018-06-27 16:47:05.352,False,2018-05-21,test project for people page,PUBLIC,,True,Other,False
rampage,slotnick,False,"<p>Beard dreamcatcher knausgaard godard, four loko wayfarers cloud bread. Pour-over echo park vinyl franzen listicle tumeric chicharrones. Heirloom microdosing intelligentsia deep v chartreuse, kinfolk photo booth. Everyday carry gentrify DIY, freegan snackwave taxidermy listicle trust fund brooklyn direct trade.</p><p>Mumblecore sriracha humblebrag +1, brooklyn pabst kickstarter. Quinoa la croix edison bulb chia, lyft selvage farm-to-table biodiesel shaman 3 wolf moon narwhal woke sriracha. Meh kinfolk mustache vaporware kogi air plant hexagon. Tbh bicycle rights vaporware deep v humblebrag..</p>",,,2018-05-22 18:59:03.439,True,2018-05-01,RAMPage,LAB,http://joi.ito.com/,True,Other,False
sample-project,jliberty,False,"<p>This is where you would type or paste in a project description. Please remember that a lay audience will be reading this, so consider rephrasing language from your abstract to be more accessible.</p>",,,2017-06-28 18:48:50.435,True,2017-06-28,Sample Project,LAB,,True,Other,False
test-project-1,jliberty,False,<p>This is to show people how it's done</p><p><br></p><p>Learn more here</p>,,,2017-10-03 19:11:00.248,True,2017-10-03,Test project,LAB,,True,Other,False
test-project-ii,jliberty,False,,,,2017-10-03 19:30:16.420,True,2017-10-03,Test project II,LAB,,True,Other,False
rampage,jliberty,False,"<p>Beard dreamcatcher knausgaard godard, four loko wayfarers cloud bread. Pour-over echo park vinyl franzen listicle tumeric chicharrones. Heirloom microdosing intelligentsia deep v chartreuse, kinfolk photo booth. Everyday carry gentrify DIY, freegan snackwave taxidermy listicle trust fund brooklyn direct trade.</p><p>Mumblecore sriracha humblebrag +1, brooklyn pabst kickstarter. Quinoa la croix edison bulb chia, lyft selvage farm-to-table biodiesel shaman 3 wolf moon narwhal woke sriracha. Meh kinfolk mustache vaporware kogi air plant hexagon. Tbh bicycle rights vaporware deep v humblebrag..</p>",,,2018-05-22 18:59:03.439,True,2018-05-01,RAMPage,LAB,http://joi.ito.com/,True,Other,False
porphyrin-imaging,kla11,False,"<p>We built a low-cost and open source 405 nm imaging device to capture red fluorescence signatures associated with the oral biomarker porphyrin, demonstrating comparable performance to an expensive commercially available device.&nbsp; We also provide a miniaturized mobile-adaptable version of the device. A step-by-step guide for device assembly and the&nbsp;associated computer vision algorithm are shared on the project website&nbsp;to facilitate open-source access to imaging technologies.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/machine-learning-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images/overview/"">Machine Learning for Combined Classification of Fluorescent Biomarkers and Expert Annotations Using White Light Images</a></li></ol>",,,2018-05-04 20:56:15.953,True,2015-06-01,Biomarker Imaging with Mobile Phones,PUBLIC,,True,Other,False
near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging,kla11,False,"<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Cone beam computed tomography (CBCT) is also widely used for diagnosis but is expensive and relatively cumbersome. Near-infrared imaging (NIR) offers a non-ionizing alternative for dental analysis. We examine and compare features in multiple extracted teeth using conventional radiographic, CBCT, and NIR transillumination imaging modes. NIR imaging can provide unique diagnostic value, primarily in its ability to reveal the extent of surface demineralization. We also provide examples where NIR illumination indicated underlying problem sites in need of further clinical attention and propose the use of NIR imaging to guide targeted and rational use of ionizing radiation in patients.</p><p><strong>Why is this work important?</strong></p><p>Two-dimensional radiographs and cone beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong><br></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. Much previous work has focused on light at 1310 nm, which strikes a balance between enamel and water attenuation, but such a wavelength often requires expensive sensors to image. NIR light at 850 nm has similar dental imaging properties, but it has not been studied as thoroughly as NIR at 1310 nm. It is not well understood what clinical features, if any, are present in NIR dental images, especially at 850 nm. Our previous work has examined the sensitivity of 850 nm NIR images to early caries lesions, but if NIR is to synergistically augment X-rays and CBCT as the standard of care, we must evaluate how well such images represent other clinical features.</p><p><strong>What are our contributions?</strong></p><p>We examine and compare features in multiple extracted teeth using conventional radiographic, CBCT, and NIR transillumination modes. NIR imaging can provide unique diagnostic value, primarily in its ability to reveal the extent of surface demineralization. We also provide examples where NIR illumination indicated underlying problem sites in need of further clinical attention and propose the use of NIR imaging to guide targeted and rational use of ionizing radiation in patients. We also show that NIR imaging identifies clinical features associated with early dimineralization and enamel caries that are not apparent upon expert visual examination.</p><p><strong>What are the next steps?</strong></p><p>Ongoing work is being done to model the interaction of light inside the tooth in order to provide even more diagnostic power.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth/overview/"">Near-Infrared Imaging for Detecting Dental Caries</a></li><li><a href=""https://www.media.mit.edu/projects/replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis/overview/"">Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography</a></li></ol>",,,2018-10-21 18:35:38.823,True,2017-05-08,Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging,PUBLIC,,True,Other,False
near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth,kla11,False,"<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Near-infrared imaging offers a non-ionizing alternative for dental analysis. We describe the construction and validation of a near-infrared imaging device to identify dental caries without the use of radiographs. It uses 850 nm light, allowing for a low-cost sensor and device construction.</p><p><strong>Why is this work important?</strong></p><p>While two-dimensional radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. Much previous work has focused on light at 1310 nm, which strikes a balance between enamel and water attenuation, but such a wavelength often requires expensive sensors to image. NIR light at 850 nm has similar dental imaging properties, but it has not been studied as thoroughly as NIR at 1310 nm. Previous studies have similarly neglected the extent to which indicators of dental health, especially early caries associated with the onset of more severe conditions, can be identified in 850 nm NIR images.</p><p><strong>What are our contributions?</strong></p><p>We describe the construction of a near-infrared imaging device to identify dental caries without the use of radiographs. Light-emitting diodes at 850 nm allow for the use of a low-cost imaging sensor. Its camera-wand design allows for multiple imaging configurations: reflectance, transillumination, and occlusal transillumination. We validate the diagnostic uses for the images produced by our device, determining that they provide insight into the location of caries without ionizing radiation. The camera-wand system was also capable of revealing demineralized areas, deep and superficial cracks, and other clinical features of teeth usually only visualized by X-rays.</p><p><strong>What are the next steps?</strong></p><p>Ongoing work is being done to analyze the extent of features made visible by our device and to model the interaction of light inside teeth in order to provide even more diagnostic power.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging/overview/"">Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging</a></li><li><a href=""https://www.media.mit.edu/projects/replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis/overview/"">Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography</a></li></ol>",,,2018-10-21 18:23:04.788,True,2016-06-01,Near-Infrared Imaging for Detecting Dental Caries,PUBLIC,,True,Other,False
machine-learning-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images,kla11,False,"<p>Biomarker imaging provides non-invasive indicators of disease and is used by human experts to augment disease diagnosis. It is, however, often expensive and reliant on experts to interpret the resulting images. We have developed a process for learning associations between standard white light images and both biomarkers and expert annotations of disease. Oral imaging is one particular example of biomarker imaging that can supplement expert knowledge; the biomarker porphyrin is associated with poor oral health and oral cancer. We report that our process learns to accurately predict the presence of porphyrin and expert-annotated conditions.</p><p><strong>Why is this work important?</strong></p><p>Biomarker imaging provides non-invasive indicators of disease and is used by human experts to augment disease diagnosis. Capturing biomarker images requires specialized and often expensive hardware, annotations, and analyses by experts, resulting in substantial diagnosis delays.</p><p><strong>What has been done before?</strong></p><p>Even when biomarker imaging is available, experts are often needed to interpret the resulting images. There is a rich literature on medical image segmentation, but many approaches—especially deep learning—require large amounts of images and operate on information from only a single given imaging modality.</p><p><strong>What are our contributions?</strong></p><p>We successfully learn assocations between images and union signatures of biomarker presence and expert disease annotations. By transforming the image-level segmentation problem into a region-based problem, we are able to learn from far fewer images than other approaches. We specifically test our approach on detecting the biomarker porphyrin and associated conditions in millions of image patches. Once trained, the classifiers predict the location of porphyrin in images without requiring specialized biomarker imaging devices or expert intervention.</p><p><strong>What are the next steps?</strong></p><p>We are developing processes incorporating numerous other biomarkers, conditions, and imaging modalities.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/porphyrin-imaging/overview/"">Biomarker Imaging with Mobile Phones</a></li></ol>",,,2018-08-15 19:45:24.252,True,2017-02-01,Machine Learning for Combined Classification of Fluorescent Biomarkers and Expert Annotations Using White Light Images,PUBLIC,,True,Other,False
replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis,kla11,False,"<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Our goal is to augment ultimately replace the prevalence of ionizing and expensive X-ray imaging and cone-beam computed tomography (CBCT) for dental care with near-infrared (NIR) imaging. Translucency of teeth in the NIR range offers non-ionizing and safe detection of dental features. NIR can be used in conjunction with multiple light sources to create three-dimensional images of teeth. By modeling the scattering of photons in teeth, we can effectively see several millimeters inside, providing additional diagnostic value.</p><p><strong>Why is this work important?</strong></p><p>Two-dimensional radiographs and cone-beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. NIR light at 850 nm and 1310 nm, which strike a balance between enamel and water attenuation, have been shown to provide helpful diagnostics that visual examination alone lacks. Our previous work has demonstrated the sensitivity of 850 nm NIR images to early caries lesions and demineralization. For NIR is to synergistically augment or eventually replace ionizing radiation as the standard of care, we aim to expand its diagnostic potential to clinical features that exist beyond the surface of the tooth.</p><p><strong>What are our contributions?</strong></p><p>This is ongoing research. Preliminary work shows promise for augmenting the diagnostic power of NIR by modeling scattering.</p><p><strong>What are the next steps?</strong></p><p>Large-scale screenings can evaluate the effectiveness of our new imaging process.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth/overview/"">Near-Infrared Imaging for Detecting Dental Caries</a></li><li><a href=""https://www.media.mit.edu/projects/near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging/overview/"">Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging</a></li></ol>",,,2018-10-26 16:23:18.913,True,2017-05-08,Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography,PUBLIC,,True,Other,False
empathy-and-the-future-of-experience,patorpey,False,"<p>Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of–as well as long-term commitment to–empathic communication.</p>",,--Choose Location,2019-04-17 19:59:42.795,True,2015-01-01,Empathy and the future of experience,PUBLIC,,True,Opera of the Future,False
fensadense,patorpey,False,"<p>Fensadense is a new work for 10-piece ensemble composed by Tod Machover, commissioned for the Lucerne Festival in summer 2015. The project represents the next generation of hyperinstruments, involving the measurement of relative qualities of many performers where previous systems only looked at a single performer. Off-the-shelf components were used to collect data about movement and muscle tension of each musician. The data was analyzed using the Hyperproduction platform to create meaningful production control for lighting and sound systems based on the connection of the performers, with a focus on qualities such as momentum, connection, and tension of the ensemble as a whole. The project premiered at the Lucerne Festival, and a spring European tour just concluded this May 2016.</p><p class=""""><a style=""font-size: 18px; font-weight: normal;"" href=""http://garrettparrish.com/about/fensadense/"">Fensadense site</a><span style=""font-size: 18px; font-weight: normal;""> created by our former UROPer, Garrett Parrish.</span><br></p><p class="""">Listen to a complete recording of the Lucerne performance <a href=""http://www.wqxr.org/#!/story/listen-tod-machovers-fensadense-hyperinstruments-and-interactive-electronics/"">here</a>.<br></p>",,--Choose Location,2017-04-03 19:36:35.564,True,2015-01-01,Fensadense,PUBLIC,,True,Opera of the Future,False
empathy-and-the-future-of-experience,sovsey,False,"<p>Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of–as well as long-term commitment to–empathic communication.</p>",,--Choose Location,2019-04-17 19:59:42.795,True,2015-01-01,Empathy and the future of experience,PUBLIC,,True,Opera of the Future,False
empathy-and-the-future-of-experience,benb,False,"<p>Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of–as well as long-term commitment to–empathic communication.</p>",,--Choose Location,2019-04-17 19:59:42.795,True,2015-01-01,Empathy and the future of experience,PUBLIC,,True,Opera of the Future,False
fensadense,benb,False,"<p>Fensadense is a new work for 10-piece ensemble composed by Tod Machover, commissioned for the Lucerne Festival in summer 2015. The project represents the next generation of hyperinstruments, involving the measurement of relative qualities of many performers where previous systems only looked at a single performer. Off-the-shelf components were used to collect data about movement and muscle tension of each musician. The data was analyzed using the Hyperproduction platform to create meaningful production control for lighting and sound systems based on the connection of the performers, with a focus on qualities such as momentum, connection, and tension of the ensemble as a whole. The project premiered at the Lucerne Festival, and a spring European tour just concluded this May 2016.</p><p class=""""><a style=""font-size: 18px; font-weight: normal;"" href=""http://garrettparrish.com/about/fensadense/"">Fensadense site</a><span style=""font-size: 18px; font-weight: normal;""> created by our former UROPer, Garrett Parrish.</span><br></p><p class="""">Listen to a complete recording of the Lucerne performance <a href=""http://www.wqxr.org/#!/story/listen-tod-machovers-fensadense-hyperinstruments-and-interactive-electronics/"">here</a>.<br></p>",,--Choose Location,2017-04-03 19:36:35.564,True,2015-01-01,Fensadense,PUBLIC,,True,Opera of the Future,False
nanoscale-imaging-of-biomolecules-in-zero-g,jskang,False,"<p>Expansion microscopy (ExM) is a new imaging modality developed by MIT Media Lab's Synthetic Neurobiology group. ExM allows biomolecules to be imaged at nanoscale resolution on conventional, high-speed, diffraction limited optics by synthesizing a swellable polymer network within the sample and thereby physically separating objects of interest isotropically. In the past, we were able to expand the brain tissue 4-20 times in the lateral dimension, achieving 20-90 nm resolution with affordable optics, including a webcam – achieving ~90nm resolution. Considering that existing optics needed for such high resolutions require bulky, expensive parts subject to misalignment and irreversible damage in a physically demanding condition like a spaceship, ExM has the potential to be the most affordable solution for nanoscale imaging of biomolecules in the reduced gravity environment. We aim to demonstrate its claimed feasibility through the project.</p>",,,2017-11-21 21:01:47.515,True,2017-06-01,Nanoscale Imaging of Biomolecules in Zero G,PUBLIC,,True,Synthetic Neurobiology,False
life-nanomachine-synergism,deblina,False,"<p>Nanoelectronics has the potential to enable radical tools for in-vivo interrogation of our biological systems in order to answer fundamental questions in biology as well as to provide novel technologies by combining diagnostics with automated, therapeutic effects at cellular precision. Realization of this promise, however, will require severe dimensional and power scaling of electronics, which is beyond the physical limitations of conventional nanoelectronics, dealing a hard blow to this dream. Our aim is to develop extremely <b>energy-efficient and ultra-scalable, next-generation nano-machines</b> that overcome these fundamental limitations and can make this dream come true, opening up entirely new avenues that were unthinkable earlier.&nbsp;These devices will possess the capabilities of&nbsp;<strong>energy harvesting</strong>,&nbsp;<strong>wireless communication with systems outside the body</strong>,&nbsp;and can be&nbsp;<strong>remotely controlled</strong>.&nbsp;&nbsp;They will be coated with biomolecules such that they can effectively camouflage and trick the body into thinking that it is a part of its own biological system. Such devices can cause a paradigm shift in life-machine synergism.</p><p>The possibilities with such bioelectronic devices are endless, and we are exploring, among other opportunities, brain activity recording at a large scale with a precision of single neuron, activity recording in spinal cord and peripheral nervous system, monitoring tumor microenvironment, observing response to pathology development or external stimulus at a single cell level, along with integrated functionalities such as stimulation and drug delivery. </p><p>The versatility of electronics is that they are inherently very fast and can be designed according to an engineer’s dream to perform unique functions, which are beyond the capabilities of biology. While our immediate aims are to develop electronic devices for probing and controlling/modulating (for therapeutics) the body and brain, our long-term goal is to achieve seamless integration of nanoelectronics-bio hybrid structures into biological systems to incorporate functionalities not otherwise enabled by biology—thus helping us transcend our biological constraints.&nbsp;</p><p>D. Sarkar, <a href=""https://www.youtube.com/watch?v=GZLKFDWtNX8"">""Could We Soon Augment Our Brains?"", TEDx</a> 2016</p>",,,2018-11-26 20:39:22.417,True,2017-01-01,Life-Nanomachine Synergism,PUBLIC,,True,Synthetic Neurobiology,False
robotics,alims,False,"<p><b>Bubble </b>is a pneumatically actuated wearable system that enables people with hand<br>disabilities to use their own hands to grasp objects without fully bending their fingers. Bubble offers<br>a novel approach to grasping, where slim, ultra-lightweight silicone actuators are attached to the<br>fingers. When the user wishes to grasp an object, the silicone units inflate pneumatically to fill the<br>available space around the object. The inflatable units are interchangeable, can be independently<br>inflated, and can be positioned anywhere on the fingers and in any orientation, thereby enabling a wide&nbsp;variety of grasping gestures.</p>",,,2019-05-06 20:24:23.803,True,2019-01-01,Bubble: Wearable assistive grasping augmentation based on soft inflatables,PUBLIC,,True,Object Based Media,False
airtap,alims,False,"<p>We demonstrate a method for augmenting existing visual interfaces, including 3D and conventional displays, with haptic feedback capabilities, by utilizing a large number of closely spaced vortex-ring generators mounted along the periphery of the display. We present our first prototype of a multimodal interactive interface platform with 16 independently-controlled air-vortex ring&nbsp; generators with one angular degree of freedom each. Our system has applications as an interactive interface, as a research tool, as an automotive control interface, and as a platform for creative expressions.</p>",,,2018-05-04 11:46:32.043,True,2017-02-02,AirTap,PUBLIC,,True,Object Based Media,False
free-space-haptic-feedback-for-3d-displays,alims,False,"<p>What if you could not only see but also feel virtual objects as you interacted with them? This would enable richer and more realistic user experiences. We have designed a low-cost air-vortex generator to provide midair haptic feedback when a user touches virtual objects displayed on holographic, aerial, and other 3D displays. The system consists of a 3D-printed chamber and nozzle, five low-frequency transducers, and a custom-designed driver board. The air-vortex generator can provide localized haptic feedback to a range of over 100cm. With increased driving power and a more optimized nozzle design, this range could be extended to several meters. </p>",,--Choose Location,2018-05-01 01:37:57.436,True,2016-01-01,Free-Space Haptic Feedback for 3D Displays,PUBLIC,,True,Object Based Media,False
modesense,alims,False,"<p>ModeSense is a full stack system that enables indoor environments to become aware of what is happening in them, and then enables the environment to locally inform (offline) all nearby phones and other electronic devices about the most appropriate operating mode in the present time and space.&nbsp; We have developed an ultra-low cost, $5 device that can be installed in conference rooms, lecture halls, movie theaters, homes, and cars, which can dynamically determine the contexts in those areas and then locally broadcast the corresponding mode. Any phones in those areas are then aware of the mode in which they ought to be at that time, and can change their behavior accordingly.</p>",,,2018-05-01 02:35:01.568,True,2016-10-01,ModeSense,PUBLIC,,True,Object Based Media,False
quiesense,alims,False,"<p>What if our mobile devices could sense and then adapt to the spatial, temporal, and social context of their local environments? Imagine if your smartphone was smart enough to know that it should not be ringing loudly when you are in an important meeting, or that it should not be in silent mode when you are trying to find where you have misplaced it at home. We have created an inexpensive secure system that delivers this goal by embedding contextual information into the environment rather than the phone.  In that way, all mobile devices at a given location can detect the broadcasted contextual information using Wi-Fi and change their behavior accordingly,  without requiring any handshake or internet connection. By leveraging the latest and most inexpensive Wi-Fi modules on the market, and by building our own embedded firmware, server-side software, and mobile app, we are able to deploy this system in a secure and massively scalable way. <br></p>",,,2019-04-17 18:20:16.620,True,2016-10-21,QuieSense:  Distributed context-awareness system for Wi-Fi enabled mobile devices,PUBLIC,,True,Object Based Media,False
andorra-innovation,devisj,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>The MIT Media Lab's City Science research group, the University of Andorra, and national and international companies are collaborating in order to bring an innovative ecosystem into the capital of Andorra. This innovation district aims to engage local citizens, researchers, and R&amp;D from the companies in order to build together an Andorran living lab, an ""innovation district"" where national and international companies can test and deploy their products and ideas and cultivate human capital.</p><p><b>Current Projects</b></p><ul><li>Andorra Innovation Space</li><li>Andorra Cultural Heritage</li><li>Drones patterns and flows, collaboration living lab<br></li><li>Young Future</li></ul>",,,2018-07-09 18:49:41.844,True,2016-09-01,Andorra | Innovation,PUBLIC,,True,City Science,False
andorra-dynamic-urban-planning,devisj,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>",,,2019-02-25 15:17:09.063,True,2016-09-01,Andorra | Dynamic Urban Planning,PUBLIC,,True,City Science,False
andorra-living-lab,devisj,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,City Science,False
andorra-mobility,devisj,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile</a></p><p>With no airport or train service, most of the 8 million tourists who visit Andorra each year arrive by car, making traffic management and parking some of the country's most important challenges. We are currently developing different projects spanning from data science to the deployment of autonomous vehicles to help address these issues.<br></p>",,,2017-10-25 05:56:26.309,True,2016-09-01,Andorra | Mobility,PUBLIC,,True,City Science,False
andorra-tourism,devisj,False,"<p><span style=""font-size: 18px; font-weight: 400;""></span><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/""><span style=""font-size: 18px; font-weight: 400;"">View the main City Science Andorra project profile.</span></a><br></p><p><span style=""font-size: 18px; font-weight: 400;"">With more than eight million visitors a year, tourism represents almost 30% of the economy of Andorra. By gathering and analyzing data from social media, call detail records, and wifi, we can understand the country's dynamics of tourism and commerce as well as design interventions that can improve the experience for tourists, encouraging them to visit Andorra more frequently, stay longer, and increase spending.&nbsp;</span><br></p><h2><b>Current Projects</b></h2><ul><li>Event Analysis<br></li><li>Social Network<br></li><li>Location Recommendation system<br></li></ul><p> </p><h2><b>EVENT ANALYSIS</b></h2><p>Based on the analysis of call detail records and social media, the goal of this project is to understand the tourist behaviors in Andorra.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">After mining those anonymized data, we have been able to learn different patterns and behaviors of the tourism in Andorra thanks to an agent-based model developed in order to represent the flow of people. This simulation is also coupled with an interactive table called CityMatrix.</span></p>",,,2019-02-25 15:33:28.936,True,2015-08-01,Andorra | Tourism,PUBLIC,,True,City Science,False
andorra-energy-environment,devisj,False,"<p><a href=""https://www.media.mit.edu/projects/city-science-andorra/overview/"">View the main City Science Andorra project profile.</a></p>",,,2018-10-22 21:46:23.783,True,2016-09-02,Andorra | Energy + Environment,PUBLIC,,True,City Science,False
city-science-andorra,devisj,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
duct-tape-network,alishap,False,"<p>The Duct Tape Network (DTN) is a series of fun, hands-on maker clubs that encourage young children (ages 7-10) to use cardboard, tape, wood, fabric, LED lights, motors, and more to bring their stories and inventions to life. We are designing an educational framework and toolkit to engage kids in the creation of things that they care about before they lose their curiosity or get pulled in by more consumer-oriented technology. Work on DTN started in 2014 as part of a collaboration with Autodesk and is now expanding to communities all around the world.</p>",,--Choose Location,2017-02-10 02:45:19.416,True,2015-01-01,Duct Tape Network,PUBLIC,,True,Lifelong Kindergarten,True
ethics-of-autonomous-vehicles,awad,False,"<p>Adoption of self-driving, Autonomous Vehicles (AVs) promises to dramatically reduce the number of traffic accidents, but some inevitable accidents will require AVs to choose the lesser of two evils, such as running over a pedestrian on the road or the sidewalk. Defining the algorithms to guide AVs confronted with such moral dilemmas is a challenge, and manufacturers and regulators will need psychologists to apply methods of experimental ethics to these situations.</p>",,--Choose Location,2018-05-01 19:43:34.342,True,2015-01-01,Ethics of Autonomous Vehicles,PUBLIC,,True,Scalable Cooperation,False
a-voting-based-system-for-ethical-decision-making,awad,False,"<p>The problem of ethical decision making presents&nbsp; a grand challenge for modern AI research. Arguably the main obstacle to automating ethical decisions is the lack of a formal specification of ground-truth ethical principles, which have been the subject of debate for centuries among philosophers (e.g., trolley problem).&nbsp;We present an algorithm to automate ethical decisions; using machine learning and computational social choice (new theory of&nbsp;swap-dominance efficient voting rules), we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1.3 million voters through the Moral Machine website.&nbsp;Our proof of concept shows that the decision the system takes is likely to be the same as if we could go to each of the 1.3 million voters, ask for their opinions, and then aggregate their opinions into a choice that satisfies mathematical notions of social justice.&nbsp;</p>",,,2019-04-19 17:41:51.688,True,2017-01-20,A voting-based system for ethical decision making,PUBLIC,,True,Scalable Cooperation,False
moral-machine,awad,False,"<p>The Moral Machine is a platform for gathering a human perspective on moral decisions made by machine intelligence, such as self-driving cars. We generate moral dilemmas, where a driverless car must choose the lesser of two evils, such as killing two passengers or five pedestrians. As an outside observer, people judge which outcome they think is more acceptable. They can then see how their responses compare with other people. If they are feeling creative, people can also design their own scenarios, for others to view, share, and discuss.</p><p>Visit the <a href=""http://moralmachine.mit.edu"">Moral Machine</a>.</p>",,--Choose Location,2018-09-26 19:22:25.166,True,2016-01-01,Moral Machine,PUBLIC,http://moralmachine.mit.edu,True,Scalable Cooperation,False
mygoodness,awad,False,"<p>There are over one million registered charities in the United States alone, and many more worldwide. How do you choose among them?&nbsp;<br></p><p>MyGoodness is a simple game that helps you understand how you give. In the game, you will make 10 giving decisions. Each decision is between two choices, and you tell us which you prefer.</p><p>At the end of the game, we give you a summary of your ‘goodness’ and how it&nbsp;compares to others. You can share that feedback with whomever you would&nbsp;like.</p>",,,2017-12-15 23:10:00.218,True,2017-12-11,MyGoodness,PUBLIC,,True,Scalable Cooperation,False
towards-understanding-the-impact-of-ai-on-labor,hyoun,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-03-22 16:27:19.712,False,2018-03-01,Towards Understanding the Impact of AI on Labor,PUBLIC,http://www.media.mit.edu/~mrfrank,True,Scalable Cooperation,True
future-of-work-ai-automation-labor,hyoun,False,"<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>",,,2019-04-03 14:42:46.705,True,2017-06-06,"AI, Automation, Labor, and Cities: How to map the future of work",PUBLIC,,True,Scalable Cooperation,True
automated-tongue-analysis,fergusoc,False,"<p>A common practice in Traditional Chinese Medicine (TCM) is visual examination of the patient's tongue. This study will examine ways to make this process more objective and to test its efficacy for understanding stress- and health-related changes in people over time. We start by developing an app that makes it comfortable and easy for people to collect tongue data in daily life together with other stress- and health-related information. We will obtain assessment from expert practitioners of TCM, and also use pattern analysis and machine learning to attempt to create state-of-the-art algorithms able to help provide better insights for health and prevention of sickness.</p>",,--Choose Location,2017-09-21 15:02:57.578,True,2015-09-01,Automated Tongue Analysis,PUBLIC,,True,Affective Computing,False
quantifyme,fergusoc,False,"<p>Unlike traditional randomized controlled trials that generalize relationships in large groups of people, single-case experiments seek to quantify an individual's reaction to an intervention by measuring an independent variable's effect on a dependent variable (i.e., an intervention's effect on an outcome behavior). These single-case experiments are then combined back together using Bayesian Statistics methods in order to learn more general patterns about a population. We are interested in single-case experiments that test the causal relationships between behaviors that have been observed to be correlated with higher wellbeing.</p><p>Thus, instead of using an RCT to find what works for the imaginary ""average"" person, we can learn what works for each individual and then carefully combine data to generalize the results to other real individuals.</p><p>To our knowledge, single-case experiments have not been implemented in a smartphone app format. We believe that a successful app will allow researchers to dramatically scale the number of participants in these studies.</p><p>Code available on <a href=""https://github.com/mitmedialab/AffectiveComputingQuantifyMeAndroid"">GitHub</a>!&nbsp;</p>",,,2018-05-01 18:27:59.835,True,2016-06-01,QuantifyMe,PUBLIC,,True,Affective Computing,False
human-adherence,fergusoc,False,"<p>The Guardians project aims to use the same game design principles used in mobile game platforms to create greater engagement with individuals. The Affective Computing group is developing a custom video game with an independent patient reporting outcome tool to increase adherence to completion of&nbsp;patient reported outcomes.</p><p>Forming positive health habits can be difficult. Whether it’s taking medication, sticking to a diet, or going to the gym, it’s tough to commit to a new schedule long enough to form a habit. It is even more difficult when a person is asked to do something regularly that does not directly and immediately benefit them. This is an&nbsp;issue when clinical researchers need study participants to report outcomes regularly over a long period of time. Adherence is lost, resulting in suboptimal clinical outcomes and the loss of important data.</p><p>Mobile video&nbsp;games, on the other hand, generate an increased amount of adherence (<a href=""https://venturebeat.com/2017/02/01/superdata-mobile-games-hit-40-6-billion-in-2016-matching-world-box-office-numbers/"">as seen by an&nbsp;estimated market revenue of over $40.6 billion in 2016</a>**). Mobile video games have captured the attention of a wide variety of demographics and are often targeted to specific subgroups in order to increase engagement with a number of in-game features. These games use common design techniques and mechanics to produce a loop that draws players to return&nbsp;on a regular schedule and encourages them to watch ads, share on social media, or pay a fee for special rewards within the game.</p><p>By using the same game design principles, we aim to replace typical video game behaviors, like watching ads or sharing on social, with new behaviors that help improve the player’s wellbeing.</p><p>This project is a collaboration between the Affective Computing&nbsp;research group and Media Lab member company Takeda Pharmaceuticals.</p><p>**according to research by SuperData Research and Unity Technologies</p>",,,2019-05-16 15:17:42.342,True,2017-04-01,The Guardians,PUBLIC,,True,Affective Computing,False
emotional-navigation-system,fergusoc,False,"<p>Before automobiles were invented and widely adopted, animals like horses were the most common mode of transportation. While this change brought significant improvements in terms of reliability and efficiency, it also removed a core component: the emotional relationship that existed between the person and the animal.</p><p>While largely ignored, the emotional states of drivers are quite important, as they influence not only driving behavior but also the safety of all road users. For instance, driving can be quite an emotionally stressful experience and, while certain amounts of stress help the driver to remain alert and attentive, too much or too little can negatively impact driving performance and safety. Furthermore, stress in large doses has been linked to a large array of adverse health conditions such as depression and various forms of cardiovascular disease.</p><p>The&nbsp;Emotion Navigation&nbsp;special interest group is led by Dr. Javier Hernandez with the goal of stimulating research efforts at the intersection of&nbsp;Automotive and&nbsp;Affective Computing.</p>",,,2019-04-19 19:34:00.344,True,2018-04-02,Emotion Navigation,PUBLIC,http://enavigation.media.mit.edu,True,Affective Computing,False
elsa,fergusoc,False,"<h2><b><i>What is ELSA?</i></b></h2><p>ELSA is an AI-powered chatbot that acts as an empathetic companion, encouraging users to talk about their day through a form of interactive journaling.</p><p>You can try some of the current ELSA bots in this online&nbsp;<a href=""http://elsaneural.net"">demo</a>.&nbsp;</p><h2><b><i>How does ELSA work?</i></b></h2><p>Our project goal is to build a more empathetic neural network conversational AI by incorporating a deeper understanding of both the affective content of the conversation and the topic.&nbsp; More specifically, we build hierarchical recurrent neural network models that can converse like people &nbsp;and use transfer learning of topic and emotional tone recognition models to improve our final model.</p><h2><b><i>What are the applications of ELSA?</i></b></h2><p>Beyond the development of chatbots that act as an empathetic companion, we have a more ambitious and longer term goal: deploy the empathetic companion bots to support mental health.&nbsp; In particular, &nbsp;we aim to make ELSA useful for:</p><ul><li>Eliciting journaling</li><li>Suggesting behavioura interventions</li><li>Using Cognition Behavioral Therapy</li><li>Detecting individuals at risk of depression or suicide</li></ul><h2><b><i>Work in progress</i></b></h2><p>ELSA is a recently started project in the Affective Computing group. You can see an example of ELSA bot conversations below. You can also try our online <a href=""http://elsaneural.net"">demo</a>. &nbsp;&nbsp;</p>",,,2019-04-22 19:06:31.702,True,2019-03-03,"ELSA: Empathy learning, socially-aware agents",PUBLIC,,True,Affective Computing,False
secure-sharing-of-wildlife-data,rfrey,False,"<h2>Leveraging the power of platforms, big data, and advanced analytics for species protection and the public good in a privacy-preserving, scalable, and sustainable manner</h2><p>Modern tracking technology enables new ways of mining data in the wild. It allows wildlife monitoring centers to permanently collect geospatial data in a non-intrusive manner and in real time. Unfortunately, such sensible data is exposed to fraud and misuse and there is already a first reported case of ""cyber-poaching."" Based on stolen geospatial data, poachers can easily track and kill animals. Meanwhile, cautious monitoring centers limited data access for research and public use. We propose a novel privacy-preserving system to allow these monitoring centers to securely answer questions from the research community and the public while the raw data is protected against unauthorized third parties. Based on the core system, several new applications are conceivable, such as a mobile app for preventing conflicts between human and wildlife or for engaging people in wildlife donation. Besides providing a solution and working on specific use cases, the intention of this project is to start a discussion about the need for data protection in the animal world. </p><p>
                    
                </p>",,,2017-05-24 15:32:00.205,True,2017-02-01,Secure Sharing of Wildlife Data,PUBLIC,http://www.WildlifeData.org,True,Human Dynamics,True
near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging,ggbhatia,False,"<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Cone beam computed tomography (CBCT) is also widely used for diagnosis but is expensive and relatively cumbersome. Near-infrared imaging (NIR) offers a non-ionizing alternative for dental analysis. We examine and compare features in multiple extracted teeth using conventional radiographic, CBCT, and NIR transillumination imaging modes. NIR imaging can provide unique diagnostic value, primarily in its ability to reveal the extent of surface demineralization. We also provide examples where NIR illumination indicated underlying problem sites in need of further clinical attention and propose the use of NIR imaging to guide targeted and rational use of ionizing radiation in patients.</p><p><strong>Why is this work important?</strong></p><p>Two-dimensional radiographs and cone beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong><br></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. Much previous work has focused on light at 1310 nm, which strikes a balance between enamel and water attenuation, but such a wavelength often requires expensive sensors to image. NIR light at 850 nm has similar dental imaging properties, but it has not been studied as thoroughly as NIR at 1310 nm. It is not well understood what clinical features, if any, are present in NIR dental images, especially at 850 nm. Our previous work has examined the sensitivity of 850 nm NIR images to early caries lesions, but if NIR is to synergistically augment X-rays and CBCT as the standard of care, we must evaluate how well such images represent other clinical features.</p><p><strong>What are our contributions?</strong></p><p>We examine and compare features in multiple extracted teeth using conventional radiographic, CBCT, and NIR transillumination modes. NIR imaging can provide unique diagnostic value, primarily in its ability to reveal the extent of surface demineralization. We also provide examples where NIR illumination indicated underlying problem sites in need of further clinical attention and propose the use of NIR imaging to guide targeted and rational use of ionizing radiation in patients. We also show that NIR imaging identifies clinical features associated with early dimineralization and enamel caries that are not apparent upon expert visual examination.</p><p><strong>What are the next steps?</strong></p><p>Ongoing work is being done to model the interaction of light inside the tooth in order to provide even more diagnostic power.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth/overview/"">Near-Infrared Imaging for Detecting Dental Caries</a></li><li><a href=""https://www.media.mit.edu/projects/replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis/overview/"">Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography</a></li></ol>",,,2018-10-21 18:35:38.823,True,2017-05-08,Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging,PUBLIC,,True,Other,False
youplural,csbishop,False,"<p>Canadian artist Emily Carr once said, “You come into the world alone and you go out of the world alone yet it seems to me you are more alone while living than even going and coming.” This observation seems more piercingly accurate now, in our 21st century reality, than it was in Carr’s early 20th century reality. If we consider how individuals connect and how their connections evolve into community, we may extend that thinking toward how to facilitate those connections—those sparks—through which individuals who know very little about each other realize they indeed share something in common, in sometimes unexpected or surprising ways. </p><p>Through the medium of photography, YOU:PLURAL aims to create that spark of connection in an all-inclusive way. Beginning with everyone in our neighborhood—E14 and E15, including the Media Lab, CBA, CMS, and ACT—and hopefully beyond, we invite you to explore connections to people you may often see but don’t know well. (And we will help you!) We will then document those nascent connections as photographic images and associated metadata. These collected portraits will be displayed as part of our first show on the Civic Media ArtCube in early 2019.</p><p>If you’re interested in building and enhancing community here in E14 and E15 and want to have fun in the process, let us know. We’d love to hear your ideas for making creative, compelling images of our community.</p><p>—Cindy Bishop + Lorrie LeJeune&nbsp;</p>",,,2019-04-08 15:15:24.252,True,2019-04-04,You:Plural,PUBLIC,http://www.youplural.org,True,Civic Media,False
rock-and-roll-spirit,csbishop,False,"<p>Concerned about your privacy online? Worried with whom and how photos of you and/or your family might be shared on social media? You probably should be. Revelations about social networks (like Facebook) sharing your personal data with unethical actors (like Cambridge Analytica) are a major cause for concern. According to a recent Pew research study, 91 percent of Americans worry that social networks might misuse or resell their sensitive personal data.</p><br><p>That's why we built RockStar, a fully-featured social network in which everyone’s identity is correlated to a rock. RockStar most literally ensures rock solid privacy. </p><p><a href=""http://rockstar.cmprod.media.mit.edu"">Get My RockStar right now!!!</a></p><br><p>Because humans are excellent at recognizing patterns, we can see faces in non-human forms— something that machine learning algorithms have difficulty replicating. As such, our most secure Rockelgangers come from samples from inorganic material—a collection of rocks at The Japanese Museum of Rocks that <a href=""http://www006.upp.so-net.ne.jp/chinseki/index-ex.html"">Look like Faces</a>.&nbsp;We captured 1,500 unique rock face images that we estimate resemble 85 percent of the human population. Using the innovative Nimoy-Brenner Algorithm developed at MIT Center for Civic Media, we match your face to that of a celebrity and from that to one of these 1,500 rocks. By cloaking your identity via your Rockelganger, you can use social media freely without worry that your or your family’s data is being resold or compromised.</p><br><p>Worried that you might be part of the 15 percent whose face doesn't map neatly to a rock? Using cutting-edge generative adversarial neural network <a href=""https://junyanz.github.io/CycleGAN/"">techniques</a>,&nbsp;we can generate a custom Rockelganger just for you. These rocks look recognizably like you, but are sufficiently inorganic, enough to fool even the most advanced facial recognition systems.</p><br><p>ADDITIONAL BENEFITS!!!</p><p><i>Escape your echo chamber!</i></p><p>Enhanced privacy is not the only benefit of RockStar. We know that social networks can lead toward echo chambers of information where you hear only from your friends and people that you spend time with offline. RockStar offers a whole new way to make friends and meet people online.</p><br><p><i>Make new friends!</i></p><p>With only 1,500 default rocks, it's almost certain that you have the same Rockelganger as someone else online. Why not meet that person and find out what interests you share in common?</p><br><p><i>Learn about geology!</i> </p><p>Meet &nbsp;RockStar members who share your same type of rock. Is your rock face an igneous rock? Meet other igneous rocks or branch out and search for sedimentary or metamorphic rocks. </p><br><p>Rockstar—smashing privacy violations and the filter bubble, one rock at a time.</p>",,,2019-05-20 14:17:56.011,True,2019-01-01,RockStar-ai,PUBLIC,,True,Civic Media,False
artcube,csbishop,False,"<p>As we know from our ‘maker’ classes and workshops, different capabilities and approaches can yield remarkable projects. Let’s facilitate a ‘smart’ building that is at least as much about the people as the tech.</p><p>Because the walls and halls of E14 and E15 are subject to building codes and approvals, we decided to create our own walls that can contain and display, conceal or reveal, and in the true MIT innovative spirit, <i>move</i>. As a result, our ArtCube was born. Once we gather the remaining materials to finish construction, we will open for submissions (beginning of 2019).&nbsp;</p>",,,2018-12-19 16:16:19.552,True,2018-12-18,Art Cube: the Ambulatory Art Exhibit,PUBLIC,,True,Civic Media,False
youplural,lorrie,False,"<p>Canadian artist Emily Carr once said, “You come into the world alone and you go out of the world alone yet it seems to me you are more alone while living than even going and coming.” This observation seems more piercingly accurate now, in our 21st century reality, than it was in Carr’s early 20th century reality. If we consider how individuals connect and how their connections evolve into community, we may extend that thinking toward how to facilitate those connections—those sparks—through which individuals who know very little about each other realize they indeed share something in common, in sometimes unexpected or surprising ways. </p><p>Through the medium of photography, YOU:PLURAL aims to create that spark of connection in an all-inclusive way. Beginning with everyone in our neighborhood—E14 and E15, including the Media Lab, CBA, CMS, and ACT—and hopefully beyond, we invite you to explore connections to people you may often see but don’t know well. (And we will help you!) We will then document those nascent connections as photographic images and associated metadata. These collected portraits will be displayed as part of our first show on the Civic Media ArtCube in early 2019.</p><p>If you’re interested in building and enhancing community here in E14 and E15 and want to have fun in the process, let us know. We’d love to hear your ideas for making creative, compelling images of our community.</p><p>—Cindy Bishop + Lorrie LeJeune&nbsp;</p>",,,2019-04-08 15:15:24.252,True,2019-04-04,You:Plural,PUBLIC,http://www.youplural.org,True,Civic Media,False
artcube,lorrie,False,"<p>As we know from our ‘maker’ classes and workshops, different capabilities and approaches can yield remarkable projects. Let’s facilitate a ‘smart’ building that is at least as much about the people as the tech.</p><p>Because the walls and halls of E14 and E15 are subject to building codes and approvals, we decided to create our own walls that can contain and display, conceal or reveal, and in the true MIT innovative spirit, <i>move</i>. As a result, our ArtCube was born. Once we gather the remaining materials to finish construction, we will open for submissions (beginning of 2019).&nbsp;</p>",,,2018-12-19 16:16:19.552,True,2018-12-18,Art Cube: the Ambulatory Art Exhibit,PUBLIC,,True,Civic Media,False
scratch-in-space,christan,False,"<p>The Scratch Team invited young people from around the world to create <a href=""http://www.scratch.mit.edu"">Scratch</a>&nbsp;projects designed specifically to be played in zero gravity. Scratch members submitted over two hundred projects to this special initiative. &nbsp;Eric Schilling from the Scratch Team deployed a diverse collection of these projects on the Space Exploration initiative's inaugural research flight in zero gravity.&nbsp;</p>",,,2018-01-25 21:35:26.771,True,2017-10-01,Scratch in Space,PUBLIC,,True,Lifelong Kindergarten,False
scratch-in-space,eschill,False,"<p>The Scratch Team invited young people from around the world to create <a href=""http://www.scratch.mit.edu"">Scratch</a>&nbsp;projects designed specifically to be played in zero gravity. Scratch members submitted over two hundred projects to this special initiative. &nbsp;Eric Schilling from the Scratch Team deployed a diverse collection of these projects on the Space Exploration initiative's inaugural research flight in zero gravity.&nbsp;</p>",,,2018-01-25 21:35:26.771,True,2017-10-01,Scratch in Space,PUBLIC,,True,Lifelong Kindergarten,False
turingbox,judyshen,False,"<p>TuringBox is a platform&nbsp;that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms.&nbsp;It is a two-sided marketplace. On one side, <i>AI contributors</i> upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">On the other side, </span><i style=""font-size: 18px; font-weight: 400;"">AI examiners</i><span style=""font-size: 18px; font-weight: 400;"">&nbsp;develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.</span></p>",,,2018-04-05 19:00:33.916,True,2018-03-21,TuringBox: Democratizing the study of AI,PUBLIC,http://turingbox.mit.edu,True,Affective Computing,False
elsa,judyshen,False,"<h2><b><i>What is ELSA?</i></b></h2><p>ELSA is an AI-powered chatbot that acts as an empathetic companion, encouraging users to talk about their day through a form of interactive journaling.</p><p>You can try some of the current ELSA bots in this online&nbsp;<a href=""http://elsaneural.net"">demo</a>.&nbsp;</p><h2><b><i>How does ELSA work?</i></b></h2><p>Our project goal is to build a more empathetic neural network conversational AI by incorporating a deeper understanding of both the affective content of the conversation and the topic.&nbsp; More specifically, we build hierarchical recurrent neural network models that can converse like people &nbsp;and use transfer learning of topic and emotional tone recognition models to improve our final model.</p><h2><b><i>What are the applications of ELSA?</i></b></h2><p>Beyond the development of chatbots that act as an empathetic companion, we have a more ambitious and longer term goal: deploy the empathetic companion bots to support mental health.&nbsp; In particular, &nbsp;we aim to make ELSA useful for:</p><ul><li>Eliciting journaling</li><li>Suggesting behavioura interventions</li><li>Using Cognition Behavioral Therapy</li><li>Detecting individuals at risk of depression or suicide</li></ul><h2><b><i>Work in progress</i></b></h2><p>ELSA is a recently started project in the Affective Computing group. You can see an example of ELSA bot conversations below. You can also try our online <a href=""http://elsaneural.net"">demo</a>. &nbsp;&nbsp;</p>",,,2019-04-22 19:06:31.702,True,2019-03-03,"ELSA: Empathy learning, socially-aware agents",PUBLIC,,True,Affective Computing,False
turingbox,felbo,False,"<p>TuringBox is a platform&nbsp;that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms.&nbsp;It is a two-sided marketplace. On one side, <i>AI contributors</i> upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">On the other side, </span><i style=""font-size: 18px; font-weight: 400;"">AI examiners</i><span style=""font-size: 18px; font-weight: 400;"">&nbsp;develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.</span></p>",,,2018-04-05 19:00:33.916,True,2018-03-21,TuringBox: Democratizing the study of AI,PUBLIC,http://turingbox.mit.edu,True,Scalable Cooperation,False
deepmoji,felbo,False,"<p><i>Emotional content is an important part of language. There are many use cases now showing that natural language processing is becoming an increasingly important part of consumer products.&nbsp;We are attempting to learn more about human emotions.</i></p><p>In his 2006 book <i>The Emotion Machine</i>, legendary computer scientist Marvin Minsky (co-founder of the field of Artificial Intelligence and one of the founding faculty members of the MIT Media Lab) wrote about the central role of emotions in reasoning—reminding us that AI will only be capable of true commonsense reasoning once it has understood emotions. To Minsky, emotions are not the opposite of rational reason, something to be weeded out before we can think clearly; rather, emotions are just a different way of thinking.</p><p><b><a href=""http://deepmoji.mit.edu/"">TRY DEEPMOJI</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href=""https://deepmoji.mit.edu/contribute/"">HELP TEACH OUR AI ABOUT EMOTIONS</a></b><br></p><p>But this is hardly helpful to a computer scientist trying to construct an emotional machine by programming a concrete set of rules. If you ask two people to explain what makes a particular sentence happy, sad, serious, or sarcastic, you will likely get at least two different opinions. Much of what determines emotional content is context-specific, culturally constructed, and difficult to describe in an explicit set of rules.<br></p>",,,2019-05-29 13:10:23.998,True,2017-08-02,DeepMoji,PUBLIC,http://deepmoji.mit.edu,True,Scalable Cooperation,False
identifying-the-human-impacts-of-climate-change,felbo,False,<p>Climate change is going to alter the environments that we depend on in myriad ways. We're using data to identify and quantify these potential human impacts.&nbsp;</p>,,,2019-04-19 17:44:02.613,True,2016-07-01,Identifying the human impacts of climate change,PUBLIC,,True,Scalable Cooperation,False
turingbox,blakeley,False,"<p>TuringBox is a platform&nbsp;that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms.&nbsp;It is a two-sided marketplace. On one side, <i>AI contributors</i> upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet.&nbsp;<span style=""font-size: 18px; font-weight: 400;"">On the other side, </span><i style=""font-size: 18px; font-weight: 400;"">AI examiners</i><span style=""font-size: 18px; font-weight: 400;"">&nbsp;develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.</span></p>",,,2018-04-05 19:00:33.916,True,2018-03-21,TuringBox: Democratizing the study of AI,PUBLIC,http://turingbox.mit.edu,True,Personal Robots,False
ai-ethics-for-middle-school,blakeley,False,"<h2>How do we raise conscientious consumers and designers of AI?</h2><p>Children today live in the age of artificial intelligence. On average, US children tend to receive their first smartphone at age 10, and by age 12 over half of all children have their own social media account. Additionally, it's estimated that by 2022, there will be 58 million new jobs in the area of artificial intelligence. Thus, it's&nbsp;important that the youth of today are both conscientious consumers and designers of AI.&nbsp;</p><p>This project seeks to develop an open source curriculum for middle school students on the topic of artificial intelligence. Through a series of lessons and activities, students learn technical concepts—such as how to train a simple classifier—and the ethical implications those technical concepts entail, such as algorithmic bias.&nbsp;</p>",,,2019-04-17 18:36:01.110,True,2018-08-01,AI + Ethics Curriculum for Middle School,PUBLIC,http://blakeleyhoffman.me/,True,Personal Robots,False
scratch-pad,khanning,False,"<p>ScratchBit is an effort to enable children to create more seamlessly in both the physical and digital world by creating a dedicated physical interface for the&nbsp;<a href=""https://scratch.mit.edu"">Scratch</a>&nbsp;programming language and environment. Designed to be rugged, low cost, and highly composable, the ScratchBit allows children to take the materials around them—such as cardboard, clothes, skateboards, and trees—and &nbsp;transform them into inputs to their digital creations on Scratch. Unlike the <a href=""http://makeymakey.com/"">Makey Makey</a> which was designed to make these connections electronically, the ScratchBit is designed to create these connections through motion and mechanism.</p>",,,2018-11-03 16:11:24.635,True,2016-09-01,ScratchBit,PUBLIC,,True,Lifelong Kindergarten,False
open-leadership-camp,yumikom,False,"<p>The Open Leadership Camp (OLC) is a new type of professional development program for senior leaders of nonprofit and public sector organizations. It aims to &nbsp;<span style=""font-size: 18px; font-weight: 400;"">apply the principles of open source, open innovation, and the decentralized nature of the web to the way some of our most crucial social sector organizations work.&nbsp;</span></p><p>This project is a collaboration between <a href=""https://www.mozilla.org"">Mozilla</a> and the <a href=""https://www.media.mit.edu/groups/ml-learning/overview/"">ML Learning Initiative</a>, and is&nbsp;<span style=""font-size: 18px; font-weight: 400;"">hosted by MIT Media Lab Director <a href=""https://www.media.mit.edu/people/joi/overview/"">Joi Ito</a> and <a href=""https://www.mozilla.org/en-US/about/leadership/"">Mitchell Baker</a>, co-founder and Executive Chairwoman of the Mozilla Corporation. In March 2017, we brought together our first cohort of 14 participants, including the CEO of Consumer Reports, the CIO of the City of Detroit, and the CEO of WGBH.&nbsp;</span></p>",,,2017-06-19 14:42:03.293,True,2017-01-01,Open Leadership Camp,PUBLIC,,True,Lifelong Kindergarten,False
robot-mindset-and-curiosity,safinah,False,"<h1>Young Learner's Companion&nbsp;</h1><h2>Developing robots' growth mindset and pro-curious behavior and fostering the same in young learners via long-term interaction</h2><p>A growth mindset and curiosity have significant impact on children's academic and social achievements. We are developing and evaluating a novel expressive cognitive-affective architecture that synergistically integrates models of curiosity, understanding of mindsets, and expressive social behaviors to advance the state-of the-art of robot companions. In doing so, we aim to contribute major advancements in the design of AI algorithms for artificial curiosity, artificial mindset, and their verbal and non-verbal expressiveness in a social robot companion for children. In our longitudinal study, we aim to evaluate the robot companion's ability to sustain engagement and promote children's curiosity and growth mindset for improved learning outcomes in an educational play context.<br></p>",,--Choose Location,2018-05-09 04:35:35.760,True,2015-09-01,Robot Mindset and Curiosity,PUBLIC,,True,Personal Robots,False
nevermind,exposito,False,"<p>NeverMind is an interface and application designed to support human memory. We combine the memory palace memorization method with augmented reality technology to create a tool to help anyone memorize more effectively. Early experiments conducted with a prototype of NeverMind suggest that the long-term memory recall accuracy of sequences of items is nearly tripled compared to paper-based memorization tasks. With this project, we hope to make the memory palace method accessible to novices and demonstrate one way augmented reality can support learning.</p>",,,2019-04-18 17:07:28.479,True,2016-09-01,NeverMind: Using AR for memorization,PUBLIC,,True,Social Machines,False
language-simplification,exposito,False,"<p><span style=""font-size: 18px; font-weight: 400;"">The Language Simplification project is developing automatic methods to simplify complex texts to be more easily read and understood by a broader audience, such as children and non-native English speakers. Using neural networks, complex words and phrases can be substituted, the sentence can be split and rephrased, and the overall text can be summarized and compressed.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">These capabilities can be wrapped into a reading assistance tool for end users, or as a pre-processing step for other NLP tasks.</span></p>",,,2019-04-17 20:51:49.717,True,2019-02-01,Language Simplification,PUBLIC,,True,Social Machines,False
storyblocks,exposito,False,"<p><span style=""font-size: 18px; font-weight: 400;"">StoryBlocks aims to promote creative expression, literacy development, and social-emotional development through storytelling for children ages six to ten. In this app, children create personally generated, comic-style stories by inserting characters, setting emotions, typing dialogue, using words to insert images that customize scenes, and recording their voices to narrate their unique stories. With StoryBlocks, we can collect a corpus of children’s stories in order to build analysis tools that&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">can document children’s narrative development over time, and support coaches in providing personalized scaffolding for children’s narratives.&nbsp;</span></p>",,,2019-04-11 15:39:53.269,True,2016-07-01,StoryBlocks,PUBLIC,,True,Social Machines,False
ecg-ambulatory,dlmocdm,False,"<p>The explosion of mHealth in both abundant and resource-constrained countries is both a cause for celebration and for concern.&nbsp;While mHealth clearly has the potential to deliver information and diagnostic decision support to the poorly trained, it is not appropriate to simply translate the technologies which the trained clinician uses into the hands of non-experts. In particular, it is important that the explosion of access does not lead to a flooding of the medical system with low quality data and false negatives. Clearly for mHealth to expand, a paradigm shift in how data is analysed must occur. Data must be vetted at the front end, using automated algorithms, to provide robust filtering of low quality data.</p><p>This project addresses the specific problem of vetting the quality of electrocardiograms (ECGs) collected by an untrained user in ambulatory scenarios using smartphone devices.</p>",,,2018-01-18 06:42:50.538,True,2017-12-01,Electrocardiogram collection in noisy ambulatory environments with Android smartphone devices,PUBLIC,,True,Affective Computing,False
comm-phys-gest,jnarain,False,"<p>We are exploring physiology and nonverbal gestures in real-life contexts, particularly for children with Autism Spectrum Disorders (ASD). Our long-term goal is to enhance understanding and communication by pairing knowledge from wearable devices with context and individualized information. Our focus is to understand longitudinal patterns of these signals in an individual's day-to-day life, and then develop personalized algorithms to interpret signals.</p>",,,2019-04-17 14:57:48.343,True,2019-03-16,ECHOS: Enhancing Communication using Holistic Observations and Sensing,PUBLIC,,True,Fluid Interfaces,False
augmentative-communication-interfaces,jnarain,False,"<p>We are creating new communication interfaces for people with speech and language disorders, incorporating technologies like physiological sensing and personalized machine learning. The goal of our work is to identify gaps in the technologies that exist and what people need, and create and test devices that meet those needs.</p><p>Do you have a speech or language impairment, or have a close friend or family member who does? We would really appreciate your feedback on our <b>AAC device survey</b>, which will help us identify promising architectures for further development.</p><p>Click this<b>&nbsp;<a href=""https://mit.co1.qualtrics.com/jfe/form/SV_6gmpNchtSf8vCLz"">link</a></b>&nbsp;to access the survey.</p><p>Sources for logo:</p><p><a href=""https://svgsilh.com/image/40466.html"">Spell deaf talk speech</a></p><p>Information Computer Technology <a href=""https://www.maxpixel.net/Information-Computer-Technology-Digital-Binary-3374456"">Digital Binary</a></p><p>Alternative Handicapped Accessible <a href=""https://commons.wikimedia.org/wiki/File:Alternative_Handicapped_Accessible_sign.svg"">sign</a></p>",,,2019-04-18 17:48:47.733,True,2019-01-13,User needs for augmentative communication interfaces,PUBLIC,,True,Fluid Interfaces,False
8k-brain-tour,bandy,False,"<p>We present an 8K (7680 x 4320 pixels) visualization system for terabyte-scale, three-dimensional microscopy images of a brain slice that can facilitate neuroscience research. High resolution, large format (85” or 188 cm x 106 cm) rendering allows the viewer to dive into the massive dataset of 700 billion voxels capturing thousands of neurons and to investigate nanoscale and macroscale structures of the neurons simultaneously.</p>",,,2019-04-17 18:31:38.208,True,2017-02-23,8K Brain Tour,PUBLIC,,True,Synthetic Neurobiology,True
8k-brain-tour,shoh,False,"<p>We present an 8K (7680 x 4320 pixels) visualization system for terabyte-scale, three-dimensional microscopy images of a brain slice that can facilitate neuroscience research. High resolution, large format (85” or 188 cm x 106 cm) rendering allows the viewer to dive into the massive dataset of 700 billion voxels capturing thousands of neurons and to investigate nanoscale and macroscale structures of the neurons simultaneously.</p>",,,2019-04-17 18:31:38.208,True,2017-02-23,8K Brain Tour,PUBLIC,,True,Synthetic Neurobiology,False
8k-brain-tour,itot,False,"<p>We present an 8K (7680 x 4320 pixels) visualization system for terabyte-scale, three-dimensional microscopy images of a brain slice that can facilitate neuroscience research. High resolution, large format (85” or 188 cm x 106 cm) rendering allows the viewer to dive into the massive dataset of 700 billion voxels capturing thousands of neurons and to investigate nanoscale and macroscale structures of the neurons simultaneously.</p>",,,2019-04-17 18:31:38.208,True,2017-02-23,8K Brain Tour,PUBLIC,,True,Collective Learning,True
biodigital-space,itot,False,"<p>Biodigital is a fictional virtual reality (VR) experience that combines VR film, immersive 3D environments, and VR data visualization. Biodigital tells the story of humanity as seen from the year 2117, after humans merged with machines in the ""biodigital"" space. The story of Biodigital is about a species that transforms each time it develops new communication technologies. It begins with the histories of printing and television, but then explores the rise of neuroimplantable devices, and the new moral dilemmas they bring to the world.&nbsp;</p><p><span style=""font-size: 18px; font-weight: normal;"">Biodigital is an experiment on mixed media created in a collaboration between Takahito Ito, from NHK, and </span>César&nbsp;<span style=""font-size: 18px; font-weight: normal;"">&nbsp;A. Hidalgo, head of the Collective Learning group. The project explores new forms of storytelling in VR.</span></p>",,,2017-04-05 18:38:52.863,True,2017-01-09,Biodigital,LAB-INSIDERS,,True,Collective Learning,True
8k-brain-tour,kanaya,False,"<p>We present an 8K (7680 x 4320 pixels) visualization system for terabyte-scale, three-dimensional microscopy images of a brain slice that can facilitate neuroscience research. High resolution, large format (85” or 188 cm x 106 cm) rendering allows the viewer to dive into the massive dataset of 700 billion voxels capturing thousands of neurons and to investigate nanoscale and macroscale structures of the neurons simultaneously.</p>",,,2019-04-17 18:31:38.208,True,2017-02-23,8K Brain Tour,PUBLIC,,True,Civic Media,True
a-voting-based-system-for-ethical-decision-making,gaikwad,False,"<p>The problem of ethical decision making presents&nbsp; a grand challenge for modern AI research. Arguably the main obstacle to automating ethical decisions is the lack of a formal specification of ground-truth ethical principles, which have been the subject of debate for centuries among philosophers (e.g., trolley problem).&nbsp;We present an algorithm to automate ethical decisions; using machine learning and computational social choice (new theory of&nbsp;swap-dominance efficient voting rules), we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1.3 million voters through the Moral Machine website.&nbsp;Our proof of concept shows that the decision the system takes is likely to be the same as if we could go to each of the 1.3 million voters, ask for their opinions, and then aggregate their opinions into a choice that satisfies mathematical notions of social justice.&nbsp;</p>",,,2019-04-19 17:41:51.688,True,2017-01-20,A voting-based system for ethical decision making,PUBLIC,,True,Space Enabled,False
big-ocean-big-data,gaikwad,False,"<p>More ocean data has been collected in the last two years than in all previous years combined, and we are on a path to continue to break that record. More than ever, we need to establish a solid foundation for processing this ceaseless stream of data. This is especially true for visual data, where ocean-going platforms are beginning to integrate multi-camera feeds for observation and navigation.&nbsp;Techniques to efficiently process and utilize visual datasets with machine learning exist and continue to be transformative, but have had limited success in the ocean world due to:</p><ul><li>Lack of data set standardization;</li><li>Sparse annotation tools for the wider oceanographic community; and&nbsp;</li><li>Insufficient formatting of existing, expertly curated imagery for use by data scientists.&nbsp;</li></ul><p>Building on successes of the machine learning community, we are developing a public platform that makes use of existing (and future) expertly curated data. Our efforts will establish a new baseline dataset, optimized to directly accelerate development of modern, intelligent, automated analysis of underwater visual data. This effort will ultimately enable scientists, explorers, policymakers, storytellers, and the public to know what’s in the ocean and where it is for effective and responsible marine stewardship.</p>",,,2019-04-22 17:49:21.666,True,2018-04-02,"Big Ocean, Big Data",PUBLIC,,True,Space Enabled,False
places,oarino,False,"<p>Changing Places researchers  are developing scalable strategies for creating hyper-efficient, technology-enabled spaces that can help make living more affordable, productive, enjoyable, and creative for urban dwellers.<br></p>",,,2019-05-24 21:06:01.300,True,2017-08-01,Theme | Changing Places,PUBLIC,,True,City Science,False
places,maitanei,False,"<p>Changing Places researchers  are developing scalable strategies for creating hyper-efficient, technology-enabled spaces that can help make living more affordable, productive, enjoyable, and creative for urban dwellers.<br></p>",,,2019-05-24 21:06:01.300,True,2017-08-01,Theme | Changing Places,PUBLIC,,True,City Science,False
city-science-guadalajara,maitanei,False,"<p>The University of Guadalajara, referred to as UdeG, is a university network composed of 15 campuses within the state of Jalisco and one online system. The University offers undergraduate and graduate studies to around 130,000 students. UdeG strives to understand urban performance metrics using evidence-based decision making tools, facilitated through a collaboration with the MIT Media Lab City Science group.</p>",,,2019-05-10 19:44:20.937,True,2018-11-01,City Science Collaboration Guadalajara,PUBLIC,,True,City Science,False
cityscope-cooper-hewitt,maitanei,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
piccolo-kitchen,maitanei,False,"<p>&nbsp;This project aims to create a modular platform for exploring micro-kitchens that are culture specific. Cooking is a personal experience that has cultural attributes.&nbsp; This project explores new modes of cooking using robotically enabled cabinets and appliances to minimize the footprint of the kitchen, while maximizing the ability for users to cook large meals, socialize, and utilize the same space during non-meal times for work. Piccolo kitchen is one of the components of the micro-units that are currently under development as part of the CityHome 02 projects.</p>",,,2019-04-19 13:39:55.600,True,2018-09-01,Piccolo Kitchen,PUBLIC,,True,City Science,False
city-science-andorra,maitanei,False,"<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an “Internationally Recognized Intelligent Country.” The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>",,,2019-05-24 18:49:18.611,True,2014-05-01,City Science Lab Andorra,PUBLIC,,True,City Science,False
places,aizpurua,False,"<p>Changing Places researchers  are developing scalable strategies for creating hyper-efficient, technology-enabled spaces that can help make living more affordable, productive, enjoyable, and creative for urban dwellers.<br></p>",,,2019-05-24 21:06:01.300,True,2017-08-01,Theme | Changing Places,PUBLIC,,True,City Science,False
getting-started-with-scratch,shrutid,False,"<p>Every day, young people around the world use the Scratch programming language to create and share thousands of interactive projects on the <a href=""https://scratch.mit.edu/"">Scratch website</a>. Yet many students aren’t sure how to get started coding their own projects.  </p><p>To address this, we have launched a new set of free resources to help students learn to create with code. The <a href=""https://scratch.mit.edu/go"">Things to Try</a> page offers a variety of project ideas, such as creating an animated story, making a pong game, or designing a virtual pet. For each theme, students can use step-by-step tutorials or printable activity cards. In addition, the site offers educator guides you can use to organize a class or workshop based on the theme.</p><p>The <a href=""https://scratch.mit.edu/info/cards/"">Scratch Activity Cards</a> is a collection of more than 80 colorful cards with 11 project themes. The front of each card illustrates an activity students can do with Scratch, such as animating a character or keeping score in a game. The back of the card shows how to snap together blocks of code to make their projects come to life.  </p><p>These resources are designed to let students learn at their own pace and personalize their projects. Students can work individually or pair up to make projects together.&nbsp;</p>",,,2017-04-19 17:58:02.568,True,2015-09-01,Getting Started with Scratch,PUBLIC,,True,Lifelong Kindergarten,False
scratch-memories,shrutid,False,"<p>Scratch Memories is a web-based visualization tool that empowers children to celebrate and reflect on their creative journey with Scratch.&nbsp;The system dynamically generates personalized visualizations in the form of a video, highlighting a user’s key moments, diverse creations, and collaborative experiences in the online community.&nbsp;</p><p>Existing tools for visualizing children’s progress in computational learning are primarily designed for educators, and often focus exclusively on <i>evaluating</i> predefined concepts in individual projects. The goal of Scratch Memories is to present a new approach towards designing positive reflective experiences that value the full range of children’s contributions as members of a creative community.<br></p><p>The tool engages young people to reflect on their personal growth over time—starting from their first experiments with code to seeing the increasing diversity and complexity of their projects over time; and from their initial interactions in the community to seeing how their projects have inspired others around the world.&nbsp;Such reflective experiences can not only help young creators feel proud about how far they have come, but also to feel inspired by their own trajectories to continue exploring new possibilities.</p>",,,2018-11-07 15:55:47.871,True,2016-12-20,Scratch Memories,PUBLIC,,True,Lifelong Kindergarten,False
scratch-blockart,shrutid,False,"<p><i>Scratch BlockArt </i>is an experimental visualization tool designed to let children discover their own computational patterns on Scratch. Existing methods often utilize data about the types of programming blocks used in children’s projects to generate a quantitative assessment of a project’s computational complexity based on limited criteria. BlockArt presents an alternative approach for revealing this data to young creators themselves.&nbsp;Rather than&nbsp;datafying children’s creations, BlockArt is designed to transform the data about their code into creative objects that can spark children's curiosity and enable them to reflect on their own styles and choices.&nbsp;</p><p>For a given username, the tool dynamically generates colorful visualizations representing the number and diversity of programming blocks used in each of their shared projects over time. Children can also click to see the project behind the visualization. The idea is not to evaluate whether they use more or less 'complex' blocks, but to reveal how the types of blocks children use are based on their motivations and interests behind creating a specific project. It also shows how looking at the diversity of code in all their projects is a better representation of their learning trajectory than providing a quantitative assessment on individual projects without additional context.&nbsp;</p>",,,2018-11-13 20:03:22.125,True,2018-01-26,Scratch BlockArt,PUBLIC,,True,Lifelong Kindergarten,False
technology-enabled-mobile-phone-screenings-augment-routine-primary-care,otkrist,False,"<p>We have developed a new process to screen patients at the point-of-care with FDA-approved technology-enabled mobile health screenings (TES) and compare the results with routine health screenings. A study of nearly 500 patients was conducted to test the effectiveness of this new screening process. This is one of the first studies to investigate using TES to augment routine health examinations. We recommend using TES in synergy with routine health screenings to identify missing sick patients who might otherwise lack comprehensive primary care.<br></p><p><strong>Why is this work important?</strong></p><p>Providing good healthcare in low- and middle-income countries (LMIC) paradoxically requires expensive equipment,&nbsp;<span style=""font-family: &quot;Neue Haas Grotesk Display&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 18px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400;"">which may not be easily available because of resource limitations,</span><span style=""font-size: 18px; font-weight: 400;"">&nbsp;for health monitoring and assessment. There is high variation in the degrees of healthcare access in LMIC, but such access is important because cardiovascular diseases, preventable blindness, oral cancer, and treatable neurological conditions constitute more than half of the disease burden in LMIC. Comprehensive TES may allow for more patients to be screened for more conditions in resource-limited settings, improving their access to primary healthcare. A lack of consensus exists about the usefulness of TES in augmenting primary health screenings in LMIC.</span></p><p><strong>What has been done before?</strong><br></p><p>Devices that allow TES have typically been evaluated in isolated silos, concentrating on individual devices or specific anatomical sites. They have additionally not been comprehensively evaluated alongside routine health screenings.</p><p><strong>What are our contributions?</strong><br></p><p>This is one of the first studies to investigate using multiple TES to augment routine health examinations. To facilitate this large-scale study, we developed and successfully used web examination platforms that enabled multiple physicians to diagnose health conditions remotely. We identified patients who would not have received the care they need in the absence of TES, and link TES to primary health outcomes.</p><p>This study led to significant insights about strategies to develop technologies at MIT that are ready for deployment for effective and scalable primary care in the real world.</p><p><strong>What technology-enabled examinations were performed?</strong></p><p>Single-lead ECG: AliveCor Mobile ECG</p><p>Blood oxygen saturation: Contec Medical Systems 50-DL Pulse Oximeter</p><p>Oral imaging: ACTEON Soprocare</p><p>Retinal scan: D-EYE direct ophthalmoscopy adapter attached to iPhone5s camera</p><p>Tympanic membrane imaging: CellScope Oto with iPhone5 LEDs and camera</p><p>Neurological examinations: Microsoft Kinect</p><p><strong>What are the next steps?</strong></p><p>We are actively working on automated diagnoses, analyses of disease co-occurrence, and patient risk stratification.</p><p>Future studies that build on our technology-enabled screening process can evaluate the process for larger numbers of patients. A future longitudinal study may allow for additional insights into time-varying conditions.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/"">Machine Learning and Automated Segmentation of Oral Diseases using Biomarker Images</a></li></ol>",,,2018-10-21 18:32:18.676,True,2015-05-15,Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care,PUBLIC,,True,Camera Culture,False
architecture-selection-for-deep-neural-networks,otkrist,False,"<p>We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Q-learning with an $\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.</p>",,,2019-04-19 17:47:51.865,True,2016-05-01,Architecture selection for deep neural networks,PUBLIC,,True,Camera Culture,False
calibration-invariant-i,otkrist,False,"<p><b>Object Classification through Scattering Media&nbsp;with Deep Learning</b></p><p>A method for classifying objects hidden behind a scattering layer with a neural network. Training on synthetic data with variations in calibration parameters allows the network to learn a model that doesn't require calibration during lab experiments.<br></p><p>Traditional techniques to see through scattering media rely on a physical model that needs to be precisely calibrated. Computationally overcoming the scattering relies heavily on accurately calibrated physical models. Thus, such systems are extremely sensitive to a precise and lengthy calibration process. </p><p>In this work we overcome this bottleneck by utilizing neural networks and their ability to learn models that are invariant to data transformation. In our case, the transformations are variations in the imaging system calibration parameters. To that end, we create a synthetic dataset that contains variations in all calibration parameters (we use a Monte Carlo forward model to render the measurements). The system is then tested on actual lab experiments without specific calibration or tuning.</p>",,,2018-10-20 00:40:19.319,True,2016-09-01,Calibration Invariant Imaging,PUBLIC,,True,Camera Culture,False
machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images,otkrist,False,"<p>We report a novel method that processes biomarker images collected at the point of care and uses machine learning algorithms to provide a first level of screening against oral diseases. A machine learning classifier is trained to learn pixel-by-pixel mappings from RGB oral images and output areas with disease. This method can be adapted to&nbsp; process biomarker images from other organs as well.</p><p><strong>Why is this work important?</strong></p><p>Visual inspection and probing techniques have been traditionally used for diagnosis of oral diseases in patients. These traditional methods are subjective and not scalable. We describe the use of RGB color images acquired by low-cost camera devices coupled with machine learning to detect areas with poor oral health.</p><p><strong>What has been done before?</strong></p><p>Currently the gold standard for oral diagnosis is visual inspections by a dentist followed by X-rays. These methods are expensive and invasive.&nbsp;</p><p><strong>What are our contributions?</strong></p><p>We implement a novel technique to combine medical expert knowledge with biomarker signatures.&nbsp; We&nbsp; use RGB color images taken directly at the point-of-care, using low-cost hand-held devices, to provide a first level machine-learning powered screening for patients.</p><p><strong>What are the next steps?</strong></p><p>We are expanding the repertoire of biomarkers that can be detected in RGB color images acquired at the point-of-care and pairing them with automated machine learning exams.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/"">Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care</a></li></ol>",,,2018-08-09 17:24:03.930,True,2017-04-24,Machine Learning and Automated Segmentation of Oral Diseases Using Biomarker Images,PUBLIC,,True,Camera Culture,False
technology-enabled-mobile-phone-screenings-augment-routine-primary-care,mrinal,False,"<p>We have developed a new process to screen patients at the point-of-care with FDA-approved technology-enabled mobile health screenings (TES) and compare the results with routine health screenings. A study of nearly 500 patients was conducted to test the effectiveness of this new screening process. This is one of the first studies to investigate using TES to augment routine health examinations. We recommend using TES in synergy with routine health screenings to identify missing sick patients who might otherwise lack comprehensive primary care.<br></p><p><strong>Why is this work important?</strong></p><p>Providing good healthcare in low- and middle-income countries (LMIC) paradoxically requires expensive equipment,&nbsp;<span style=""font-family: &quot;Neue Haas Grotesk Display&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 18px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400;"">which may not be easily available because of resource limitations,</span><span style=""font-size: 18px; font-weight: 400;"">&nbsp;for health monitoring and assessment. There is high variation in the degrees of healthcare access in LMIC, but such access is important because cardiovascular diseases, preventable blindness, oral cancer, and treatable neurological conditions constitute more than half of the disease burden in LMIC. Comprehensive TES may allow for more patients to be screened for more conditions in resource-limited settings, improving their access to primary healthcare. A lack of consensus exists about the usefulness of TES in augmenting primary health screenings in LMIC.</span></p><p><strong>What has been done before?</strong><br></p><p>Devices that allow TES have typically been evaluated in isolated silos, concentrating on individual devices or specific anatomical sites. They have additionally not been comprehensively evaluated alongside routine health screenings.</p><p><strong>What are our contributions?</strong><br></p><p>This is one of the first studies to investigate using multiple TES to augment routine health examinations. To facilitate this large-scale study, we developed and successfully used web examination platforms that enabled multiple physicians to diagnose health conditions remotely. We identified patients who would not have received the care they need in the absence of TES, and link TES to primary health outcomes.</p><p>This study led to significant insights about strategies to develop technologies at MIT that are ready for deployment for effective and scalable primary care in the real world.</p><p><strong>What technology-enabled examinations were performed?</strong></p><p>Single-lead ECG: AliveCor Mobile ECG</p><p>Blood oxygen saturation: Contec Medical Systems 50-DL Pulse Oximeter</p><p>Oral imaging: ACTEON Soprocare</p><p>Retinal scan: D-EYE direct ophthalmoscopy adapter attached to iPhone5s camera</p><p>Tympanic membrane imaging: CellScope Oto with iPhone5 LEDs and camera</p><p>Neurological examinations: Microsoft Kinect</p><p><strong>What are the next steps?</strong></p><p>We are actively working on automated diagnoses, analyses of disease co-occurrence, and patient risk stratification.</p><p>Future studies that build on our technology-enabled screening process can evaluate the process for larger numbers of patients. A future longitudinal study may allow for additional insights into time-varying conditions.</p><p><strong>Related projects</strong></p><ol><li><a href=""https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/"">Machine Learning and Automated Segmentation of Oral Diseases using Biomarker Images</a></li></ol>",,,2018-10-21 18:32:18.676,True,2015-05-15,Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care,PUBLIC,,True,Camera Culture,False
synthetic-genome-engineering,pranam,False,"<p>We are currently developing novel DNA editing technologies to broaden the scope of genome engineering. Our strategy is based on identifying and engineering endonucleases from diverse living systems, along with targeting with synthetic molecules. Together these components confer greater stability, minimize off-target DNA cleavage, and eliminate sequence restrictions for precision genetic manipulations within cells.</p>",,,2017-09-23 06:18:07.043,True,2016-06-06,Synthetic Genome Engineering,PUBLIC,,True,Molecular Machines,False
opening-wider-genomic-access-with-a-flexible-crispr-enzyme,pranam,False,"<p>The CRISPR-Cas9 system has proven to be a versatile tool for genome editing, with&nbsp;<span style=""font-size: 18px; font-weight: 400;"">numerous implications in medicine, agriculture, bioenergy, food security, and beyond. The&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">range of targetable DNA sequences is limited, however, by the need for a short sequence of DNA beside the target site, called the PAM. In total, there are only a handful of CRISPR enzymes with a short enough PAM sequence to be able to target a large portion of the total DNA in a genome. In this study, we identify a natural Cas9 enzyme from the bacterial genome of </span><i style=""font-size: 18px; font-weight: 400;"">Streptococcus canis</i><span style=""font-size: 18px; font-weight: 400;""> that has a PAM sequence with only a single G as its PAM sequence (5’-NNG-3’), allowing flexible targeting of up to 50% of all DNA sequences in living organisms. This new molecular tool potentially grants unprecedented access to correct disease-related mutations, enhance agricultural methods, and expand research efforts.&nbsp;</span></p>",,,2018-10-25 16:09:08.650,True,2017-01-04,Opening wider genomic access with a flexible CRISPR enzyme,PUBLIC,,True,Molecular Machines,False
synthetic-genome-engineering,njakimo,False,"<p>We are currently developing novel DNA editing technologies to broaden the scope of genome engineering. Our strategy is based on identifying and engineering endonucleases from diverse living systems, along with targeting with synthetic molecules. Together these components confer greater stability, minimize off-target DNA cleavage, and eliminate sequence restrictions for precision genetic manipulations within cells.</p>",,,2017-09-23 06:18:07.043,True,2016-06-06,Synthetic Genome Engineering,PUBLIC,,True,Molecular Machines,False
opening-wider-genomic-access-with-a-flexible-crispr-enzyme,njakimo,False,"<p>The CRISPR-Cas9 system has proven to be a versatile tool for genome editing, with&nbsp;<span style=""font-size: 18px; font-weight: 400;"">numerous implications in medicine, agriculture, bioenergy, food security, and beyond. The&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">range of targetable DNA sequences is limited, however, by the need for a short sequence of DNA beside the target site, called the PAM. In total, there are only a handful of CRISPR enzymes with a short enough PAM sequence to be able to target a large portion of the total DNA in a genome. In this study, we identify a natural Cas9 enzyme from the bacterial genome of </span><i style=""font-size: 18px; font-weight: 400;"">Streptococcus canis</i><span style=""font-size: 18px; font-weight: 400;""> that has a PAM sequence with only a single G as its PAM sequence (5’-NNG-3’), allowing flexible targeting of up to 50% of all DNA sequences in living organisms. This new molecular tool potentially grants unprecedented access to correct disease-related mutations, enhance agricultural methods, and expand research efforts.&nbsp;</span></p>",,,2018-10-25 16:09:08.650,True,2017-01-04,Opening wider genomic access with a flexible CRISPR enzyme,PUBLIC,,True,Molecular Machines,False
flipfeed,annyuan,False,"<p class=""""><a href=""https://chrome.google.com/webstore/detail/flipfeed/glfjakcglibkihmaaekjcaefcbebgcfg?hl=en-US&amp;gl=US"">FlipFeed is a Google Chrome Extension</a> that enables Twitter users to replace their own feed with that of another real Twitter user. Powered by deep learning and social network analysis, feeds are selected based on inferred political ideology (""left"" or ""right"") and served to users of the extension. For example, a right-leaning user who uses FlipFeed may load and navigate a left-leaning user's feed to observe the news stories, commentary, and other content they consume. The user can then decide to flip back to their own feed or repeat the process with another feed. We hope tools like FlipFeed will enable us to explore how social media platforms can be used to mitigate, rather than exacerbate, ideological polarization by helping people explore and empathize with different perspectives.<br></p>",,,2017-03-08 16:10:50.709,True,2016-11-11,FlipFeed,PUBLIC,http://flipfeed.media.mit.edu,True,Social Machines,False
collectivedebate,annyuan,False,"<p>On&nbsp;<a href=""http://collectivedebate.mit.edu/"">Collective Debate</a>,&nbsp;users take a test of their morality, then debate an artificial agent regarding a controversial claim: that differences in professional outcomes between men and women arise from bias as opposed to biology. Users indicate how much they agree with the claim, then they exchange arguments with the agent (who assumes the opposite position). After the debate, users are asked to re-evaluate their position. The artificial agent is trained to select arguments that nudge the user to become more moderate.</p>",,,2018-02-21 18:31:02.278,True,2017-11-14,Collective Debate,PUBLIC,http://collectivedebate.mit.edu/,True,Social Machines,False
social-media-mirror,annyuan,False,"<p>Social Mirror is a web application that helps Twitter users interactively explore the politically active parts of their social network.  Worsening political polarization over the past several years has exacerbated ideological echo chambers, which in turn have further fueled polarization by widening knowledge and empathy gaps between disparate groups.  We hope digital tools like Social Mirror can help inspire self-reflection, and ultimately, intellectual humility by providing people with a new view of their social media ecosystems and helping them form new network connections.</p>",,,2018-11-15 19:09:55.909,True,2017-06-01,Social Mirror,PUBLIC,,True,Social Machines,False
flipfeed,ngillani,False,"<p class=""""><a href=""https://chrome.google.com/webstore/detail/flipfeed/glfjakcglibkihmaaekjcaefcbebgcfg?hl=en-US&amp;gl=US"">FlipFeed is a Google Chrome Extension</a> that enables Twitter users to replace their own feed with that of another real Twitter user. Powered by deep learning and social network analysis, feeds are selected based on inferred political ideology (""left"" or ""right"") and served to users of the extension. For example, a right-leaning user who uses FlipFeed may load and navigate a left-leaning user's feed to observe the news stories, commentary, and other content they consume. The user can then decide to flip back to their own feed or repeat the process with another feed. We hope tools like FlipFeed will enable us to explore how social media platforms can be used to mitigate, rather than exacerbate, ideological polarization by helping people explore and empathize with different perspectives.<br></p>",,,2017-03-08 16:10:50.709,True,2016-11-11,FlipFeed,PUBLIC,http://flipfeed.media.mit.edu,True,Social Machines,False
pathways,ngillani,False,"<p>Our social networks influence our sense of what's possible: we can't aspire to be cancer researchers, activists, or artificial intelligence engineers if we've never been exposed to these as possibilities.  Unfortunately, many children grow up in environments replete with exposure gaps, impeding awareness and ultimately limiting their conceptions of which opportunities are available to them.</p><p>Pathways is a web application that seeks to scaffold career exploration and introspection among young people in order to help them explore a) what kinds of topics they might pursue in the future, b) in which capacities they might pursue these topics, and c) examples of education and career pathways others have traversed to get where they are today.  The tool uses several data science and machine learning techniques to process self-reported education and career data from thousands of individuals in the Greater Boston area.</p><p>Ultimately, we hope tools like Pathways can help enhance exposure and spark new social network ties that help foster greater upward mobility and an improved quality of life.</p>",,,2019-04-10 23:26:21.291,True,2018-05-01,Pathways,PUBLIC,,True,Social Machines,False
language-simplification,ngillani,False,"<p><span style=""font-size: 18px; font-weight: 400;"">The Language Simplification project is developing automatic methods to simplify complex texts to be more easily read and understood by a broader audience, such as children and non-native English speakers. Using neural networks, complex words and phrases can be substituted, the sentence can be split and rephrased, and the overall text can be summarized and compressed.&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">These capabilities can be wrapped into a reading assistance tool for end users, or as a pre-processing step for other NLP tasks.</span></p>",,,2019-04-17 20:51:49.717,True,2019-02-01,Language Simplification,PUBLIC,,True,Social Machines,False
social-media-mirror,ngillani,False,"<p>Social Mirror is a web application that helps Twitter users interactively explore the politically active parts of their social network.  Worsening political polarization over the past several years has exacerbated ideological echo chambers, which in turn have further fueled polarization by widening knowledge and empathy gaps between disparate groups.  We hope digital tools like Social Mirror can help inspire self-reflection, and ultimately, intellectual humility by providing people with a new view of their social media ecosystems and helping them form new network connections.</p>",,,2018-11-15 19:09:55.909,True,2017-06-01,Social Mirror,PUBLIC,,True,Social Machines,False
the-telemetron,sands,False,"<p>Today, the environments that humans occupy in space are designed for survival. Humans are carefully shuttled to and from space, and during their relatively short stays, they are provided with minimum supplies to remain alive and able to perform experiments. As we begin to plan less for short visits and more for life in space (such as a six to eight month trip to Mars and beyond) the question becomes: What does human culture look like in space?</p><p><a href=""https://www.instagram.com/nico_lh/"">Nicole L'Huillier</a> and <a href=""https://www.instagram.com/sandsfish"">Sands Fish</a> decided to explore how design and creativity might evolve as we begin to do more than merely survive in space. <b>The Telemetron</b> is a unique mode of musical performance that takes advantage of the poetics of zero gravity, and opens a new field of musical creativity. The project attempts to expand expression beyond the limits of earth-based instruments and performers. Leveraging sensors, data transmission and capture (for performance after flight), as well as their experience as composers and performers, Sands and Nicole explore a new body language for music. </p><p>The Telemetron was played for the first time during the inaugural Media Lab Space Exploration Initiative's Zero G flight. This instrument is a clear dodecahedron chamber that contains customized ""chimes"" containing gyroscopes. The chimes emit their telemetry as they spin and collide. Sensors record the position, direction, and spin of each chime. These elements create the composition. The performers play the instrument by moving it in space, shaking it, colliding it. The performance can be recorded to be experienced on earth or used as a live instrument during future space flights. The instrument can be played inside space craft or in the vacuum of space without the benefit of sound waves.</p><p>Recorded as a beautiful audio-visual experience, this experiment opens the doors for new forms of creative expression, and brings the magic of space to musicians. We hope to reach beyond the utilitarian, and toward the inspiring.</p>",,,2018-08-20 20:13:42.164,True,2017-08-01,The Telemetron,PUBLIC,,True,Civic Media,False
the-telemetron-adventures,sands,False,"<p>The&nbsp;Telemetron&nbsp;is a musical instrument specially designed to be performed in microgravity environments. It is created to explore the poetics of movement in outer space and the relational aspects of an antigravitational performance between human and non-human bodies. Through this line of work, we explore how the creation of culture might evolve as we leave Earth. The Telemetron project proposes a space in space for everybody - a space to share, to create, to listen.</p><p>During the summer of 2018,&nbsp; the Telemetron was presented on different occasions:</p><p>The Telemetron was exhibited&nbsp;at <a href=""https://ars.electronica.art/news/en/"">Ars Electronica</a>&nbsp;as part of the exhibition <a href=""https://ars.electronica.art/error/en/a-glitch-in-the-stars/"">""A Glitch in the Stars""</a> curated by the MIT Media Lab Space Exploration Initiative. Nicole L'Huillier designed the exhibition along with Sands Fish and Xin Liu.</p><p>The Telemetron was featured at <a href=""https://sonarplusd.com/"">Sónar+D</a>, a festival that explores how creativity is changing our present and imagining new futures. Nicole was invited to speak in the <a href=""https://sonarplusd.com/en/programs/barcelona-2018/areas/talks/making-music-in-space"">""Making Music in Space""</a> panel. She also gave a workshop called <a href=""https://sonarplusd.com/en/programs/barcelona-2018/areas/workshops/antigravitational-luthiers"">""Antigravitational Luthiers""</a>. Also, the Telemetron was part of <a href=""https://sonarplusd.com/en/programs/barcelona-2018/areas/the-zero-gravity-band/the-zero-gravity-band"">The Zero Gravity Band</a> Exhibition.</p><p>We published the paper <a href=""http://www.nime.org/proceedings/2018/nime2018_paper0066.pdf"">“Telemetron: a musical instrument for performance in zero gravity”</a> at <a href=""http://nime2018.icat.vt.edu/"">NIME</a>, and Sands Fish presented it at the international conference on New Interfaces for Musical Expression, describing the technical design of our first Telemetron.&nbsp;</p><p>Nicole also gave a talk about the Telemetron at the<a href=""https://www.eventrid.cl/eventos/atenea/en-orbita-2018""> En Orbita Festival</a> in NYC.</p><p>The Telemetron was created by Nicole L'Huillier and Sands Fish. With the assistance of Thomas Sanchez Lengeling, Sarah Hua, and Matt Carney. It was created on the context of the Space Exploration Initiative first zero gravity research flight. We are currently working on more space instruments, stay tuned.</p>",,,2019-02-14 19:38:59.554,True,2018-06-13,The Telemetron Adventures,PUBLIC,,True,Civic Media,False
ai-ethics-and-governance,tenzin,False,"<p>This project will support social scientists, philosophers, and policy and legal scholars who undertake research that aims to impact how artificial intelligence technologies are designed, implemented, understood, and held accountable. It will also provide a platform to create, convene, and support a diverse and powerful network of people and institutions who are working to steer AI in ethically conscious directions, both in fields of specialized AI as well as general AI. &nbsp;The project will investigate the social implications of the maturation and proliferation of AI. It will help catalyze and support research that advances AI in the public interest and fund engineers who want to help define public interest in AI through the code they write and machines they build. </p><p>	The initiative also organized a high-level symposium at the Media Lab on the topic that took place in April 2016 between the academic community and industry leaders working on AI.</p>",,,2018-02-21 21:42:35.499,True,2016-04-01,AI Ethics and Governance,PUBLIC,,True,Other,False
learning-empathy,tenzin,False,"<p>As part of its broader work around learning, this project is exploring both individualized and community-based models for promoting empathy by designing training methods and developing complementary &nbsp;technologies. The initiative launched 20 Day Stranger app with Playful Systems . The initiative is also working with the Opera of the Future group on a Vocal Vibrations/ Finding Your Voice interactive exhibition targeted towards awareness, empathy, and empowerment. The exhibition debuted in Paris and Cambridge with plans for Mexico City in 2017.
                    
                </p>",,,2018-12-11 18:23:20.052,True,2017-05-01,"Strangers, Voices, and Society",PUBLIC,,True,Other,False
ethics-learning-and-education,tenzin,False,"<p>This initiative is designing and disseminating new tools and pedagogies for introducing “non-prescriptive” ways of learning about ethical frameworks and values in educational and governmental organizations, including law enforcement agencies, around the world. &nbsp;The program is collaborating with high schools and middle schools to develop new curricula and resources &nbsp;for teachers that integrate critical thinking methodologies with hands-on workshops. &nbsp;The current project has involved designing apps and games and is working with a variety of collaborators including Education Arcade, Learning Games Network, MasterCard Foundation, Hope Lab, WGBH, and Televisa. Programs have already been deployed through organizations in the US, Mexico, Colombia, India, and parts of Africa.
                    
                </p>",,,2017-07-24 18:13:30.834,True,2017-05-01,Ethics and Empathy Learning,PUBLIC,,True,Other,False
gene-editing-and-biomedical-ethics,tenzin,False,"<p>The initiative, working with George Church, professor of genetics at Harvard Medical School, and Kevin Esvelt, head of the Sculpting Evolution Group &nbsp;at the Media Lab, will explore the broader ethical dimensions for developing tools that involve CRISPR and the expansion of gene editing technologies. This work aims to create a dialog between scientists and citizens, and will include spiritual leaders, religious leaders, and community leaders in a wider conversation about the ethical implications and potential repercussions of the introduction and deployment of these emerging technological interventions. This project is exploring novel ways to inform citizens about science and how they can affect policy based on news scientific developments.
                    
                </p>",,,2017-05-30 15:47:22.536,True,2017-05-01,Gene Editing and Biomedical Ethics,PUBLIC,,True,Other,False
transformative-leadership,tenzin,False,"<p>Despite of gazillions of leadership programs across university campuses and organizations, we increasingly live in an era of leadership deficit. Transformative Leadership helps participants explore how to align professional values with their deep personal values and develop reflective thinking skills for ethics and values-based leadership. The program is tailored for various demographics including leadership in business organizations, governments and law enforcement agencies. This program is equipped with longitudinal assessment protocols to study in-depth transformational abilities of individuals and how they leverage it for organizations.<br><br>Transformative Leadership is currently offered in the US, Mexico, Colombia, India &nbsp;and Canada. &nbsp;At MIT, Transformative Leadership modules are offered through MIT Sloan and Open Leadership at MIT Media Lab.</p>",,,2017-07-31 18:25:57.668,True,2017-02-01,Transformative Leadership,PUBLIC,,True,Other,False
aneye-extending-the-reach-of-anterior-segment-ophthalmic-imaging,sssinha,False,"<p>Eye exams via a slit lamp are critical in early diagnosis of diseases such as cataracts, corneal injury, and pterygia, in order to avert vision loss. The slit lamp is one of the most versatile tools in an ophthalmologist's clinic, but is big, expensive, and is designed with specialized ophthalmic clinics in mind. AnEye is a suite of portable, computationally driven solutions that leverage modern optics and commercially available consumer electronics to extend the reach of examinations of the anterior segment of the eye well beyond large hospitals and clinics, into resource-constrained settings such as rural mass-screening camps, mobile ophthalmology clinics, and even primary care.</p>",,--Choose Location,2019-04-19 17:46:56.865,True,2015-09-01,AnEye: Extending the reach of anterior segment ophthalmic imaging,PUBLIC,,True,Camera Culture,False
mathland,jsirera,False,"<p>Mathematical experiences are intrinsic to our everyday lives, yet mathematics education is mostly confined to textbooks. Seymour Papert used the term ""Mathland"" to propose a world where one would learn mathematics as naturally as one learns French while growing up in France. We built a mixed reality application that augments the physical world with interactive mathematical concepts and annotations to create a real-life Mathland. Using Mathland, people can collaboratively explore, experience, and experiment with mathematical phenomena in their real, physical environments using tangible objects. Mathland opens up new opportunities for mathematical learning using Papert's constructionist principles in an immersive environment that affords situated learning, embodied interaction and playful constructionism.</p>",,,2019-04-17 20:11:43.769,True,2017-03-08,Mathland: Play with math in mixed reality,PUBLIC,,True,Fluid Interfaces,False
tidzam,duhart,False,"<p>Tid'Zam is an ambient sound analysis system for outdoor environments. It is a component of the Tidmarsh Farms project which monitors the environmental evolution of an industrial cranberry farm during its ecological restoration of wetland. Tid'Zam analyzes the audio streams generated by the deployed microphones in the wild in order to detect the sonic events happening on the site, such as bird calls, insects, frogs, rain, storms, car noise, human voices, and more.&nbsp;</p><p>This system is used to cross-validate other sensors for weather monitoring to identify, geolocalize, and track present wildlife and bird specimens over time. It also controls the audio mixers in order to mute or change the gain on noisy microphones.</p>",,--Choose Location,2017-10-02 15:14:53.312,True,2016-01-01,Tid'Zam,PUBLIC,http://deep-resenv.media.mit.edu:8080,True,Responsive Environments,True
tidmarsh-living-observatory-portal,duhart,False,"<p>The Tidmarsh Living Observatory Portal is a research project that focuses on the realization of a pavilion that will&nbsp;generate an immersive experience about the Tidmarsh Living Observatory. This&nbsp;site has been restored from a former cranberry farm to natural wetland.&nbsp;Through an extensive Responsive Environments research, this networked&nbsp; and&nbsp;outdoor instrumented site streams live&nbsp; data that will be part of the portal&nbsp;experience.</p>",,,2019-04-19 18:16:55.498,False,2019-04-19,Tidmarsh Living Observatory Portal,PUBLIC,,True,Responsive Environments,True
tega-a-new-robot-platform-for-long-term-interaction,lukulele,False,"<p>Tega is a new robot platform designed to support long-term, in-home interactions with children, with applications in early-literacy education from vocabulary to storytelling.<br></p>",,--Choose Location,2018-02-02 12:45:47.861,True,2015-01-01,Tega: A New Social Robot Platform,PUBLIC,http://www.jinjoolee.com,True,Personal Robots,False
pev,inigo,False,"<h1><b>An Alternative Autonomous Revolution&nbsp;</b></h1><h2><i>System design for emerging urban contexts and societal aspirations</i></h2><p>The Persuasive Electric Vehicle (PEV) aims to solve urban mobility challenges with a healthy, convenient, sustainable alternative to cars. The PEV is a low-cost, agile, shared-use autonomous bike that can be either an electrically assisted tricycle for passenger commuting or an autonomous carrier for package delivery.</p><p>The PEV uses standard bicycle components and is lightweight (&lt;50kg) yet robust. Its sensors are easy to reconfigure and it has a 250W mid-drive electric motor and 10Ah battery pack that provides 25 miles of travel per charge and a top speed of 20 miles per hour.</p><p>Our vision for the PEV: a rider summons the PEV through a phone app, and the nearest available PEV arrives autonomously to meet the rider. Upon completing the trip, the PEV simply moves on to its next passenger or package pickup.&nbsp; The PEV can be autonomous, operated by the rider, or provide the rider with an electric assist. PEV's operate in bike lanes, avoiding the congestion and adding incentives to make more bikeable cities.</p>",,--Choose Location,2018-07-19 21:12:27.919,True,2014-09-01,Persuasive Electric Vehicle (PEV),PUBLIC,,True,City Science,False
shibuya-360-8k,eno,False,"<h1><b>Go Beyond 8K to experience telepresence without VR</b></h1><h2><b>[Concept] </b></h2><p><b>Experience telepresence without VR headset. </b></p><p>This application combines</p><ul><li>&nbsp;<b>360 degree video</b></li><li>&nbsp;<b>8K resolution display (7680pxls x 4320pxls)</b></li><li><b>AR technology</b></li><li><b>Stereophonic sound</b></li></ul><p>...to taste super-realistic sensation.</p><br><h2><b>360 video in 16K resolution</b></h2><p>Six DLR cameras are connected into one, to enable full spherical shooting with 20 K high resolution (down-converted into 16K in this prototype).&nbsp;</p>",,,2018-05-09 14:26:47.302,True,2018-03-01,Shibuya 360/8K,LAB-INSIDERS,,True,Responsive Environments,False
design-daydreams,pip,False,"<p>The Design Daydreams augmented drafting table projects the Looking Sideways exploration tool onto a tabletop to allow a more tangible interaction with the information being explored. Combined with a low-tech augmented reality tool that uses any mobile device in a simple holder to project digital animations on top of objects around the user, the tool allows digital and physical concepts to be overlaid on top of each other to provoke new reinterpretations and creative inspiration.&nbsp;</p>",,,2018-10-22 21:40:14.068,True,2017-10-27,Design Daydreams,LAB-INSIDERS,,True,Object Based Media,False
design-human-design,pip,False,"<p>design(human)design is a tool that builds on insights about the design process inspired by research carried out at IDEO Cambridge. The idea is that there are a few design ""variables"" that designers play with, and that they often like to provoke their creativity with ""random but purposeful"" inspirations—which present the designer with a random selection of design variables to act as a ""structured serendipitous"" creative prompt.</p><p>design(human)design comprises a deck of cards that act as a creative game to prompt new design ideas. The deck consists of sets of cards containing examples of each of the design variables, e.g. an object, an app, a book, etc., for the artifact design variable. Designers randomly select a card from each of the variable sets and create their design inspired by the MadLibs-style prompt sentence construct. Blank cards for each of the variables are also included so that designers can add their own examples.</p><p>To make the complete and selective randomization and personalization of the variables even easier, an interactive design(human)design website&nbsp;<span style=""font-size: 18px; font-weight: 400;"">was created at <a href=""http://designhumandesign.media.mit.edu"">designhumandesign.media.mit.edu</a>. The list of design variables are contained in a Google Spreadsheet that again can be added to by designers to customise their prompts.</span></p>",,,2017-09-13 15:06:37.907,True,2016-07-18,design(human)design,PUBLIC,,True,Object Based Media,False
looking-sideways,pip,False,"<p>Looking Sideways is an online inspiration browsing tool that seeks to provoke unexpected inspiration and guide pathways to new ideas by providing users with a selection of semi-randomly chosen, loosely related, diverse sources from art, design, history, and literature for every search query.&nbsp;</p>",,,2018-10-21 20:23:24.814,True,2018-04-16,Looking Sideways,LAB-INSIDERS,,True,Object Based Media,False
storyboards,shantell,False,"<p>Giving opaque technology a glass house, Storyboards present the tinkerers or owners of electronic devices with stories of how their devices work. Just as the circuit board is a story of star-crossed lovers—Anode and Cathode—with its cast of characters (resistor, capacitor, transistor), Storyboards have their own characters driving a parallel visual narrative.</p>",,--Choose Location,2017-04-03 01:11:34.352,True,2014-09-01,Storyboards,PUBLIC,,True,Social Computing,True
3d-printed-hemi-ellipsoidal-dome,cail,False,"<p>The Digital Construction Environment is the first architectural-scale structure fabricated with the <a href=""https://www.media.mit.edu/projects/digital-construction-platform-v-2/overview/"">Digital Construction Platform (DCP)</a>.  Using the Mediated Matter group’s Print-In-Place construction technique, an open-domed structure with a diameter of 14.6 m and a height of 3.7 m was manufactured over a print time of 13.5 hours.&nbsp;</p>",,,2019-02-11 20:03:42.274,True,2016-07-17,DCP: Digital Construction Environment,PUBLIC,,True,Mediated Matter,False
digital-construction-platform-v-2,cail,False,"<p>The Digital Construction Platform (DCP) is an experimental enabling technology for large-scale digital manufacturing. In contrast to the typical gantry-based approach to digital construction, robotic arm systems offer the promise of greater task flexibility, dynamically expandable workspaces, rapid setup times, and easier implementation with existing construction techniques. Potential applications for this system include fabrication of non-standard architectural forms; incorporation of data gathered on-site in real time into fabrication processes; improvements in construction efficiency, quality, and safety; and exploration of autonomous construction systems for use in disaster relief, hazardous environments, and extraterrestrial exploration.</p>",,,2017-05-05 16:41:04.046,True,2015-08-01,Digital Construction Platform,PUBLIC,,True,Mediated Matter,False
water-templating-and-biomaterial-skins-in-zero-gravity,cail,False,"<p>Water is life.&nbsp;</p><p>Extending the research that we are involved in with Mediated Matter’s water-based fabrication platform, we brought water-based biomaterial solutions into zero gravity observe their behavior. We created several small scaffolds of different geometries and surface textures that used the water-based solutions as their skins, taking advantage of&nbsp;the fact that van der Waal forces are the primary force affecting water in zero g, which allows water to maintain more structure. The solutions covered the body in a noticeable layering that would otherwise collapse at normal gravity.&nbsp;</p><p>The scaffolds were made of a hydrophilic polymer that is graded in its hydrophilicity, which combined with varying geometries, enabled us to vary the amount and type of liquid the scaffold can retain.&nbsp;Aqueous solution was rapidly applied to the scaffolds in zero gravity to see how thick of a skin we could create that is essentially all fluid.&nbsp;</p><p>This project has significance for research in dynamic cladding systems in space that could have biological functionality and carry mobile life-sustaining functions, as well as providing a skin that shields EM radiation.</p>",,,2018-05-10 18:43:35.423,True,2017-09-01,Water templating and biomaterial skins in zero gravity,LAB,,True,Mediated Matter,False
fiberbots,cail,False,"<p>FIBERBOTS is a digital fabrication platform fusing cooperative robotic manufacturing with abilities to generate highly sophisticated material architectures. The platform can enable design and digital fabrication of large-scale structures with high spatial resolution leveraging mobile fabrication nodes, or robotic ""agents"" designed to <i>tune</i> the material make-up of the structure being constructed on the fly as informed by their environment.<br></p><p>Some of nature’s most successful organisms collaborate in a swarm fashion. Nature’s builders leverage hierarchical structures in order to control and optimize multiple material properties. Spiders, for instance, spin protein fibers to weave silk webs with tunable local and global material properties, adjusting their material composition and fiber placement to create strong yet flexible structures optimized to capture prey. Other organisms, such as bees, ants and termites cooperate to rapidly build structures much larger than themselves. </p>",,,2019-02-13 16:36:22.730,True,2016-01-01,"FIBERBOTS: Design of a multi-agent, fiber composite digital fabrication system",PUBLIC,,True,Mediated Matter,False
3d-printed-hemi-ellipsoidal-dome,jleland,False,"<p>The Digital Construction Environment is the first architectural-scale structure fabricated with the <a href=""https://www.media.mit.edu/projects/digital-construction-platform-v-2/overview/"">Digital Construction Platform (DCP)</a>.  Using the Mediated Matter group’s Print-In-Place construction technique, an open-domed structure with a diameter of 14.6 m and a height of 3.7 m was manufactured over a print time of 13.5 hours.&nbsp;</p>",,,2019-02-11 20:03:42.274,True,2016-07-17,DCP: Digital Construction Environment,PUBLIC,,True,Mediated Matter,False
digital-construction-platform-v-2,jleland,False,"<p>The Digital Construction Platform (DCP) is an experimental enabling technology for large-scale digital manufacturing. In contrast to the typical gantry-based approach to digital construction, robotic arm systems offer the promise of greater task flexibility, dynamically expandable workspaces, rapid setup times, and easier implementation with existing construction techniques. Potential applications for this system include fabrication of non-standard architectural forms; incorporation of data gathered on-site in real time into fabrication processes; improvements in construction efficiency, quality, and safety; and exploration of autonomous construction systems for use in disaster relief, hazardous environments, and extraterrestrial exploration.</p>",,,2017-05-05 16:41:04.046,True,2015-08-01,Digital Construction Platform,PUBLIC,,True,Mediated Matter,False
3d-printed-hemi-ellipsoidal-dome,darweesh,False,"<p>The Digital Construction Environment is the first architectural-scale structure fabricated with the <a href=""https://www.media.mit.edu/projects/digital-construction-platform-v-2/overview/"">Digital Construction Platform (DCP)</a>.  Using the Mediated Matter group’s Print-In-Place construction technique, an open-domed structure with a diameter of 14.6 m and a height of 3.7 m was manufactured over a print time of 13.5 hours.&nbsp;</p>",,,2019-02-11 20:03:42.274,True,2016-07-17,DCP: Digital Construction Environment,PUBLIC,,True,Mediated Matter,False
aguahoja,darweesh,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
fiberbots,darweesh,False,"<p>FIBERBOTS is a digital fabrication platform fusing cooperative robotic manufacturing with abilities to generate highly sophisticated material architectures. The platform can enable design and digital fabrication of large-scale structures with high spatial resolution leveraging mobile fabrication nodes, or robotic ""agents"" designed to <i>tune</i> the material make-up of the structure being constructed on the fly as informed by their environment.<br></p><p>Some of nature’s most successful organisms collaborate in a swarm fashion. Nature’s builders leverage hierarchical structures in order to control and optimize multiple material properties. Spiders, for instance, spin protein fibers to weave silk webs with tunable local and global material properties, adjusting their material composition and fiber placement to create strong yet flexible structures optimized to capture prey. Other organisms, such as bees, ants and termites cooperate to rapidly build structures much larger than themselves. </p>",,,2019-02-13 16:36:22.730,True,2016-01-01,"FIBERBOTS: Design of a multi-agent, fiber composite digital fabrication system",PUBLIC,,True,Mediated Matter,False
blind-and-reference-free-fluorescence-lifetime-estimation-via-consumer-time-of-flight-sensors,ayush,False,"<p>Fluorescence lifetime imaging is a significant bio-imaging tool that finds important applications in life-sciences. Widely known applications include cancer detection and DNA sequencing. To that end, fluorescence microscopy which is at the heart of bio-imaging is an electronically and optically sophisticated device which is prohibitively expensive. Our work is demonstrates the fluorescence microscopy like functionality can be achieved by a simple, consumer sensor such as the Microsoft Kinect which costs about $100. This is done by trading-off the precision in optics and electronics for sophistication in computational methods. Not only this allows for massive cost reduction but leads to several advances in the area. For example, our method is calibration-free in that we do not assume sample's relative placement with respect to the sensor. Furthermore, our work opens new pathways of interaction between bio-imaging, optics and computer vision communities.</p>",,--Choose Location,2019-04-19 17:48:42.345,True,2015-01-01,Blind and reference-free fluorescence lifetime estimation via consumer time-of-flight sensors,PUBLIC,,True,Camera Culture,False
sensorchimes-musical-mapping-for-sensor-networks,eflynch,False,"<p>SensorChimes aims to create a new canvas for artists leveraging ubiquitous sensing and data collection. Real-time data from environmental sensor networks are realized as musical composition. Physical processes are manifested as musical ideas, with the dual goal of making meaningful music and rendering an ambient display. The Tidmarsh Living Observatory initiative, which aims to document the transformation of a reclaimed cranberry bog, provides an opportunity to explore data-driven musical composition based on a large-scale environmental sensor network. The data collected from Tidmarsh are piped into a mapping framework, which a composer configures to produce music driven by the data.</p>",,--Choose Location,2019-04-19 14:33:18.956,True,2015-09-01,SensorChimes: Musical mapping for sensor networks,PUBLIC,,True,Responsive Environments,False
doppelmarsh-cross-reality-environmental-sensor-data-browser,eflynch,False,"<p>Doppelmarsh is a cross-reality sensor data browser built for experimenting with presence and multimodal sensory experiences. Built on evolving terrain data from a physical wetland landscape, the software integrates real-time data from an environmental sensor network with real-time audio streams and other media from the site. Sensor data is rendered in the scene in both visual representations and as 3D sonification. Users can explore this data by walking on the virtual terrain in a first person view, or flying high above it. This flexibility allows Doppelmarsh to serve as an interface to other research platforms on the site, such as Quadrasense, an augmented reality UAV system that blends a flying live camera view with a virtual camera from Doppelmarsh. We are currently investigating methods for representing subsurface data, such as soil and water temperatures at depth, as well as automation in scene and terrain painting.</p>",,--Choose Location,2019-04-19 14:24:11.949,True,2014-09-01,Doppelmarsh: Cross-reality environmental sensor data browser,PUBLIC,,True,Responsive Environments,False
scale2018,taka_y,False,"<p>The SCALE platform adds <b>interactivity and trackability to everyday objects.&nbsp;</b>This load-sensitive surface is composed of only three loadcels beneath the surface, so that the system achieves <b>unobtrusive sensing</b>.</p><p>To capture and analyze human activities without any body-attached devices, a bunch of enabling technologies has been proposed. However, there have still been limitations in conventional approaches: RFID requires costly modification of the objects, and computer vision causes privacy issues. To address these problems, we propose ubiquitous load-sensing systems, networked platforms for giving interactivity and trackability to everyday objects by applying object-localization technology to every surface in our living environment and connecting all platforms with each other. Each platform is capable of localizing the position of external objects, utilizing pressure patterns acquired over time from multiple sensors as variables for the detection of location and weight. In addition, we treat the mass of an object as a unique identifier so that the system can trace the flow or detect consumption across platforms. As applications, we introduce a prototype that makes everyday objects tangible interfaces, privacy-preserving observation systems for family through medicine pill bottles and tooth brushes, and augmentation of existing interfaces.&nbsp;</p>",,,2018-10-22 20:46:39.996,True,2018-01-01,SCALE (2018),LAB-INSIDERS,,True,Tangible Media,False
aeromorph,nv2247,False,"<p>The project investigates how to make origami structure with inflatables with various materials. We introduce a universal bending mechanism that creates programmable shape-changing behaviors with paper, plastics, and fabrics. We developed a software tool that generates this bending mechanism for a given geometry, simulates its transformation, and exports the compound geometry as digital fabrication files. A custom heat-sealing head that can be mounted on usual three-axis CNC machines to precisely fabricate the designed transforming material is presented. We envision this technology could be used for designing interactive wearables and toys, and for the packaging industry.
                    
                </p><p>Visit&nbsp;<a href=""http://tangible.media.mit.edu/project/aeromorph/"">http://tangible.media.mit.edu/project/aeromorph/</a>.<br></p><p>Honorable Mention Paper Award, UIST 2016</p>",,,2017-04-24 18:56:38.724,True,2016-11-01,aeroMorph,PUBLIC,http://tangible.media.mit.edu/project/aeromorph/,True,Tangible Media,False
cultural-lens,nv2247,False,"<p>Cultural Lens is a speculative design project that reverses traditional gender responsibilities around clothing and personal appearance, interrogating the status of prevalent historical arguments around modesty and etiquette. Every society, culture, and religion has implicit or explicit expectations for women's clothing and public appearance, which has recently led to debates around issues for human rights, freedom, and self-expression.</p><p>Cultural Lens asks society-at-large to share the burden of enforcing public appearance for women. Instead of forcing women to wear clothing styles that are deemed acceptable by the public, Cultural Lens allows members of the public that might be offended–by, for example, perceived immodesty or improper etiquette–to have the freedom to selectively filter the appearance of the women they see in public to conform  literally to their view of how women should look.&nbsp;</p><p>The system uses a Microsoft HoloLens to implement an Augmented Reality visual field for the user.  The system can identify people and faces in the view, classify gender, and apply visual filters to their appearance according to the user's preference. For example, Cultural Lens can add digital veil to the faces of all women the user observes.&nbsp;<br></p>",,,2017-03-29 20:15:11.938,True,2017-01-03,Cultural Lens,PUBLIC,,True,Tangible Media,False
kinetix,nv2247,False,"<p>kinetiX is a transformable material featuring a design that resembles a cellular structure. It consists of rigid plates or rods and elastic hinges. These modular elements can be combined in a wide variety of ways and assembled into multifarious forms.</p><p>This project describes a group of auxetic-inspired material structures that can transform into various shapes upon compression. While the majority of the studies of auxetic materials focus on their mechanical properties and topological variations, our work proposes a parametric design approach that gives auxetic structures the ability to deform beyond shrinking or expanding. To do so, we see the auxetic structure as a parametric four-bar linkage. We developed four cellular-based material structure units composed of rigid plates and elastic/rotary hinges. Different compositions of these units lead to a variety of tunable shape-changing possibilities, such as uniform scaling, shearing, bending and rotating. By tessellating those transformations together, we can create various higher level transformations for design. The simulation is validated by the 3D printed structures.&nbsp;</p><p>&nbsp;We hope this work will inspire research in metamaterials design, shape-changing materials, and transformable architecture.</p>",,,2019-01-24 14:26:13.216,True,2017-03-01,kinetiX,PUBLIC,,True,Tangible Media,False
wireless-sensing-for-drones-agile-robots-robotics,nselby,False,"<p>Can drones find missing items? Every year, companies lose billions of dollars due to misplaced items and faulty inventory records in their warehouses. Consider that the smallest Walmart warehouse is larger than 17 football fields, making it impossible to keep track of all items in the warehouse. </p><p>To overcome this challenge, we introduce RFly, a drone-based wireless system that can scan and locate items in warehouses. The system leverages cheap, battery-free RFID (Radio Frequency Identifier) stickers, which are attached to every item in the warehouse similar to barcodes. These RFIDs power up and respond with a unique identifier when commanded by a wireless device called a reader. To scan a warehouse, a drone operator dispatches a small, inexpensive, and safe drone which flies throughout a warehouse, cataloging and localizing all the RFIDs in a warehouse. The video below shows how the system operates.</p>",,,2018-08-22 16:00:28.691,True,2016-11-01,RFly: Drones that find missing objects using battery-free RFIDs,PUBLIC,,True,Signal Kinetics,False
rfid-localization,nselby,False,"<p>Presenting RFind, a new technology that allows us to locate almost any object with extreme accuracy by transforming low-cost, battery-free wireless stickers into powerful radars. At a high level, our technology operates by measuring the time it takes the signal to travel from the wireless sticker to an access point. By taking into account the speed of propagation of light, we can then map the time to an exact location (with sub-centimeter precision) in 3D space.</p>",,,2018-10-19 14:06:36.714,True,2016-12-04,RFind: Extreme localization for billions of items,PUBLIC,,True,Signal Kinetics,False
warehouse-RFID,nselby,False,"<p>Every year, billions of dollars are wasted in lost objects which could be avoided by tagging them with RFIDs.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">Between 2003 and 2011, the US army lost track of $5.8&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">billion of supplies among its warehouses. In 2013&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">alone, Walmart lost $3 billion in sales because of mismatches between&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">its recorded inventory and actual stock. In 2016,&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">the US National Retail Federation reported that shrinkage—i.e.,&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">loss of items in retail stores—averaged 1.38% of retail sales,&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">which translates to $45.2 billion annually.</span></p><p>The fundamental challenge with RFIDs ,however, is that they are only reliable at fairly short distances of a few meters. The distance becomes much lower if the RFID is buried under other objects or if the tag’s antenna is oriented away from that of the reader.<br></p><p>This projects develops a technology that overcomes the range challenge of RFIDs by leveraging drones. By mounting a new compact circuit on the drone, we demonstrate how we can significantly increase the coverage for RFID localization systems. The technology does not &nbsp;require changing the RFID or the readers, and can achieve two orders of magnitude improvement in coverage area of RFIDs—increasing them from few squared meters to hundreds of squared meters.<span style=""font-size: 16px;""><br></span></p>",,,2017-11-02 01:34:55.226,True,2016-10-10,Warehouse-scale RFID Localization using Drones,LAB-INSIDERS,,True,Signal Kinetics,False
wireless-sensing-for-drones-agile-robots-robotics,fadel,False,"<p>Can drones find missing items? Every year, companies lose billions of dollars due to misplaced items and faulty inventory records in their warehouses. Consider that the smallest Walmart warehouse is larger than 17 football fields, making it impossible to keep track of all items in the warehouse. </p><p>To overcome this challenge, we introduce RFly, a drone-based wireless system that can scan and locate items in warehouses. The system leverages cheap, battery-free RFID (Radio Frequency Identifier) stickers, which are attached to every item in the warehouse similar to barcodes. These RFIDs power up and respond with a unique identifier when commanded by a wireless device called a reader. To scan a warehouse, a drone operator dispatches a small, inexpensive, and safe drone which flies throughout a warehouse, cataloging and localizing all the RFIDs in a warehouse. The video below shows how the system operates.</p>",,,2018-08-22 16:00:28.691,True,2016-11-01,RFly: Drones that find missing objects using battery-free RFIDs,PUBLIC,,True,Signal Kinetics,False
programming-wireless-networks-computer-networks,fadel,False,"<p>Wireless networks—consisting of WiFi, LTE, RFIDs, and millimeter-wave devices—have become integral parts of our everyday lives. Our research explores how we can make these networks faster, more robust, and seamlessly mobile. It also explores how we can use these networks for purposes other than communication, such as localization, sensing, and control.
                    
                </p>",,,2018-08-22 01:30:56.157,True,2016-11-01,Programming Wireless Networks (Computer Networks),PUBLIC,,True,Signal Kinetics,False
seeing-through-walls-computer-vision,fadel,False,"<p>Our group develops technologies that can see through walls and perform motion capture through occlusions. To do so, we rely on wireless signals, like WiFi. These signals traverse walls and reflect off humans behind the wall before returning to a wireless receiver. We design and develop new algorithms and software-hardware systems that can extract these signals and analyze them to capture human motion from behind a wall.
                    
                </p>",,,2017-08-22 14:30:57.619,True,2016-11-01,Seeing Through Walls,PUBLIC,,True,Signal Kinetics,False
millimeter-wave,fadel,False,<p>We are developing new millimeter wave technologies for health sensing and imaging. The technology can potentially detect tumors and track blood flow inside the human body.</p>,,,2018-08-22 01:32:29.325,True,2017-05-01,Millimeter Wave Health Sensing,LAB-INSIDERS,,True,Signal Kinetics,False
rfid-localization,fadel,False,"<p>Presenting RFind, a new technology that allows us to locate almost any object with extreme accuracy by transforming low-cost, battery-free wireless stickers into powerful radars. At a high level, our technology operates by measuring the time it takes the signal to travel from the wireless sticker to an access point. By taking into account the speed of propagation of light, we can then map the time to an exact location (with sub-centimeter precision) in 3D space.</p>",,,2018-10-19 14:06:36.714,True,2016-12-04,RFind: Extreme localization for billions of items,PUBLIC,,True,Signal Kinetics,False
warehouse-RFID,fadel,False,"<p>Every year, billions of dollars are wasted in lost objects which could be avoided by tagging them with RFIDs.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">Between 2003 and 2011, the US army lost track of $5.8&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">billion of supplies among its warehouses. In 2013&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">alone, Walmart lost $3 billion in sales because of mismatches between&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">its recorded inventory and actual stock. In 2016,&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">the US National Retail Federation reported that shrinkage—i.e.,&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">loss of items in retail stores—averaged 1.38% of retail sales,&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">which translates to $45.2 billion annually.</span></p><p>The fundamental challenge with RFIDs ,however, is that they are only reliable at fairly short distances of a few meters. The distance becomes much lower if the RFID is buried under other objects or if the tag’s antenna is oriented away from that of the reader.<br></p><p>This projects develops a technology that overcomes the range challenge of RFIDs by leveraging drones. By mounting a new compact circuit on the drone, we demonstrate how we can significantly increase the coverage for RFID localization systems. The technology does not &nbsp;require changing the RFID or the readers, and can achieve two orders of magnitude improvement in coverage area of RFIDs—increasing them from few squared meters to hundreds of squared meters.<span style=""font-size: 16px;""><br></span></p>",,,2017-11-02 01:34:55.226,True,2016-10-10,Warehouse-scale RFID Localization using Drones,LAB-INSIDERS,,True,Signal Kinetics,False
cyber-physical-security-and-privacy-system-network-security,fadel,False,"<p>In the age of ubiquitous connectivity and the Internet of Things, our security and privacy have taken on new dimensions. For example, how can we ensure that our locations are not being tracked from our cellphones? And, how can we prevent an unauthorized user from hacking into our smart home systems? Our research aims at developing primitives that can address these challenges. To do so, we explore intrinsically new security mechanisms that operate across all computing stacks to secure not only the bits but also the integrity of the sensed signals, and to protect the privacy of the sensed environments.
                    
                </p>",,,2018-09-05 01:10:16.706,True,2016-11-01,Cyber-Physical Security and Privacy (System & Network Security),PUBLIC,,True,Signal Kinetics,False
health-sensing-human-computer-interaction,fadel,False,"<p>Today’s health sensors (which monitor breathing, heartbeats, steps, etc.) require their users to wear them on their bodies. In contrast, our technologies can monitor human health without requiring the user to wear any device on his/her body. To do so, we capture and analyze wireless signals reflected off the human body; we then use these reflected signals to extract breathing and heartbeats without any physical contact with the human body. We are currently exploring techniques to remotely sense additional health metrics like blood pressure, oxygen saturation, and glucose levels. Monitoring these health metrics can render ICU (intensive care unit) vital sign monitors completely noninvasive and enable continuous monitoring of diabetes patients.</p>",,,2019-04-17 18:06:52.094,True,2016-11-01,Health sensing using wireless signals,PUBLIC,,True,Signal Kinetics,False
translational-acoustic-rf-tarf-communication,fadel,False,"<p>Did you know that submarines today still cannot wirelessly communicate with airplanes? For decades, communicating between underwater and the air has remained an unsolved problem. Underwater, submarines use acoustic signals (or SONAR) to communicate; in the air, airplanes use radio signals like cellular or WiFi. But neither of these signals can work across both water and air.</p><p>We present TARF (Translational Acoustic-RF&nbsp;communication), the&nbsp;first technology that enables communication between underwater and the air. A TARF transmitter sends standard sound (or SONAR signals).&nbsp; Sound travels as pressure waves; when these waves hit the surface, they cause it to vibrate. To pick up these vibrations, a TARF receiver in the air uses a very sensitive radar. The radar transmits a signal which reflects off the water surface and comes back. As the water surface vibrates, it causes small changes to the received radar signal, enabling a TARF receiver to sense the tiny vibrations caused by the underwater acoustic transmitter.</p><p>The video below explains how TARF works and some of its applications.</p>",,,2019-04-17 18:09:35.809,True,2018-08-21,Wireless communication from underwater to the air,PUBLIC,,True,Signal Kinetics,False
learning-food-quality-and-safety-using-wireless-stickers,fadel,False,"​<p>We have developed a wireless system that leverages the inexpensive RFID tags already on hundreds of billions of products to sense potential food contamination. Our system, called RFIQ (Radio Frequency IQ), aims at democratizing food quality and safety, bringing it to the hands of consumers.&nbsp;</p>",,,2019-02-13 16:42:33.755,True,2018-09-01,RFIQ: Food quality and safety detection using wireless stickers,PUBLIC,,True,Signal Kinetics,False
ivn-in-vivo-networking,fadel,False,"<p>In-Vivo Networking (IVN)&nbsp;is the new technology that can wirelessly power and communicate with tiny devices implanted deep within the human body. Such devices could be used to deliver drugs, monitor conditions inside the body, or treat disease by stimulating the brain with electricity or light.&nbsp;&nbsp;</p><p>The implants are powered by radio frequency waves, which are safe for humans. In tests in animals, we showed that the waves can power devices located 10 centimeters deep in tissue, from a distance of one meter.</p>",,,2019-02-14 19:47:21.377,True,2017-09-01,In-Vivo Networking: Powering and communicating with tiny battery-free devices inside the body,PUBLIC,,True,Signal Kinetics,False
turbotrack1,fadel,False,,2021-07-31,,2019-02-19 17:21:52.504,False,2019-01-01,TurboTrack,PUBLIC,,True,Signal Kinetics,False
turbotrack-3d-backscatter-localization-for-fine-grained,fadel,False,"<p>TurboTrack is a 3D localization system for fine-grained robotic tasks, with unique capability to localize backscatter nodes with sub-centimeter accuracy without any constraints on their locations or mobility. We showed that TurboTrack can work in multiple collaborative applications with robotic arms and nanodrones including indoor tracking, packaging, assembly, and handover.</p><p>This research is partially funded by the National Science Foundation (NSF) and a Google Faculty Research Award.</p>",,,2019-04-17 18:08:41.636,True,2017-06-01,TurboTrack: 3D backscatter localization for fine-grained robotics,PUBLIC,,True,Signal Kinetics,False
wireless-sensing-for-drones-agile-robots-robotics,yunfeima,False,"<p>Can drones find missing items? Every year, companies lose billions of dollars due to misplaced items and faulty inventory records in their warehouses. Consider that the smallest Walmart warehouse is larger than 17 football fields, making it impossible to keep track of all items in the warehouse. </p><p>To overcome this challenge, we introduce RFly, a drone-based wireless system that can scan and locate items in warehouses. The system leverages cheap, battery-free RFID (Radio Frequency Identifier) stickers, which are attached to every item in the warehouse similar to barcodes. These RFIDs power up and respond with a unique identifier when commanded by a wireless device called a reader. To scan a warehouse, a drone operator dispatches a small, inexpensive, and safe drone which flies throughout a warehouse, cataloging and localizing all the RFIDs in a warehouse. The video below shows how the system operates.</p>",,,2018-08-22 16:00:28.691,True,2016-11-01,RFly: Drones that find missing objects using battery-free RFIDs,PUBLIC,,True,Signal Kinetics,False
millimeter-wave,yunfeima,False,<p>We are developing new millimeter wave technologies for health sensing and imaging. The technology can potentially detect tumors and track blood flow inside the human body.</p>,,,2018-08-22 01:32:29.325,True,2017-05-01,Millimeter Wave Health Sensing,LAB-INSIDERS,,True,Signal Kinetics,False
rfid-localization,yunfeima,False,"<p>Presenting RFind, a new technology that allows us to locate almost any object with extreme accuracy by transforming low-cost, battery-free wireless stickers into powerful radars. At a high level, our technology operates by measuring the time it takes the signal to travel from the wireless sticker to an access point. By taking into account the speed of propagation of light, we can then map the time to an exact location (with sub-centimeter precision) in 3D space.</p>",,,2018-10-19 14:06:36.714,True,2016-12-04,RFind: Extreme localization for billions of items,PUBLIC,,True,Signal Kinetics,False
warehouse-RFID,yunfeima,False,"<p>Every year, billions of dollars are wasted in lost objects which could be avoided by tagging them with RFIDs.&nbsp;<span style=""font-size: 18px; font-weight: normal;"">Between 2003 and 2011, the US army lost track of $5.8&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">billion of supplies among its warehouses. In 2013&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">alone, Walmart lost $3 billion in sales because of mismatches between&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">its recorded inventory and actual stock. In 2016,&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">the US National Retail Federation reported that shrinkage—i.e.,&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">loss of items in retail stores—averaged 1.38% of retail sales,&nbsp;</span><span style=""font-size: 18px; font-weight: normal;"">which translates to $45.2 billion annually.</span></p><p>The fundamental challenge with RFIDs ,however, is that they are only reliable at fairly short distances of a few meters. The distance becomes much lower if the RFID is buried under other objects or if the tag’s antenna is oriented away from that of the reader.<br></p><p>This projects develops a technology that overcomes the range challenge of RFIDs by leveraging drones. By mounting a new compact circuit on the drone, we demonstrate how we can significantly increase the coverage for RFID localization systems. The technology does not &nbsp;require changing the RFID or the readers, and can achieve two orders of magnitude improvement in coverage area of RFIDs—increasing them from few squared meters to hundreds of squared meters.<span style=""font-size: 16px;""><br></span></p>",,,2017-11-02 01:34:55.226,True,2016-10-10,Warehouse-scale RFID Localization using Drones,LAB-INSIDERS,,True,Signal Kinetics,False
learning-food-quality-and-safety-using-wireless-stickers,yunfeima,False,"​<p>We have developed a wireless system that leverages the inexpensive RFID tags already on hundreds of billions of products to sense potential food contamination. Our system, called RFIQ (Radio Frequency IQ), aims at democratizing food quality and safety, bringing it to the hands of consumers.&nbsp;</p>",,,2019-02-13 16:42:33.755,True,2018-09-01,RFIQ: Food quality and safety detection using wireless stickers,PUBLIC,,True,Signal Kinetics,False
ivn-in-vivo-networking,yunfeima,False,"<p>In-Vivo Networking (IVN)&nbsp;is the new technology that can wirelessly power and communicate with tiny devices implanted deep within the human body. Such devices could be used to deliver drugs, monitor conditions inside the body, or treat disease by stimulating the brain with electricity or light.&nbsp;&nbsp;</p><p>The implants are powered by radio frequency waves, which are safe for humans. In tests in animals, we showed that the waves can power devices located 10 centimeters deep in tissue, from a distance of one meter.</p>",,,2019-02-14 19:47:21.377,True,2017-09-01,In-Vivo Networking: Powering and communicating with tiny battery-free devices inside the body,PUBLIC,,True,Signal Kinetics,False
turbotrack-3d-backscatter-localization-for-fine-grained,yunfeima,False,"<p>TurboTrack is a 3D localization system for fine-grained robotic tasks, with unique capability to localize backscatter nodes with sub-centimeter accuracy without any constraints on their locations or mobility. We showed that TurboTrack can work in multiple collaborative applications with robotic arms and nanodrones including indoor tracking, packaging, assembly, and handover.</p><p>This research is partially funded by the National Science Foundation (NSF) and a Google Faculty Research Award.</p>",,,2019-04-17 18:08:41.636,True,2017-06-01,TurboTrack: 3D backscatter localization for fine-grained robotics,PUBLIC,,True,Signal Kinetics,False
playful-words,pbansal,False,"<p>While there are a number of literacy technology solutions developed for individuals, the role of social—or networked—literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.</p><p>By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.</p><p>To learn more about this project, please check out:&nbsp;<a href=""http://playfulwords.org/"">http://playfulwords.org/</a></p>",,--Choose Location,2018-04-30 20:28:15.298,True,2014-09-01,Playful Words,PUBLIC,,True,Social Machines,False
playful-words,isysoev,False,"<p>While there are a number of literacy technology solutions developed for individuals, the role of social—or networked—literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.</p><p>By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.</p><p>To learn more about this project, please check out:&nbsp;<a href=""http://playfulwords.org/"">http://playfulwords.org/</a></p>",,--Choose Location,2018-04-30 20:28:15.298,True,2014-09-01,Playful Words,PUBLIC,,True,Social Machines,False
play-analytics,isysoev,False,"<p>Analyzing detailed data from SpeechBlocks&nbsp;to understand how kids engage with constructionist literacy learning technologies, with the goal of empowering caregivers (e.g. parents, older siblings, tutors) with these insights.</p>",,,2018-04-30 20:56:57.445,True,2016-02-01,Play Analytics,PUBLIC,,True,Social Machines,False
speech-blocks,isysoev,False,"<p>SpeechBlocks is a medium that allows children (4-5 years old) to engage in open-ended play with writing. They can build arbitrary compositions out of words and associated images, which can become cards, signs, stories, and ""books."" We hypothesize that such creative, self-expressive play can foster development of basic literacy skills, like phonological awareness. However, because users of SpeechBlocks are not yet in command of writing, it is necessary for the system to scaffold and guide them. We study a variety of ways to accomplish this.<br></p>",,,2019-05-21 17:11:02.880,True,2015-03-01,SpeechBlocks,PUBLIC,,True,Social Machines,False
digitization-of-asset-registries,mrweber,False,"<h1><b><i>A blockchain protocol for verifiable records</i></b></h1><p>b_verify is the name of an applied research project and experimental protocol developed at the Digital Currency Initiative at the MIT Media Lab. Its purpose is to provide an improved technical foundation for the issuance, verification, and transaction of certain financial instruments and tradable securities, especially in markets pursuing the digitization of paper records. Upon this foundation, applications servicing the b_verify protocol can be customized for different use cases and contexts.</p><p>Forthcoming in 2018 are academic and industry papers along with a “pilot kit” containing open-source reference code (Java), the system architecture for the b_verify protocol, template desktop and mobile applications, and additional considerations for real world experimentation. Once complete, the pilot kit and associated materials will be distributed to governments, multilateral organizations, and industry associations around the world; advisory to pilot implementations may be provided upon request.</p><h2><b><i>Warehouse receipts</i></b></h2><p>The selected use case informing this research is the negotiable warehouse receipt for agricultural commodities. Warehouse receipts are legally defined title documents attesting to a particular quantity, type, and quality of a commodity at a specific storage facility. These instruments can be used to secure inventory as collateral for loans, to facilitate trade, and to settle expiring futures contracts.</p><p>Development agencies and multilaterals have championed the benefits of warehouse receipts for price discovery and access to credit for even the poorest of farmers. One USAID program in Tanzania produced a doubling of the prices farmers were able to command for their harvest immediately upon the installation of a storage and warehouse receipt program. Strengthening the agricultural sector also improves food security and competitiveness at a national level.</p><p>Three problems prevent warehouse receipts from realizing their full potential for farmers and society: forged documents, high transaction costs, and the potential for disparities between the receipt attestation and the physical goods. Applications using the b_verify protocol can help mitigate these problems.</p><p>First, high profile frauds involving forged or duplicated warehouse receipts have cost banks hundreds of millions of dollars; this makes banks wary of lending against them and traders wary of buying them. The b_verify protocol addresses this problem by posting warehouse receipt issuances as cryptographic commitments to the data structure of the Bitcoin blockchain as a secure, public source of record.</p><p>Second, assuming the receipts are authentic, the transaction costs involved in verifying and transporting paper records are extremely high, especially in countries with poor infrastructure. The b_verify protocol addresses this problem using a novel method of coordinating updates to the records using Merkle proofs constructed by a designated server, which need not be trusted.</p><p>Third, again assuming authentic receipts, banks and traders worry about the quality and honesty of warehouse custodianship; perhaps the goods are removed illegally for example. While this problem cannot be completely eliminated by technology, access control measures and Internet of Things (IoT) integrations can combined with the b_verify protocol to reduce these risks. For example, outflows of grain from a silo could require authentication via a query of the blockchain record, while digital devices measuring the outflow can independently commit updates to the record without human interference.</p><p>An added feature the b_verify protocol is the opportunity for the programmatic enforcement of covenants and contracts (also known as “smart contracts”). For example, using an application servicing the b_verify protocol, the pledging of collateral with a warehouse receipt could automate the transfer of the collateral to the lender upon a hard loan default. Covenants such as maximum debt-to-asset ratios or minimum allowed commodity price fluctuations could also be constructed within the b_verify system.</p><p>The verifiable activities of a given business over time, such as inventory turnover and repayment history, can also provide valuable insight into the health of the business, which is the chief consideration in assessing creditworthiness.</p><p>Lastly, the transparency provided by this publicly accessible and verifiable system of record may contribute to safer, more transparent in asset-backed securities and derivatives markets as these develop in emerging economies.</p><p>Thus, we form the following hypothesis: the b_verify protocol can contribute to meaningful reductions in the aforementioned problems, thereby maximizing the potential for warehouse receipts to improve price discovery and access to finance, as well as national food security and competitiveness.</p><h2><b><i>Progress to Date — From Mexico to Ukraine</i></b></h2><p>This research began in 2016, and originally focused on blockchain-based land title. This grew into a knowledge partnership with the Mexican ministries of Finance and Economy, which in turn motivated a pivot to the warehouse receipt use case, per Mexico’s priorities and a compelling opportunity to promote financial inclusion. 2016 stakeholder interviews and warehouse visits in Mexico informed the development of the first b_verify prototype in 2017. Upon seeing a demo, the Inter-American Development Bank joined the Digital Currency Initiative and contributed funding to further research with the aim of piloting b_verify in Latin America. Meanwhile, new members to the research team from Ukraine motivated an examination of that country’s warehouse receipt system and potential as a pilot destination. 2018 stakeholder interviews and warehouse visits in Ukraine provided valuable insights, which are now informing the development of an updated protocol, pilot kit, and associated papers.</p>",,,2018-04-08 02:27:49.458,True,2017-02-01,b_verify,PUBLIC,,True,Initiatives,False
andorra-living-lab,agnis,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,City Science,False
persuasive-cities,agnis,False,"<p>Persuasive Cities research is aimed at advancing urban spaces to facilitate societal changes. According to social science research, any well-designed environment can become a strong influencer of what people think and do. There is an endlessly dynamic interaction between a person, a particular behavior, and a specific environment. Persuasive Cities research leverages this knowledge to engineer persuasive environments and interventions for altering human behavior on a societal level. This research is focused on socially engaging environments for supporting entrepreneurship and innovation, reshaping routines and behavioral patterns in urban spaces, deploying intelligent outdoor sensing for shifting mobility modes, enhancing environmentally friendly behaviors through social norms, introducing interactive public feedback channels to alter attitudes at scale, engaging residents through socially influencing systems, exploring methods for designing persuasive neighborhoods, and fostering adoption of novel urban systems. More: http://bit.ly/TEDxp</p>",,--Choose Location,2017-03-31 20:52:58.972,True,2015-09-01,Persuasive Cities,PUBLIC,http://cp.media.mit.edu/agnis-stibe,True,City Science,False
andorra-living-lab,lrocher,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,Human Dynamics,False
andorra-living-lab,noriega,False,"<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>",,--Choose Location,2017-05-30 17:05:50.125,True,2015-01-01,Andorra Living Lab,PUBLIC,,True,Human Dynamics,False
active-fairness,noriega,False,"<h2>Algorithmic Fairness</h2><p>Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Substantial work in algorithmic fairness has surged, focusing on either post-processing trained models, constraining learning processes, or pre-processing training data.&nbsp;Recent work has proposed optimal post-processing methods that randomize classification decisions on a fraction of individuals in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concerns due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail.&nbsp;</p><h2>Active Fairness</h2><p>The present work proposes an alternative <b>active framework for fair classification</b>, where, in deployment, a decision maker adaptively acquires information according to the needs of different groups or individuals towards balancing disparities in classification performance. We propose two such methods where information collection is adapted to group- and individual-level needs, respectively. We show on real-world datasets that these can achieve: 1) <b>calibration and single error parity</b> (e.g., equal opportunity) and 2) <b>parity in both false positive and false negative rates</b> (e.g., equal odds). Moreover, we show that, by leveraging their additional degree of freedom, active approaches can <b>outperform randomization-based classifiers previously considered optimal</b>, while also avoiding limitations such as intra-group unfairness.</p>",,,2019-01-02 20:52:50.453,True,2018-01-31,Active Fairness in Algorithmic Decision Making,PUBLIC,,True,Human Dynamics,False
thermal-nlos-imaging,tomotomo,False,"<h2><b>Seeing around corners with a thermal camera. Our technique exploits unique properties of long-wave IR surface reflectance to see around corners.</b></h2><p>Seeing around corners is challenging in the visible spectrum as photons scatter at diffuse surfaces. However, the surface reflectance of common materials in the long-wave IR spectrum has a strong specular component, making it easier for us to see around corners. Furthermore, any heat sources are long-wave IR sources while common objects are not visible light source. We exploit these to recover 2D shape and 3D location of objects around corners with completely passive sensing.<br></p><p><b>Key Idea</b></p><p>Long-wave IR has much stronger specular surface reflectance on common surfaces than visible light. Any heat sources act as light sources in the long-wave IR spectrum, and their emission can be estimated if the temperature is known. We exploit these two properties of long-wave IR to see around corners. We first estimate the bidirectional reflectance distribution function (BRDF) of the wall at a corner. The estimated BRDF gives the mapping between the hidden object's long-wave IR emission and the measurements of a thermal camera. Using this mapping, our technique recovers the 2D shape and the 3D location of the hidden heat sources around corners. This method is completely passive and does not require occlusion geometries that other passive techniques require.&nbsp;</p>",,,2019-04-18 13:40:06.043,True,2018-10-01,Seeing around corners with thermal imaging,LAB-INSIDERS,,True,Camera Culture,False
codespace,sanjayg,False,"<p>Many behaviors spread through social contact. However, different behaviors seem to require different degrees of social reinforcement to spread within a network. We study and visualize the&nbsp;learning paths of hundreds of thousands of programmers on the social coding platform Github. We show the influence of collaborators on technology and knowledge diffusion. This research has interesting applications in understanding key influencers and early adopters in technology teams and software organizations</p>",,,2018-05-03 20:05:46.108,True,2018-04-01,CodeSpace,LAB-INSIDERS,,True,Collective Learning,False
replot,sanjayg,False,"<p>Replot is a new and soon-to-be-open sourced visualization library for the web, built natively on the ReactJS, the extremely popular open-source web framework that powers most modern websites today.</p><p>Replot is written from the ground up in React, rendering native SVG visualizations and fully leveraging the idea of user interfaces being pure functions of an underlying data ""state."" This enables developers to construct rich and engaging visualizations with very few lines of code that bind automatically to their datasets, and animate automatically when their data change.&nbsp;</p><p>Replot also introduces a paradigm shift in customizability of your visualizations, enabling you to trigger transformations in your visualizations by simply manipulating state variables.</p>",,,2017-10-11 19:58:05.810,True,2017-06-01,Replot,LAB-INSIDERS,,True,Collective Learning,False
aguahoja,yjtai,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
water-based-additive-manufacturing,yjtai,False,"<p>This research presents water-based robotic fabrication as a design approach and enabling technology for additive manufacturing (AM) of biodegradable hydrogel composites. We focus on expanding the dimensions of the fabrication envelope, developing structural materials for additive deposition, incorporating material-property gradients, and manufacturing architectural-scale biodegradable systems. The technology includes a robotically controlled AM system to produce biodegradable composite objects, combining natural hydrogels with other organic aggregates. It demonstrates the approach by designing, building, and evaluating the mechanics and controls of a multi-chamber extrusion system. Finally, it provides evidence of large-scale composite objects fabricated by our technology that display graded properties and feature sizes ranging from micro- to macro-scale. Fabricated objects may be chemically stabilized or dissolved in water and recycled within minutes. Applications include the fabrication of fully recyclable products or temporary architectural components, such as tent structures with graded mechanical and optical properties.</p>",,--Choose Location,2019-06-04 21:35:42.589,True,2014-01-01,Water-Based Additive Manufacturing,PUBLIC,,True,Mediated Matter,False
aguahoja,dumo,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
water-based-additive-manufacturing,dumo,False,"<p>This research presents water-based robotic fabrication as a design approach and enabling technology for additive manufacturing (AM) of biodegradable hydrogel composites. We focus on expanding the dimensions of the fabrication envelope, developing structural materials for additive deposition, incorporating material-property gradients, and manufacturing architectural-scale biodegradable systems. The technology includes a robotically controlled AM system to produce biodegradable composite objects, combining natural hydrogels with other organic aggregates. It demonstrates the approach by designing, building, and evaluating the mechanics and controls of a multi-chamber extrusion system. Finally, it provides evidence of large-scale composite objects fabricated by our technology that display graded properties and feature sizes ranging from micro- to macro-scale. Fabricated objects may be chemically stabilized or dissolved in water and recycled within minutes. Applications include the fabrication of fully recyclable products or temporary architectural components, such as tent structures with graded mechanical and optical properties.</p>",,--Choose Location,2019-06-04 21:35:42.589,True,2014-01-01,Water-Based Additive Manufacturing,PUBLIC,,True,Mediated Matter,False
aguahoja,jvanzak,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
empowered-brains,jvanzak,False,"<h2><b>EEEeb Spring 2019:&nbsp;&nbsp;Urban Oceans</b></h2><p>March 24,&nbsp;April 7 and 21,&nbsp;May 19,&nbsp;June 2&nbsp;<br><span style=""font-size: 18px; font-weight: 400;"">To register, please </span><a href=""https://www.eventbrite.com/e/ecology-evolution-and-engineering-for-empowered-brains-spring-2019-tickets-54681164836"" style=""font-size: 18px; font-weight: 400;"">visit this link</a><span style=""font-size: 18px; font-weight: 400;"">.&nbsp;</span></p><p>Sponsored and run by members of the MIT Media Lab and the <a href=""http://www.empoweredbrain.org/"">Empowered Brain Institute</a>,&nbsp;<i>Ecology, Evolution, and Engineering for Empowered Brains</i> is an eight-week, sensory-friendly series of related educational workshops for neurodiverse individuals (ages 8 - 14) which aims to hone skills in understanding, interpreting, and protecting the natural environment. Through creative, hands-on teaching exercises and field visits, participants become comfortable with basic ecological principles, as well as emerging technologies used to sculpt ecological and evolutionary processes. We discuss contemporary issues related to conservation and highlight engineering strategies with which to address these obstacles. Through project-based learning, students will have the opportunity to develop understanding by experimentation—or play—and workshops will emphasize immersion, rather than memorization.  Wholly, we seek to foster a safe and creative learning space in which students are able to develop the necessary technical literacy to become future leaders in the myriad realms of environmental science.&nbsp;</p><p>For questions, please contact Avery Normandin (ave@media.mit.edu).<br></p>",,,2019-01-10 16:02:03.396,True,2019-01-10,"Ecology, Evolution, and Engineering for Empowered Brains",PUBLIC,,True,Mediated Matter,False
water-based-additive-manufacturing,jvanzak,False,"<p>This research presents water-based robotic fabrication as a design approach and enabling technology for additive manufacturing (AM) of biodegradable hydrogel composites. We focus on expanding the dimensions of the fabrication envelope, developing structural materials for additive deposition, incorporating material-property gradients, and manufacturing architectural-scale biodegradable systems. The technology includes a robotically controlled AM system to produce biodegradable composite objects, combining natural hydrogels with other organic aggregates. It demonstrates the approach by designing, building, and evaluating the mechanics and controls of a multi-chamber extrusion system. Finally, it provides evidence of large-scale composite objects fabricated by our technology that display graded properties and feature sizes ranging from micro- to macro-scale. Fabricated objects may be chemically stabilized or dissolved in water and recycled within minutes. Applications include the fabrication of fully recyclable products or temporary architectural components, such as tent structures with graded mechanical and optical properties.</p>",,--Choose Location,2019-06-04 21:35:42.589,True,2014-01-01,Water-Based Additive Manufacturing,PUBLIC,,True,Mediated Matter,False
aguahoja,nah6cz,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
water-based-additive-manufacturing,nah6cz,False,"<p>This research presents water-based robotic fabrication as a design approach and enabling technology for additive manufacturing (AM) of biodegradable hydrogel composites. We focus on expanding the dimensions of the fabrication envelope, developing structural materials for additive deposition, incorporating material-property gradients, and manufacturing architectural-scale biodegradable systems. The technology includes a robotically controlled AM system to produce biodegradable composite objects, combining natural hydrogels with other organic aggregates. It demonstrates the approach by designing, building, and evaluating the mechanics and controls of a multi-chamber extrusion system. Finally, it provides evidence of large-scale composite objects fabricated by our technology that display graded properties and feature sizes ranging from micro- to macro-scale. Fabricated objects may be chemically stabilized or dissolved in water and recycled within minutes. Applications include the fabrication of fully recyclable products or temporary architectural components, such as tent structures with graded mechanical and optical properties.</p>",,--Choose Location,2019-06-04 21:35:42.589,True,2014-01-01,Water-Based Additive Manufacturing,PUBLIC,,True,Mediated Matter,False
aguahoja,asling,False,"<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature’s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach—and often match—those of natural ecologies.</p>",2019-07-31,,2019-05-07 13:42:35.058,True,2016-01-01,Aguahoja,PUBLIC,,True,Mediated Matter,False
water-templating-and-biomaterial-skins-in-zero-gravity,asling,False,"<p>Water is life.&nbsp;</p><p>Extending the research that we are involved in with Mediated Matter’s water-based fabrication platform, we brought water-based biomaterial solutions into zero gravity observe their behavior. We created several small scaffolds of different geometries and surface textures that used the water-based solutions as their skins, taking advantage of&nbsp;the fact that van der Waal forces are the primary force affecting water in zero g, which allows water to maintain more structure. The solutions covered the body in a noticeable layering that would otherwise collapse at normal gravity.&nbsp;</p><p>The scaffolds were made of a hydrophilic polymer that is graded in its hydrophilicity, which combined with varying geometries, enabled us to vary the amount and type of liquid the scaffold can retain.&nbsp;Aqueous solution was rapidly applied to the scaffolds in zero gravity to see how thick of a skin we could create that is essentially all fluid.&nbsp;</p><p>This project has significance for research in dynamic cladding systems in space that could have biological functionality and carry mobile life-sustaining functions, as well as providing a skin that shields EM radiation.</p>",,,2018-05-10 18:43:35.423,True,2017-09-01,Water templating and biomaterial skins in zero gravity,LAB,,True,Mediated Matter,False
water-based-additive-manufacturing,asling,False,"<p>This research presents water-based robotic fabrication as a design approach and enabling technology for additive manufacturing (AM) of biodegradable hydrogel composites. We focus on expanding the dimensions of the fabrication envelope, developing structural materials for additive deposition, incorporating material-property gradients, and manufacturing architectural-scale biodegradable systems. The technology includes a robotically controlled AM system to produce biodegradable composite objects, combining natural hydrogels with other organic aggregates. It demonstrates the approach by designing, building, and evaluating the mechanics and controls of a multi-chamber extrusion system. Finally, it provides evidence of large-scale composite objects fabricated by our technology that display graded properties and feature sizes ranging from micro- to macro-scale. Fabricated objects may be chemically stabilized or dissolved in water and recycled within minutes. Applications include the fabrication of fully recyclable products or temporary architectural components, such as tent structures with graded mechanical and optical properties.</p>",,--Choose Location,2019-06-04 21:35:42.589,True,2014-01-01,Water-Based Additive Manufacturing,PUBLIC,,True,Mediated Matter,False
food-computer-project-open-agriculture-initiative,calebh,False,"<p>This project aims to bridge the current gap between controlled-environment agriculture and the computer vision and robotics fields. The intended outcome is to develop an open source toolkit for plant analysis that can be adapted to a wide range of controlled-environment devices (ranging from small personal devices to container-size smart farms), enabling the possibility of accurately measuring and quantifying plant morphological traits.</p>",,,2019-03-19 18:10:32.113,True,2016-10-03,Computer Vision and Machine Learning,PUBLIC,,True,Open Agriculture,False
npcp,calebh,False,"<p>Chemical constituents are the most essential components of the fresh produce items consumed by people around the world. That chemistry can be extremely variable based on a number of factors—but wealthy or poor, privileged or vulnerable, we humans are ultimately subject to the destiny meted out by the chemistry of the food we consume.<br></p><p>Indeed, nutritional content, flavor, color, texture, aroma, and even medicinal benefits are dictated by this chemistry, yet there has never been a thorough and comprehensive analysis of that chemistry in fresh produce items.&nbsp;The National Produce Chemotyping Project sets the stage to assess the chemical variability, or chemotypes, of those items. Ultimately this project will lead to:</p><ol><li>Unprecedented studies seeking to correlate specific chemical attributes of food to health and well-being for all people.</li><li>Improved understanding of how the agricultural pipeline impacts food at the point of consumption, with the specific inclusion of food sources located in underserved communities.</li><li>The piloting of a free, public-facing resource (The National Produce Chemotyping Database) with interpreted data available in a format that is user-friendly for even the most vulnerable populations.</li></ol>",,,2019-04-18 13:44:15.691,True,2019-01-01,National produce chemotyping project,PUBLIC,,True,Open Agriculture,False
tree-computer,calebh,False,"<p>The&nbsp;OpenAg™&nbsp;<b>Tree Computer</b> is a food computer built to explore plant stress, health, longevity, and productivity across a wide range of crops.&nbsp;</p><p>The Tree Computer modulates&nbsp;<b>photosynthetic intensity</b>,&nbsp;<b>spectral range</b>,&nbsp;<b>wind velocity</b>,&nbsp;<b>nutrient levels, </b><b>soil and ambient temperature, and moisture content </b>in order&nbsp;to&nbsp;replicate growing conditions around the globe.&nbsp;</p><p>By reproducing actual and predicted weather patterns in near real time, the Tree Computer&nbsp;helps farmers determine what to plant, where&nbsp;to plant, when to harvest, and how to recover from drought, flood, frost, and disease.&nbsp;</p>",,,2019-04-16 14:37:15.732,True,2017-10-10,Tree Computer,PUBLIC,,True,Open Agriculture,False
open-phenome-project,calebh,False,"<p>The aim of this project is to create an open source digital library&nbsp;with open data sets that cross link phenotypic response in plants (<i style=""font-size: 18px; font-weight: 400;"">taste, nutrition, etc</i>) to environmental variables, biologic variables, genetic variables, and resources required in cultivation (<i style=""font-size: 18px; font-weight: 400;"">inputs).</i>&nbsp;While plants can be altered genetically to produce different or more desirable traits, plants with the same genetics may naturally vary in color, size, texture growth rate, yield, flavor, and nutrient density depending on the environmental conditions in which they are grown.&nbsp;</p><p>Each specific set of conditions can be thought of as a ""Climate Recipe"" that produces unique phenotypic results. As users experiment with new Climate Recipes, their input data and phenotypic results will be recorded and filed in an open source digital platform so that it can be shared, borrowed, scaled up, and improved upon around the world, instantly.&nbsp;</p>",,--Choose Location,2019-05-14 14:19:49.983,True,2015-01-01,Open Phenome Project,PUBLIC,,True,Open Agriculture,False
personal-food-computer,calebh,False,"<p>The OpenAg™ Personal Food Computer is a tabletop-sized, controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber. Climate variables such as carbon dioxide, air temperature, humidity, dissolved oxygen, potential hydrogen, electrical conductivity, and root-zone temperature are among the many conditions that can be controlled and monitored within the growing chamber to yield various phenotypic expressions in the plants.&nbsp;</p><p>Our latest version—the <a href=""https://wiki.openag.media.mit.edu/pfc_edu_3.0""><b>PFC v3.0, or ""PFC_EDU""</b></a>—has been scaled down from previous PFC's in terms of cost, size, and complexity, and designed specifically with educators and children aged 8-14 in mind. It offers a spectrum of control so that users can make their growing experience as manual or as automated as they would like.</p><p><b><a href=""https://www.media.mit.edu/posts/build-a-food-computer/"">Click here to Build a Food Computer</a>.</b></p><p>Like all OpenAg's Food Computers, the PFC_EDU is open source and can be made from easily accessible components so that #nerdfarmers&nbsp;with a broad spectrum of skills, resources, and interests can build, modify, share, and upgrade over time. Build instructions, design files, and helpful resources for all our OpenAg™ Personal Food Computers are on our <a href=""https://wiki.openag.media.mit.edu/start""><b>OpenAg Wiki</b></a>, <a href=""https://github.com/OpenAgInitiative""><b>OpenAg Github</b></a>&nbsp;so&nbsp;nerd farmers can band together (using the <a href=""http://forum.openag.media.mit.edu/""><b>OpenAg Forum</b></a>) to conduct scientifically rigorous citizen-science experimentation, all over the world.</p>",,--Choose Location,2019-05-23 17:57:23.969,True,2015-09-01,Personal Food Computer,PUBLIC,,True,Open Agriculture,False
adrena,calebh,False,"<h1><b>aDRENA</b></h1><h2><br></h2><h2>A Digital Research and Experimentation Network for Agriculture </h2><p>Addressing complex and interlinked challenges requires a globally collaborative, digitally-enabled agricultural revolution.&nbsp;</p><p>This project focuses on three main elements:</p><ol><li><b>The Platform:</b> building an open access platform of &nbsp;hardware, software, and data libraries</li><li><b>The Community: </b>developing and engaging a community of people around the world who are collaborating and sharing data via a networked approach</li><li><b>OpenAg Proliferation:</b> supporting the launch of the OpenAg platform in 10 diverse locations around the world, creating a global network of research, replication, data collection, sharing, and learning from various applications.</li></ol><p>The three main elements enable the networking, scaling, and deployment of agricultural research while simultaneously educating and building capacity for the next generation of digital farmers. In addition, this approach to data collection and sharing creates a de facto standard to ensure interoperability of agricultural and climatic data sets that are open and accessible. &nbsp;</p>",,,2019-04-28 23:47:42.260,True,2019-01-01,aDRENA,PUBLIC,,True,Open Agriculture,False
food-server,calebh,False,"<p>The OpenAg™ Food Server is a shipping container-sized, <b>controlled environment agriculture technology</b> that can be built to utilize hydroponic or aeroponic technology. It can serve as both a<b> research platform for simulating precise environments at scale</b> (see <a href=""https://www.media.mit.edu/projects/openag-flavor-ecology/overview/"">Flavor, environment, and the phenome</a>) , and a <b>production unit</b> for any specified crop of interest. It is intended to produce <b>larger quantities of food</b> than a <a href=""https://www.media.mit.edu/projects/personal-food-computer/overview/"">Personal Food Computer</a> and appeals to <b>interdisciplinary researchers</b> as well as&nbsp;<b>small-scale cafeterias, restaurants, and boutique operators.</b></p>",,--Choose Location,2019-03-20 12:41:06.829,True,2015-09-01,Food Server,PUBLIC,https://www.linkedin.com/in/johndelaparra/,True,Open Agriculture,False
openag-education,calebh,False,"<p>Today's students have grown up in a world of rapidly evolving technology, and they are natural-born experimenters, programmers, and tinkerers. By introducing OpenAg™&nbsp; Food Computers into the education context, including in schools and museums, we hope to inspire and empower the high-tech farmers of the future.&nbsp;</p><p>Our latest testing in schools and libraries shows&nbsp;<b style=""font-size: 18px;"">more than half of students </b><span style=""font-size: 18px;"">who tested a PFC_EDU&nbsp;<b>think </b></span><b><span style=""font-size: 18px;""><i>more</i></span><span style=""font-size: 18px;""> about food production, hunger, and where their food comes from</span></b><span style=""font-size: 18px;""><b>.</b>&nbsp;</span></p><h2><b>Build a Food Computer</b></h2><ul><li>Visit the<a href=""https://wiki.openag.media.mit.edu/pfc_edu_3.0"">&nbsp;<b>OpenAg Wiki page</b></a>&nbsp;for the latest version of <a href=""https://www.media.mit.edu/projects/personal-food-computer/overview/""><b>Personal Food Computer </b>-<b>&nbsp;the ""PFC_EDU""</b></a>&nbsp;including activity guides, materials lists, and helpful tips on how to get parts manufactured.</li></ul><ul><li>Join&nbsp;<a href=""https://forum.openag.media.mit.edu/c/education"">our forum and&nbsp;<b>meet other educators</b></a>, or follow along with <a style=""font-size: 18px;"" href=""https://forum.openag.media.mit.edu/t/libraries-food-computers-plix-build-public-library-innovation-exchange-at-mit-media-lab/2895/67""><b>public libraries</b> who are integrating Food Computers into their activities</a><span style=""font-size: 18px;""> for students of all ages.</span></li><li>Download or contribute activities, materials, lesson plans, curricula, and other education resources on our <a href=""https://wiki.openag.media.mit.edu/education""><b>OpenAg EDU Wiki Page</b></a>.</li></ul>",,--Choose Location,2019-03-25 17:22:50.561,True,2015-01-01,OpenAg EDU,PUBLIC,,True,Open Agriculture,False
food-computer-project-open-agriculture-initiative,rbaynes,False,"<p>This project aims to bridge the current gap between controlled-environment agriculture and the computer vision and robotics fields. The intended outcome is to develop an open source toolkit for plant analysis that can be adapted to a wide range of controlled-environment devices (ranging from small personal devices to container-size smart farms), enabling the possibility of accurately measuring and quantifying plant morphological traits.</p>",,,2019-03-19 18:10:32.113,True,2016-10-03,Computer Vision and Machine Learning,PUBLIC,,True,Open Agriculture,False
personal-food-computer,rbaynes,False,"<p>The OpenAg™ Personal Food Computer is a tabletop-sized, controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber. Climate variables such as carbon dioxide, air temperature, humidity, dissolved oxygen, potential hydrogen, electrical conductivity, and root-zone temperature are among the many conditions that can be controlled and monitored within the growing chamber to yield various phenotypic expressions in the plants.&nbsp;</p><p>Our latest version—the <a href=""https://wiki.openag.media.mit.edu/pfc_edu_3.0""><b>PFC v3.0, or ""PFC_EDU""</b></a>—has been scaled down from previous PFC's in terms of cost, size, and complexity, and designed specifically with educators and children aged 8-14 in mind. It offers a spectrum of control so that users can make their growing experience as manual or as automated as they would like.</p><p><b><a href=""https://www.media.mit.edu/posts/build-a-food-computer/"">Click here to Build a Food Computer</a>.</b></p><p>Like all OpenAg's Food Computers, the PFC_EDU is open source and can be made from easily accessible components so that #nerdfarmers&nbsp;with a broad spectrum of skills, resources, and interests can build, modify, share, and upgrade over time. Build instructions, design files, and helpful resources for all our OpenAg™ Personal Food Computers are on our <a href=""https://wiki.openag.media.mit.edu/start""><b>OpenAg Wiki</b></a>, <a href=""https://github.com/OpenAgInitiative""><b>OpenAg Github</b></a>&nbsp;so&nbsp;nerd farmers can band together (using the <a href=""http://forum.openag.media.mit.edu/""><b>OpenAg Forum</b></a>) to conduct scientifically rigorous citizen-science experimentation, all over the world.</p>",,--Choose Location,2019-05-23 17:57:23.969,True,2015-09-01,Personal Food Computer,PUBLIC,,True,Open Agriculture,False
adrena,rbaynes,False,"<h1><b>aDRENA</b></h1><h2><br></h2><h2>A Digital Research and Experimentation Network for Agriculture </h2><p>Addressing complex and interlinked challenges requires a globally collaborative, digitally-enabled agricultural revolution.&nbsp;</p><p>This project focuses on three main elements:</p><ol><li><b>The Platform:</b> building an open access platform of &nbsp;hardware, software, and data libraries</li><li><b>The Community: </b>developing and engaging a community of people around the world who are collaborating and sharing data via a networked approach</li><li><b>OpenAg Proliferation:</b> supporting the launch of the OpenAg platform in 10 diverse locations around the world, creating a global network of research, replication, data collection, sharing, and learning from various applications.</li></ol><p>The three main elements enable the networking, scaling, and deployment of agricultural research while simultaneously educating and building capacity for the next generation of digital farmers. In addition, this approach to data collection and sharing creates a de facto standard to ensure interoperability of agricultural and climatic data sets that are open and accessible. &nbsp;</p>",,,2019-04-28 23:47:42.260,True,2019-01-01,aDRENA,PUBLIC,,True,Open Agriculture,False
high-frequency-lidar-using-beat-notes,achoo,False,"<p>Time of Flight 3D cameras like the Microsoft Kinect are prevalent in computer vision and computer graphics. In such devices, the power of an integrated laser is amplitude modulated at megahertz (MHz) frequencies and demodulated using a specialized imaging sensor to obtain sub-cm range precision. To use a similar architecture and obtain micron range precision, this paper incorporates beat notes. To bring telecommunications ideas to correlation ToF imaging, we study a form of ""cascaded Time of Flight"" that uses a Hertz-scale intermediate frequency to encode high-frequency pathlength information. We show synthetically and experimentally that a bulk implementation of opto-electronic mixers offers: (a) robustness to environmental vibrations; (b) programmability; and (c) stability in frequency tones. A fiberoptic prototype is constructed, which demonstrates three micron range precision over a range of two meters. A key contribution of this paper is to study and evaluate the proposed architecture for use in machine vision.<br></p>",,,2019-04-19 17:53:06.677,True,2017-12-21,High-frequency LIDAR using beat notes,PUBLIC,,True,Camera Culture,False
spacehuman,muccillo,False,"<p>SpaceHuman is a soft robotics device designed to facilitate the exploration of environments with reduced gravity in a view of democratization and openness towards access to space and its exploration. &nbsp;It is based on the idea that one day, people who have not received a long preparation and training, as happens today with the astronauts, will be able to have access to the space having a type of conformation and physical configuration that is not adapted to this kind of setting.&nbsp;</p><p>The analysis of the unique seahorse's tail structure became the insight of the overall biomimetic design process. In fact, seahorse tail movement, gripping and protection to the seahorse while floating.&nbsp;Moreover, seahorses do not use their tails to swim; instead, they use them to grasp objects in their environment while they camouflage to hide from predators and hunts for prey. Flexibility and resiliency are key features that enable these behaviours.</p><p>SpaceHuman is an additive prosthesis or otherwise definable as a ""supernumerary robot."" SpaceHuman will facilitate the use of space in zero gravity or reduced gravity restoring the right motion and balance of our body and assigning a new function to a part of our body that until now has not been fully exploited except for the transport of loads, our back. Users will thus be able to build a new poetics of the body and its movements within this radically different space through SpaceHuman, creating new scenarios of its application. Through air chambers specifically designed to be able to change their shape and bend along a reinforcing rib of the material, the people who will use SpaceHuman will be able to cling to useful surfaces inside orbital housing or in Lunar or Martian villages.&nbsp;</p>",,,2019-05-13 21:47:19.873,True,2018-08-01,SpaceHuman,PUBLIC,,True,Responsive Environments,False
bricoleur,hisean,False,"<p>Bricoleur allows makers of all ages to explore the creative possibilities of video and audio as programmable media on mobile devices. Using hand-drawn gestures and a Scratch Blocks-based interface, makers can quickly create complex interactive stories, animations, and artworks by capturing and programming images and sounds from the world around them.</p>",,,2018-10-21 00:33:16.471,True,2018-06-01,Bricoleur,PUBLIC,,True,Lifelong Kindergarten,False
persuasive-cities,rchin,False,"<p>Persuasive Cities research is aimed at advancing urban spaces to facilitate societal changes. According to social science research, any well-designed environment can become a strong influencer of what people think and do. There is an endlessly dynamic interaction between a person, a particular behavior, and a specific environment. Persuasive Cities research leverages this knowledge to engineer persuasive environments and interventions for altering human behavior on a societal level. This research is focused on socially engaging environments for supporting entrepreneurship and innovation, reshaping routines and behavioral patterns in urban spaces, deploying intelligent outdoor sensing for shifting mobility modes, enhancing environmentally friendly behaviors through social norms, introducing interactive public feedback channels to alter attitudes at scale, engaging residents through socially influencing systems, exploring methods for designing persuasive neighborhoods, and fostering adoption of novel urban systems. More: http://bit.ly/TEDxp</p>",,--Choose Location,2017-03-31 20:52:58.972,True,2015-09-01,Persuasive Cities,PUBLIC,http://cp.media.mit.edu/agnis-stibe,True,City Science,False
CityscopeBostonbrt,rchin,False,"<p>The&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://mfc.mit.edu/"">Mobility Futures Collaborative</a>&nbsp;in the MIT Department of Urban Studies and Planning (DUSP) and the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://cp.media.mit.edu/"">Changing Places group</a>&nbsp;at the MIT Media Lab have developed new interactive tools aimed to better communicate the possible impacts of new transit systems. The Media Lab and DUSP have partnered with the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""https://www.barrfoundation.org/"">Barr Foundation</a>&nbsp;to test these tools in a series of community engagement workshops to examine the impacts of Bus Rapid Transit (BRT) systems in greater Boston. These tools include the&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://cp.media.mit.edu/city-simulation/"">CityScope</a>&nbsp;— an interactive platform that utilizes physical models (built from LEGO bricks) and 3-D projection — to enable community members to engage in neighborhood and street-level decisions including alternative bus corridor designs and station-level variations (such as pre-pay boarding). The second tool,&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://mfc.mit.edu/innovations-participatory-design-brt-systems"">CoAXs</a>&nbsp;is a new interactive platform for collaborative transit planning that builds on open-source urban analytics tools such as&nbsp;<a style=""font-size: 18px; font-weight: 400;"" href=""http://conveyal.com/projects/analyst/"">Conveyal Transport Analyst</a></p>",,--Choose Location,2017-10-16 18:32:03.412,True,2015-01-01,CityScope Boston BRT,PUBLIC,,True,City Science,False
tree-computer,hildreth,False,"<p>The&nbsp;OpenAg™&nbsp;<b>Tree Computer</b> is a food computer built to explore plant stress, health, longevity, and productivity across a wide range of crops.&nbsp;</p><p>The Tree Computer modulates&nbsp;<b>photosynthetic intensity</b>,&nbsp;<b>spectral range</b>,&nbsp;<b>wind velocity</b>,&nbsp;<b>nutrient levels, </b><b>soil and ambient temperature, and moisture content </b>in order&nbsp;to&nbsp;replicate growing conditions around the globe.&nbsp;</p><p>By reproducing actual and predicted weather patterns in near real time, the Tree Computer&nbsp;helps farmers determine what to plant, where&nbsp;to plant, when to harvest, and how to recover from drought, flood, frost, and disease.&nbsp;</p>",,,2019-04-16 14:37:15.732,True,2017-10-10,Tree Computer,PUBLIC,,True,Open Agriculture,False
open-phenome-project,hildreth,False,"<p>The aim of this project is to create an open source digital library&nbsp;with open data sets that cross link phenotypic response in plants (<i style=""font-size: 18px; font-weight: 400;"">taste, nutrition, etc</i>) to environmental variables, biologic variables, genetic variables, and resources required in cultivation (<i style=""font-size: 18px; font-weight: 400;"">inputs).</i>&nbsp;While plants can be altered genetically to produce different or more desirable traits, plants with the same genetics may naturally vary in color, size, texture growth rate, yield, flavor, and nutrient density depending on the environmental conditions in which they are grown.&nbsp;</p><p>Each specific set of conditions can be thought of as a ""Climate Recipe"" that produces unique phenotypic results. As users experiment with new Climate Recipes, their input data and phenotypic results will be recorded and filed in an open source digital platform so that it can be shared, borrowed, scaled up, and improved upon around the world, instantly.&nbsp;</p>",,--Choose Location,2019-05-14 14:19:49.983,True,2015-01-01,Open Phenome Project,PUBLIC,,True,Open Agriculture,False
personal-food-computer,hildreth,False,"<p>The OpenAg™ Personal Food Computer is a tabletop-sized, controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber. Climate variables such as carbon dioxide, air temperature, humidity, dissolved oxygen, potential hydrogen, electrical conductivity, and root-zone temperature are among the many conditions that can be controlled and monitored within the growing chamber to yield various phenotypic expressions in the plants.&nbsp;</p><p>Our latest version—the <a href=""https://wiki.openag.media.mit.edu/pfc_edu_3.0""><b>PFC v3.0, or ""PFC_EDU""</b></a>—has been scaled down from previous PFC's in terms of cost, size, and complexity, and designed specifically with educators and children aged 8-14 in mind. It offers a spectrum of control so that users can make their growing experience as manual or as automated as they would like.</p><p><b><a href=""https://www.media.mit.edu/posts/build-a-food-computer/"">Click here to Build a Food Computer</a>.</b></p><p>Like all OpenAg's Food Computers, the PFC_EDU is open source and can be made from easily accessible components so that #nerdfarmers&nbsp;with a broad spectrum of skills, resources, and interests can build, modify, share, and upgrade over time. Build instructions, design files, and helpful resources for all our OpenAg™ Personal Food Computers are on our <a href=""https://wiki.openag.media.mit.edu/start""><b>OpenAg Wiki</b></a>, <a href=""https://github.com/OpenAgInitiative""><b>OpenAg Github</b></a>&nbsp;so&nbsp;nerd farmers can band together (using the <a href=""http://forum.openag.media.mit.edu/""><b>OpenAg Forum</b></a>) to conduct scientifically rigorous citizen-science experimentation, all over the world.</p>",,--Choose Location,2019-05-23 17:57:23.969,True,2015-09-01,Personal Food Computer,PUBLIC,,True,Open Agriculture,False
openag-education,hildreth,False,"<p>Today's students have grown up in a world of rapidly evolving technology, and they are natural-born experimenters, programmers, and tinkerers. By introducing OpenAg™&nbsp; Food Computers into the education context, including in schools and museums, we hope to inspire and empower the high-tech farmers of the future.&nbsp;</p><p>Our latest testing in schools and libraries shows&nbsp;<b style=""font-size: 18px;"">more than half of students </b><span style=""font-size: 18px;"">who tested a PFC_EDU&nbsp;<b>think </b></span><b><span style=""font-size: 18px;""><i>more</i></span><span style=""font-size: 18px;""> about food production, hunger, and where their food comes from</span></b><span style=""font-size: 18px;""><b>.</b>&nbsp;</span></p><h2><b>Build a Food Computer</b></h2><ul><li>Visit the<a href=""https://wiki.openag.media.mit.edu/pfc_edu_3.0"">&nbsp;<b>OpenAg Wiki page</b></a>&nbsp;for the latest version of <a href=""https://www.media.mit.edu/projects/personal-food-computer/overview/""><b>Personal Food Computer </b>-<b>&nbsp;the ""PFC_EDU""</b></a>&nbsp;including activity guides, materials lists, and helpful tips on how to get parts manufactured.</li></ul><ul><li>Join&nbsp;<a href=""https://forum.openag.media.mit.edu/c/education"">our forum and&nbsp;<b>meet other educators</b></a>, or follow along with <a style=""font-size: 18px;"" href=""https://forum.openag.media.mit.edu/t/libraries-food-computers-plix-build-public-library-innovation-exchange-at-mit-media-lab/2895/67""><b>public libraries</b> who are integrating Food Computers into their activities</a><span style=""font-size: 18px;""> for students of all ages.</span></li><li>Download or contribute activities, materials, lesson plans, curricula, and other education resources on our <a href=""https://wiki.openag.media.mit.edu/education""><b>OpenAg EDU Wiki Page</b></a>.</li></ul>",,--Choose Location,2019-03-25 17:22:50.561,True,2015-01-01,OpenAg EDU,PUBLIC,,True,Open Agriculture,False
tree-computer,snowak,False,"<p>The&nbsp;OpenAg™&nbsp;<b>Tree Computer</b> is a food computer built to explore plant stress, health, longevity, and productivity across a wide range of crops.&nbsp;</p><p>The Tree Computer modulates&nbsp;<b>photosynthetic intensity</b>,&nbsp;<b>spectral range</b>,&nbsp;<b>wind velocity</b>,&nbsp;<b>nutrient levels, </b><b>soil and ambient temperature, and moisture content </b>in order&nbsp;to&nbsp;replicate growing conditions around the globe.&nbsp;</p><p>By reproducing actual and predicted weather patterns in near real time, the Tree Computer&nbsp;helps farmers determine what to plant, where&nbsp;to plant, when to harvest, and how to recover from drought, flood, frost, and disease.&nbsp;</p>",,,2019-04-16 14:37:15.732,True,2017-10-10,Tree Computer,PUBLIC,,True,Open Agriculture,False
tree-computer,poitrast,False,"<p>The&nbsp;OpenAg™&nbsp;<b>Tree Computer</b> is a food computer built to explore plant stress, health, longevity, and productivity across a wide range of crops.&nbsp;</p><p>The Tree Computer modulates&nbsp;<b>photosynthetic intensity</b>,&nbsp;<b>spectral range</b>,&nbsp;<b>wind velocity</b>,&nbsp;<b>nutrient levels, </b><b>soil and ambient temperature, and moisture content </b>in order&nbsp;to&nbsp;replicate growing conditions around the globe.&nbsp;</p><p>By reproducing actual and predicted weather patterns in near real time, the Tree Computer&nbsp;helps farmers determine what to plant, where&nbsp;to plant, when to harvest, and how to recover from drought, flood, frost, and disease.&nbsp;</p>",,,2019-04-16 14:37:15.732,True,2017-10-10,Tree Computer,PUBLIC,,True,Open Agriculture,False
personal-food-computer,poitrast,False,"<p>The OpenAg™ Personal Food Computer is a tabletop-sized, controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber. Climate variables such as carbon dioxide, air temperature, humidity, dissolved oxygen, potential hydrogen, electrical conductivity, and root-zone temperature are among the many conditions that can be controlled and monitored within the growing chamber to yield various phenotypic expressions in the plants.&nbsp;</p><p>Our latest version—the <a href=""https://wiki.openag.media.mit.edu/pfc_edu_3.0""><b>PFC v3.0, or ""PFC_EDU""</b></a>—has been scaled down from previous PFC's in terms of cost, size, and complexity, and designed specifically with educators and children aged 8-14 in mind. It offers a spectrum of control so that users can make their growing experience as manual or as automated as they would like.</p><p><b><a href=""https://www.media.mit.edu/posts/build-a-food-computer/"">Click here to Build a Food Computer</a>.</b></p><p>Like all OpenAg's Food Computers, the PFC_EDU is open source and can be made from easily accessible components so that #nerdfarmers&nbsp;with a broad spectrum of skills, resources, and interests can build, modify, share, and upgrade over time. Build instructions, design files, and helpful resources for all our OpenAg™ Personal Food Computers are on our <a href=""https://wiki.openag.media.mit.edu/start""><b>OpenAg Wiki</b></a>, <a href=""https://github.com/OpenAgInitiative""><b>OpenAg Github</b></a>&nbsp;so&nbsp;nerd farmers can band together (using the <a href=""http://forum.openag.media.mit.edu/""><b>OpenAg Forum</b></a>) to conduct scientifically rigorous citizen-science experimentation, all over the world.</p>",,--Choose Location,2019-05-23 17:57:23.969,True,2015-09-01,Personal Food Computer,PUBLIC,,True,Open Agriculture,False
food-server,poitrast,False,"<p>The OpenAg™ Food Server is a shipping container-sized, <b>controlled environment agriculture technology</b> that can be built to utilize hydroponic or aeroponic technology. It can serve as both a<b> research platform for simulating precise environments at scale</b> (see <a href=""https://www.media.mit.edu/projects/openag-flavor-ecology/overview/"">Flavor, environment, and the phenome</a>) , and a <b>production unit</b> for any specified crop of interest. It is intended to produce <b>larger quantities of food</b> than a <a href=""https://www.media.mit.edu/projects/personal-food-computer/overview/"">Personal Food Computer</a> and appeals to <b>interdisciplinary researchers</b> as well as&nbsp;<b>small-scale cafeterias, restaurants, and boutique operators.</b></p>",,--Choose Location,2019-03-20 12:41:06.829,True,2015-09-01,Food Server,PUBLIC,https://www.linkedin.com/in/johndelaparra/,True,Open Agriculture,False
tree-computer,zandera,False,"<p>The&nbsp;OpenAg™&nbsp;<b>Tree Computer</b> is a food computer built to explore plant stress, health, longevity, and productivity across a wide range of crops.&nbsp;</p><p>The Tree Computer modulates&nbsp;<b>photosynthetic intensity</b>,&nbsp;<b>spectral range</b>,&nbsp;<b>wind velocity</b>,&nbsp;<b>nutrient levels, </b><b>soil and ambient temperature, and moisture content </b>in order&nbsp;to&nbsp;replicate growing conditions around the globe.&nbsp;</p><p>By reproducing actual and predicted weather patterns in near real time, the Tree Computer&nbsp;helps farmers determine what to plant, where&nbsp;to plant, when to harvest, and how to recover from drought, flood, frost, and disease.&nbsp;</p>",,,2019-04-16 14:37:15.732,True,2017-10-10,Tree Computer,PUBLIC,,True,Open Agriculture,False
tree-computer,tsavas,False,"<p>The&nbsp;OpenAg™&nbsp;<b>Tree Computer</b> is a food computer built to explore plant stress, health, longevity, and productivity across a wide range of crops.&nbsp;</p><p>The Tree Computer modulates&nbsp;<b>photosynthetic intensity</b>,&nbsp;<b>spectral range</b>,&nbsp;<b>wind velocity</b>,&nbsp;<b>nutrient levels, </b><b>soil and ambient temperature, and moisture content </b>in order&nbsp;to&nbsp;replicate growing conditions around the globe.&nbsp;</p><p>By reproducing actual and predicted weather patterns in near real time, the Tree Computer&nbsp;helps farmers determine what to plant, where&nbsp;to plant, when to harvest, and how to recover from drought, flood, frost, and disease.&nbsp;</p>",,,2019-04-16 14:37:15.732,True,2017-10-10,Tree Computer,PUBLIC,,True,Open Agriculture,False
open-phenome-project,tsavas,False,"<p>The aim of this project is to create an open source digital library&nbsp;with open data sets that cross link phenotypic response in plants (<i style=""font-size: 18px; font-weight: 400;"">taste, nutrition, etc</i>) to environmental variables, biologic variables, genetic variables, and resources required in cultivation (<i style=""font-size: 18px; font-weight: 400;"">inputs).</i>&nbsp;While plants can be altered genetically to produce different or more desirable traits, plants with the same genetics may naturally vary in color, size, texture growth rate, yield, flavor, and nutrient density depending on the environmental conditions in which they are grown.&nbsp;</p><p>Each specific set of conditions can be thought of as a ""Climate Recipe"" that produces unique phenotypic results. As users experiment with new Climate Recipes, their input data and phenotypic results will be recorded and filed in an open source digital platform so that it can be shared, borrowed, scaled up, and improved upon around the world, instantly.&nbsp;</p>",,--Choose Location,2019-05-14 14:19:49.983,True,2015-01-01,Open Phenome Project,PUBLIC,,True,Open Agriculture,False
personal-food-computer,tsavas,False,"<p>The OpenAg™ Personal Food Computer is a tabletop-sized, controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber. Climate variables such as carbon dioxide, air temperature, humidity, dissolved oxygen, potential hydrogen, electrical conductivity, and root-zone temperature are among the many conditions that can be controlled and monitored within the growing chamber to yield various phenotypic expressions in the plants.&nbsp;</p><p>Our latest version—the <a href=""https://wiki.openag.media.mit.edu/pfc_edu_3.0""><b>PFC v3.0, or ""PFC_EDU""</b></a>—has been scaled down from previous PFC's in terms of cost, size, and complexity, and designed specifically with educators and children aged 8-14 in mind. It offers a spectrum of control so that users can make their growing experience as manual or as automated as they would like.</p><p><b><a href=""https://www.media.mit.edu/posts/build-a-food-computer/"">Click here to Build a Food Computer</a>.</b></p><p>Like all OpenAg's Food Computers, the PFC_EDU is open source and can be made from easily accessible components so that #nerdfarmers&nbsp;with a broad spectrum of skills, resources, and interests can build, modify, share, and upgrade over time. Build instructions, design files, and helpful resources for all our OpenAg™ Personal Food Computers are on our <a href=""https://wiki.openag.media.mit.edu/start""><b>OpenAg Wiki</b></a>, <a href=""https://github.com/OpenAgInitiative""><b>OpenAg Github</b></a>&nbsp;so&nbsp;nerd farmers can band together (using the <a href=""http://forum.openag.media.mit.edu/""><b>OpenAg Forum</b></a>) to conduct scientifically rigorous citizen-science experimentation, all over the world.</p>",,--Choose Location,2019-05-23 17:57:23.969,True,2015-09-01,Personal Food Computer,PUBLIC,,True,Open Agriculture,False
food-server,tsavas,False,"<p>The OpenAg™ Food Server is a shipping container-sized, <b>controlled environment agriculture technology</b> that can be built to utilize hydroponic or aeroponic technology. It can serve as both a<b> research platform for simulating precise environments at scale</b> (see <a href=""https://www.media.mit.edu/projects/openag-flavor-ecology/overview/"">Flavor, environment, and the phenome</a>) , and a <b>production unit</b> for any specified crop of interest. It is intended to produce <b>larger quantities of food</b> than a <a href=""https://www.media.mit.edu/projects/personal-food-computer/overview/"">Personal Food Computer</a> and appeals to <b>interdisciplinary researchers</b> as well as&nbsp;<b>small-scale cafeterias, restaurants, and boutique operators.</b></p>",,--Choose Location,2019-03-20 12:41:06.829,True,2015-09-01,Food Server,PUBLIC,https://www.linkedin.com/in/johndelaparra/,True,Open Agriculture,False
conversation-trees-1,bridgitm,False,"<p>An alternative viewing experience for large-scale conversations on Twitter. Each conversation tree visualizes the structure of replies to a single tweet. The purpose of this project is to explore a hidden dimension of virality. Not all tweets that elicit a large volume of comments behave the same under the hood.&nbsp; These structural patterns, when combined with sentiment analysis and/or toxicity, can reveal a deeper story.</p>",,,2019-04-18 14:50:10.698,True,2019-04-17,Conversation Trees,PUBLIC,,True,Initiatives,False
story-comprehension,bridgitm,False,"<p>The ability to automatically understand and infer characters' goals and their emotional states is key towards better narrative comprehension. Reasoning about mental representations of various characters in a narrative has been referred to as Theory of Mind (ToM) reasoning. In this work, we propose an unsupervised neural network that  exploits the personal stories on social media and incorporates commonsense knowledge about characters' motivations and reactions to generate interpretable trajectories of characters' mental states. We find that our model is capable of learning coherent mental representations from characters' actions and their affect states. We evaluate our model using a publicly available dataset for mental state tracking of characters in short commonsense stories.&nbsp;</p>",,,2019-04-18 15:04:09.320,True,2019-02-10,Story Comprehension,PUBLIC,,True,Initiatives,False
conversation-trees-1,bcroy,False,"<p>An alternative viewing experience for large-scale conversations on Twitter. Each conversation tree visualizes the structure of replies to a single tweet. The purpose of this project is to explore a hidden dimension of virality. Not all tweets that elicit a large volume of comments behave the same under the hood.&nbsp; These structural patterns, when combined with sentiment analysis and/or toxicity, can reveal a deeper story.</p>",,,2019-04-18 14:50:10.698,True,2019-04-17,Conversation Trees,PUBLIC,,True,Social Machines,False
conversation-health-twitter-toxicity-network-structure,bcroy,False,"<p>We investigate the nature of toxic conversations on Twitter, focusing on the important subset of public discourse related to mainstream news. We analyze 71,000 conversations prompted by tweets from five major news outlets, comprising more than 12.3 million tweets and replies posted by 1.13 million users. We study the relationship between tweet toxicity, the reply-tree structure of the tweets, and the network of social connections between the tweet authors at three main levels: the individual level of the tweet authors, the dyadic level (between a tweet author and a reply author), and the overall reply-tree and follow graph structure of the conversation.</p>",,,2019-04-10 20:19:57.770,True,2018-09-01,Conversation health: Twitter toxicity and network structure,LAB-INSIDERS,,True,Social Machines,False
tv-audience-tribes,bcroy,False,"<p>How can we make sense of the expressed—and latent—interests of millions of TV show audience members online? We analyze millions of users on Twitter who follow a set of TV shows, discovering affinities between users and identifying clusters that reflect what they're interested in (e.g., soccer, hip hop, the environment). This ""interest map"" enables us to identify audience clusters that might be likely to engage with certain TV shows or genres, potentially supporting the creation, distribution, and promotion of more impactful stories.</p>",,,2019-04-18 17:30:58.538,True,2018-11-26,TV Audience Tribes,LAB-INSIDERS,,True,Social Machines,False
rhythm,orenled,False,"<p>Rhythm is a collection of open-source tools to make it easier for researchers to examine, analyze, and augment human interaction.&nbsp;Rhythm includes hardware to measure face to face interaction, software platforms to quantify social dynamics from online videoconferencing, and analysis and visualization tools to craft interventions that affect social behavior. For more information, visit &nbsp;<a href=""http://rhythm.mit.edu"">rhythm.mit.edu</a>, or our main <a href=""https://github.com/HumanDynamics/openbadge"">github repository</a>.</p>",,,2019-01-21 17:59:43.190,True,2016-09-01,Rhythm: Open measurement and feedback tools for human interaction,PUBLIC,http://rhythm.mit.edu/,True,Human Dynamics,False
open-badges,orenled,False,"<p>We present Open Badges, an open-source framework and toolkit for measuring and shaping face-to-face social interactions using either custom hardware devices or smart phones, and real-time web-based visualizations. Open Badges is a modular system that allows researchers to monitor and collect interaction data from people engaged in real-life social settings.</p>",,--Choose Location,2019-01-10 19:19:48.517,True,2014-09-01,Open Badges,PUBLIC,,True,Human Dynamics,False
unbounded-high-dynamic-range-photography-using-a-modulo-camera,hangzhao,False,"<p>We present a novel framework to extend the dynamic range of images called Unbounded High Dynamic Range (UHDR) photography with a modulo camera. A modulo camera could theoretically take unbounded radiance levels by keeping only the least significant bits. We show that with limited bit depth, very high radiance levels can be recovered from a single modulus image with our newly proposed unwrapping algorithm for natural images. We can also obtain an HDR image with details equally well preserved for all radiance levels by merging the least number of modulus images. Synthetic experiments and experiments with a real modulo camera show the effectiveness of the proposed approach.</p>",,--Choose Location,2019-04-19 18:38:01.471,True,2014-09-01,Unbounded high dynamic range photography using a modulo camera,PUBLIC,,True,Camera Culture,False
unbounded-high-dynamic-range-photography-using-a-modulo-camera,shiboxin,False,"<p>We present a novel framework to extend the dynamic range of images called Unbounded High Dynamic Range (UHDR) photography with a modulo camera. A modulo camera could theoretically take unbounded radiance levels by keeping only the least significant bits. We show that with limited bit depth, very high radiance levels can be recovered from a single modulus image with our newly proposed unwrapping algorithm for natural images. We can also obtain an HDR image with details equally well preserved for all radiance levels by merging the least number of modulus images. Synthetic experiments and experiments with a real modulo camera show the effectiveness of the proposed approach.</p>",,--Choose Location,2019-04-19 18:38:01.471,True,2014-09-01,Unbounded high dynamic range photography using a modulo camera,PUBLIC,,True,Camera Culture,False
space-food,mcoblent,False,"<p>The human experience of food extends far beyond that of mere nourishment. Debriefs with astronauts tell us that food is a key creature comfort in spaceflight, and it will play an even more significant role for long duration space travel and future life in other orbits. How can we best meet the nutritional, performance, and emotional needs of astronauts through food? Our Space Food and Sensory Experiences research area aims to address the unique challenges associated with eating in space—from the microbiome scale to the “envirome” scale—including fermentation and probiotics, preservation of freshness and nutrient quality, improving waning or shifting appetite, food stimuli (e.g., spice) to counteract loss of smell, varying the food forms and packaging interaction for increased interest in eating, and communal, cultural experience sharing for mental health and wellbeing. This research area explicitly addresses earth-based markets as well as how the foods, packaging, and eating experiences developed for space can be re-used in many Earth contexts.<br></p><p><b>Research Areas</b></p><p><b>Social Wellbeing.</b> Immersive augmented reality experiences can promote wellbeing and enhance the experience of eating. Recent space food workshops at SEI with veteran astronauts Paolo Nespoli (ESA) and Cady Coleman (NASA ret.) proved that VR was a highly effective and emotive tool to connect to home and engage with food. For example, homesick astronauts could visit family and friends at their favorite restaurant on Earth by way of a virtual, communal dinner gathering. We are designing augmented reality experiences to pair with space food.</p><p><b>Nutritional Science and Edible Forms. </b>We are exploring 3D printing to create new food forms in space, starting from a suite of highly nutritious or taste-stimulating basic ingredients. Innovative packaging materials (based on polyethylene) can be recycled in a new process we are developing, to transform all non-food waste into a 3D printer source of material as well.</p><p><b>Appetite. </b>Many astronauts suffer from loss of appetite in outer space. Appetite is influenced by cross-modal sensory interactions, the physiological elements of oral processing, mood, and food structure and texture. We are designing a) foods and holistic eating experiences to help increase appetite and fulfill nutritional requirements and b) multi-sensory, pre-meal experiences to promote the anticipation of eating and stimulate digestion (e.g., the smell of onions frying, or sounds of cooking preparation). In addition, we are compiling a “Zero Gravity Cookbook” to gather space-tested and newly proposed recipes from astronauts and chefs around the world.&nbsp;</p><p><b>Prototypes, Testing, and Product Development</b></p><p>A tasting menu will be deployed on upcoming zero gravity flights and shared with ISS-mission-veteran astronauts for taste testing and further development. We are also developing fermentation food products for longer term food consumption and habitation in space.</p><p>Stay tuned for results from our zero gravity flight, upcoming Blue Origin suborbital launches, and deployments on the ISS. </p><p>For inquiries about the research, please direct questions to <a href=""https://www.media.mit.edu/people/mcoblent/overview/"">Maggie Coblentz</a> (mcoblent@media.mit.edu) and <a href=""https://www.media.mit.edu/people/aekblaw/overview/"">Ariel Ekblaw</a>&nbsp;(aekblaw@media.mit.edu).&nbsp;</p>",,,2019-06-07 19:51:33.238,True,2018-09-01,Space Food and Sensory Experiences,PUBLIC,https://www.maggiecoblentz.com/,True,Initiatives,False
interplanetary-cookbook,mcoblent,False,"<p>The MIT Media Lab Space Exploration Initiative is developing “The Interplanetary Cookbook”—a collection of thought-provoking recipes, tools for eating, and sensory experiences for life in space.</p><h1><b>Open call for entries!</b></h1><p><b>Deadline to apply: July 15, 2019</b></p><p><b><a href=""https://docs.google.com/forms/d/e/1FAIpQLSc7hi7n_4Z5XlYqVTKT8LmKTtBGf525cG1qeKjSlYt_pa4xCg/viewform"">SUBMIT ENTRY HERE&nbsp;</a></b></p><p>Food is a key creature comfort in spaceflight, and it will play an even more significant role on long duration space travel and future life in space habitats. Currently, space food is freeze-dried and prepackaged in ways consistent with the demands of present day space travel. The advancement of deepspace exploration and the development of an interplanetary space tourism industry will make new cultural events and experiences never encountered before in human history possible. How can we design new and unforeseen foodways and gastronomic experiences that extend beyond basic sustenance?</p><p>Thoughtfully designed foods and culinary experiences could allow humans to feel more connected to their loved ones and histories on Earth, as well as promote the beginning of a food culture that fosters deeper relationships with new worlds. The function of food is not simply to provide nourishment—it evokes the imagination, engages our senses, and contains our cultural heritage. “The Interplanetary Cookbook” seeks to explore the unique food culture that will undoubtedly emerge as humans venture into new orbits through reconsidering, and even redesigning, the future of food beyond Earth-based practices.&nbsp;</p><p>We welcome all forms of submissions from project briefs to recipes and designs.&nbsp;</p><p>Topics may include, but are not limited to, the following:<br></p><ul><li>Recipe based on environment (e.g., zero gravity, geography) </li><li>Recipe inspired by space (e.g., Sci-Fi, aesthetics)</li><li>Mealtime tradition (e.g., cultural, daily routine, special occasion) &nbsp;</li><li>Cuisine style (e.g., regional, fusion, haute cuisine and homestyle/comfort food) </li><li>Space tourism (e.g., restaurant concept) </li><li>Tool for eating (e.g., utensil, dishware) </li><li>Food packaging design </li><li>Sensory experience design (e.g., aroma, flavor, texture) </li></ul><p>Space food products and eating experiences should celebrate the unique affordances of zero gravity while addressing the challenges associated with eating in space—decreased sense of smell, waning and shifting appetites, food flyaways in zero gravity, degrading nutritional value, and a need for communal and cultural experience sharing for mental health and wellbeing.</p><p>For inquiries about the project please email Maggie Coblentz (<a href=""mailto:mcoblent@media.mit.edu"">mcoblent@media.mit.edu</a>).&nbsp;<br></p>",,,2019-06-06 14:31:21.257,True,2018-09-01,The Interplanetary Cookbook,PUBLIC,,True,Initiatives,False
realtime-detection-of-social-cues,gelso,False,"<h2>Realtime detection of social cues in children’s voices</h2><p>In everyday conversation, people use what are known as backchannels to signal to someone that they are still listening, paying attention, and engaged. As listeners, we smile, nod, and say “uh-huh” to convey attentiveness, and we do this naturally with little thought. We give this feedback not randomly but at certain moments in the conversation because speakers give off social cues that signal upcoming backchanneling opportunities.</p>",,,2018-05-07 17:34:51.716,True,2017-03-01,Realtime Detection of Social Cues,PUBLIC,http://www.haewonpark.com/,True,Personal Robots,False
the-electome-measuring-responsiveness-in-the-2016-election,aheyward,False,"<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who’s winning/losing (traditional “horse race” polls and projections) to the issues the campaign is being fought over (the “<a href=""http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/"">Horse Race of Ideas</a>”).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day “fire hose”) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf"">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/""> share of conversation</a> or coverage that <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/"">any given issue </a>or candidate commands on Twitter and in the news media, respectively—and how the two platforms are aligned<br></li><li>which issues are <a href=""https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/"">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates’ character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of “<a href=""http://fusion.net/story/304384/election-incivility-on-twitter/"">incivility</a>” (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=""http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/""> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM’s deployment of Electome analytics has been <a href=""http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will"">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets—including the <a href=""https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/"">Washington Post</a>, <a href=""http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps"">Bloomberg News</a>, <a href=""http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf"">CNN Politics</a> and Fusion—as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=""http://www.debates.org/"">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates’ moderators and credentialed journalists<br></li><li>also collaborated with the <a href=""http://ropercenter.cornell.edu/"">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center’s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms —and, importantly, creators—of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>",,--Choose Location,2019-04-19 18:40:58.879,True,2015-09-01,The Electome: Measuring responsiveness in the 2016 election,PUBLIC,http://electome.org/,True,Social Machines,True
wanderers,kolb,False,"<p>The Wanderers&nbsp;were unveiled as part of the exhibition: ‘The  Sixth Element: Exploring the Natural Beauty of 3D Printing' on display  at EuroMold, 25-28 November, Frankfurt, Germany. This work was done in collaboration with <a href=""http://www.deskriptiv.de/"">Christoph Bader and Dominik Kolb</a>.  The wearables were 3D-printed with Stratasys multi-material 3D printing  technology. Members of the Mediated Matter group led by <a href=""http://matter.media.mit.edu/people/bio/william-patrick"">Will Patrick</a>&nbsp;and <a href=""http://matter.media.mit.edu/people/bio/sunanda-sharma"">Sunanda Sharma</a>  are currently working on embedding living matter in the form of  engineered bacteria within the 3D structures in order to augment the  environment.</p>",,,2018-04-27 17:06:00.697,True,2014-07-01,Wanderers,PUBLIC,,True,Mediated Matter,False
living-mushtari,kolb,False,<p>How can we design relationships between the most primitive and the most  sophisticated life forms? Can we design wearables embedded with  synthetic microorganisms that can enhance and augment biological  functionality? Can we design wearables that generate consumable energy  when exposed to the sun?</p>,,--Choose Location,2017-10-13 23:28:11.103,True,2015-01-01,Living Mushtari,PUBLIC,,True,Mediated Matter,False
rottlace,kolb,False,"<p>Rottlace is a family of masks designed for Icelandic singer-songwriter Björk. Inspired by Björk’s most recent album—Vulnicura—the Mediated Matter Group explored themes associated with self-healing and expressing ""the face without a skin."" The series originates with a mask that emulates Björk’s facial structure and concludes with a mask that reveals a new identity, independent of its origin. What originates as a form of portraiture culminates in reincarnation.</p>",,--Choose Location,2018-05-07 19:47:18.431,True,2016-01-01,Rottlace,PUBLIC,,True,Mediated Matter,False
vespers,kolb,False,"<p>Novel technologies for additive manufacturing are enabling design and production at nature’s scale. We can seamlessly vary the physical properties of materials at the resolution of a sperm cell, a muscle cell, or a nerve cell. Stiffness, color, hygroscopy, transparency, conductivity, even scent, can be individually tuned for each three-dimensional pixel within a physical object. The generation of products is therefore no longer limited to assemblages of discrete parts with homogeneous properties. Rather like organs, objects can be computationally ""grown"" and 3D printed to form materially heterogeneous and multi-functional products.</p>",,,2018-05-07 19:48:07.209,True,2016-12-12,Vespers II,PUBLIC,,True,Mediated Matter,False
making-data-matter,kolb,False,"<p>We present a multimaterial voxel-printing method enabling the physical visualization of data sets commonly associated with scientific imaging. Leveraging voxel-based control of multimaterial 3D printing, our method enables additive manufacturing of discontinuous data types such as point cloud data, curve and graph data, image-based data, and volumetric data. By converting data sets into dithered material deposition descriptions, through modifications to rasterization processes, we demonstrate that data sets frequently visualized on screen can be converted into physical, materially heterogeneous objects.&nbsp;</p><p>Our approach alleviates the need to post-process data sets to boundary representations, preventing alteration of data and loss of information in the produced physicalizations. Therefore, it bridges the gap between digital information representation and physical material composition. We evaluate the visual characteristics and features of our method, assess its relevance and applicability in the production of physical visualizations, and detail the conversion of data sets for multimaterial 3D printing. We conclude with exemplary 3D printed datasets produced by our method pointing towards potential applications across scales, disciplines, and problem domains.</p>",,,2019-04-18 19:51:47.144,True,2018-05-30,Making Data Matter: Voxel-printing for the digital fabrication of data across scales and domains,PUBLIC,,True,Mediated Matter,False
immersion-teams,jingxian,False,<p>MITeams is an email network visualization tool for teams that allows people to see collaboration among team members and the structure of teams.<br></p>,,,2017-10-25 01:00:12.114,True,2016-10-01,MITeams,LAB-INSIDERS,,True,Scalable Cooperation,False
immersion-teams,dorghian,False,<p>MITeams is an email network visualization tool for teams that allows people to see collaboration among team members and the structure of teams.<br></p>,,,2017-10-25 01:00:12.114,True,2016-10-01,MITeams,LAB-INSIDERS,,True,Collective Learning,False
immersion-teams,xiaojiao,False,<p>MITeams is an email network visualization tool for teams that allows people to see collaboration among team members and the structure of teams.<br></p>,,,2017-10-25 01:00:12.114,True,2016-10-01,MITeams,LAB-INSIDERS,,True,Collective Learning,True
fiftynifty,lemeb,False,"<p>This is a grassroots challenge to get friends to participate in democracy by making calls to congresspeople in all 50 states. Live phone calls are the best way to directly express your opinion on an issue to your elected officials. Your mission is to pass message this along to friends who will make calls and also pass the message/link along to others who will do the same. It's a social chain letter and a call to action for a better participatory democracy. &nbsp;<span style=""font-size: 18px; font-weight: normal;"">We help you make your call and you pass on an invitation for your friends to do the same. Your invite can stress your opinion on a given issue.&nbsp;</span></p><p>The winners are the first ten chains to reach 50 states and accumulate the most challenge points. You get 250 points for making a call, 125 points for a call that your friend makes, 65 points for the call their friend makes, on and on. Everyone on the chain earns points. Points count for your first call to each of your two senators and your representative. You get a bonus for a ""grand slam""—a network that reaches all 435 representatives and 100 senators.</p><p>There is a leaderboard and a network view so you can track how you are doing. You can also see how much of the country your chain is covering.</p>",,,2019-06-04 20:46:21.258,True,2017-02-13,FiftyNifty,PUBLIC,https://fiftynifty.org,True,Civic Media,False
mekatilili,jaleesat,False,"<p><a href=""https://www.mekatilili.com"">Mekatilili</a>&nbsp;is a learning initiative that provides a platform for African youth to enhance technical skills through creative learning approaches that strive to empower learners and enable access to broader job opportunities and meaningful work. Founded in 2016, the program has reached over 400 young people whose average age demographic ranges from 14 – 25 years. The program is conducted through hands on, interactive workshops focusing on human-centered design, rapid prototyping, electronics, computer science, and professional development.&nbsp;<br></p><h2>Mekatilili Fellowship Program</h2><p>In 2019, the initiative launched the Mekatilili Fellowship Program (MFP), which is an annual gathering of African innovators that aims to foster open ended, playful, and peer-driven learning to promote the development of appropriate and sustainable local technical solutions.&nbsp;</p>",,,2019-03-29 21:06:55.831,True,2018-07-03,Mekatilili Fellowship Program,PUBLIC,http://mekatilili.media.mit.edu/,True,Lifelong Kindergarten,False
perform-1,jaleesat,False,"<p>PerForm explores the intuitive meanings associated with the shape of objects, and how a shape-changing tool can allow for different forms of tangible interaction.&nbsp;Is it possible to map how ideas feel, and use the connections between senses to create more intuitive interfaces?&nbsp;PerForm addresses that question by allowing users to transform a physical tool&nbsp;to fit their intentions. This way, a user can play different musical instruments or take different actions in games, simply by varying the shape of the tool. Since the meanings associated with the shapes would be dependent on context, we are giving special focus to studying possible mappings of between the perception of sound and shape.</p><p><b>SOUND-SHAPE CORRESPONDENCES</b></p><p><b>“<i>Music is not limited to the world of sound. There exists a music of the visual world.</i>”</b></p><p><b>—Oskar Fischinger, 1951.</b></p><p>When the German-American animator and filmmaker Oskar Fischinger created musically inspired animations and works of art, he touched on the intuitive associations our minds make between all the different sensory stimuli received from the environment.&nbsp;There is strong evidence that our brains forge relationships between shapes and seemingly corresponding sounds.</p><p>PerForm explores how the associations between visual and auditory perception can be used in interaction design. We developed a physical interface that users can transform by bending to create geometric shapes or symbols.&nbsp;By investigating possible correlations, natural or forged, between perceptual components of shape and its correlates in sound, we enable the&nbsp;tool to become a new instrument, with different sound timbre depending on the geometry of the object.</p><p><b>A SHAPE-SHIFTING GAMING CONTROLLER</b></p><p>One of the applications of this shape-shifting device would be to enable different modes of interaction through changes in shape. Instead of having to buy multiple controller devices for each genre of gaming or kind of interaction, or simply using a single, fixed-form controller that limits the embodied experience, a device capable of transformation would enable users to have a more imaginative and creative gaming experience, even enabling new kinds of games in which the user can invent tools by varying shapes.</p>",2019-09-01,,2019-04-18 14:46:14.807,True,2018-05-01,PerForm,PUBLIC,,True,Lifelong Kindergarten,False
teen-summit,jaleesat,False,"<p>Teen Summit is a biennial week-long Youth Leadership event that brings Clubhouse youth together from each of the 100 Clubhouses internationally. Youth leaders explore issues relevant to them and propose solutions through the creative use of innovative, high-end technologies. The 2018 Teen Summit will take place in late July at Boston University, featuring a college and career fair, collaborative cross-cultural activities, and many other opportunities for educational, career, and personal growth.</p>",,,2018-11-03 16:33:08.852,True,2017-10-02,Clubhouse Teen Summit,PUBLIC,http://www.computerclubhouse.org/teensummit,True,Lifelong Kindergarten,False
mekatilili,muthui,False,"<p><a href=""https://www.mekatilili.com"">Mekatilili</a>&nbsp;is a learning initiative that provides a platform for African youth to enhance technical skills through creative learning approaches that strive to empower learners and enable access to broader job opportunities and meaningful work. Founded in 2016, the program has reached over 400 young people whose average age demographic ranges from 14 – 25 years. The program is conducted through hands on, interactive workshops focusing on human-centered design, rapid prototyping, electronics, computer science, and professional development.&nbsp;<br></p><h2>Mekatilili Fellowship Program</h2><p>In 2019, the initiative launched the Mekatilili Fellowship Program (MFP), which is an annual gathering of African innovators that aims to foster open ended, playful, and peer-driven learning to promote the development of appropriate and sustainable local technical solutions.&nbsp;</p>",,,2019-03-29 21:06:55.831,True,2018-07-03,Mekatilili Fellowship Program,PUBLIC,http://mekatilili.media.mit.edu/,True,Lifelong Kindergarten,False
creative-learning-africa,muthui,False,"<p>The Lifelong Kindergarten group develops technologies and activities that engage people in creative learning. We’re exploring a few directions with our work in Africa:</p><p><b>1. Supporting creative learning across contexts</b></p><p>Instruction-focused learning and an emphasis on narrow outcomes continue to dominate formal and informal environments. As school systems seek to adapt to changes in technology, we’re exploring ways to support school systems, educators, and informal learning organizations in reconstructing outcomes and pedagogy that better prepare young people for lifelong, creative learning.</p><p><b>2. Creative Learning Communities</b></p><p>We’re exploring two kinds of communities in particular: (a) a maker-focused community that connects makers with employers while helping makers document and translate their skills to professional contexts and (b) supporting the development of creative learning communities—connecting individuals and organizations working to support creative learning in formal and informal learning environments.&nbsp;</p><p>Building on the work of <a href=""https://www.media.mit.edu/projects/creative-learning-in-brazil/overview/"">Aprendizagem Criativa no Brasil</a> (Creative Learning in Brazil—a decentralized network of educators, designers, systems leaders, foundations, and companies all involved in or hoping to support creative learning) and other initiatives, we hope to first gather stories of educators across a range of contexts, connect them with one another (including at the 2019 Africa Scratch conference), support co-development of resources, and hopefully engage in movement building.&nbsp;</p><p><b>3. Creative <i>vocational</i> learning</b></p><p>Vocational, 21st-century skills trainings are in high demand across the world—and especially in areas with high unemployment. Trainings tend to be dominated by traditional instruction and often limited to a particular set of skills. Moreover, while some of these trainings might help a student get a specific job or progress on a particular skill, they don’t prepare students for lifelong, creative learning—and the world that students will be entering.<br></p><p>In this environment, we’re exploring how to bring creative learning principles to the design and facilitation of “vocational trainings”—with the aspiration to create a <b>model</b>&nbsp;for more open-ended, playful, passionate, and peer-driven vocational learning that provides a springboard for lifelong learning.&nbsp;&nbsp;</p><p>We're piloting this work with the <a href=""http://mekatilili.media.mit.edu"">Mekatilili Initiative</a> in Nairobi, Kenya.&nbsp;</p>",,,2019-02-14 16:32:44.034,True,2018-10-03,Creative Learning Africa,PUBLIC,,True,Lifelong Kindergarten,False
HAL,cbarabas,False,"<p>The&nbsp;&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Humanizing AI in Law (HAL)&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">project aims to build the technical and legal foundations necessary to establish a due-process framework for auditing and improving decisions made by artificial intelligence systems as they evolve over time. This work is directed at the concerning software that has been deployed within the criminal justice system to aid judges in the sentencing of criminal defendants.</span></p>",,,2018-05-07 14:46:00.241,True,2017-06-01,Humanizing AI in Law (HAL),PUBLIC,,True,Other,False
HAL,nsaltiel,False,"<p>The&nbsp;&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Humanizing AI in Law (HAL)&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">project aims to build the technical and legal foundations necessary to establish a due-process framework for auditing and improving decisions made by artificial intelligence systems as they evolve over time. This work is directed at the concerning software that has been deployed within the criminal justice system to aid judges in the sentencing of criminal defendants.</span></p>",,,2018-05-07 14:46:00.241,True,2017-06-01,Humanizing AI in Law (HAL),PUBLIC,,True,Other,False
HAL,madars,False,"<p>The&nbsp;&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Humanizing AI in Law (HAL)&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">project aims to build the technical and legal foundations necessary to establish a due-process framework for auditing and improving decisions made by artificial intelligence systems as they evolve over time. This work is directed at the concerning software that has been deployed within the criminal justice system to aid judges in the sentencing of criminal defendants.</span></p>",,,2018-05-07 14:46:00.241,True,2017-06-01,Humanizing AI in Law (HAL),PUBLIC,,True,Other,False
zkledger-privacy-preserving-auditing,madars,False,"<h1>Privacy-preserving auditing on distributed ledgers</h1><p>zkLedger is a project that combines techniques from modern cryptography to analyze private data, while at the same time ensuring the integrity of that analysis by committing to the private data on a blockchain that is verified by all participants.</p><p>zkLedger uses permissioned blockchains, zero-knowledge proofs, and additively homomorphic commitment schemes to create a tamper-resistant, verifiable ledger of transactions which hides the amounts, senders, and recipients of transactions, and still allows for rich auditing.</p><h2>Auditing complex systems increases confidence that said systems work as intended</h2><p>Lack of auditability or inaccurate results from auditing can have devastating effects, as demonstrated by the 2008 financial crisis. Traditionally, auditability for companies has been solved by the use of trusted third party auditors, such as the “Big Four”: Deloitte, PriceWaterhouseCoopers, Ernst and Young, and KPMG. Auditability for financial institutions and exchanges has been insured by federal and state government agencies such as the OCC, the FDIC, and SEC, to name just a few. Unfortunately, this type of auditing is a laborious, time-consuming process, that is far from real-time. Blockchain technology proposes an alternative, yet for that alternative to work, direct competitors would need to share information that they consider proprietary.</p><h2>Permissioned blockchains</h2><p>Recently, financial institutions have formed consortia to investigate the use of a different architecture for securities settlement, inspired by blockchain technology. Bitcoin’s success has motivated institutions to consider upgrading their technical infrastructure by using permissioned blockchains, often maintained by participants with a consensus protocol. There are many strong players in this area that are making an impact, such as R3’s Corda system and IBM’s Hyperledger. With a large number of financial institutions already participating in these ledgers, what stands in the way of real-time auditing is a way to run computations on data while allowing participants to maintain the privacy of their data. This is where zkLedger can help.</p><h2>Zero-knowledge proofs</h2><p>Using zero-knowledge proofs, one party can prove that they know some secret information without revealing what that information is. One way to understand this is to look at an example: suppose that Alice has two billiard balls, one red and one green (they are otherwise identical). Bob, who is colorblind, cannot tell the difference between the balls, so he assumes that they are the same color. Alice wants to convince Bob that they are in fact different without revealing the colors of the balls to Bob, so Bob takes both balls, puts them behind his back, and either switches them or keeps them in the same hand. If Alice can correctly answer each time whether they have been switched or not, then she has some knowledge about the balls, but has never revealed what the color of either ball is. If she were to answer incorrectly once, then we know that she was guessing each time. We use zero-knowledge proofs in APL to ensure that transactions added to the ledger are consistent, and that auditing computations are performed correctly. Going further, zero-knowledge proofs can impact many aspects of the financial sector by providing both secrecy and accountability to financial institutions, and we’re exploring new ways to leverage this technology.</p><h2>zkLedger uses and current status</h2><p>We are exploring non-financial uses cases for zkLedger.&nbsp; Our paper ""zkLedger: Privacy-Preserving Auditing on Distributed Ledgers"" will appear at&nbsp;<a href=""https://www.usenix.org/conference/nsdi18/technical-sessions"">NSDI 2018</a>, and our prototype software will be released soon.</p><p><b>Read the paper below:</b></p>",,,2018-05-07 02:08:11.910,True,2018-01-01,zkLedger: Privacy-Preserving Auditing,PUBLIC,https://dci.mit.edu,True,Other,False
HAL,joi,False,"<p>The&nbsp;&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Humanizing AI in Law (HAL)&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">project aims to build the technical and legal foundations necessary to establish a due-process framework for auditing and improving decisions made by artificial intelligence systems as they evolve over time. This work is directed at the concerning software that has been deployed within the criminal justice system to aid judges in the sentencing of criminal defendants.</span></p>",,,2018-05-07 14:46:00.241,True,2017-06-01,Humanizing AI in Law (HAL),PUBLIC,,True,Initiatives,False
mit-knowledge-futures-group,joi,False,"<p>The MIT Knowledge Futures Group (KFG), a joint venture of the MIT Media Lab and the MIT Press, is an incubator for early-stage technologies that form part of a new open knowledge ecosystem. The partnership is the first of its kind between a leading publisher and a world-class research lab designing technologies of the future. The KFG seeks to incubate projects that enrich our open knowledge infrastructure, and leading by example, to spark a movement towards greater institutional ownership of that infrastructure. <br></p><p>The KFG currently incubates PubPub, an open authoring and publishing platform initially developed as a Media Lab project, by deploying it with dozens of MIT Press books and journals. PubPub socializes the process of knowledge creation by integrating conversation, annotation, and versioning into short and long-form digital publication. One of the flagship publications on PubPub is the Journal of Design and Science, which forges new connections between science and design and breaks down the barriers between academic disciplines. We envision JoDS as the node in a global online community rooted in the Media Lab’s research and design ethos. </p><p>The KFG also incubates The Underlay, an open, distributed knowledge store architected to capture, connect, and archive publicly available knowledge and its provenance. The Underlay provides mechanisms for distilling the knowledge graph from openly available publications, along with the archival and access technology to make the data and content hosted on PubPub available to other platforms. <br></p>",,,2018-08-23 19:38:55.666,False,2018-08-01,MIT Knowledge Futures Group,PUBLIC,,True,Initiatives,False
HAL,zittrain,False,"<p>The&nbsp;&nbsp;<span style=""font-size: 18px; font-weight: 400;"">Humanizing AI in Law (HAL)&nbsp;</span><span style=""font-size: 18px; font-weight: 400;"">project aims to build the technical and legal foundations necessary to establish a due-process framework for auditing and improving decisions made by artificial intelligence systems as they evolve over time. This work is directed at the concerning software that has been deployed within the criminal justice system to aid judges in the sentencing of criminal defendants.</span></p>",,,2018-05-07 14:46:00.241,True,2017-06-01,Humanizing AI in Law (HAL),PUBLIC,,True,Affiliates,True
artificial-intelligence-for-drug-discovery-and-clinical-trials,fkendall,False,"<p>Future of clinical development is on the verge of a major transformation due to convergence of large new digital data sources, the computing power to identify clinically-meaningful patterns in the data using efficient artificial intelligence (AI) and machine-learning (ML) algorithms, and regulators embracing this change through new collaborations. This perspective summarizes insights and recommendations for a new digital paradigm for healthcare from academy, biotechnology industry, non-profit foundations, regulators and technology corporations. Analysis and learning from publically available biomedical and clinical trial datasets, real world evidence from sensors and health records by machine learning architectures are discussed. Strategies for modernizing the clinical development process by integration of AI and ML based digital methods and secure computing technologies through recently announced regulatory pathways at the United States Food and Drug Administration are outlined. We conclude by discussing impact of digital algorithmic evidence to improve medical care for patients.</p>",,,2019-03-11 17:36:08.527,True,2017-01-23,Artificial Intelligence and Machine Learning in Clinical Development: a Translational Perspective,PUBLIC,,True,Other,False
affinity-tensorflow,mkkr,False,"<p>Affinity is a high-level machine learning API (Application Programming Interface) dedicated exclusively to molecular geometry. Affinity is written in TensorFlow; a small proportion of high-performance code is in low-level C++.  Depending on the application it can be configured as multi-CPU, multi-CPU single GPU, or multi-GPU system. Affinity has  its own web page at <a href=""http://affinity.mit.edu"">affinity.mit.edu </a><br></p>",,,2019-04-17 19:38:38.863,True,2017-08-01,Affinity: Deep Learning API for Molecular Geometry,PUBLIC,https://affinity.mit.edu,True,Molecular Machines,False
Biological-Enhancement,avujic,False,"<h2><b>Lab on Body, Synthetic Biology, and Bio-Digital Systems for Health and Human Enhancement</b></h2>",,,2019-05-10 15:15:39.601,True,2019-02-03,Theme | Wearable Biotech Enhancement,PUBLIC,,True,Fluid Interfaces,False
human-microbe-interaction,avujic,False,"<p>One of the current foci within the HCI community is to understand and augment human capabilities using physiological and biological data. Microbes living on, inside, and around the human play significant roles in life, from improving health to causing infectious diseases. As the knowledge of human-microbe interaction continues to unfold, we propose a framework for microbial HCI based on a growing body of work aiming to observe, integrate, and modify microorganisms in interactive systems. Our motivation for the framework is to advancing the next generation of biological HCI and exploring novel human-microbe interfaces across contexts, scales, and species.</p>",,,2019-04-29 16:39:12.651,True,2018-11-01,Microbial Augmentation Interfaces,LAB-INSIDERS,,True,Fluid Interfaces,False
serosa,avujic,False,"<p>Exploring the connection between our two minds: the one in our head and the one in our body.&nbsp;</p><p>The mind-gut connection has flourished as a research area in the past few decades, elucidating the&nbsp; key role of the enteric nervous system (ENS or ""gut-brain"") in stress, affect, and memory. However, this connection has not been explored for wearable technology—applications and research for cognitive phenomena remain biased towards the cerebrum. In this project, we are non-invasively acquiring gastric myoelectric activity from the abdomen to evaluate the potential for a new area of wearable technology that can inform users on affective, stress, or memory states based on signals produced by the ENS.</p>",,,2018-05-09 13:05:47.805,True,2017-10-01,Serosa,LAB-INSIDERS,,True,Fluid Interfaces,False
making-scratch-accessible,quacht,False,"<p>A conversational, voice-based interface for creating and playing Scratch projects makes Scratch accessible to children regardless of visual ability. Just as Scratch’s visual language lowers the barrier to entry for sighted children, the conversational interface lowers the barrier for children with visual impairments. The screenless interface is inspired by voice assistants and demonstrates the potential for programming through conversation.</p>",,,2019-04-19 18:38:49.254,True,2018-02-13,Agent-based programming interfaces for children,PUBLIC,,True,Lifelong Kindergarten,False
g3p,j_klein,False,"<p>Ancient yet modern, enclosing yet invisible, glass was first created in  Mesopotamia and Ancient Egypt 4,500 years ago. Precise recipes for its  production - the chemistry and techniques - often remain closely guarded  secrets. Glass can be molded, formed, blown, plated or sintered; its  formal qualities are closely tied to techniques used for its  formation.&nbsp;From the discovery of core-forming process for bead-making in  ancient Egypt, through the invention of the metal blow pipe during  Roman times, to the modern industrial Pilkington process for making  large-scale flat glass; each new breakthrough in glass technology  occurred as a result of prolonged experimentation and ingenuity, and has  given rise to a new universe of possibilities for uses of the material.  <br></p>",,--Choose Location,2017-10-13 19:43:29.330,True,2014-01-01,Glass I,PUBLIC,,True,Mediated Matter,False
fiberbots,sfalcone,False,"<p>FIBERBOTS is a digital fabrication platform fusing cooperative robotic manufacturing with abilities to generate highly sophisticated material architectures. The platform can enable design and digital fabrication of large-scale structures with high spatial resolution leveraging mobile fabrication nodes, or robotic ""agents"" designed to <i>tune</i> the material make-up of the structure being constructed on the fly as informed by their environment.<br></p><p>Some of nature’s most successful organisms collaborate in a swarm fashion. Nature’s builders leverage hierarchical structures in order to control and optimize multiple material properties. Spiders, for instance, spin protein fibers to weave silk webs with tunable local and global material properties, adjusting their material composition and fiber placement to create strong yet flexible structures optimized to capture prey. Other organisms, such as bees, ants and termites cooperate to rapidly build structures much larger than themselves. </p>",,,2019-02-13 16:36:22.730,True,2016-01-01,"FIBERBOTS: Design of a multi-agent, fiber composite digital fabrication system",PUBLIC,,True,Center for Bits and Atoms,False
human-adherence,fpeng,False,"<p>The Guardians project aims to use the same game design principles used in mobile game platforms to create greater engagement with individuals. The Affective Computing group is developing a custom video game with an independent patient reporting outcome tool to increase adherence to completion of&nbsp;patient reported outcomes.</p><p>Forming positive health habits can be difficult. Whether it’s taking medication, sticking to a diet, or going to the gym, it’s tough to commit to a new schedule long enough to form a habit. It is even more difficult when a person is asked to do something regularly that does not directly and immediately benefit them. This is an&nbsp;issue when clinical researchers need study participants to report outcomes regularly over a long period of time. Adherence is lost, resulting in suboptimal clinical outcomes and the loss of important data.</p><p>Mobile video&nbsp;games, on the other hand, generate an increased amount of adherence (<a href=""https://venturebeat.com/2017/02/01/superdata-mobile-games-hit-40-6-billion-in-2016-matching-world-box-office-numbers/"">as seen by an&nbsp;estimated market revenue of over $40.6 billion in 2016</a>**). Mobile video games have captured the attention of a wide variety of demographics and are often targeted to specific subgroups in order to increase engagement with a number of in-game features. These games use common design techniques and mechanics to produce a loop that draws players to return&nbsp;on a regular schedule and encourages them to watch ads, share on social media, or pay a fee for special rewards within the game.</p><p>By using the same game design principles, we aim to replace typical video game behaviors, like watching ads or sharing on social, with new behaviors that help improve the player’s wellbeing.</p><p>This project is a collaboration between the Affective Computing&nbsp;research group and Media Lab member company Takeda Pharmaceuticals.</p><p>**according to research by SuperData Research and Unity Technologies</p>",,,2019-05-16 15:17:42.342,True,2017-04-01,The Guardians,PUBLIC,,True,Affective Computing,False
personlized-animation,fpeng,False,"<p>Storytelling is a fundamental way in which human beings understand the world. Imagine watching a movie telling the story of your life, how would you respond to it and how would it change your perception of your own memories? Personalized animated movies are generated from Unity, customized to each user's mood and behavior date collected through self-reports. Our study shows that personalized animations can elicit strong emotional responses from participants and lengthier writing of self-reflection compared to a non-personalized control. Moving forward, we're looking at using personalized animation to encourage cognitive reappraisal and positive thinking.<br></p>",,,2019-06-05 18:33:04.324,True,2016-10-01,Personalized Animated Movies,PUBLIC,,True,Affective Computing,False
conversational-health-loneliness-on-reddit,lnfrat,False,"<p>Loneliness is becoming a global epidemic. As many as 33 percent of Americans report being chronically lonely, with similar percentages being reported in countries around the world. Additionally, this percentage has risen in recent years. Many are turning to online forums as a way to connect with others about their feelings of loneliness and to begin to reduce these feelings. However, oftentimes, posts go unresponded to and online conversations do not take place, perhaps because those conversing did not find a connection with each other, potentially leaving the poster feeling even more lonely. This research explores the how health of conversation should be defined in online support conversations and analyzes the characteristics of conversation that contribute to healthier conversation.&nbsp;</p>",,,2019-04-17 13:57:33.414,True,2018-09-01,Conversational Health: Loneliness on Reddit,PUBLIC,,True,Social Machines,False
networked-playscapes-dig-deep,edwinapn,False,"<p>Networked Playscapes re-imagines outdoor play by merging the flexibility of the digital world with the tangible, sensorial properties of physical play to create hybrid interactions for the urban environment.&nbsp;</p><p>Dig Deep takes the classic sandbox found in children's playgrounds and merges it with the common fantasy of ""digging your way to the other side of the world"" to create a networked interaction in tune with child cosmogony. </p>",,--Choose Location,2017-10-13 18:29:16.756,True,2014-01-01,Networked Playscapes: Dig Deep,PUBLIC,,True,Object Based Media,False
creative-learning-africa,yusufa,False,"<p>The Lifelong Kindergarten group develops technologies and activities that engage people in creative learning. We’re exploring a few directions with our work in Africa:</p><p><b>1. Supporting creative learning across contexts</b></p><p>Instruction-focused learning and an emphasis on narrow outcomes continue to dominate formal and informal environments. As school systems seek to adapt to changes in technology, we’re exploring ways to support school systems, educators, and informal learning organizations in reconstructing outcomes and pedagogy that better prepare young people for lifelong, creative learning.</p><p><b>2. Creative Learning Communities</b></p><p>We’re exploring two kinds of communities in particular: (a) a maker-focused community that connects makers with employers while helping makers document and translate their skills to professional contexts and (b) supporting the development of creative learning communities—connecting individuals and organizations working to support creative learning in formal and informal learning environments.&nbsp;</p><p>Building on the work of <a href=""https://www.media.mit.edu/projects/creative-learning-in-brazil/overview/"">Aprendizagem Criativa no Brasil</a> (Creative Learning in Brazil—a decentralized network of educators, designers, systems leaders, foundations, and companies all involved in or hoping to support creative learning) and other initiatives, we hope to first gather stories of educators across a range of contexts, connect them with one another (including at the 2019 Africa Scratch conference), support co-development of resources, and hopefully engage in movement building.&nbsp;</p><p><b>3. Creative <i>vocational</i> learning</b></p><p>Vocational, 21st-century skills trainings are in high demand across the world—and especially in areas with high unemployment. Trainings tend to be dominated by traditional instruction and often limited to a particular set of skills. Moreover, while some of these trainings might help a student get a specific job or progress on a particular skill, they don’t prepare students for lifelong, creative learning—and the world that students will be entering.<br></p><p>In this environment, we’re exploring how to bring creative learning principles to the design and facilitation of “vocational trainings”—with the aspiration to create a <b>model</b>&nbsp;for more open-ended, playful, passionate, and peer-driven vocational learning that provides a springboard for lifelong learning.&nbsp;&nbsp;</p><p>We're piloting this work with the <a href=""http://mekatilili.media.mit.edu"">Mekatilili Initiative</a> in Nairobi, Kenya.&nbsp;</p>",,,2019-02-14 16:32:44.034,True,2018-10-03,Creative Learning Africa,PUBLIC,,True,Lifelong Kindergarten,False
pathways,pbeshai,False,"<p>Our social networks influence our sense of what's possible: we can't aspire to be cancer researchers, activists, or artificial intelligence engineers if we've never been exposed to these as possibilities.  Unfortunately, many children grow up in environments replete with exposure gaps, impeding awareness and ultimately limiting their conceptions of which opportunities are available to them.</p><p>Pathways is a web application that seeks to scaffold career exploration and introspection among young people in order to help them explore a) what kinds of topics they might pursue in the future, b) in which capacities they might pursue these topics, and c) examples of education and career pathways others have traversed to get where they are today.  The tool uses several data science and machine learning techniques to process self-reported education and career data from thousands of individuals in the Greater Boston area.</p><p>Ultimately, we hope tools like Pathways can help enhance exposure and spark new social network ties that help foster greater upward mobility and an improved quality of life.</p>",,,2019-04-10 23:26:21.291,True,2018-05-01,Pathways,PUBLIC,,True,Social Machines,True
local-voices-network,pbeshai,False,"<p><b>What is Cortico and the Local Voices Network?</b></p><p><a href=""https://www.cortico.ai/"">Cortico</a>, a non-profit 501(c)(3) in cooperation with <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">MIT’s Laboratory for Social Machines</a>, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we’re developing a <b>public conversation network</b>&nbsp;called the&nbsp;<a href=""https://lvn.org/"">Local Voices Network</a>&nbsp;(LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.</p><p>LVN combines in-person and digital listening to host, analyze and connect community conversations at scale.&nbsp;Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network&nbsp;is designed around three core efforts:</p><ul><li>Facilitating in-person community dialogue that enables participants to listen, learn, and be heard</li><li>Connecting facilitators and conversations digitally across boundaries</li><li>Opening a new listening channel for journalists, leaders, and the community at large<br></li></ul><p><b>Why is this work important?</b></p><p>Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular “tribes” hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.</p>",,,2019-04-16 16:12:25.376,True,2018-09-01,Cortico: The Local Voices Network,PUBLIC,http://www.lvn.org,True,Social Machines,True
pathways,belen,False,"<p>Our social networks influence our sense of what's possible: we can't aspire to be cancer researchers, activists, or artificial intelligence engineers if we've never been exposed to these as possibilities.  Unfortunately, many children grow up in environments replete with exposure gaps, impeding awareness and ultimately limiting their conceptions of which opportunities are available to them.</p><p>Pathways is a web application that seeks to scaffold career exploration and introspection among young people in order to help them explore a) what kinds of topics they might pursue in the future, b) in which capacities they might pursue these topics, and c) examples of education and career pathways others have traversed to get where they are today.  The tool uses several data science and machine learning techniques to process self-reported education and career data from thousands of individuals in the Greater Boston area.</p><p>Ultimately, we hope tools like Pathways can help enhance exposure and spark new social network ties that help foster greater upward mobility and an improved quality of life.</p>",,,2019-04-10 23:26:21.291,True,2018-05-01,Pathways,PUBLIC,,True,Social Machines,False
affective-network,belen,False,"<p><a href=""https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US"">Try Affective Network!</a></p><p><span style=""font-size: 18px;"">Emotional contagion in online social networks has been of great interest over the past years. Previous studies have mainly focused on finding evidence of affection contagion in homophilic atmospheres. However, these studies have overlooked users' awareness of the sentiments they share and consume online. In this work, we present an experiment with Twitter users that aims to help them better understand which emotions they experience on this social network. We introduce&nbsp;</span><a href=""https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US"" style=""font-size: 18px;""><b>Affective Network</b></a><span style=""font-size: 18px;"">&nbsp;(Aff-Net), a Google Chrome extension that enables Twitter users to filter and make explicit (through colored visual marks) the emotional content in their news feed.</span><br></p><p>The extension is powered by machine learning algorithms that classify tweets into different sentiment categories: positive posts tend to use happy or surprising language; negative posts tend to use sad, angry, or disgusting language; and posts without strong emotional language are classified as neutral.</p><p>Affective Network aims to help social media users better understand which emotions they tend to consume on social media, and how these emotions can spread through their social networks. It was built by researchers at the <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">Laboratory for Social Machines&nbsp;</a>and the&nbsp;<a href=""https://www.media.mit.edu/groups/affective-computing/overview/"">Affective Computing</a> group at the <a href=""https://www.media.mit.edu/"">MIT Media Lab</a>.</p><p>Note that Affective Network does not necessarily reflect the official position of the MIT Media Lab regarding the benefits and drawbacks of filtering out specific emotional content.</p><p><a href=""https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US"">Try Affective Network!</a></p>",,,2019-05-17 19:51:18.500,True,2018-12-01,Affective Network,PUBLIC,https://affectivenetwork.media.mit.edu,True,Social Machines,False
local-voices-network,eyi,False,"<p><b>What is Cortico and the Local Voices Network?</b></p><p><a href=""https://www.cortico.ai/"">Cortico</a>, a non-profit 501(c)(3) in cooperation with <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">MIT’s Laboratory for Social Machines</a>, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we’re developing a <b>public conversation network</b>&nbsp;called the&nbsp;<a href=""https://lvn.org/"">Local Voices Network</a>&nbsp;(LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.</p><p>LVN combines in-person and digital listening to host, analyze and connect community conversations at scale.&nbsp;Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network&nbsp;is designed around three core efforts:</p><ul><li>Facilitating in-person community dialogue that enables participants to listen, learn, and be heard</li><li>Connecting facilitators and conversations digitally across boundaries</li><li>Opening a new listening channel for journalists, leaders, and the community at large<br></li></ul><p><b>Why is this work important?</b></p><p>Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular “tribes” hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.</p>",,,2019-04-16 16:12:25.376,True,2018-09-01,Cortico: The Local Voices Network,PUBLIC,http://www.lvn.org,True,Social Machines,False
local-voices-network,dougb5,False,"<p><b>What is Cortico and the Local Voices Network?</b></p><p><a href=""https://www.cortico.ai/"">Cortico</a>, a non-profit 501(c)(3) in cooperation with <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">MIT’s Laboratory for Social Machines</a>, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we’re developing a <b>public conversation network</b>&nbsp;called the&nbsp;<a href=""https://lvn.org/"">Local Voices Network</a>&nbsp;(LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.</p><p>LVN combines in-person and digital listening to host, analyze and connect community conversations at scale.&nbsp;Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network&nbsp;is designed around three core efforts:</p><ul><li>Facilitating in-person community dialogue that enables participants to listen, learn, and be heard</li><li>Connecting facilitators and conversations digitally across boundaries</li><li>Opening a new listening channel for journalists, leaders, and the community at large<br></li></ul><p><b>Why is this work important?</b></p><p>Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular “tribes” hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.</p>",,,2019-04-16 16:12:25.376,True,2018-09-01,Cortico: The Local Voices Network,PUBLIC,http://www.lvn.org,True,Social Machines,False
local-voices-network,chrwang,False,"<p><b>What is Cortico and the Local Voices Network?</b></p><p><a href=""https://www.cortico.ai/"">Cortico</a>, a non-profit 501(c)(3) in cooperation with <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">MIT’s Laboratory for Social Machines</a>, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we’re developing a <b>public conversation network</b>&nbsp;called the&nbsp;<a href=""https://lvn.org/"">Local Voices Network</a>&nbsp;(LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.</p><p>LVN combines in-person and digital listening to host, analyze and connect community conversations at scale.&nbsp;Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network&nbsp;is designed around three core efforts:</p><ul><li>Facilitating in-person community dialogue that enables participants to listen, learn, and be heard</li><li>Connecting facilitators and conversations digitally across boundaries</li><li>Opening a new listening channel for journalists, leaders, and the community at large<br></li></ul><p><b>Why is this work important?</b></p><p>Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular “tribes” hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.</p>",,,2019-04-16 16:12:25.376,True,2018-09-01,Cortico: The Local Voices Network,PUBLIC,http://www.lvn.org,True,Social Machines,False
local-voices-network,wschen,False,"<p><b>What is Cortico and the Local Voices Network?</b></p><p><a href=""https://www.cortico.ai/"">Cortico</a>, a non-profit 501(c)(3) in cooperation with <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">MIT’s Laboratory for Social Machines</a>, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we’re developing a <b>public conversation network</b>&nbsp;called the&nbsp;<a href=""https://lvn.org/"">Local Voices Network</a>&nbsp;(LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.</p><p>LVN combines in-person and digital listening to host, analyze and connect community conversations at scale.&nbsp;Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network&nbsp;is designed around three core efforts:</p><ul><li>Facilitating in-person community dialogue that enables participants to listen, learn, and be heard</li><li>Connecting facilitators and conversations digitally across boundaries</li><li>Opening a new listening channel for journalists, leaders, and the community at large<br></li></ul><p><b>Why is this work important?</b></p><p>Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular “tribes” hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.</p>",,,2019-04-16 16:12:25.376,True,2018-09-01,Cortico: The Local Voices Network,PUBLIC,http://www.lvn.org,True,Social Machines,True
local-voices-network,ajking,False,"<p><b>What is Cortico and the Local Voices Network?</b></p><p><a href=""https://www.cortico.ai/"">Cortico</a>, a non-profit 501(c)(3) in cooperation with <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">MIT’s Laboratory for Social Machines</a>, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we’re developing a <b>public conversation network</b>&nbsp;called the&nbsp;<a href=""https://lvn.org/"">Local Voices Network</a>&nbsp;(LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.</p><p>LVN combines in-person and digital listening to host, analyze and connect community conversations at scale.&nbsp;Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network&nbsp;is designed around three core efforts:</p><ul><li>Facilitating in-person community dialogue that enables participants to listen, learn, and be heard</li><li>Connecting facilitators and conversations digitally across boundaries</li><li>Opening a new listening channel for journalists, leaders, and the community at large<br></li></ul><p><b>Why is this work important?</b></p><p>Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular “tribes” hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.</p>",,,2019-04-16 16:12:25.376,True,2018-09-01,Cortico: The Local Voices Network,PUBLIC,http://www.lvn.org,True,Social Machines,False
local-voices-network,wesc,False,"<p><b>What is Cortico and the Local Voices Network?</b></p><p><a href=""https://www.cortico.ai/"">Cortico</a>, a non-profit 501(c)(3) in cooperation with <a href=""https://www.media.mit.edu/groups/social-machines/overview/"">MIT’s Laboratory for Social Machines</a>, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we’re developing a <b>public conversation network</b>&nbsp;called the&nbsp;<a href=""https://lvn.org/"">Local Voices Network</a>&nbsp;(LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.</p><p>LVN combines in-person and digital listening to host, analyze and connect community conversations at scale.&nbsp;Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network&nbsp;is designed around three core efforts:</p><ul><li>Facilitating in-person community dialogue that enables participants to listen, learn, and be heard</li><li>Connecting facilitators and conversations digitally across boundaries</li><li>Opening a new listening channel for journalists, leaders, and the community at large<br></li></ul><p><b>Why is this work important?</b></p><p>Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular “tribes” hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.</p>",,,2019-04-16 16:12:25.376,True,2018-09-01,Cortico: The Local Voices Network,PUBLIC,http://www.lvn.org,True,Social Machines,True
soundsignaling,ishwarya,False,"<p>Drawing inspiration from the notion of cognitive incongruence associated with Stroop's famous experiment, from musical principles, and from the observation that music consumption on an individual basis is becoming increasingly ubiquitous, we present the SoundSignaling system—a software platform designed to make real-time, stylistically relevant modifications to a personal corpus of music as a means of conveying information or notifications. From the substantial body of HCI research demonstrating the negative attentional implications of a daily inundation of notifications, we highlight two challenges associated with standard audio notifications—a ""switch cost"" that impedes productivity, and a lack of awareness of a user's cognitive load—that have the potential to be addressed by such a system without active activity estimation. Through this work, we suggest a re-evaluation of the age-old paradigm of binary notifications in favor of a system designed to operate upon the relatively unexplored medium of a user's musical preferences.</p>",,,2018-10-19 20:43:41.491,True,2017-11-01,SoundSignaling,PUBLIC,,True,Responsive Environments,False
visualsoundtrack,ishwarya,False,"<p>We present VisualSoundtrack, a system designed  as a tool for soundtrack composers to experiment with original musical  content in differing musical “styles."" The system allows a user to  rapidly prototype musical ideas with respect to the target media (such  as a film or podcast) by having him/ her input original musical motifs,  capitalizing on a corpus of existing soundtrack samples to source  various styles, and allowing the user to identify the most appropriate  style sources for the target media by visually architecting a path  through a highly abstracted feature space.</p>",,,2018-10-19 20:45:43.275,True,2016-12-01,VisualSoundtrack,PUBLIC,,True,Responsive Environments,False
cognitive-audio,ishwarya,False,"<p>When we form memories, not everything that we perceive is noticed; not  everything that we notice is remembered. Humans are excellent at  filtering and retaining only the most important parts of their  experience—what if our audio compression had the same ability?&nbsp;</p><p>Our goal is to understand what makes sound memorable. With this  work, we hope to gain insight into the cognitive processes that drive  auditory perception and predict the memorability of sounds in the world  around us more accurately than ever before.  Ultimately, these models  will give us the ability to generate and manipulate the sounds that  surround us to be more or less memorable.&nbsp;</p><p>We envision this research introducing new paradigms into the  space of audio compression, attention-driven user interactions, and  auditory AR, amongst others.</p>",,,2018-05-06 23:28:44.204,True,2017-11-01,Cognitive Audio,PUBLIC,,True,Responsive Environments,False
compression-by-content-curation,ishwarya,False,"<p>As we move towards an increasingly IoT-enabled ecosystem, we find that it is easier than ever before to capture vast amounts of audio data.  However, there are many scenarios in which we may seek a ""compressed"" representation of an audio stream, consisting of an intentional curation of content to achieve a specific presentation—a background soundtrack for studying or working; a summary of salient events over the course of a day; or an aesthetic soundscape that evokes nostalgia of a time and place.  In this work, we present a novel, automated approach to the task of content-driven ""compression,"" built upon the tenets of auditory cognition, attention, and memory.  We expand upon our previous experimental findings, which demonstrate the relative importance of higher-level gestalt and lower level spectral principles in determining auditory memory, to design corresponding computational implementations enabled by auditory saliency models, deep neural networks for audio classification, and spectral feature extraction.  We demonstrate the approach by generating a number of 30 second binaural mixes from eight-hour recordings captured in three contrasting locations at the Media Lab, and conduct a qualitative evaluation illustrating the relationship between our feature space and a user's perception of the resulting presentations.  Through this work, we suggest rethinking traditional paradigms of compression in favor of an approach that is goal-oriented and modulated by human perception.</p>",,,2019-04-16 20:15:50.983,True,2019-03-01,Cognition-driven audio summarization,PUBLIC,,True,Responsive Environments,False
codec-perceptual-loss,ishwarya,False,"<p>Generative audio models based on neural networks have led to considerable improvements across fields including speech enhancement, source separation, and text-to-speech synthesis. These systems are typically trained in a supervised fashion using simple element-wise l1 or l2 losses. However, because they do not capture properties of the human auditory system, such losses encourage modeling perceptually meaningless aspects of the output, wasting capacity and limiting performance. Additionally, while adversarial models have been employed to encourage outputs that are statistically indistinguishable from ground truth and have resulted in improvements in this regard, such losses do not need to explicitly model perception as their task; furthermore, training adversarial networks remains an unstable and slow process.</p><p><br>In this work, we investigate an idea fundamentally rooted in psychoacoustics. We train a neural network to emulate an MP3 codec as a differentiable function. Feeding the output of a generative model through this MP3 function, we remove signal components that are perceptually irrelevant before computing a loss. To further stabilize gradient propagation, we employ intermediate layer outputs to define our loss, as found useful in image domain methods. Our experiments using an autoencoding task show an improvement over standard losses in listening tests, indicating the potential of psychoacoustically motivated models for audio generation.</p>",,,2019-04-16 20:20:38.400,True,2019-02-01,Towards a Perceptual Loss:  Using a neural network codec approximation as a loss for generative audio models,PUBLIC,,True,Responsive Environments,False
artboat,perovich,False,"​<p><br>ArtBoat is a tool for communities to make collaborative light paintings in public spaces and reimagine the future of their cities.&nbsp;&nbsp;Visit our project website at&nbsp;<a href=""http://artboatcommunity.com/"">artboatcommunity.com</a>.</p><p>ArtBoat consists of a remote control boat with light strips that change color in response to a custom light design board. During ArtBoat events, community members can drive remote controlled boats, control the color of lights on the boats, or take long exposure photographs to make collaborative light graffiti that explores how art can help communities claim public spaces.&nbsp;</p><p>Our first ArtBoat community event was on July 29, 2018 at <a href=""https://magazinebeach.org/"">Magazine Beach Park</a> in Cambridge, MA. Thanks to our Magazine Beach host Cathie Zusy, our photographers&nbsp;<a href=""http://www.jorgevaldezphotography.net/"">Jorge Valdez</a>&nbsp;(IG: jorgelvma), Ron Hoffmann (IG: archiscapes.us),&nbsp;<a href=""http://www.facebook.com/exploreThePlanetEarth"">Neil Gaikwad</a>&nbsp;(IG: Neil Gaikwad Photography), Garance Malivel, and Jimmy Day, our&nbsp;boat facilitators, and our participants.&nbsp; Our second ArtBoat event was on&nbsp;September 22, 2018&nbsp; at Herter Park Amphitheater in Boston, MA, as part of the 15th anniversary&nbsp;<a href=""https://www.revels.org/shows-events/riversing/"">Revels RiverSing</a>!&nbsp;</p><p>If you'd like to join our mailing list to find out about future ArtBoat events, email artboat@lauraperovich.com or fill out this <a href=""https://goo.gl/forms/TWoXkU7jCR4j8dpx1"">form</a>.&nbsp;</p><p>ArtBoat is a sister project of <a href=""https://www.media.mit.edu/projects/thermal-fishing-bob-in-place-environmental-data-visualization/overview/"">SeeBoat</a>. It is an open source project licensed under GPLv2 and Creative Commons.&nbsp;<br></p>",,,2019-02-25 15:19:42.293,True,2018-04-01,ArtBoat,PUBLIC,,True,Object Based Media,False
thermal-fishing-bob-in-place-environmental-data-visualization,perovich,False,"<p>Two of the most important traits of environmental hazards today are their invisibility and the fact that they are experienced by communities, not just individuals. Yet we don't have a good way to make hazards like chemical pollution visible and intuitive.&nbsp; SeeBoat and the thermal fishing bob seek to visceralize rather than simply visualize data by creating a physical data experience that makes water pollution data present in communities.&nbsp;</p><p>SeeBoat is a remote control boat with sensors (temperature, turbidity, conductivity, pH) that measure local water quality and LEDs that&nbsp;display the data on site by changing color in real time. Data is also logged to be physically displayed elsewhere and can be further recorded using long-exposure photos.&nbsp;Making environmental data experiential and interactive will help both communities and researchers better understand pollution and its implications.</p><p>The Thermal Fishing Bob is an early version of this tool that has a spherical form factor and focuses on measuring water temperature as a marker for combined sewer overflows (CSOs) that may pollute rivers.&nbsp;&nbsp;</p><p>This project began in partnership with Sara Wylie (Northeastern University) in Spring 2015. Early work included Thermal Fishing Bob workshops, design iteration, prototyping, system testing with users in the Mystic River and Charles River, long exposure photography events, and further concept development. In Spring of 2017, Perovich and Wylie began a collaboration with Roseann Bongiovanni of GreenRoots, an&nbsp;environmental justice community group in Chelsea, MA, to test and iterate on the devices so they best suit the environmental and social context in the local community.&nbsp; As part of this process,&nbsp;Perovich continued to develop the technical side of the project to create SeeBoat, a remote control boat based system, including sensors for&nbsp;turbidity, conductivity, pH, radio based data communication, and designs for and early implementation of an Android app for collecting and viewing quantitative sensor data. Perovich, Wylie, and Bongiovanni are also pursuing related routes of research and community engagement around open access environmental data, the politics of space, community based data installations, and evaluating individual and group learning through extended participatory action research projects.&nbsp;<br></p><p>A publication describing their first year of collaboration can be found in their paper:&nbsp;&nbsp;</p><p>Laura J. Perovich, Sara Wylie, Roseann Bongiovanni (2018)&nbsp;Pokémon Go, pH, and projectors: Applying transformation design and participatory action research to an environmental justice collaboration in Chelsea, MA,&nbsp;Cogent Arts &amp; Humanities,&nbsp;5:1,&nbsp;1-22. (<a href=""https://www.tandfonline.com/doi/pdf/10.1080/23311983.2018.1483874"">Link to PDF</a>.)&nbsp;</p><p>In July of 2018, the team began to collaborate with high school students and staff at the Microsoft Garage Makerspace to test the ease of fabrication of SeeBoat in a more general audience and to continue development of the SeeBoat Android app for numeric data display.&nbsp;</p><p>Thanks to&nbsp;ECO, David Ortiz, Adela Gonzalez, Leo Martinez, GreenRoots staff, Don Blair, Catherine D’Ignazio, the Boston University Law Clinic, and Dr. Sharon Harlan for their support and input on this project. Thanks to MIT undergraduates Sophia Struckman, Rod Bayliss, Robert Henning, and Claudia Chen who contributed to the technical aspect of these workshops and citizen science tool development, photographers Jorge Valdez and Shirin Adhami,&nbsp; the Wylie Lab at Northeastern University, Dr. V. Michael Bove and members of the Object-Based Media group at the MIT Media Lab, the MIT Arts Scholars, the Public Lab community,&nbsp; Mare Librum, the MIT Sailing Pavilion, and the Council for the Arts at MIT.</p>",,--Choose Location,2019-04-17 18:22:32.202,True,2015-01-01,SeeBoat (Thermal Fishing Bob): In-place environmental data visualization,PUBLIC,,True,Object Based Media,False
open-water-data,perovich,False,"<p>The Open Water Data project explores data physicalization as a path to community engagement and action on important environmental issues. For our 2018 installation&nbsp;<i>Chemicals in the Creek</i>, we released glowing lanterns representing water quality permit violations from local facilities onto the river as part of a performance of local environmental challenges that informed a community conversation on these issues.&nbsp;</p>",,,2019-04-23 13:14:44.647,True,2017-01-01,Open Water Data,PUBLIC,http://datalanterns.com/,True,Object Based Media,False
dancing-data,perovich,False,"<p>Every day, data about our environment, education, routes and other aspects of our lives become available.&nbsp;&nbsp;Yet, most people often struggle to understand, use and relate to these data. Especially because the access to all this data is often in formats that only few can retrieve, read, and understand.</p><p>In this project, we seek to&nbsp;disrupt the way we present and interact with datasets, using art as a vehicle to tell our stories informed by datasets.&nbsp;&nbsp;<b>Data dancing is a data experience.</b> “<i>A data experience takes data off the screen and puts it into the physical world</i>” [<a href=""http://lauraperovich.com/thesisPerovich.pdf"">Perovich L., 2015</a>].&nbsp;<br></p><p>This project explores&nbsp;the ways in which data can be physicalized through human expression.&nbsp;We are working with&nbsp;members of the community to co-compose dance performances informed by datasets and, validate the use of dance and acting as methods to enhance data literacy.&nbsp;</p>",2020-05-01,,2019-06-07 20:04:38.751,True,2019-05-01,Data Dancing,PUBLIC,,True,Object Based Media,False
origami-simulator,ghassaei,False,"<p>This WebGL app created in the <b>MIT Center for Bits and Atoms</b> simulates how any origami crease pattern will fold. It may look a little different from what you typically think of as ""origami"" - rather than folding paper in a set of sequential steps, this simulation attempts to fold every crease simultaneously. It does this by iteratively solving for small displacements in the geometry of an initially flat sheet due to forces exerted by creases. It calculates the geometry of folded or partially folded origami using a dynamic, GPU-accelerated solver; the solver extends work from the papers&nbsp;<a href=""http://www2.eng.cam.ac.uk/~sdg/preprint/5OSME.pdf"">Origami Folding: A Structural Engineering Approach</a>&nbsp;and&nbsp;<a href=""http://www.tsg.ne.jp/TT/cg/TachiFreeformOrigami2010.pdf"">Freeform Variations of Origami</a>. It also supports an immersive, interactive VR mode using&nbsp;<a href=""https://webvr.info/"">WebVR</a>. More information about the solver and the app is being compiled into a paper and will be posted here soon.&nbsp;<br></p>",,,2017-11-29 21:37:37.554,True,2017-03-01,Origami Simulator,LAB,,True,Center for Bits and Atoms,False
orbit-weaver-suit,alauer,False,"​​​​​​​​​<p>The Orbit Weaver Suit for&nbsp;zero gravity was&nbsp;designed by Media Lab Director's Fellow Andrea Lauer in collaboration with Xin Liu.&nbsp;</p><p>The design was inspired by a drawing Jordan Piantedosi made for Xin Liu, in which she&nbsp;is in a suit and casting&nbsp;strings out into space.&nbsp;The drawing is a reference to Orbit Weaver, a performance piece Xin created to test in a zero-gravity environment in November 2017.&nbsp;</p>",,,2017-11-30 15:54:37.271,True,2017-10-17,Orbit Weaver Suit,PUBLIC,,True,Initiatives,False
grey-mirror,rhysl,False,"<p>This podcast features interviews with experts on technology, society, and ethics. Check out our interviews <a href=""https://podcasts.apple.com/us/podcast/grey-mirror-mit-media-labs-digital-currency-initiative/id1254196635?mt=2"">here</a>!&nbsp;</p>",,,2019-04-16 17:47:05.046,True,2018-12-20,"Grey Mirror: A podcast on technology, society, and ethics",PUBLIC,,True,Initiatives,False
cryptocurrency-research-review,rhysl,False,"<p>The Digital Currency Initiative is helping ""build a field"" of cryptocurrency and blockchain technology by starting a new peer-reviewed journal and conference. Check out our first step (and experimental newsletter), <a href=""https://mitcryptocurrencyresearch.substack.com/"">here</a>.</p>",,,2019-04-16 17:50:24.675,True,2018-12-07,Cryptocurrency Research Review,PUBLIC,,True,Initiatives,False
linkedout,rubezc,False,"<p>LinkedOut aims to define and build solutions to facilitate societal reentry for formerly incarcerated individuals.</p><p>In collaboration with the Office of Returning Citizens (ORC), an office under the City of Boston that facilitates reentry, LinkedOut is working to design a reentry infrastructure to shut the revolving door of incarceration and reincarceration.</p><p>Our work seeks to break down the structural and societal barriers to successful reentry. At the structural level, we are building technologies to develop richer data biographies of the lives of returning citizens, and in doing so provide policymakers with the robust data required for effective reentry programs. Importantly, our goal is to embed values in technology design to humanize the reentry process. On the societal level, we have begun to expose holes in the moral fabric of society as we draw attention to the dehumanization and stigmatization of returning citizens. These invisible societal norms disempower returning citizens from rebuilding their lives. Thus, our work demands a reimagination of our current justice system—one that rehabilitates rather than retributes, that embraces rather than excludes—to design for a successful reentry journey.</p><h1><b>Ongoing Project</b></h1><h2><b>Pathfinder: How it works</b></h2><p>Pathfinder is a personalized case management platform that helps case managers design and keep track of returning citizens’ transition back into society. It makes real-time recommendations in the categories of healthcare, housing, employment, education, financial health, and community engagement based on the unique needs of each returning citizen. Beyond tracking the reentry journey of returning citizens, Pathfinder helps service providers identify and track community service gaps in meeting the needs of their clients.</p><p>In summary, Pathfinder has three key aims:</p><ol><li>Develop individualized case management plans for returning citizens</li><li>Streamline work process for service providers to improve efficacy of services</li><li>Capture community-level determinants of reentry for more informed reentry policies and programs</li></ol><p><b>Collaborators</b></p><p>Pathfinder is a collaboration between, with, and for returning citizens. We are partnering with <a href=""https://www.codersbeyondbars.org/"">Coders Beyond Bars</a>, a non-profit that teaches returning citizens to code; through this partnership, returning citizens will co-design and co-develop Pathfinder with the MIT Media Lab.</p>",,,2019-04-16 16:34:52.993,True,2018-09-17,LinkedOut: Codesigning societal reentry with returning citizens,PUBLIC,https://rubezchong.com/,True,Civic Media,False
linkedout,mavipasi,False,"<p>LinkedOut aims to define and build solutions to facilitate societal reentry for formerly incarcerated individuals.</p><p>In collaboration with the Office of Returning Citizens (ORC), an office under the City of Boston that facilitates reentry, LinkedOut is working to design a reentry infrastructure to shut the revolving door of incarceration and reincarceration.</p><p>Our work seeks to break down the structural and societal barriers to successful reentry. At the structural level, we are building technologies to develop richer data biographies of the lives of returning citizens, and in doing so provide policymakers with the robust data required for effective reentry programs. Importantly, our goal is to embed values in technology design to humanize the reentry process. On the societal level, we have begun to expose holes in the moral fabric of society as we draw attention to the dehumanization and stigmatization of returning citizens. These invisible societal norms disempower returning citizens from rebuilding their lives. Thus, our work demands a reimagination of our current justice system—one that rehabilitates rather than retributes, that embraces rather than excludes—to design for a successful reentry journey.</p><h1><b>Ongoing Project</b></h1><h2><b>Pathfinder: How it works</b></h2><p>Pathfinder is a personalized case management platform that helps case managers design and keep track of returning citizens’ transition back into society. It makes real-time recommendations in the categories of healthcare, housing, employment, education, financial health, and community engagement based on the unique needs of each returning citizen. Beyond tracking the reentry journey of returning citizens, Pathfinder helps service providers identify and track community service gaps in meeting the needs of their clients.</p><p>In summary, Pathfinder has three key aims:</p><ol><li>Develop individualized case management plans for returning citizens</li><li>Streamline work process for service providers to improve efficacy of services</li><li>Capture community-level determinants of reentry for more informed reentry policies and programs</li></ol><p><b>Collaborators</b></p><p>Pathfinder is a collaboration between, with, and for returning citizens. We are partnering with <a href=""https://www.codersbeyondbars.org/"">Coders Beyond Bars</a>, a non-profit that teaches returning citizens to code; through this partnership, returning citizens will co-design and co-develop Pathfinder with the MIT Media Lab.</p>",,,2019-04-16 16:34:52.993,True,2018-09-17,LinkedOut: Codesigning societal reentry with returning citizens,PUBLIC,https://rubezchong.com/,True,Civic Media,False
dancing-data,mavipasi,False,"<p>Every day, data about our environment, education, routes and other aspects of our lives become available.&nbsp;&nbsp;Yet, most people often struggle to understand, use and relate to these data. Especially because the access to all this data is often in formats that only few can retrieve, read, and understand.</p><p>In this project, we seek to&nbsp;disrupt the way we present and interact with datasets, using art as a vehicle to tell our stories informed by datasets.&nbsp;&nbsp;<b>Data dancing is a data experience.</b> “<i>A data experience takes data off the screen and puts it into the physical world</i>” [<a href=""http://lauraperovich.com/thesisPerovich.pdf"">Perovich L., 2015</a>].&nbsp;<br></p><p>This project explores&nbsp;the ways in which data can be physicalized through human expression.&nbsp;We are working with&nbsp;members of the community to co-compose dance performances informed by datasets and, validate the use of dance and acting as methods to enhance data literacy.&nbsp;</p>",2020-05-01,,2019-06-07 20:04:38.751,True,2019-05-01,Data Dancing,PUBLIC,,True,Civic Media,False
miniaturized-neural-system-for-chronic-local-intracerebral-drug-delivery,canand,False,"<p>Recent advances in medications for neurodegenerative disorders are expanding opportunities for improving the debilitating symptoms suffered by patients. Existing pharmacologic treatments, however, often rely on systemic drug administration, which result in broad drug distribution and consequent increased risk for toxicity. Given that many key neural circuitries have sub-cubic millimeter volumes and cell-specific characteristics, small-volume drug administration into affected brain areas with minimal diffusion and leakage is essential. We report the development of an implantable, remotely controllable, miniaturized neural drug delivery system permitting dynamic adjustment of therapy with pinpoint spatial accuracy. We demonstrate that this device can chemically modulate local neuronal activity in small-animal (rodent) and large-animal (nonhuman primate) models, while simultaneously allowing the recording of neural activity to enable feedback control.&nbsp;</p>",,,2018-09-18 17:32:14.261,True,2018-01-24,"Miniaturized Neural System for Chronic, Local Intracerebral Drug Delivery (MiNDS)",PUBLIC,,True,Conformable Decoders,False
lead-zirconate-titanate-gastrointestinal-sensor-pzt-gi-s,canand,False,"<p>Improvements in ingestible electronics with the capacity to sense physiological and pathophysiological states have transformed the standard of care for patients. Yet, despite advances in device development, significant risks associated with solid, non-flexible gastrointestinal transiting systems remain. Here, we report the design and use of an ingestible, flexible piezoelectric device that senses mechanical deformation within the gastric cavity. We demonstrate the capabilities of the sensor in both in vitro and ex vivo simulated gastric models, quantify its key behaviours in the gastrointestinal tract using computational modelling and validate its functionality in awake and ambulating swine. Our proof-of-concept device may lead to the development of ingestible piezoelectric devices that might safely sense mechanical variations and harvest mechanical energy inside the gastrointestinal tract for the diagnosis and treatment of motility disorders, as well as for monitoring ingestion in bariatric applications.</p>",,,2018-06-29 21:58:46.618,True,2017-10-10,Flexible piezoelectric devices for gastrointestinal motility sensing,PUBLIC,,True,Conformable Decoders,False
a-protocol-to-characterize-ph-sensing-materials-and-systems,canand,False,"​<p>Although significant progress is being made in identifying pH sensing materials&nbsp;and device configurations, a standard protocol for benchmarking performance&nbsp;of next-generation pH devices is still lacking. In particular, key properties&nbsp;of characterization systems, such as inherent component contributions, time plots for extended-gate field-effect transistor (EGFET) measurements,&nbsp;and the input resistance (Rin), often go unreported in studies of pH sensing&nbsp;systems. These properties strongly influence the characterization system and&nbsp;can lead to mistaken attribution of properties to the device. In this project, a series of essential characterization tests and parameters are reported to&nbsp;evaluate pH systems, such as the zinc oxide (ZnO) EGFET, in a standardized protocol.&nbsp;This EGFET ZnO sensor has a sensitivity of −58.1 mV pH−1, drift range from&nbsp;2.5 to 14.2 μA h−1, and response time of 136 s. By using a ZnO sensing&nbsp;electrode, it is demonstrated that i) intrinsic contributions of reference&nbsp;electrode and commercial transistor (for EGFET) are not negligible; ii) time&nbsp;plots for EGFET configuration and defining a critical point at the onset of drift&nbsp;are essential for accurate sensitivity, response time, and drift reporting; and&nbsp;iii) the results of the pH sensing system are strongly dependent on the input resistance of the used characterization instruments.</p>",,,2019-04-17 18:50:52.506,True,2018-08-23,A Protocol to Characterize pH Sensing Materials and Systems,PUBLIC,,True,Conformable Decoders,False
conformal-piezoelectric-mechanical-energy-harvesters,canand,False,"<p>Nearly all classes of wearable and implantable biomedical devices depend on battery power for continuous operation. However, the life span of batteries is limited, rarely exceeding a few hours for wearables and a few years for implants. Consequently, battery replacements and, often times, surgical procedures are required to change the depleted batteries of implants, exposing people to high risks of surgical complications and/or high financial costs. This project seeks to develop conformal piezoelectric patches integrated to personal garments to extract energy from body movements such as motion of arms, fingers, and legs. The completion of this project could improve quality life for people and potentially provide environmentally friendly power.</p>",,--Choose Location,2019-04-17 18:52:17.221,True,2016-01-01,Conformal Piezoelectric Mechanical Energy Harvesters: Mechanically Invisible Human Dynamos,PUBLIC,,True,Conformable Decoders,False
computational-tinkering,tarmelop,False,"<p>As children tinker with materials in the world, they are constantly putting things together and taking them apart. They are learning through play—trying out new ideas, exploring alternate paths, making adjustments, imagining new possibilities, expressing themselves creatively. In the process, they learn about the creative process and develop as creative thinkers.</p><p>As digital technologies enter the lives of children, there is risk that they will crowd out tinkering, with children spending more time watching screens than tinkering with materials. Yet, in our work, we have seen how digital technologies can also be used to open up new opportunities for tinkering.</p><p>Working in collaboration with the Tinkering Studio at the Exploratorium, Fondazione Reggio Children and the LEGO Foundation, we are developing a new generation of tools, activities, and spaces to support playful investigation and experimentation, integrating digital and physical materials.&nbsp;</p><p>The new activities will enable children to engage in new types of inquiry into light, sound, motion, and storytelling. In the initial set of activities, called ""light play,"" children can program colored lights and moving objects to make dynamic patterns of shadows.<br></p>",,,2018-03-27 10:44:38.668,True,2016-09-01,Computational Tinkering,PUBLIC,,True,Lifelong Kindergarten,False
accessibility-of-the-microgravity-research-ecosystem,cjoseph1,False,"<p>For decades, the International Space Station (ISS) has operated as a bastion of international cooperation and a unique testbed for microgravity research. Beyond enabling insights into human physiology in space, the ISS has served as a microgravity platform for numerous science experiments. In recent years, private industry has also been affiliating with NASA and international partners to offer transportation, logistics management, and payload demands. As the costs of flying projects to the ISS decrease, the barriers limiting non-traditional partners, like emerging space nations and startups, from accessing the ISS as a platform also decrease.&nbsp; </p><p>However, the ISS in its current form cannot be sustained forever. As NASA looks towards commercialization of the low Earth orbit (LEO) space and the development of a cislunar station, concrete plans for shifting the public-private relationship of the ISS are unclear. With the consistent need to continue microgravity research—from governments and private industry—<b>understanding the socio-technical and policy issues that affect the marketplace for future microgravity platforms is essential to maintaining an accessible and sustainable space economy.&nbsp;</b></p><p>How will the US and other governments design public-private partnerships to pursue economic and social goals in the LEO microgravity ecosystem? What governance structures will influence who is eligible to operate platforms for activities including tourism, research, manufacturing, and outreach? How will international collaboration occur in the future LEO microgravity ecosystem?&nbsp;</p><p>This project contributes to progress on these questions by offering technology policy insight using methods from Systems Engineering. Through case study research and numerous expert interviews, this project examines the stakeholders, needs, objectives, system functions and forms for the ISS and microgravity research platforms now and in the future. Particular attention is paid towards explaining the market dynamics that affect the administrative and economic barriers to entry for emerging space nations and non-traditional spaceflight participants.</p>",,,2019-03-13 17:40:41.842,True,2018-04-01,Accessibility of the microgravity research ecosystem,PUBLIC,,True,Space Enabled,False
lead-zirconate-titanate-gastrointestinal-sensor-pzt-gi-s,zijunw,False,"<p>Improvements in ingestible electronics with the capacity to sense physiological and pathophysiological states have transformed the standard of care for patients. Yet, despite advances in device development, significant risks associated with solid, non-flexible gastrointestinal transiting systems remain. Here, we report the design and use of an ingestible, flexible piezoelectric device that senses mechanical deformation within the gastric cavity. We demonstrate the capabilities of the sensor in both in vitro and ex vivo simulated gastric models, quantify its key behaviours in the gastrointestinal tract using computational modelling and validate its functionality in awake and ambulating swine. Our proof-of-concept device may lead to the development of ingestible piezoelectric devices that might safely sense mechanical variations and harvest mechanical energy inside the gastrointestinal tract for the diagnosis and treatment of motility disorders, as well as for monitoring ingestion in bariatric applications.</p>",,,2018-06-29 21:58:46.618,True,2017-10-10,Flexible piezoelectric devices for gastrointestinal motility sensing,PUBLIC,,True,Biomechatronics,False
unspoken-news,eickhoff,False,"<p>How something is presented can be as important as the message itself.</p><p>In the age of political polarization and election meddling, it is of vital importance to understand which factors contribute to the formation of public opinions. Television is one of the main sources of information for a large portion of the general population.</p><p>In order to understand discrepancies in news perception, what they are caused by and which implications they might have on shaping public political debate, we first need to understand how television news are constructed and define presentational aspects of the news. Given that, we can build tools that analyze news consumption by the public.</p><p>We aim to use various artificial intelligence techniques to model the ""subcarriers of information"" present in a TV newscast, to automatically detect and understand visual and auditory cues beyond the spoken word including the layout of the set, the affect of the participants, the nature of the motion, and other cues. Our goal is to develop an algorithmic understanding of journalistic choices in the way news content is presented. We also attempt develop an understanding of higher-level characteristics of television news such as television set atmosphere or political bias. This altogether would enable a broad-range, comprehensive algorithmic analysis of <b>how news presentation is trying to shape the public political debate</b>.</p><p><a href=""https://viral.media.mit.edu/pub/unspoken"">Project updates</a> via PubPub</p>",,,2019-04-18 16:51:12.445,True,2018-11-01,Unspoken News,PUBLIC,,True,Viral Communications,False
youtune,eickhoff,False,"<p>&nbsp;Seemingly an old TV box which turns out to offer an abundance of control over content selection: each aspect of news can be selected for using old-style controls. The main goal is to make people aware of their choices and biases and to make them curious to tune the controls and to watch something different. Hitting the TV resets the controls to random values and lets user discover completely new unexpected content.&nbsp;</p><p>The main underlying assumption behind the idea of YouTune is that while it is very difficult to influence people’s opinions and change their beliefs explicitly without a willingness on their part to change these beliefs, we could attempt to influence people implicitly by providing a tool that, first, would make users aware of the choices they take: to watch what they want or are used to, the user has to explicitly set controls. Unlike traditional broadcast and online media platforms, YouTune does not allow users to select content based on channel or program, so in order to find content that the person is usually interested in, with YouTune they would need to manually select characteristics of the content they want to watch. And second, we believe that YouTune will make people curious to change the control values they are used to and therefore to discover unexpected content that could potentially influence their opinions in an implicit way.&nbsp;</p><p>With YouTune the user can choose between different characteristics and tune into some value of the chosen characteristic to see short news stories. The idea is to make statistics experienceable without showing graphs: a person using YouTune would subconsciously aggregate statistical patterns and understand which tricks and stylistic tools channels tend to use to cover certain topics.</p><p><a href=""https://viral.media.mit.edu/pub/youtune"">Project updates</a> via PubPub</p>",,,2019-04-18 16:51:54.465,True,2019-01-01,YouTune,PUBLIC,,True,Viral Communications,False
youtune-1,eickhoff,False,"<p>A web app that is a television with topical tuning rather than channel tuning. You can select the subject, the slant, and the emotional intensity of the people and the captions. Our question is whether these controls will broaden people's perspective and help create a common framework with which we can discuss our attitudes, sentiments, and positions on public issues. This project is a visualization of the infrastructure of Unspoken News and Superglue. These are an evolving resource for broadcast news understanding.</p>",,,2019-04-18 16:50:41.771,False,2018-10-15,YouTune,LAB-INSIDERS,,True,Viral Communications,False
which-industries-follow-relatedness,mkaltenb,False,"<p>Industries are more likely to enter and less likely to exit regions that are densely populated by related industries. Unfortunately, the measures used to estimate the relatedness of industries often combine information about multiple forms of relatedness. Here, we use data on the entire formal sector economy of a large country to construct five different measures of relatedness and compare their ability to predict diversification events. We interpret differences in the ability of these metrics to predict entry events as evidence of the relative importance of each relatedness channel for specific industries. These findings advance our understanding of the forms of relatedness that are more likely to predict regional diversification events for specific industries.</p>",,,2018-05-03 20:55:25.540,True,2017-02-01,Untangling Relatedness: What forms of relatedness predict diversification?,LAB-INSIDERS,,True,Other,False
personalized-interaction-for-language-learning,hchen25,False,"<p>The process by which children learn native languages is markedly different from the process of learning a second, or non-native, language. Children are typically immersed in their native languages. They receive input from the adults and other children surrounding them, based on immediate need and interaction, during every waking hour. &nbsp;</p><p>Second language learners are exposed to input from the new language in very different ways, most commonly in a classroom setting.&nbsp;The second language learner relies heavily on memory skills with sparse interaction, in contrast to the first language learner that can rely on environmental reinforcement and social interaction to learn words.&nbsp;</p><p>Social robots have the potential to drastically improve on this paradigm, making the second-language learning experience more like the experience of learning a native language by engaging the child in a rich, interactive exposure to the target language, especially aspects not typically covered by traditional technological solutions, such as prosody, fundamental phonetics, common linguistic structures, etc.</p><p>Our project explores how to design child-robot &nbsp;interactions that encourage child-driven language learning, that adapt and personalize each child’s learning experience. We incorporate game design and machine learning into the child-robot interaction design. The child and robot play through a suite of educational games together. Using real-time sensor data and gameplay features, the robot constructs a model of each child's learning and emotional trajectory, then uses these models to inform its own decision making during the game. Thus, the robot's behaviors become personalized to individual children based on their learning style, personality and knowledge/emotional states during gameplay.&nbsp;</p><p></p><p></p><p></p><p></p><p></p><p></p>",,,2018-05-03 20:56:50.720,True,2017-01-01,Personalized  Interaction for Language Learning,PUBLIC,,True,Personal Robots,False
collaborative-robot-storyteller,hchen25,False,"<p>Could a social robot collaboratively exchange stories with children as a peer and help improve their linguistic and storytelling skills? Tega uses machine learning algorithms to learn actions that improve children's storytelling and keep them engaged. &nbsp;We are also interested in how Tega can personalize its interaction with each child over multiple encounters, because every child learns and engages differently.&nbsp;</p><p>In Spring 2017, Tega went to twelve preschool classrooms in the Greater Boston area for&nbsp;three months, pioneering the field of long-term human-robot interaction.&nbsp;Using Q-learning, a policy was trained to tell stories optimized for each child’s engagement and linguistic skill progression. Tega monitored children's affect signals and asked dialogic questions during storytelling to gauge their engagement. Tega also invited children to&nbsp;tell&nbsp;it stories, which Tega used to assess each child's linguistic skill development. Our results show robot's interaction policy indeed personalized to each child. At the end of the sessions, the policy significantly differed from one child to the other. Children who interacted and built relationships with a personalized robot showed higher engagement, learned and retained more vocabularies, and used more complex syntax structure in their speech compared to where they had started.</p>",,--Choose Location,2019-01-22 17:42:46.144,True,2015-09-01,Personalized Robot Storytelling Companion,PUBLIC,,True,Personal Robots,False
skinbot-a-wearable-skin-climbing-robot,sfollmer,False,"<p>We introduce SkinBot: a lightweight robot that moves over the skin's surface with a two-legged suction-based locomotion mechanism and captures a wide range of body parameters with an exchangeable multipurpose sensing module. We believe that robots that live on our skin, such as SkinBot, will enable a more systematic study of the human body and offer great opportunities to advance our knowledge in many areas such as telemedicine, human-computer interfaces, body care, and fashion.</p>",,,2019-04-19 14:35:05.713,True,2016-11-01,"SkinBot: A wearable, skin-climbing robot",PUBLIC,http://www.artemdementyev.com,True,Tangible Media,False
participatory-biotechnology,nbakker,False,"<p>The development of biotechnologies since the era of recombinant DNA in the 1970s has occurred largely via the interaction of academic, industrial, and governmental institutions.&nbsp;Largely absent from this ecosystem are the informed inputs of grassroots communities at any point in the technology developmental cycle.</p><p>The parallel rise of high-throughput, next generation DNA sequencing and advanced DNA synthesis technologies (reading and writing DNA), along with invention of precise genome editing technologies (e.g., CRISPR) has humanity at the brink of a new era, one where living technologies rule. Given the vital importance of living technologies, not only to human health, manufacturing, the economy, and environment, but to our social fabric and culture, we ask:</p><ul><li>How should living technologies be developed?<br></li><li>How can we ensure there is broad, diverse participation in biotechnology?<br></li><li>How can marginalized, under-represented and indigenous communities be agents of change in this era?<br></li><li>What types of institutions and design practices can be employed to ensure just outcomes?<br></li><li>How can humanity work harmoniously, in concert with nature, to co-evolve and flourish?</li></ul><p><br></p><p>We will host a <a href=""https://www.media.mit.edu/events/participatory-design-conference/"">workshop</a>&nbsp;on this topic during the biennial Participatory Design Conference in Genk and Hasselt, Belgium.&nbsp;</p>",,,2018-05-02 14:59:36.627,True,2018-05-27,Participatory Biotechnology,PUBLIC,,True,Other,True
food-server,jrye,False,"<p>The OpenAg™ Food Server is a shipping container-sized, <b>controlled environment agriculture technology</b> that can be built to utilize hydroponic or aeroponic technology. It can serve as both a<b> research platform for simulating precise environments at scale</b> (see <a href=""https://www.media.mit.edu/projects/openag-flavor-ecology/overview/"">Flavor, environment, and the phenome</a>) , and a <b>production unit</b> for any specified crop of interest. It is intended to produce <b>larger quantities of food</b> than a <a href=""https://www.media.mit.edu/projects/personal-food-computer/overview/"">Personal Food Computer</a> and appeals to <b>interdisciplinary researchers</b> as well as&nbsp;<b>small-scale cafeterias, restaurants, and boutique operators.</b></p>",,--Choose Location,2019-03-20 12:41:06.829,True,2015-09-01,Food Server,PUBLIC,https://www.linkedin.com/in/johndelaparra/,True,Open Agriculture,False
sonic-enrichment-at-the-zoo,janetb,False,<p>This project is a collaboration between the MIT Media Lab and the San Diego Zoo to design and build interactive sonic enrichment systems for animals in managed care. Our approach is based on the potential of animal-animal and human-animal relationship as an environmental enrichment for the welfare of zoo-housed animals specifically in terms of animal vocal communication.&nbsp; Enrichment is a way for caregivers to provide animals with the opportunity to express natural behaviors and reduce stereotypic behaviors.&nbsp;</p>,,,2019-04-17 20:03:16.443,True,2018-04-01,Sonic enrichment at the zoo,PUBLIC,,True,Fluid Interfaces,True
translational-acoustic-rf-tarf-communication,tonolini,False,"<p>Did you know that submarines today still cannot wirelessly communicate with airplanes? For decades, communicating between underwater and the air has remained an unsolved problem. Underwater, submarines use acoustic signals (or SONAR) to communicate; in the air, airplanes use radio signals like cellular or WiFi. But neither of these signals can work across both water and air.</p><p>We present TARF (Translational Acoustic-RF&nbsp;communication), the&nbsp;first technology that enables communication between underwater and the air. A TARF transmitter sends standard sound (or SONAR signals).&nbsp; Sound travels as pressure waves; when these waves hit the surface, they cause it to vibrate. To pick up these vibrations, a TARF receiver in the air uses a very sensitive radar. The radar transmits a signal which reflects off the water surface and comes back. As the water surface vibrates, it causes small changes to the received radar signal, enabling a TARF receiver to sense the tiny vibrations caused by the underwater acoustic transmitter.</p><p>The video below explains how TARF works and some of its applications.</p>",,,2019-04-17 18:09:35.809,True,2018-08-21,Wireless communication from underwater to the air,PUBLIC,,True,Signal Kinetics,False
translational-acoustic-rf-tarf-communication,junsuj,False,"<p>Did you know that submarines today still cannot wirelessly communicate with airplanes? For decades, communicating between underwater and the air has remained an unsolved problem. Underwater, submarines use acoustic signals (or SONAR) to communicate; in the air, airplanes use radio signals like cellular or WiFi. But neither of these signals can work across both water and air.</p><p>We present TARF (Translational Acoustic-RF&nbsp;communication), the&nbsp;first technology that enables communication between underwater and the air. A TARF transmitter sends standard sound (or SONAR signals).&nbsp; Sound travels as pressure waves; when these waves hit the surface, they cause it to vibrate. To pick up these vibrations, a TARF receiver in the air uses a very sensitive radar. The radar transmits a signal which reflects off the water surface and comes back. As the water surface vibrates, it causes small changes to the received radar signal, enabling a TARF receiver to sense the tiny vibrations caused by the underwater acoustic transmitter.</p><p>The video below explains how TARF works and some of its applications.</p>",,,2019-04-17 18:09:35.809,True,2018-08-21,Wireless communication from underwater to the air,PUBLIC,,True,Responsive Environments,False
rampage,chialynn,False,"<p>Beard dreamcatcher knausgaard godard, four loko wayfarers cloud bread. Pour-over echo park vinyl franzen listicle tumeric chicharrones. Heirloom microdosing intelligentsia deep v chartreuse, kinfolk photo booth. Everyday carry gentrify DIY, freegan snackwave taxidermy listicle trust fund brooklyn direct trade.</p><p>Mumblecore sriracha humblebrag +1, brooklyn pabst kickstarter. Quinoa la croix edison bulb chia, lyft selvage farm-to-table biodiesel shaman 3 wolf moon narwhal woke sriracha. Meh kinfolk mustache vaporware kogi air plant hexagon. Tbh bicycle rights vaporware deep v humblebrag..</p>",,,2018-05-22 18:59:03.439,True,2018-05-01,RAMPage,LAB,http://joi.ito.com/,True,Other,False
social-capital-accounting,nishikat,False,"<p>To better understand and improve the quality of our lives, there has been a need for measuring non-economic capital such as social capital and natural capital in addition to economic capital. Quantifying non-economic capital, however, is not easy and has not been widespread. In this project, we propose a system where individuals can start measuring their social capital, turning them into a real-world asset that enables the improvement their economic wellbeing, while preserving individual privacy and security.</p>",,,2018-11-15 19:26:04.996,True,2018-06-01,Social Capital Accounting,PUBLIC,,True,Human Dynamics,True
agonist-antagonist-myoneural-interface-ami,mcarty,False,"<h2><b>Humans can accurately sense the position, speed, and torque of their limbs, even with their eyes shut. This sense, known as proprioception, allows humans to precisely control their body movements. </b></h2><p>Today’s conventional prosthetic limbs do not provide feedback to the nervous system. Because of this, people with amputated limbs cannot feel the position, speed, and torque of their prosthetic joints without looking at them, making it difficult to control their movement. In order to create a more complete prosthetic control experience, researchers at the Center for Extreme Bionics at the MIT Media Lab invented the&nbsp;<b>agonist-antagonist myoneural interface (AMI)</b>. The AMI is a method to restore proprioception to persons with amputation.</p>",,,2018-08-17 16:20:19.891,True,2014-06-01,Agonist-antagonist Myoneural Interface (AMI),PUBLIC,,True,Biomechatronics,True
learning-food-quality-and-safety-using-wireless-stickers,unsoo,False,"​<p>We have developed a wireless system that leverages the inexpensive RFID tags already on hundreds of billions of products to sense potential food contamination. Our system, called RFIQ (Radio Frequency IQ), aims at democratizing food quality and safety, bringing it to the hands of consumers.&nbsp;</p>",,,2019-02-13 16:42:33.755,True,2018-09-01,RFIQ: Food quality and safety detection using wireless stickers,PUBLIC,,True,Signal Kinetics,False
learning-food-quality-and-safety-using-wireless-stickers,jleng,False,"​<p>We have developed a wireless system that leverages the inexpensive RFID tags already on hundreds of billions of products to sense potential food contamination. Our system, called RFIQ (Radio Frequency IQ), aims at democratizing food quality and safety, bringing it to the hands of consumers.&nbsp;</p>",,,2019-02-13 16:42:33.755,True,2018-09-01,RFIQ: Food quality and safety detection using wireless stickers,PUBLIC,,True,Signal Kinetics,False
candlewax-rockets-a-green-propellant-alternative,stober,False,"<p>Paraffin wax (common candlewax) shows promise as a high-performing hybrid rocket propellant for chemical propulsion systems.&nbsp; Its inherent safety and simplicity advantages and low cost (less than $4/kg) make it well-suited for widespread adoption for launch and in-space applications.&nbsp; Its benign nature compared to the toxicity and carcinogenicity which characterize currently-used propellants, such as hydrazine and nitrogen tetroxide, make paraffin an especially strong candidate for new entrants to the propulsion community.</p><p>The Space Enabled Research Group is focused on the use of paraffin wax for small satellite missions.&nbsp; Specifically, we are investigating the centrifugal casting of paraffin into annular geometries on Earth as well as in microgravity.&nbsp; The research group envisions the repurposing of paraffin thermal insulation at end of life for deorbit maneuvers.&nbsp; However, such a mission would require centrifugal casting of paraffin in orbit—a task which has never been done before.&nbsp; The microgravity environment is expected to reduce rotation rate demands for uniform casting, and the overall experimental investigation aims to quantify the differences between 1-g and microgravity centrifugal casting.</p>",,,2019-04-19 20:14:35.126,True,2018-04-01,Candlewax Rockets: A green alternative for in-space propulsion,PUBLIC,,True,Space Enabled,False
myths-of-the-cosmos-indigenous-cosmologies,andreuhl,False,,,,2019-05-30 15:05:50.758,True,2018-01-01,Myths of the Cosmos: Indigenous Cosmologies,LAB,,True,Other,False
myths-of-the-cosmos-indigenous-cosmologies,prathima,False,,,,2019-05-30 15:05:50.758,True,2018-01-01,Myths of the Cosmos: Indigenous Cosmologies,LAB,,True,Space Enabled,False
opal-health,shada,False,<h1><b>Open Algorithms (OPAL)</b></h1>,,,2018-10-19 21:07:55.443,True,2017-11-01,OPAL 4 Health,PUBLIC,https://www.shadaalsalamah.com/,True,Human Dynamics,False
open-trialchain,shada,False,"<p>In this project, we are motivated to address clinical trials issues using Blockchain technology. We propose, OPEN TrialChain, a privacy-preserving Blockchain-based data sharing infrastructure that uses open algorithms between stakeholder of the federation in the clinical trials ecosystem. OPEN TrialChain balances between the sharing of clinical data and the need for subject’s privacy protection by allowing queries on decentralized raw datasets from which it returns aggregated safe answers that are blinded (i.e. anonymized). Using Type II Diabetes as a case study, we study the adoption of OPEN TrialChain and how it can address clinical trials issues. Results show that OPEN TrialChain, first, encourages pharmaceutical companies to report their trial results with higher fidelity, as well as, federated ones to provide more detailed results in return for their peers’ detailed results. Furthermore, it allows for multiple studies to be queried for results from underrepresented demographics, producing greater insight from previously ignored minorities. Finally, analysis done using OPEN TrialChain should illustrate meaningful results without violating the privacy of individuals. Eventually, OPEN TrialChain is expected to optimize the clinical trial ecosystem by improving patient safety, saving lives, cutting drug development costs, encouraging transparent results, preserving patients privacy, and maintaining pharmaceuticals integrity.</p>",,,2019-04-01 17:38:57.336,False,2018-01-01,OPEN TrialChain,PUBLIC,,True,Human Dynamics,False
smart-2-opal,shada,False,"<p>Privacy-preserving mHealth application using Open Algorithm (OPAL) architecture to address urgent care challenges in Riyadh, Saudi Arabia.</p>",,,2019-04-01 17:39:27.492,False,2017-11-01,SMART^2 OPAL,PUBLIC,,True,Human Dynamics,False
healthy-blockchain,shada,False,<p>Achieving a safe privacy-preserving information sharing environment for individualized care using blockchain-based technology in multiple use cases in the healthcare space.</p>,,,2019-04-01 17:40:08.489,False,2017-09-01,Healthy Blockchain,PUBLIC,,True,Human Dynamics,False
at-home-sleep-apnea-screening,rohan,False,"<p>A large proportion of the American population currently suffers from sleep disorders. Among them are patients with obstructive sleep apnea (OSA), who repeatedly stop breathing while asleep. Current screening methods and devices are impractical for widespread screening. We introduce a new model for low-cost OSA screening consisting of an at-home, wearable sleep mask that can easily track the wearer's sleep patterns. The data collected overnight by this sensory mask provides a determination of a patient's OSA risk.<br></p><p><strong>Why is this work important?</strong></p><p>There are 7-18 million Americans suffering from sleep disorders. Among them are patients with OSA, who stop breathing either completely or partially while asleep. This is a serious condition with few reliable low-cost devices available for primary diagnosis without expert supervision.</p><p><strong>What has been done before?</strong></p><p>The gold standard for OSA diagnosis is overnight polysomnography (PSG). Apart from that there are many home diagnostics devices available. However, many at-home devices offer poor diagnostic quality and some of them also require expert intervention, from installation of the device to analysis of the data.</p><p><strong>What are our contributions?</strong><br></p><p>We report the construction and validation of a design for low-cost OSA screening built around a simplified screening device embedded in an at-home wearable sleep mask. This simplified screening system allows for OSA diagnosis without imposing the costs or time commitment of a full PSG.</p><p><strong>What are the next steps?</strong></p><p>In the next iterations of the device, we aim to improve the mechanical design and ease of use, as well as automate data analysis and screening so that the device can be evaluated in larger studies.</p>",,,2018-05-06 23:20:12.777,True,2016-05-09,At-Home Sleep Apnea Screening,PUBLIC,,True,Camera Culture,False
alterego,arnavk,False,"<p>AlterEgo&nbsp;is a&nbsp; non-invasive, wearable, peripheral neural interface that allows humans to converse in natural language with machines, artificial intelligence assistants, services, and other people without any voice—without opening their mouth, and without externally observable movements—simply by articulating words internally.&nbsp; The feedback to the user is given through audio, via bone conduction,&nbsp; without disrupting the user's usual auditory perception, and making the interface closed-loop. This enables an human-computer interaction that is subjectively experienced as completely internal to the human user—like speaking to one's self.&nbsp;&nbsp;AlterEgo seeks to combine humans and computers—such that computing, the Internet, and AI would weave into human personality as an internal “second self” and augment human cognition and abilities.</p><p>The wearable system captures peripheral neural signals when internal speech articulators are volitionally and neurologically activated, during a user's internal articulation of words. This enables a user to transmit and receive streams of information to and from a computing device or any other person without any observable action, in discretion, without unplugging the user from her environment, without invading the user's&nbsp; privacy.&nbsp;</p>",,,2019-05-17 00:56:53.288,True,2017-09-01,AlterEgo,PUBLIC,,True,Fluid Interfaces,False
alterego,ewadkins,False,"<p>AlterEgo&nbsp;is a&nbsp; non-invasive, wearable, peripheral neural interface that allows humans to converse in natural language with machines, artificial intelligence assistants, services, and other people without any voice—without opening their mouth, and without externally observable movements—simply by articulating words internally.&nbsp; The feedback to the user is given through audio, via bone conduction,&nbsp; without disrupting the user's usual auditory perception, and making the interface closed-loop. This enables an human-computer interaction that is subjectively experienced as completely internal to the human user—like speaking to one's self.&nbsp;&nbsp;AlterEgo seeks to combine humans and computers—such that computing, the Internet, and AI would weave into human personality as an internal “second self” and augment human cognition and abilities.</p><p>The wearable system captures peripheral neural signals when internal speech articulators are volitionally and neurologically activated, during a user's internal articulation of words. This enables a user to transmit and receive streams of information to and from a computing device or any other person without any observable action, in discretion, without unplugging the user from her environment, without invading the user's&nbsp; privacy.&nbsp;</p>",,,2019-05-17 00:56:53.288,True,2017-09-01,AlterEgo,PUBLIC,,True,Fluid Interfaces,False
brainstorm-anima-mundi,lunshof,False,"<p>We'd like to introduce you to a very special neuroscience project that we are currently conducting in the setting of a traditional fine arts museum.</p><h1><b>Join the conversation on the&nbsp;<a href=""https://www.responsivescience.org/brainstorm"">Responsive Science Brainstorm project site</a>. </b></h1><p>Responsive Science uses the&nbsp;<a href=""https://www.pubpub.org/"">PubPub platform</a>, which allows for direct interaction. PubPub was developed at MIT Media Lab.</p>",,,2018-10-23 15:25:53.714,True,2018-06-09,Brainstorm: Anima Mundi,PUBLIC,,True,Sculpting Evolution,False
central-banks-and-digital-currency,robleh,False,<p>Money is at the heart of the financial system—its most basic element. Fundamental reform of the system starts with addressing how money works today and how it could work in the future. The emergence of digital currency has led several central banks to consider how this new technology affects their ability to discharge their mandates. One of the most significant questions is whether digital versions of fiat currencies can be issued and what the role of the central bank should be in a financial system being changed by new technology.</p><p>We are working to address some of the fundamental questions which need to be addressed to bring about this reformation of the financial system.</p>,,,2019-04-18 19:24:02.529,True,2017-03-28,Digital Fiat Currency,PUBLIC,,True,Initiatives,False
totems,josephk,False,"<p>Biodiversity on planet Earth is under momentous threat, with extinction rates estimated between 100 and 1,000 times their pre-human level. The Mediated Matter group has been in search of materials and chemical substances that can sustain and enhance biodiversity across living systems, and that have so far endured the perils of climate change. Melanin is one such substance illustrating biodiversity at the genetic, species, and ecosystem levels.</p>",,,2019-05-07 13:44:31.523,True,2019-02-27,Totems,PUBLIC,,True,Mediated Matter,False
totems,fkraemer,False,"<p>Biodiversity on planet Earth is under momentous threat, with extinction rates estimated between 100 and 1,000 times their pre-human level. The Mediated Matter group has been in search of materials and chemical substances that can sustain and enhance biodiversity across living systems, and that have so far endured the perils of climate change. Melanin is one such substance illustrating biodiversity at the genetic, species, and ecosystem levels.</p>",,,2019-05-07 13:44:31.523,True,2019-02-27,Totems,PUBLIC,,True,Mediated Matter,False
maiden-flight,fkraemer,False,"<p>Maiden Flight is an autonomous biological laboratory environment designed for studying the impact of space flight on the sole reproductive node of a bee colony: <b>the queen bee and her retinue.&nbsp;&nbsp;</b></p><p>It represents the first space module of its kind built specifically to cater to queen bees. The hybrid-ecology of the capsule was created to take into account the distributed and uniquely non-human nature of bee biology, in order to consider how to extend the bee reproductive system for environmental extremes. This aim is reflected in the structure of the capsule interior, which was assembled by humans and augmented by the bees’ natural fabrication.&nbsp;</p><p>In May 2019, the Mediated Matter group traveled to Texas to launch two laboratory capsules on Blue Origin’s sub-orbital rocket system, New Shepard. Each custom-designed&nbsp;<b>metabolic support capsule&nbsp;</b>comprised an experimental environment for one queen bee and an attending retinue of 10-20 nurse bees for a parabolic flight to a 100-kilometer micro-gravitational space apogee, and back.</p>",,,2019-05-17 13:11:46.844,True,2019-04-19,Maiden Flight,PUBLIC,,True,Mediated Matter,False
ivn-in-vivo-networking,zhluo,False,"<p>In-Vivo Networking (IVN)&nbsp;is the new technology that can wirelessly power and communicate with tiny devices implanted deep within the human body. Such devices could be used to deliver drugs, monitor conditions inside the body, or treat disease by stimulating the brain with electricity or light.&nbsp;&nbsp;</p><p>The implants are powered by radio frequency waves, which are safe for humans. In tests in animals, we showed that the waves can power devices located 10 centimeters deep in tissue, from a distance of one meter.</p>",,,2019-02-14 19:47:21.377,True,2017-09-01,In-Vivo Networking: Powering and communicating with tiny battery-free devices inside the body,PUBLIC,,True,Signal Kinetics,False
turbotrack1,zhluo,False,,2021-07-31,,2019-02-19 17:21:52.504,False,2019-01-01,TurboTrack,PUBLIC,,True,Signal Kinetics,False
turbotrack-3d-backscatter-localization-for-fine-grained,zhluo,False,"<p>TurboTrack is a 3D localization system for fine-grained robotic tasks, with unique capability to localize backscatter nodes with sub-centimeter accuracy without any constraints on their locations or mobility. We showed that TurboTrack can work in multiple collaborative applications with robotic arms and nanodrones including indoor tracking, packaging, assembly, and handover.</p><p>This research is partially funded by the National Science Foundation (NSF) and a Google Faculty Research Award.</p>",,,2019-04-17 18:08:41.636,True,2017-06-01,TurboTrack: 3D backscatter localization for fine-grained robotics,PUBLIC,,True,Signal Kinetics,False
distributed-learning-and-collaborative-learning-1,vepakom,False,"<h1>Split Learning: Distributed deep learning without sharing raw data</h1><p><strong>Abstract:</strong>&nbsp;Can a server utilize deep learning models for training or inference without accessing raw data from clients?&nbsp;Split learning naturally allows for various configurations of cooperating entities to train (and infer from) machine learning models without sharing any raw data or detailed information about the model.&nbsp;</p><p><strong>Key idea:</strong>&nbsp;In the simplest of configurations of split learning, each client (for example, radiology center) trains a partial deep network up to a specific layer known as the cut layer. The outputs at the cut layer are sent to another entity (server/another client) which completes the rest of the training without looking at raw data from any client that holds the raw data. This completes a round of forward propagation without sharing raw data. The gradients are now back propagated again from its last layer until the cut layer in a similar fashion. The gradients at the cut layer (and only these gradients) are sent back to radiology client centers. The rest of back propagation is now completed at the radiology client centers. This process is continued until the distributed split learning network is trained without looking at each others raw data.</p><p><b>Split Learning Papers:</b></p><p><b>1.) Split learning for health: Distributed deep learning without sharing raw patient data, Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, Ramesh Raskar,&nbsp;<a href=""https://arxiv.org/pdf/1812.00564.pdf"">(PDF)</a>&nbsp;(2018)</b></p><p><b>2.) “Distributed learning of deep neural network over multiple agents”, Otkrist Gupta and Ramesh Raskar, In: Journal of Network and Computer Applications 116,&nbsp;<a href=""https://www.sciencedirect.com/science/article/pii/S1084804518301590"">(PDF)</a>&nbsp;(2018)</b></p><p><b>3.) Survey paper: No Peek: A Survey of private distributed deep learning, Praneeth Vepakomma, Tristan Swedish, Ramesh Raskar, Otkrist Gupta, Abhimanyu Dubey,&nbsp;<a href=""https://arxiv.org/pdf/1812.03288.pdf"">(PDF)</a>&nbsp;(2018)</b></p><h2>Split learning’s computational and communication efficiency on clients:</h2><p>Client-side communication costs are significantly reduced as the data to be transmitted is restricted to initial layers of the split learning network (splitNN) prior to the split. The client-side computation costs of learning the weights of the network are also significantly reduced for the same reason. In terms of model performance, the accuracies of Split NN remained competitive to other distributed deep learning methods like federated learning and large batch synchronous SGD with a drastically smaller client side computational burden when training on a larger number of clients as shown below in terms of teraflops of computation and gigabytes of communication when split learning is used to train Resnet and VGG architectures over 100 and 500 clients with CIFAR 10 and CIFAR 100 datasets.</p><p><b>Versatile plug-and-play configurations of split learning</b></p><p>Versatile configurations of split learning configurations cater to various practical settings of&nbsp;i) multiple entities holding different modalities of patient data, ii) centralized and local health entities collaborating on multiple tasks, iii) learning without sharing labels, iv) multi-task split learning, v) multi-hop split learning&nbsp;and other hybrid possibilities to name a few as shown below and further detailed in our paper here&nbsp;<a href=""https://arxiv.org/pdf/1812.00564.pdf"">(PDF)</a>&nbsp;</p><h2>News stories about this work</h2><p><strong><a href=""https://www.technologyreview.com/the-download/612567/a-new-ai-method-can-train-on-medical-records-without-revealing-patient-data/"">MIT Technology Review</a></strong></p>",,,2018-12-12 20:53:48.275,True,2018-08-27,Distributed and collaborative learning,PUBLIC,,True,Camera Culture,False
physiohmd,yangtao,False,"<p>Virtual and augmented reality headsets are unique as they have access to our facial area, an area that presents an excellent opportunity for always-available input and insight into the user's state. Their position on the face makes it possible to capture bio-signals as well as facial expressions.&nbsp; The&nbsp; PhysioHMD platform&nbsp;introduces a software and hardware modular interface built for collecting affect and physiological data from users wearing a head-mounted display. The platform enables researchers and developers to aggregate and interpret signals in real-time and use them to develop novel, personalized interactions, as well as evaluate virtual experiences. Our design offers seamless integration with standard HMDs, requiring minimal setup effort for developers and those with less experience using game engines. The PhysioHMD platform is a flexible architecture that offers an interface that is not only easy to extend but also complemented by a suite of tools for testing and analysis. We hope that PhysioHMD can become a universal, publicly available testbed for VR and AR researchers.</p><p>To create a seamless experience, we have integrated several bio-signal sensors into the faceplate of an HTC VIVE VR headset and utilized the Shimmer3 sensor for emotion-sensing. For the collection of Galvanic Skin Response, dry electrodes were positioned on the forehead area due to the fact that it is one of the areas most dense with sweat glands. GSR data reflects emotional arousal, but in order to identify how arousal, valence, motivation, and cognition interact in response to physical or psychological stimuli, it becomes necessary to complement GSR with other biosensors. For the heart rate, a PPG (photoplethysmogram) sensor, which senses the rate of blood flow by utilizing light to monitor the heart’s pumping action, was placed in the temple region of the user. This is done to get insights into the respondent's physical state, anxiety and stress levels (arousal), and to determine how changes in their physiological state relate to their actions and decisions.<br></p>",,,2018-11-14 19:05:00.306,True,2017-02-01,PhysioHMD,PUBLIC,,True,Fluid Interfaces,False
children-s-book-ch,ufuoma,False,"<p>OpenIdeo put out a challenge to create a book targeting children ages 0-3 that would inspire children and their caregivers to read together. The challenge encouraged writers to create stories that center underrepresented identities and reflect the lives of urban communities like Philadelphia.</p><p>We are creating a story that draws parallels between the life of a little Black girl and the life of an astronaut by juxtaposing typical activities like getting ready in the morning, taking the bus or having family gatherings with the activities that an astronaut is expected to conduct for their job requirements. The message of the story is that astronauts, while few in number, are still just normal human beings with a career that children of any identity should be able to see themselves in.</p>",,,2019-04-22 18:25:19.612,True,2019-04-12,Early Childhood Book Challenge,LAB-INSIDERS,,True,Space Enabled,False
technology-design-and-assessment-for-coastal-water-ecosystem-management-a-case-study-of-benin,ufuoma,False,"<p>All over Africa, experts use satellite Earth Observation (EO) data for applications such as monitoring crop health or assessing the risk of disease vectors. These applications are often done at a national scale meaning there is a challenge to ensure that end users such as small companies, rural communities or otherwise marginalized groups benefit from EO systems. This project explores an EO application with the enterprise Green Keeper Africa (GKA) based in Cotonou, Benin, that addresses the management of an invasive plant species that is threatening local economic activities such as fishing. GKA helps control the infestation of the water hyacinth on Lake Nokoue by repurposing the plant into a product that absorbs oil-based waste. The EO application is an online Environmental Observatory that utilizes satellite, aerial and ground data to map the location of the water hyacinth over time, providing valuable information for government, private and public users. The research outcomes presented in this project address processes that (i) outline the steps for a small company in Benin to setup and operate a new EO technological capability, and (ii) enable low cost data collection of parameters describing the coastal water ecosystem.&nbsp;</p><p>In the observatory, the technique Normalized Difference Vegetation Index (NDVI) is applied to free satellite data to identify likely locations of the hyacinth in the target region of Lake Nokoue.&nbsp;</p>",,,2019-04-22 18:27:41.358,True,2018-08-01,Low-cost invasive species management in coastal ecosystems: A case study in Benin,LAB-INSIDERS,,True,Space Enabled,False
public-library-innovation-exchange,hbailey,False,"<p>Public libraries are one of most trusted public institutions in the US and increasingly provide a broad range of education services, ranging from early learning programs, to maker spaces, to adult training. Libraries are not storage places for books, but communities for social change and innovation. The Public Library Innovation Exchange connects Media Lab researchers with librarians to develop new creative learning programs together. We will host how-to materials that help public libraries deploy projects developed at the Media Lab, such as &nbsp;<a href=""http://www.chicagotribune.com/bluesky/series/gadgets/ct-innovative-gifts-bsi-photos-20151210-001-photo.html"">Circuit Stickers</a> or <a href=""https://www.youtube.com/watch?v=rfQqh7iCcOU"">Makey Makey</a>, and we will offer scholarships to support exchanges and residencies to foster collaborative research going forward. The project is funded by Knight Foundation.&nbsp;</p><p>Visit our project website to learn more: <a href=""http://plix.media.mit.edu"">plix.media.mit.edu</a></p>",,,2019-05-30 18:56:51.688,True,2017-03-15,Public Library Innovation Exchange,PUBLIC,https://plix.media.mit.edu,True,Initiatives,False
public-library-innovation-exchange,lguterma,False,"<p>Public libraries are one of most trusted public institutions in the US and increasingly provide a broad range of education services, ranging from early learning programs, to maker spaces, to adult training. Libraries are not storage places for books, but communities for social change and innovation. The Public Library Innovation Exchange connects Media Lab researchers with librarians to develop new creative learning programs together. We will host how-to materials that help public libraries deploy projects developed at the Media Lab, such as &nbsp;<a href=""http://www.chicagotribune.com/bluesky/series/gadgets/ct-innovative-gifts-bsi-photos-20151210-001-photo.html"">Circuit Stickers</a> or <a href=""https://www.youtube.com/watch?v=rfQqh7iCcOU"">Makey Makey</a>, and we will offer scholarships to support exchanges and residencies to foster collaborative research going forward. The project is funded by Knight Foundation.&nbsp;</p><p>Visit our project website to learn more: <a href=""http://plix.media.mit.edu"">plix.media.mit.edu</a></p>",,,2019-05-30 18:56:51.688,True,2017-03-15,Public Library Innovation Exchange,PUBLIC,https://plix.media.mit.edu,True,Initiatives,False
public-library-innovation-exchange,cohenm,False,"<p>Public libraries are one of most trusted public institutions in the US and increasingly provide a broad range of education services, ranging from early learning programs, to maker spaces, to adult training. Libraries are not storage places for books, but communities for social change and innovation. The Public Library Innovation Exchange connects Media Lab researchers with librarians to develop new creative learning programs together. We will host how-to materials that help public libraries deploy projects developed at the Media Lab, such as &nbsp;<a href=""http://www.chicagotribune.com/bluesky/series/gadgets/ct-innovative-gifts-bsi-photos-20151210-001-photo.html"">Circuit Stickers</a> or <a href=""https://www.youtube.com/watch?v=rfQqh7iCcOU"">Makey Makey</a>, and we will offer scholarships to support exchanges and residencies to foster collaborative research going forward. The project is funded by Knight Foundation.&nbsp;</p><p>Visit our project website to learn more: <a href=""http://plix.media.mit.edu"">plix.media.mit.edu</a></p>",,,2019-05-30 18:56:51.688,True,2017-03-15,Public Library Innovation Exchange,PUBLIC,https://plix.media.mit.edu,True,Camera Culture,False
functionally-graded-filament-wound-carbon-fiber-prosthetic-sockets,cdgu,False,"<p>Prosthetic sockets belong to a family of orthoic devices designed for amputee rehabilitation and performance augmentation. Although such products are fabricated out of lightweight composite materials and designed for optimal shape and size, they are limited in their capacity to offer local control of material properties for optimizing load distribution and ergonomic fit over surface and volume areas. Our research offers a novel workflow to enable the digital design and fabrication of customized prosthetic sockets with variable impedance informed by MRI data. We implement parametric environments to enable the controlled distribution of functional gradients of a filament-wound carbon fiber socket.</p>",,--Choose Location,2019-04-18 19:50:23.822,True,2014-01-01,Functionally graded filament-wound carbon-fiber prosthetic sockets,PUBLIC,,True,Mediated Matter,False
openag-education,pcerqu,False,"<p>Today's students have grown up in a world of rapidly evolving technology, and they are natural-born experimenters, programmers, and tinkerers. By introducing OpenAg™&nbsp; Food Computers into the education context, including in schools and museums, we hope to inspire and empower the high-tech farmers of the future.&nbsp;</p><p>Our latest testing in schools and libraries shows&nbsp;<b style=""font-size: 18px;"">more than half of students </b><span style=""font-size: 18px;"">who tested a PFC_EDU&nbsp;<b>think </b></span><b><span style=""font-size: 18px;""><i>more</i></span><span style=""font-size: 18px;""> about food production, hunger, and where their food comes from</span></b><span style=""font-size: 18px;""><b>.</b>&nbsp;</span></p><h2><b>Build a Food Computer</b></h2><ul><li>Visit the<a href=""https://wiki.openag.media.mit.edu/pfc_edu_3.0"">&nbsp;<b>OpenAg Wiki page</b></a>&nbsp;for the latest version of <a href=""https://www.media.mit.edu/projects/personal-food-computer/overview/""><b>Personal Food Computer </b>-<b>&nbsp;the ""PFC_EDU""</b></a>&nbsp;including activity guides, materials lists, and helpful tips on how to get parts manufactured.</li></ul><ul><li>Join&nbsp;<a href=""https://forum.openag.media.mit.edu/c/education"">our forum and&nbsp;<b>meet other educators</b></a>, or follow along with <a style=""font-size: 18px;"" href=""https://forum.openag.media.mit.edu/t/libraries-food-computers-plix-build-public-library-innovation-exchange-at-mit-media-lab/2895/67""><b>public libraries</b> who are integrating Food Computers into their activities</a><span style=""font-size: 18px;""> for students of all ages.</span></li><li>Download or contribute activities, materials, lesson plans, curricula, and other education resources on our <a href=""https://wiki.openag.media.mit.edu/education""><b>OpenAg EDU Wiki Page</b></a>.</li></ul>",,--Choose Location,2019-03-25 17:22:50.561,True,2015-01-01,OpenAg EDU,PUBLIC,,True,Open Agriculture,False
openag-education,davidzac,False,"<p>Today's students have grown up in a world of rapidly evolving technology, and they are natural-born experimenters, programmers, and tinkerers. By introducing OpenAg™&nbsp; Food Computers into the education context, including in schools and museums, we hope to inspire and empower the high-tech farmers of the future.&nbsp;</p><p>Our latest testing in schools and libraries shows&nbsp;<b style=""font-size: 18px;"">more than half of students </b><span style=""font-size: 18px;"">who tested a PFC_EDU&nbsp;<b>think </b></span><b><span style=""font-size: 18px;""><i>more</i></span><span style=""font-size: 18px;""> about food production, hunger, and where their food comes from</span></b><span style=""font-size: 18px;""><b>.</b>&nbsp;</span></p><h2><b>Build a Food Computer</b></h2><ul><li>Visit the<a href=""https://wiki.openag.media.mit.edu/pfc_edu_3.0"">&nbsp;<b>OpenAg Wiki page</b></a>&nbsp;for the latest version of <a href=""https://www.media.mit.edu/projects/personal-food-computer/overview/""><b>Personal Food Computer </b>-<b>&nbsp;the ""PFC_EDU""</b></a>&nbsp;including activity guides, materials lists, and helpful tips on how to get parts manufactured.</li></ul><ul><li>Join&nbsp;<a href=""https://forum.openag.media.mit.edu/c/education"">our forum and&nbsp;<b>meet other educators</b></a>, or follow along with <a style=""font-size: 18px;"" href=""https://forum.openag.media.mit.edu/t/libraries-food-computers-plix-build-public-library-innovation-exchange-at-mit-media-lab/2895/67""><b>public libraries</b> who are integrating Food Computers into their activities</a><span style=""font-size: 18px;""> for students of all ages.</span></li><li>Download or contribute activities, materials, lesson plans, curricula, and other education resources on our <a href=""https://wiki.openag.media.mit.edu/education""><b>OpenAg EDU Wiki Page</b></a>.</li></ul>",,--Choose Location,2019-03-25 17:22:50.561,True,2015-01-01,OpenAg EDU,PUBLIC,,True,Open Agriculture,False
scratch-project-recommendation,labdalla,False,"<p>Scratch is built on the principle of “wide walls,” encouraging a wide array of diverse projects, reflecting the diverse interests of Scratchers. In order for Scratch to fulfill this promise, we&nbsp;propose a personalized Scratch project recommendation algorithm that will recommend projects for users to explore, while also taking into account their interests, preferences, and learning pathway. The objective is for such an algorithm to increase the distribution of project attention across the platform as well as broaden the experience of Scratchers. To that end, this project involves using machine learning to classify Scratch projects by type and complexity, making it easier for the project recommendation algorithm to pull specific types of projects to suggest to the user.</p>",,,2019-04-30 15:14:49.042,True,2018-09-05,Scratch Project Recommendation,LAB-INSIDERS,,True,Lifelong Kindergarten,False
civic-entertainment,anushkas,False,"<p>Civic Entertainment is a project based at the Center for Civic Media that explores the intersection of civic engagement with film, television, radio, theatre, literature, and digital entertainment. The project aims to study the modes in which entertainment can create greater knowledge of public institutions, motivate citizens towards democratic duties, and present effective strategies of social and political change.</p><p>The research focuses on studying the ways in which fiction media can affect change in thought and behavior, develops case studies of past and existing films and television shows that reflect or carry elements of civic engagement, explores the representation of protest and activism in popular culture, and experiments with techniques to balance civic education with humor or drama within entertainment.</p><p>The project has a key focus on Indian entertainment and works with Civic Studios (www.civicstudios.com), a Mumbai-based production firm, on creating civic entertainment content for young people across India.</p>",,,2019-03-15 19:38:40.415,True,2018-03-20,Civic Entertainment,PUBLIC,,True,Civic Media,False
cityscope-livingline-shanghai,liuymit,False,"<p>College of Design and Innovation of Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform to support the urban decision making that promotes urban vibrancy and innovation potential. </p><p>The “NICE2035 LivingLine” project in Shanghai, China, is a design-driven, community-based urban innovation initiated by Professor Yongqi Lou, Dean of College of Design of Innovation. LivingLine is a crowdsourcing and co-creation project aiming at building an ecosystem of innovation and entrepreneurship on the internal street of a typical gated residential neighborhood. By introducing radical programs such as living labs, co-working space, and startup-incubators into underutilized storefront space, LivingLine’s goal is to revitalize the urban space and to prototype diverse future lifestyles.<br></p>",,,2019-06-06 16:01:44.891,True,2018-03-01,CityScope LivingLine Shanghai,PUBLIC,,True,City Science,False
implosion-fabrication-1,tillberg,False,"<h2>Shrinking problems in 3D printing</h2><p>Although a range of materials can now be fabricated using additive manufacturing techniques, these usually involve assembly of a series of stacked layers, which restricts three-dimensional (3D) geometry. Oran&nbsp;et al.&nbsp;developed a method to print a range of materials, including metals and semiconductors, inside a gel scaffold (see the Perspective by Long and Williams). When the hydrogels were dehydrated, they shrunk 10-fold, which pushed the feature sizes down to the nanoscale.</p><p>Lithographic nanofabrication is often limited to successive fabrication of two-dimensional (2D) layers. We present a strategy for the direct assembly of 3D nanomaterials consisting of metals, semiconductors, and biomolecules arranged in virtually any 3D geometry. We used hydrogels as scaffolds for volumetric deposition of materials at defined points in space. We then optically patterned these scaffolds in three dimensions, attached one or more functional materials, and then shrank and dehydrated them in a controlled way to achieve nanoscale feature sizes in a solid substrate. We demonstrate that our process, Implosion Fabrication (ImpFab), can directly write highly conductive, 3D silver nanostructures within an acrylic scaffold via volumetric silver deposition. Using ImpFab, we achieve resolutions in the tens of nanometers and complex, non–self-supporting 3D geometries of interest for optical metamaterials.</p>",,,2018-12-13 20:17:08.530,True,2018-01-02,Implosion Fabrication,PUBLIC,,True,Synthetic Neurobiology,False
implosion-fabrication-1,chenf,False,"<h2>Shrinking problems in 3D printing</h2><p>Although a range of materials can now be fabricated using additive manufacturing techniques, these usually involve assembly of a series of stacked layers, which restricts three-dimensional (3D) geometry. Oran&nbsp;et al.&nbsp;developed a method to print a range of materials, including metals and semiconductors, inside a gel scaffold (see the Perspective by Long and Williams). When the hydrogels were dehydrated, they shrunk 10-fold, which pushed the feature sizes down to the nanoscale.</p><p>Lithographic nanofabrication is often limited to successive fabrication of two-dimensional (2D) layers. We present a strategy for the direct assembly of 3D nanomaterials consisting of metals, semiconductors, and biomolecules arranged in virtually any 3D geometry. We used hydrogels as scaffolds for volumetric deposition of materials at defined points in space. We then optically patterned these scaffolds in three dimensions, attached one or more functional materials, and then shrank and dehydrated them in a controlled way to achieve nanoscale feature sizes in a solid substrate. We demonstrate that our process, Implosion Fabrication (ImpFab), can directly write highly conductive, 3D silver nanostructures within an acrylic scaffold via volumetric silver deposition. Using ImpFab, we achieve resolutions in the tens of nanometers and complex, non–self-supporting 3D geometries of interest for optical metamaterials.</p>",,,2018-12-13 20:17:08.530,True,2018-01-02,Implosion Fabrication,PUBLIC,,True,Synthetic Neurobiology,False
smart-2-opal,hhao,False,"<p>Privacy-preserving mHealth application using Open Algorithm (OPAL) architecture to address urgent care challenges in Riyadh, Saudi Arabia.</p>",,,2019-04-01 17:39:27.492,False,2017-11-01,SMART^2 OPAL,PUBLIC,,True,Human Dynamics,False
kinetix,jannik,False,"<p>kinetiX is a transformable material featuring a design that resembles a cellular structure. It consists of rigid plates or rods and elastic hinges. These modular elements can be combined in a wide variety of ways and assembled into multifarious forms.</p><p>This project describes a group of auxetic-inspired material structures that can transform into various shapes upon compression. While the majority of the studies of auxetic materials focus on their mechanical properties and topological variations, our work proposes a parametric design approach that gives auxetic structures the ability to deform beyond shrinking or expanding. To do so, we see the auxetic structure as a parametric four-bar linkage. We developed four cellular-based material structure units composed of rigid plates and elastic/rotary hinges. Different compositions of these units lead to a variety of tunable shape-changing possibilities, such as uniform scaling, shearing, bending and rotating. By tessellating those transformations together, we can create various higher level transformations for design. The simulation is validated by the 3D printed structures.&nbsp;</p><p>&nbsp;We hope this work will inspire research in metamaterials design, shape-changing materials, and transformable architecture.</p>",,,2019-01-24 14:26:13.216,True,2017-03-01,kinetiX,PUBLIC,,True,Tangible Media,True
cities-without,gba,False,"<p>Cities are becoming increasingly complex. As of&nbsp;2008, more than half of the world's population has been living in cities; this number is expected to be as high as 70 percent by 2050, with the most growth occurring in the southern hemisphere. According to the Word Bank, 85 percent of the global GDP is created in cites.&nbsp;We must find new solutions to meet the complex challenges of the future.&nbsp;</p><p>We pose an opportunity to rethink current models and invent new systems and strategies. We consider&nbsp;<i>cities without</i>, and the opportunities this can pose for a more livable, equitable, and resilient future.&nbsp;</p>",,,2019-04-08 14:52:00.977,True,2019-03-27,Cities Without,GROUP,,True,City Science,False
cityscope-cooper-hewitt,gba,False,"<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>",,,2019-04-02 21:27:39.368,True,2018-11-01,CityScope Cooper Hewitt,PUBLIC,https://collection.cooperhewitt.org/exhibitions/2318794479/page1,True,City Science,False
piccolo-kitchen,alhadidi,False,"<p>&nbsp;This project aims to create a modular platform for exploring micro-kitchens that are culture specific. Cooking is a personal experience that has cultural attributes.&nbsp; This project explores new modes of cooking using robotically enabled cabinets and appliances to minimize the footprint of the kitchen, while maximizing the ability for users to cook large meals, socialize, and utilize the same space during non-meal times for work. Piccolo kitchen is one of the components of the micro-units that are currently under development as part of the CityHome 02 projects.</p>",,,2019-04-19 13:39:55.600,True,2018-09-01,Piccolo Kitchen,PUBLIC,,True,City Science,False
piccolo-kitchen,nanaco,False,"<p>&nbsp;This project aims to create a modular platform for exploring micro-kitchens that are culture specific. Cooking is a personal experience that has cultural attributes.&nbsp; This project explores new modes of cooking using robotically enabled cabinets and appliances to minimize the footprint of the kitchen, while maximizing the ability for users to cook large meals, socialize, and utilize the same space during non-meal times for work. Piccolo kitchen is one of the components of the micro-units that are currently under development as part of the CityHome 02 projects.</p>",,,2019-04-19 13:39:55.600,True,2018-09-01,Piccolo Kitchen,PUBLIC,,True,City Science,False
piccolo-kitchen,apaolaza,False,"<p>&nbsp;This project aims to create a modular platform for exploring micro-kitchens that are culture specific. Cooking is a personal experience that has cultural attributes.&nbsp; This project explores new modes of cooking using robotically enabled cabinets and appliances to minimize the footprint of the kitchen, while maximizing the ability for users to cook large meals, socialize, and utilize the same space during non-meal times for work. Piccolo kitchen is one of the components of the micro-units that are currently under development as part of the CityHome 02 projects.</p>",,,2019-04-19 13:39:55.600,True,2018-09-01,Piccolo Kitchen,PUBLIC,,True,City Science,False
speech-blocks,jhgray,False,"<p>SpeechBlocks is a medium that allows children (4-5 years old) to engage in open-ended play with writing. They can build arbitrary compositions out of words and associated images, which can become cards, signs, stories, and ""books."" We hypothesize that such creative, self-expressive play can foster development of basic literacy skills, like phonological awareness. However, because users of SpeechBlocks are not yet in command of writing, it is necessary for the system to scaffold and guide them. We study a variety of ways to accomplish this.<br></p>",,,2019-05-21 17:11:02.880,True,2015-03-01,SpeechBlocks,PUBLIC,,True,Personal Robots,False
hol-deep-sense,yuefw,False,"<p>The Hol-Deep-Sense project (funded by the&nbsp;EU’s Horizon 2020 Marie Skłodowska-Curie grant)&nbsp;aims at holistic machine perception of human phenomena such as personal attributes (e.g., age, gender), emotion, health, as well as cognitive and physical states. The machine learning methods developed in this project help personalize AI technologies and enable natural human-machine communication. In particular, this project addresses the shortcoming of today's recognition systems that regard affective states as isolated patterns. Using novel multi-task and transfer learning techniques, we aim to shed light on the interrelations between the facets of human phenomena.&nbsp;The overarching goal of the Hol-Deep-Sense project is to create an end-to-end, multi-input, and multi-output learning framework that learns from multi-modal sensory inputs (e.g., audio, visual, physiological signals), an acoustic model to jointly recognize multiple output targets.&nbsp;</p>",2020-04-15,,2019-05-31 19:54:27.551,True,2018-10-15,HOL-DEEP-SENSE,PUBLIC,,True,Affective Computing,False
a-protocol-to-characterize-ph-sensing-materials-and-systems,asadraei,False,"​<p>Although significant progress is being made in identifying pH sensing materials&nbsp;and device configurations, a standard protocol for benchmarking performance&nbsp;of next-generation pH devices is still lacking. In particular, key properties&nbsp;of characterization systems, such as inherent component contributions, time plots for extended-gate field-effect transistor (EGFET) measurements,&nbsp;and the input resistance (Rin), often go unreported in studies of pH sensing&nbsp;systems. These properties strongly influence the characterization system and&nbsp;can lead to mistaken attribution of properties to the device. In this project, a series of essential characterization tests and parameters are reported to&nbsp;evaluate pH systems, such as the zinc oxide (ZnO) EGFET, in a standardized protocol.&nbsp;This EGFET ZnO sensor has a sensitivity of −58.1 mV pH−1, drift range from&nbsp;2.5 to 14.2 μA h−1, and response time of 136 s. By using a ZnO sensing&nbsp;electrode, it is demonstrated that i) intrinsic contributions of reference&nbsp;electrode and commercial transistor (for EGFET) are not negligible; ii) time&nbsp;plots for EGFET configuration and defining a critical point at the onset of drift&nbsp;are essential for accurate sensitivity, response time, and drift reporting; and&nbsp;iii) the results of the pH sensing system are strongly dependent on the input resistance of the used characterization instruments.</p>",,,2019-04-17 18:50:52.506,True,2018-08-23,A Protocol to Characterize pH Sensing Materials and Systems,PUBLIC,,True,Conformable Decoders,False
conformal-piezoelectric-mechanical-energy-harvesters,asadraei,False,"<p>Nearly all classes of wearable and implantable biomedical devices depend on battery power for continuous operation. However, the life span of batteries is limited, rarely exceeding a few hours for wearables and a few years for implants. Consequently, battery replacements and, often times, surgical procedures are required to change the depleted batteries of implants, exposing people to high risks of surgical complications and/or high financial costs. This project seeks to develop conformal piezoelectric patches integrated to personal garments to extract energy from body movements such as motion of arms, fingers, and legs. The completion of this project could improve quality life for people and potentially provide environmentally friendly power.</p>",,--Choose Location,2019-04-17 18:52:17.221,True,2016-01-01,Conformal Piezoelectric Mechanical Energy Harvesters: Mechanically Invisible Human Dynamos,PUBLIC,,True,Conformable Decoders,False
a-protocol-to-characterize-ph-sensing-materials-and-systems,mghoneim,False,"​<p>Although significant progress is being made in identifying pH sensing materials&nbsp;and device configurations, a standard protocol for benchmarking performance&nbsp;of next-generation pH devices is still lacking. In particular, key properties&nbsp;of characterization systems, such as inherent component contributions, time plots for extended-gate field-effect transistor (EGFET) measurements,&nbsp;and the input resistance (Rin), often go unreported in studies of pH sensing&nbsp;systems. These properties strongly influence the characterization system and&nbsp;can lead to mistaken attribution of properties to the device. In this project, a series of essential characterization tests and parameters are reported to&nbsp;evaluate pH systems, such as the zinc oxide (ZnO) EGFET, in a standardized protocol.&nbsp;This EGFET ZnO sensor has a sensitivity of −58.1 mV pH−1, drift range from&nbsp;2.5 to 14.2 μA h−1, and response time of 136 s. By using a ZnO sensing&nbsp;electrode, it is demonstrated that i) intrinsic contributions of reference&nbsp;electrode and commercial transistor (for EGFET) are not negligible; ii) time&nbsp;plots for EGFET configuration and defining a critical point at the onset of drift&nbsp;are essential for accurate sensitivity, response time, and drift reporting; and&nbsp;iii) the results of the pH sensing system are strongly dependent on the input resistance of the used characterization instruments.</p>",,,2019-04-17 18:50:52.506,True,2018-08-23,A Protocol to Characterize pH Sensing Materials and Systems,PUBLIC,,True,Conformable Decoders,False
personalized-emotional-wellness-coach,aymerich,False,"<p>The diagnosis and tracking of mood disorders still rely on clinical assessments, originating more than 50 years ago, of self-reported depressive symptoms via surveys and interviews. Such methods are known to provide limited accuracy and reliability in addition to being costly to track and scale. Once a problem is detected, providing personalized daily intervention and support is also too costly and does not scale. The goal of this pilot project is to develop a proof of concept of personalized emotional wellness coach focusing on key technology modules to create an emotionally intelligent social robot for this targeted domain. We shall also conduct a pilot evaluation with a five-week user study to evaluate the robot coach with respect to its ability to successfully sustain a user long-term adherence (i.e., daily self-report and efficacious advice)—with the expected result that it is more effective than state-of-the-art, gamified mobile apps currently used.&nbsp;</p>",2019-09-30,,2019-04-22 15:26:39.440,True,2018-09-01,Personalized Emotional Wellness Coach,PUBLIC,,True,Personal Robots,False
maiden-flight,renri,False,"<p>Maiden Flight is an autonomous biological laboratory environment designed for studying the impact of space flight on the sole reproductive node of a bee colony: <b>the queen bee and her retinue.&nbsp;&nbsp;</b></p><p>It represents the first space module of its kind built specifically to cater to queen bees. The hybrid-ecology of the capsule was created to take into account the distributed and uniquely non-human nature of bee biology, in order to consider how to extend the bee reproductive system for environmental extremes. This aim is reflected in the structure of the capsule interior, which was assembled by humans and augmented by the bees’ natural fabrication.&nbsp;</p><p>In May 2019, the Mediated Matter group traveled to Texas to launch two laboratory capsules on Blue Origin’s sub-orbital rocket system, New Shepard. Each custom-designed&nbsp;<b>metabolic support capsule&nbsp;</b>comprised an experimental environment for one queen bee and an attending retinue of 10-20 nurse bees for a parabolic flight to a 100-kilometer micro-gravitational space apogee, and back.</p>",,,2019-05-17 13:11:46.844,True,2019-04-19,Maiden Flight,PUBLIC,,True,Mediated Matter,False
elsa,ncjones,False,"<h2><b><i>What is ELSA?</i></b></h2><p>ELSA is an AI-powered chatbot that acts as an empathetic companion, encouraging users to talk about their day through a form of interactive journaling.</p><p>You can try some of the current ELSA bots in this online&nbsp;<a href=""http://elsaneural.net"">demo</a>.&nbsp;</p><h2><b><i>How does ELSA work?</i></b></h2><p>Our project goal is to build a more empathetic neural network conversational AI by incorporating a deeper understanding of both the affective content of the conversation and the topic.&nbsp; More specifically, we build hierarchical recurrent neural network models that can converse like people &nbsp;and use transfer learning of topic and emotional tone recognition models to improve our final model.</p><h2><b><i>What are the applications of ELSA?</i></b></h2><p>Beyond the development of chatbots that act as an empathetic companion, we have a more ambitious and longer term goal: deploy the empathetic companion bots to support mental health.&nbsp; In particular, &nbsp;we aim to make ELSA useful for:</p><ul><li>Eliciting journaling</li><li>Suggesting behavioura interventions</li><li>Using Cognition Behavioral Therapy</li><li>Detecting individuals at risk of depression or suicide</li></ul><h2><b><i>Work in progress</i></b></h2><p>ELSA is a recently started project in the Affective Computing group. You can see an example of ELSA bot conversations below. You can also try our online <a href=""http://elsaneural.net"">demo</a>. &nbsp;&nbsp;</p>",,,2019-04-22 19:06:31.702,True,2019-03-03,"ELSA: Empathy learning, socially-aware agents",PUBLIC,,True,Affective Computing,False
integrated-complex-systems-modeling,jackreid,False,"<p>One of the six research methods used by the Space Enabled research group is creating models of complex systems by drawing on techniques from systems engineering, social science, and earth science. The environment, human vulnerability and societal impact, human decision-making, and technology design are four domains with complex interactions that can be simulated in computer models. Environmental models use physics-based simulations to estimate the behavior of natural features in the atmosphere, water, or vegetation. Human vulnerability and societal impact models estimate how people are impacted by environmental hazards, such as hurricanes, or ecosystems services, such as the benefits of forests, using physics-based simulations or economic regressions. Human decision-making models, which can be decentralized agent-based&nbsp; or centralized decision-logic, simulate the actions taken by humans in response to environmental features. The technology design model allows humans to explore options for technical systems (such as earth observation satellites) to improve awareness of the state of the environment.</p><p>While significant benefit has come from addressing each domain individually in existing models, and yet more from considering certain groupings (such as the economic valuations that combine both decision-making and societal impact), capturing all four together in an integrated software model can enable us to overcome important challenges that lie at the intersections of these domains.<br></p><p>Work is underway to make such integrated models a reality. We are developing applications aimed at facilitating the targeted harvesting of water hyacinth in southern Benin and at assisting the city of Rio de Janeiro with the conservation of their mangrove forests (see the images below). Once complete, these models will accomplish the dual objectives of improving remote observation-informed decision-making and to enable the exploration of remote observation system options, including specifying the architecture of a custom satellite.</p><p>The Space Enabled research group aims to develop an open-source standard for software model interfaces and begin to build an open-source library of new and existing computational models for use in a variety of applications. Hopefully this library will be used to facilitate the use of remote sensing data, craft policy here on Earth, and design new remote sensing platforms. As we progress and work towards version 1.0, we hope others will join us in this endeavor.</p>",,,2019-04-19 13:29:24.236,True,2018-09-03,Integrated Complex Systems Modeling,PUBLIC,,True,Space Enabled,False
improving-well-being-prediction-performance-using-temporal-machine-learning-models,terumi,False,"<p>This project aims to improve the prediction accuracy of wellbeing (stress, mood, and health levels) using temporal machine learning models. We extend our previous approach using Long Short-Term Memory models and time series data from the <a href=""https://www.media.mit.edu/projects/snapshot-study/overview/"">SNAPSHOT study</a>. In addition, we consider adaptive methods to fill in missing data with time series information. We also develop the model using modifiable behavioral features such as bedtime, and examine how these contribute to wellbeing, so that people can get better control over how to improve their personal well-being.</p>",,,2019-05-24 18:28:52.701,True,2018-04-02,Improving wellbeing prediction performance using temporal machine learning models,PUBLIC,,True,Affective Computing,False

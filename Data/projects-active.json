[{"website": "", "description": "<p>Data portraits are a series of visualizations of information traces collected from various social application feeds. Using salient words and phrases from an individual's postings or collection, we visualize the topical and temporal patterns to create an abstract portrait of the author. Displayed as a group, these portraits both feature the individuality of each subject and highlight the external events that unite them.</p>", "people": ["judith@media.mit.edu"], "title": "Data Portraits", "modified": "2016-12-05T00:16:20.516Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "data-portraits"}, {"website": "", "description": "<p>We study different methods to depict the ways that people are spending their time in their lived environments. We look at people's relationships with each other in physical space through a variety of visualization techniques. By combining different sources of data from sensors, mobile phone logs, and other social media, we build rich and meaningful histories from the less-recognizable details of our lives to understand how our identities are shaped in time.</p>", "people": ["judith@media.mit.edu"], "title": "Data Portrait Study Series: Social Maps of Time and Space", "modified": "2016-12-05T00:17:10.350Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-391", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "data-portrait-study-series-social-maps-of-time-and-space"}, {"website": "", "description": "<p>NewsPix is a simple news-engagement application that helps users encounter breaking news in the form of high-impact photos. It is currently a Chrome browser extension (mobile app to come) that is customizable for small and large news organizations. Currently, when users open a new, blank page in Chrome, they get a new tab with tiles that show recently visited pages. NewsPix replaces that view with a high-quality picture from a news site. Users interested in more information about the photo can click through to the news site. News organizations can upload photos ranging from breaking news to historic sporting events, with photos changing every time a new tab is clicked.</p>", "people": ["dignazio@media.mit.edu", "ethanz@media.mit.edu", "matt54@media.mit.edu"], "title": "NewsPix", "modified": "2016-12-05T00:17:03.242Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["future-of-news", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "newspix"}, {"website": "", "description": "..", "people": ["aithpao@media.mit.edu"], "title": "aithpao's untitled project", "modified": "2016-09-27T20:24:23.900Z", "visibility": "LAB", "start_on": "2014-01-01", "location": "--Choose Location", "groups": [], "published": false, "active": false, "end_on": null, "slug": "aithpaos-untitled-project-2"}, {"website": "", "description": "<p>Data-Pop Alliance is a joint initiative on big data and development with a goal of helping to craft and leverage the new ecosystem of big data--new personal data, new tools, new actors--to improve decisions and empower people in a way that avoids the pitfalls of a new digital divide, de-humanization, and de-democratization. Data-Pop Alliance aims to serve as a designer, broker, and implementer of ideas and activities, bringing together institutions and individuals around common principles and objectives through collaborative research, training and capacity building, technical assistance, convening, knowledge curation, and advocacy. Our thematic areas of focus include official statistics, socio-economic and demographic methods, conflict and crime, climate change and environment, literacy, and ethics.</p>", "people": ["sandy@media.mit.edu"], "title": "Data-Pop Alliance", "modified": "2016-12-05T00:16:20.681Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "data-pop-alliance"}, {"website": "", "description": "<p>First Upload is a tool for verifying the authenticity of news imagery. It helps find the first upload of imagery, particularly videos. Finding the person who uploaded a video is a key to determining authenticity, because often it is necessary to contact that person directly. It is being developed with input from YouTube and Bloomberg. Currently we have a working prototype, built for the YouTube site.</p>", "people": ["ethanz@media.mit.edu", "matt54@media.mit.edu"], "title": "First Upload", "modified": "2016-12-05T00:16:25.181Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["future-of-news", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "first-upload"}, {"website": "", "description": "<h1><b>Open-Source Instructions for Building SPRING System</b></h1>", "people": ["ktj@media.mit.edu"], "title": "Open-Source SPRING", "modified": "2017-12-24T14:02:54.051Z", "visibility": "PUBLIC", "start_on": "2017-12-20", "location": "", "groups": ["ml-learning", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "open-source-spring"}, {"website": "https://guadalupebabio.com", "description": "<p><i style=\"font-size: 18px; font-weight: 400;\">Unfolding the way we move.&nbsp;</i><br></p><p>Mobility has shaped the built environment since humans started settling together. From industrial towns to post-industrial innovation and service hubs, the mobility mode of the era was key in shaping not only the physical attributes of cities, but also the efficacy. In order to allocate the massive migration from rural areas, cities are growing and becoming more dense. Although high density can minimize transportation cost and energy, several problems start to appear if they are not planned carefully. Urban ventilation potential is reduced and open seen spaces are limited, compromising our experience and life quality. Residential, office, and retail get closer but remain arranged in conventional ways. A two-dimensional street that organizes the way we live and keeps transportation methods are in permanent conflict. Too fast for those who live in it, and too slow and congested for those that go by.</p><p>Urban mobility is becoming more electric, more autonomous, more shared, and more connected, indicators that call for a mobility revolution, and designers have the chance to reinvent the way city is experienced.</p><p>Today, more than ever, the scale and rate of urban expansion is making mobility solutions a key concern, which will impact large segments of the global population since it is estimated that by 2050, more that two thirds of the global population will be living in cities. We&nbsp; propose a new experience and mobility around cities, unfolding the city networks and using its third dimensions, different mobility, speed modes (static, mass transportation, internal transportation), public areas appearing in rooftops, and mix-use spaces in&nbsp;intersticial&nbsp;parts of buildings. Through simulation as a tool, we can understand the impact of this new disrupting mobility system, avoiding to repeat mistakes like those made in the past.&nbsp;</p>", "people": ["agrignar@media.mit.edu", "kll@media.mit.edu", "gbabio@media.mit.edu"], "title": "3D Mobility", "modified": "2019-06-12T14:47:59.274Z", "visibility": "PUBLIC", "start_on": "2018-07-09", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "3d-mobility"}, {"website": "", "description": "", "people": ["pattie@media.mit.edu"], "title": "AutoEmotive", "modified": "2016-10-05T23:50:05.077Z", "visibility": "PUBLIC", "start_on": "2015-12-01", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "autoemotive"}, {"website": "", "description": "<p></p>", "people": ["fracchia@media.mit.edu", "neilg@media.mit.edu"], "title": "BioGlasses: a platform for Human Physiological Measurement", "modified": "2016-12-05T00:17:07.612Z", "visibility": "LAB", "start_on": "2012-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "bioglasses-a-platform-for-human-physiological-measurement"}, {"website": "", "description": "<p>The building blocks we grow up with and the coordinate systems we are introduced to at an early age shape the design space with which we think. Complex systems are difficult to understand because they often require transition from one coordinate system to another. We could even begin to say that empathy is precisely this ability to map easily to many different coordinates. Troxes is a building blocks kit based on the triangle, where kids get to build their building blocks and then assemble Platonic and Archimedean solids.</p>", "people": ["jbobrow@media.mit.edu"], "title": "Troxes", "modified": "2016-12-15T03:10:14.309Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "troxes"}, {"website": "", "description": "<p>An open-source, low-cost electrocardiography device.</p>", "people": ["fracchia@media.mit.edu", "neilg@media.mit.edu"], "title": "FabECG: An Open-Source Electrocardiography Device", "modified": "2016-12-05T00:17:11.760Z", "visibility": "LAB", "start_on": "2012-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "fabecg-an-open-source-electrocardiography-device"}, {"website": "", "description": "", "people": [], "title": "Networked Playscapes: TriCycle", "modified": "2016-10-25T14:43:03.828Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "networked-playscapes-tricycle"}, {"website": "", "description": "<p>This project explores how modern graphical interface techniques and explicit support for the user's problem-solving activities can make more productive interfaces for debugging, which accounts for half the cost of software development. Animated representations of code, a reversible control structure, and instant connections between code and graphical output are some of the techniques used.</p>", "people": ["lieber@media.mit.edu"], "title": "Graphical Interfaces for Software Visualization and Debugging", "modified": "2016-12-05T00:17:13.585Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "graphical-interfaces-for-software-visualization-and-debugging"}, {"website": "", "description": "<p>SensorTape is a modular and dense sensor network in a form factor of a tape. SensorTape is composed of interconnected and programmable sensor nodes on a flexible electronics sub-strate. Each node can sense its orientation with an inertial measurement unit, allowing deformation self-sensing of the whole tape. Also, nodes sense proximity using time-of-flight infrared. We developed network architecture to automatically determine the location of each sensor node, as SensorTape is cut and rejoined. We also made an intuitive graphical interface to program the tape. Our user study suggested that SensorTape enables users with different skill sets to intuitively create and program large sensor network arrays. We developed diverse applications ranging from wearables to home sensing, to show low-deployment effort required by the user. We showed how SensorTape could be produced at scale and made a 2.3-meter long prototype.</p>", "people": ["artemd@media.mit.edu", "joep@media.mit.edu", "cindykao@media.mit.edu"], "title": "SensorTape: Modular and programmable 3D-aware dense sensor network on a tape", "modified": "2019-04-19T14:34:20.486Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["living-mobile", "responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "sensortape-modular-and-programmable-3d-aware-dense-sensor-network-on-a-tape"}, {"website": "", "description": "<p>With common-sense computing, we can discover trends in the topics that people are talking about right now. Red Fish, Blue Fish takes input in real time from many political blogs, and creates a visualization of what topics are being discussed by the left and the right.</p>", "people": [], "title": "Red Fish, Blue Fish", "modified": "2016-12-05T00:17:21.765Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["digital-intuition"], "published": true, "active": false, "end_on": null, "slug": "red-fish-blue-fish"}, {"website": "", "description": "<p>We have developed technology that enables easy analysis of semantic data, blended in various ways with common-sense world knowledge. The results support reasoning by analogy and association. A packaged library of code is being made available to all sponsors.</p>", "people": ["havasi@media.mit.edu"], "title": "Divisi: Reasoning Over Semantic Relationships", "modified": "2016-12-05T00:16:21.698Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-383", "groups": ["digital-intuition"], "published": true, "active": false, "end_on": null, "slug": "divisi-reasoning-over-semantic-relationships"}, {"website": "", "description": "<p>Food offers a rich multi-modal experience that can deeply affect emotion and memory. We're interested in exploring the artistic and expressive potential of food beyond mere nourishment, as a means of creating memorable experiences that involve multiple senses. For instance, music can change our eating experience by altering our emotions during the meal, or by evoking a specific time and place. Similarly, sight, smell, and temperature can all be manipulated to combine with food for expressive effect. In addition, by drawing upon people's physiology and upbringing, we seek to create individual, meaningful sensory experiences. Specifically, we are exploring the connection between music and flavor perception.</p>", "people": ["slavin@media.mit.edu"], "title": "Designing Immersive Multi-Sensory Eating Experiences", "modified": "2016-12-05T00:17:10.552Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "designing-immersive-multi-sensory-eating-experiences"}, {"website": "", "description": "<p>The web enables massive realtime communication and collaboration, but most media on the web does not take advantage of these features. Media on the internet typically uses the web only as a distribution medium.</p><p>If we are going to make next-generation internet media, we need to think about how to integrate the unique properties of the web into the media itself. This involves rethinking the role and design of web servers so they facilitate realtime interaction instead of serving requests.</p><p>Models for internet-enabled interaction and collaboration like forums, chatroom, live documents, metrics and A/B testing, are not designed with interactive media in mind.&nbsp;</p><p>This project is our very first exploration using custom web server technology and a new interaction model to facilitate online collaboration.</p>", "people": ["holbrow@media.mit.edu"], "title": "Eternal", "modified": "2018-05-01T13:57:18.927Z", "visibility": "PUBLIC", "start_on": "2017-10-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "eternal"}, {"website": "", "description": "<p>Video or broadcast news is viewed in a far wider set of circumstances than it ever has been before. It is composed with the assumption of a complete, situated viewing, but in fact it is often grabbed on-the-fly as a momentary experience. As You Need It is a semantic summarizer that deconstructs a multi-part segment for presentation as \"chunks of importance.\" We are learning if a story can be cut down to a useful update that takes less time than a traffic light, or as much time as a given user has. This project uses and contributes to another group project, SuperGlue.</p>", "people": ["lip@media.mit.edu", "jasrub@media.mit.edu"], "title": "As You Need It", "modified": "2016-12-05T00:16:49.848Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "viral-communications"], "published": true, "active": false, "end_on": null, "slug": "as-you-need-it"}, {"website": "", "description": "<p>Lyme disease is the most common vector-borne infection in North America. People are infected when bitten by ticks; ticks are typically infected when they bite white-footed mice, the primary \"reservoir\" of the disease. We are exploring the possibility of permanently immunizing mouse populations to block transmission by making and releasing mice that produce protective mouse antibodies from birth and pass immunity on to their pups. The project has been guided by representatives in offshore island communities from inception. Communities will choose which type of antibodies, pick uninhabited islands to serve as field trial sites, select independent monitors, and ultimately decide whether to volunteer their own islands for the next stage. If successful, prevention could be expanded to the mainland using local or global gene drive systems. Whether or not communities decide to proceed, we hope the process will become a model for responsive science worldwide.</p>", "people": ["buchthal@media.mit.edu", "esvelt@media.mit.edu"], "title": "Preventing tick-borne disease by permanently immunizing mice", "modified": "2019-04-17T18:53:26.237Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "preventing-tick-borne-disease-by-permanently-immunizing-mice"}, {"website": "", "description": "<p>A peer-to-peer network, enabling different parties to jointly store and run computations on data while keeping the data completely private. Enigma's computational model is based on a highly optimized version of secure multi-party computation, guaranteed by a verifiable secret-sharing scheme. For storage, we use a modified distributed hashtable for holding secret-shared data. An external blockchain is utilized as the controller of the network, manages access control and identities, and serves as a tamper-proof log of events. Security deposits and fees incentivize operation, correctness, and fairness of the system. Similar to Bitcoin, Enigma removes the need for a trusted third party, enabling autonomous control of personal data. For the first time, users are able to share their data with cryptographic guarantees regarding their privacy.</p>", "people": ["sandy@media.mit.edu", "guyzys@media.mit.edu"], "title": "Enigma", "modified": "2018-02-12T20:37:17.158Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "enigma"}, {"website": "", "description": "<p>We propose a novel shape-changing interface that consists of a single line. Lines have several interesting characteristics from the perspective of interaction design: abstractness of data representation; a variety of inherent interactions/affordances; and constraints such as boundaries or borderlines. By using such aspects of lines together with added transformation capability, we present various applications in different scenarios: shape-changing cords, mobiles, body constraints, and data manipulation to investigate the design space of line-based shape-changing interfaces.</p>", "people": ["ishii@media.mit.edu", "ken_n@media.mit.edu"], "title": "LineFORM", "modified": "2018-05-04T15:31:24.514Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "lineform"}, {"website": "", "description": "<p>We present an interactive virtual reality (VR) experience&nbsp;that uses biometric information for reflection and relaxation.&nbsp;We monitor in real-time brain activity using a modified version&nbsp;of the Muse EEG and track heart rate (HR) and electro&nbsp;dermal activity (EDA) using an Empatica E4 wristband. We&nbsp;use this data to procedurally generate 3D creatures and&nbsp;change the lighting of the environment to reflect the internal&nbsp;state of the viewer in a set of visuals depicting an underwater&nbsp;audiovisual composition. These 3D creatures are&nbsp;created to unconsciously influence the body signals of the&nbsp;observer via subtle pulses of light, movement and sound.&nbsp;We aim to decrease heart rate and respiration by subtle,&nbsp;almost imperceptible light flickering, sound pulsations and&nbsp;slow movements of these creatures to increase relaxation.</p>", "people": ["richer@media.mit.edu", "afuste@media.mit.edu", "amores@media.mit.edu"], "title": "Deep Reality: An underwater VR experience to promote relaxation by unconscious HR, EDA and brain activity biofeedback", "modified": "2019-05-09T19:18:17.333Z", "visibility": "PUBLIC", "start_on": "2017-04-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "deep-reality"}, {"website": "", "description": "<p>HydroMorph is an interactive display based on shapes formed by a stream of water. Inspired by the membrane formed when a water stream hits a smooth surface (e.g., a spoon), we developed a system that dynamically controls the shape of a water membrane. This project explores a design space of interactions around water shapes, and proposes a set of user scenarios in applications across scales, from the faucet to the fountain. Through this work, we look to enrich our interaction with water, an everyday material, with the added dimension of transformation.</p>", "people": ["thariq@media.mit.edu", "ishii@media.mit.edu", "ken_n@media.mit.edu"], "title": "HydroMorph", "modified": "2016-12-05T00:17:14.252Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "hydromorph"}, {"website": "", "description": "<p>Masca is a flexible mask for sleep stage detection. Our device adapts to the human body using conformable piezoresistive fabric and silicone, enabling eyelid motion detection in a comfortable, affordable form factor.&nbsp; <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.1995.tb03410.x\">Eyes and eyelids change movement frequency predictably</a>&nbsp;as sleep stage transitions occur, allowing for a simpler, more portable system than the typical high-density EEG required for sleep tracking. Tracking and influencing of sleep cognition opens up doors to targeted reactivation of daytime experience during sleep: a future in which the consolidation of emotion, memory, and learning in sleep is rendered controllable by wearable electronics.&nbsp;</p><p>This device is modeled after Prof. Robert Stickgold's <a href=\"https://www.sciencedirect.com/science/article/pii/S1053810084710026\">Nightcap</a>, and we are grateful for his ongoing assistance with this project, aiming at further extending the benefits of sleep neuroscience.<br></p>", "people": ["irmandy@media.mit.edu", "adamjhh@media.mit.edu", "tomasero@media.mit.edu", "rosello@media.mit.edu"], "title": "Masca", "modified": "2019-03-26T13:32:12.436Z", "visibility": "PUBLIC", "start_on": "2018-04-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "masca"}, {"website": "", "description": "<p>Urban populations around the world are rapidly growing. To improve livability, urban residents must reduce dependency on fossil fuels and private cars, while needing efficient equitable access to inexpensive and reliable transportation.</p><p>Urbanization has outpaced transportation innovation as we know it, and urban transportation issues are far more complex and diverse than they appear when viewed from a car seat. The private sector frequently offers self-driving and electric cars as a catch-all solution; additionally, the sharing/on-demand model with connected vehicles has become mainstream, human-scaled, and increasingly electrified, from cars (ZipCar, Car2Go) to bicycles (Hubway, MoBike).<br></p><p>The&nbsp;Mobility Revolution<i>\u2019s</i>&nbsp;autonomous vehicles and car sharing have created open questions about the needed participation of the public sector, questionable suitability of automobiles in the emerging urban context, and unintended negative externalities like sprawl or autonomous congestions.</p>", "people": ["lukeji@media.mit.edu", "cq_zhang@media.mit.edu", "mcllin@media.mit.edu", "kll@media.mit.edu", "jerryao@media.mit.edu", "yagol@media.mit.edu", "cassiano@media.mit.edu", "ptinn@media.mit.edu", "taiyu@media.mit.edu", "abhia@media.mit.edu"], "title": "Theme   |   Mobility On-Demand", "modified": "2019-04-23T14:03:41.132Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "mod"}, {"website": "", "description": "<p>We are expanding the home-video viewing experience by generating imagery to extend the TV screen and give the impression that the scene wraps completely around the viewer. Optical flow, color analysis, and heuristics extrapolate beyond the screen edge, where projectors provide the viewer's perceptual vision with low-detail dynamic patterns that are perceptually consistent with the video imagery and increase the sense of immersive presence and participation. We perform this processing in real time using standard microprocessors and GPUs.</p>", "people": ["novysan@media.mit.edu", "vmb@media.mit.edu"], "title": "Infinity-by-Nine", "modified": "2016-12-05T00:16:31.538Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "infinity-by-nine"}, {"website": "", "description": "<p>fja;sldjfa;boq</p>", "people": ["neri@media.mit.edu", "j_duro@media.mit.edu"], "title": "Sample project for Neri", "modified": "2016-10-06T17:12:11.589Z", "visibility": "PUBLIC", "start_on": "2016-10-06", "location": "", "groups": ["mediated-matter"], "published": false, "active": false, "end_on": null, "slug": "sample-project-for-neri"}, {"website": "", "description": "<p>This project unites embodied cognition and on-body device design to ask questions about the origin of emotions and the potential to hack our brains and behavior by hacking the body.&nbsp;</p><p>Frisson, also known as aesthetic chills, are the wave of chills you get while experience peak emotional moments in a song or peak meaning-making moments in a speech.&nbsp;&nbsp;At once transcendent and physiological, the subtle signals of beauty and semantics meet mechanism as the sublime literally cascades across skin. Conveniently, frisson seems to be an<a href=\"https://www.researchgate.net/publication/227131588_Aesthetic_Chills_as_a_Universal_Marker_of_Openness_to_Experience\">&nbsp;almost universal marker</a>&nbsp;of peak emotional experiences across a wide range of cultures and continents.&nbsp;Embodied cognition has done much to illuminate links between our physical experience and our psychological experience, and studies on<a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2420110403\">&nbsp;misattribution of arousal</a>&nbsp;show us we can drive cognition by driving physical sensation. This points to opportunities: if we can drive frisson, perhaps we can also drive the openness to experience, relief in stress, increase in empathy, and experience of meaning that frisson has been tied to in past research.</p><p>So we built a device to trigger frisson.&nbsp;Alongside&nbsp;<a href=\"https://www.researchgate.net/publication/305892666_Aesthetic_Chills_Knowledge-Acquisition_Meaning-Making_and_Aesthetic_Emotions\">F\u00e9lix Schoeller</a>, first we're testing whether inducing chills can increase&nbsp;<a href=\"http://journals.sagepub.com/doi/abs/10.1177/0305735615572358\">deep attention and openness to experiences.&nbsp;</a>&nbsp;Psychophysiology driving thought from the spine upwards!</p>", "people": ["adamjhh@media.mit.edu", "tomasero@media.mit.edu", "abyjain@media.mit.edu"], "title": "Frisson", "modified": "2019-03-26T13:36:01.178Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "frisson"}, {"website": "", "description": "", "people": [], "title": "test pjrojea", "modified": "2018-10-01T16:34:31.092Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "test-pjrojea"}, {"website": "https://www.linkedin.com/in/johndelaparra/", "description": "<p><b>Flavor</b>, in addition to making our food delicious, is one<b>&nbsp;</b>way of <b>sensing biochemical richness</b>. A highly flavorful plant generally contains a greater quantity and diversity of molecules\u2014often with useful functional roles in our own metabolism\u2014than a bland-tasting plant.&nbsp;</p><p>Flavor is a built-in reward for eating plants that has fueled our drive to domesticate and breed a massive biodiversity of vegetation over the last 10,000 years. OpenAg is going deep into the biochemical <b>machinery, evolution, and ecology of plants&nbsp;</b>to make growing food for the optimization of specific chemical profiles (flavor, pharmaceutical properties, nutrition) a reality.</p><p>Plants rely on <b>rich and diverse chemistry&nbsp;</b>for self-defense and stress adaptation. OpenAg is working <b>to induce a plant to synthesize these molecules&nbsp;</b>by adding specific stresses to the plant\u2019s environment and measuring chemical shifts.</p><p>These <b>specialized metabolite molecules&nbsp;</b>can manifest as flavor, pharmaceutical compounds, and rich nutritional profiles. Flavor itself is frequently tied to additional healthful bio-activities for humans\u2014such as <b>vitamins, antioxidants, stimulants, and nutrients.</b></p>", "people": ["rebekahj@media.mit.edu", "delapa@media.mit.edu"], "title": "Optimizing plants for flavor, nutrition, and pharmaceutical content", "modified": "2019-04-23T21:12:54.149Z", "visibility": "PUBLIC", "start_on": "2017-10-02", "location": "", "groups": ["open-agriculture-openag"], "published": true, "active": false, "end_on": null, "slug": "openag-flavor-ecology"}, {"website": "http://aberke.com", "description": "<p>As bikes navigate city streets after dark, they are often equipped with lights. The lights make the bikes visible to cars or other bikers, and the hazards of traffic less dangerous.</p><p>Imagine that as solitary bikes come together, their lights begin to pulsate at the same cadence. The bikers may not know each other, or may only be passing each other briefly, but for the moments they are together, their lights synchronize. The effect is a visually united presence, as groups of bikes illuminate themselves with a gently pulsing, collective light source.</p>", "people": ["thomassl@media.mit.edu", "kll@media.mit.edu", "aberke@media.mit.edu"], "title": "[bike] swarm", "modified": "2019-06-11T16:37:33.942Z", "visibility": "PUBLIC", "start_on": "2019-02-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "bike-swarm"}, {"website": "", "description": "<p>Have you ever wondered what a friend&nbsp;would&nbsp;do if she was in your decision-making situation? Or thought about where a family member might go if he was visiting a travel destination with you? In many cases, you can only guess what a person would do if they were in your shoes. But now you may be able to securely \"<b>borrow their&nbsp; identity</b>\" and ask a question with the confidence of receiving a relevant and &nbsp;valuable answer.&nbsp;</p><p>Can software agents become our digital heirs? Can a head of state, a scientist, or a business owner leverage machine intelligence to complement succession planning?&nbsp; What if you could select the digital identity of a deceased person from a social network and activate it as a pluggable ontology into your iPhone\u2019s Siri and ask a question?</p><p>Our digital identity has become so rich and intrinsic that without it, it may feel like a part of us is missing. The number of sensors we carry daily and the digital footprints we leave behind have given us enough granular patterns and data clusters that we can now use them for prediction and reasoning on behalf of an individual. We believe that by enabling our digital identity to perpetuate, we can significantly contribute to global expertise and enable a new form of an intergenerational collective intelligence.</p>", "people": ["rahnama@media.mit.edu"], "title": "Augmented Eternity and Swappable Identities", "modified": "2019-04-19T14:37:08.576Z", "visibility": "PUBLIC", "start_on": "2017-05-22", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "augmented-eternity"}, {"website": "", "description": "", "people": [], "title": "Creating New Jazz Musical Instruments and New Jazz Idioms", "modified": "2018-05-08T01:54:44.319Z", "visibility": "PUBLIC", "start_on": "2018-05-07", "location": "", "groups": ["code-next"], "published": false, "active": false, "end_on": null, "slug": "creating-new-jazz-musical-instruments-and-new-jazz-idioms"}, {"website": "", "description": "", "people": [], "title": "adsfawdfa", "modified": "2018-10-01T16:53:19.437Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "adsfawdfa"}, {"website": "", "description": "<p><i>A Pedagogy of Noise</i>&nbsp;(in development)&nbsp;is a&nbsp;project-based approach to learning about sound, and to using sound to learn about the world around us. This is an ongoing project consisting of models, software systems, and more that are intended to enable this kind of creative learning. <i>Noise</i>&nbsp;here refers to the emergent texture of the sounds around us, in combination. This dense surface, especially in urban environments, can be perceived as noise, but there is a rich network of sounds that generate this texture.</p><p>Working with sound teaches us about sound itself, as well as engages us in creative processes, allows for artistic expression, and builds skills in a variety of traditional disciplines.&nbsp;Engaging with sound can also serve as a way of studying objects, environments, and more, as well as a medium for creative storytelling.</p><p>As we continue to make sound and noise as participants in our acoustical environments, and also continue to generate large quantities of audio for a variety of reasons, I'm very interested in how we can effectively tap into these as resources for creative projects, play, interaction, and learning.</p><p><b>In progress:</b>&nbsp;A construction kit for creative sound-based project-oriented learning.</p><h2>Example Projects</h2><p><b>Kronospaces</b></p><p><i>Kronospaces</i> (2018) is an interactive experience created using excerpts from the Kronos Quartet's recorded catalog, which spans about 40 years.</p><p>This project initially involved Investigating this catalog to find moments that could be excerpted to represent the range of music in the quartet's repertoire, to create a database of musical motifs and gestures.</p><p>Custom software was written to extract information about these audio excerpts, to organize them into 6 different \"spaces\" or categories using this extracted information, and to turn these into an interactive experience using concatenative synthesis to retrieve these excerpts from the database and stitch them into a sound \"mosaic\" in real-time based on the user's navigation of the visual interface, which also consists of custom software written for this project.</p><p><i>Kronospaces</i>&nbsp;is a playful, interactive window into the depth and diversity of the Kronos Quartet's history.</p><p><b>Scream</b></p><p><i>Scream</i> (2018) is an audiovisual installation that uses face-tracking as an input to control an instrument built from the audio portion of <a href=\"https://zenodo.org/record/1188976\">The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)</a> by Livingstone &amp; Russo (licensed under <a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\">CC BY-NA-SC 4.0</a>). Facial features and gestures are used to construct textures from the database, involving custom audio and visual software.</p><p>This work is part of an ongoing series exploring artistic applications of audio datasets, bodily interfaces to recorded sound, and new visual and physical interfaces to the exploration of sound corpuses.</p><p><i>Scream&nbsp;</i>uses perhaps the most primal sound-making device, the mouth, in a silent, visual capacity. It creates a one-to-many mapping in terms of people, allowing the gestures of one person to be expressed through the voices of many.</p><p>The installation\u2019s title doubles as an instruction, encouraging participants to scream, for whatever reason they may want to, through the voices of others without needing to make any sound acoustically.&nbsp;</p>", "people": ["nsingh1@media.mit.edu"], "title": "A Pedagogy of Noise", "modified": "2019-04-18T19:20:06.357Z", "visibility": "LAB", "start_on": "2019-04-18", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "apedagogyofnoise"}, {"website": "", "description": "<p>\"Know Your Exit\" is an interactive, web-based music composition. To create this piece, we crowdsourced over 1100 tracks from all over the world. People were asked to sing, clap, or tell a short story. The web-based version is dynamic and incorporates live tweets and geo-locations of the audio contributions. </p>", "people": ["rmorris@media.mit.edu"], "title": "Know Your Exit", "modified": "2016-12-05T00:16:33.314Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "know-your-exit"}, {"website": "", "description": "<p>The rise in wearable devices and the desire to quantify various aspects of everyday activities has provided the opportunity to offer just-in-time triggers to aid in achieving pre-determined goals. While a lot is known about the effectiveness of messaging in marketing efforts, less is known about the effectiveness of these marketing techniques on in-the-moment decision-making. We designed an experiment to determine if a simple solution of using just-in-time persuasive messaging could influence participants' eating habits and what types of messaging could be most effective in this effort. Our solution utilizes a head-mounted display to present health-based messages to users as they make real-time snack choices. We are able show that this method is effective and more feasible than current efforts to influence eating habits.</p>", "people": ["nfarve@media.mit.edu", "pattie@media.mit.edu"], "title": "Food Attack", "modified": "2016-12-05T00:16:44.086Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "food-attack"}, {"website": "http://www.judithamores.com", "description": "<p>Automatic and real-time sleep scoring is necessary to&nbsp;develop user interfaces that trigger stimuli in specific sleep stages.&nbsp;However, most automatic sleep scoring systems have been focused&nbsp;on offline data analysis. We present the first, real-time sleep staging&nbsp;system that uses deep learning without the need for servers&nbsp;in a smartphone application for a wearable EEG. We employ&nbsp;real-time adaptation of a single channel Electroencephalography&nbsp;(EEG) to infer from a Time-Distributed Convolutional Neural&nbsp;Network (CNN). Polysomnography (PSG)\u2014<b>the gold standard&nbsp;for sleep staging\u2014requires a human scorer and is both complex&nbsp;and resource-intensive</b>. Our work demonstrates an end-to-end,&nbsp;smartphone-based pipeline that can infer sleep stages in just&nbsp;single 30-second epochs, with an overall accuracy of 83.5% on&nbsp;20-fold cross validation for 5-stage classification of sleep stages&nbsp;using the open Sleep-EDF dataset.&nbsp;</p>", "people": ["amores@media.mit.edu"], "title": "Real-time Smartphone-based Sleep Staging using 1-Channel EEG", "modified": "2019-05-09T19:22:28.325Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "sleep-staging-EEG"}, {"website": "", "description": "<p><b>The Launch Team: </b>Christian McBride (winner of six Grammy Awards), David Gage, Mas Hino, and Topper Carew.</p>", "people": ["tcarew@media.mit.edu"], "title": "Creating New Jazz Musical Instruments and New Jazz Idioms", "modified": "2018-05-08T12:03:39.097Z", "visibility": "PUBLIC", "start_on": "2018-05-07", "location": "", "groups": ["code-next"], "published": true, "active": false, "end_on": null, "slug": "creating-new-jazz-musical-instruments-and-new-jazz-idioms-1"}, {"website": "", "description": "<p>Structure and function studies of membrane proteins, particularly G protein-coupled receptors (GPCRs) and multipass transmembrane proteins, require detergents after removing them from cell membranes. We have invented a simple tool, the QTY code, that is named for three amino acids: glutamine (Q), threonine (T), and tyrosine (Y) for making water-insoluble domains become water-soluble without detergents. Despite substantial transmembrane domain changes, the detergent-free QTY variants maintain stable structures and ligand-binding activities. We believe the QTY code will be useful for designing water-soluble variants of membrane proteins and other water-insoluble aggregated proteins. The QTY code designed detergent-free chemokine receptors may be useful in many applications. The QTY variants may not only be useful as reagents in deorphanization studies, but also for designing biologic drugs to treat cancer, autoimmune, or infectious diseases. The QTY code allows membrane proteins to be systematically designed through simple, specific amino acid substitutions. The QTY code is robust and straightforward: it is the simplest tool to carry out membrane protein design without sophisticated computer algorithms. Thus, it can be used broadly. The QTY code has implications for designing additional GPCRs and other membrane proteins, or for rendering water-insoluble and aggregated proteins to become water soluble.</p>", "people": ["shuguang@media.mit.edu", "vex@media.mit.edu", "junni@media.mit.edu", "ruiqing@media.mit.edu"], "title": "The QTY code: A simple tool for protein design", "modified": "2019-06-12T19:13:17.729Z", "visibility": "LAB-INSIDERS", "start_on": "2019-04-01", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "the-qty-code-a-simple-tool-for-protein-design"}, {"website": "", "description": "<p>We exploit the sub-picosecond time resolution along with spectral resolution provided by terahertz time-domain spectroscopy to extract occluding content from layers whose thicknesses are wavelength comparable. The method uses the statistics of the THz E-field at subwavelength gaps to lock into each layer position and then uses a time-gated spectral kurtosis to tune to highest spectral contrast of the content on that specific layer. To demonstrate, occluding textual content was successfully extracted from a sample similar to a closed book down to nine pages without human supervision. The method provides over an order of magnitude enhancement in the signal contrast and can impact inspection of structural defects in wooden objects, plastic components, composites, drugs, and especially cultural artifacts with subwavelength or wavelength comparable layers.</p>", "people": ["raskar@media.mit.edu", "aghasi@media.mit.edu", "barmak@media.mit.edu", "redosan@media.mit.edu"], "title": "Reading through Closed Books: THz Time-Gated Spectral Imaging for Content Extraction through Layered Structures", "modified": "2016-10-24T19:44:56.028Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["terrestrial-sensing", "camera-culture"], "published": false, "active": false, "end_on": null, "slug": "barmaks-untitled-project-2"}, {"website": "", "description": "<p>Ubiquitous computing has been focusing on creating smart agents that are submerged into everyday environments, however, recent development on physical computing is demanding a shift from calm computing to a physically engaging form. Computing is no more limited to increasing our comfort through passive and pervasive deployment, they can now be created as being more actively and physically intermeshed into our tasks. We present L\u2019evolved, autonomous ubiquitous utilities that assist in user tasks through active physical participation. They not only dynamically adapt to individual user needs and actions, but also work in close tandem with the users. Among explorations on potential applications, we harness drone technology to realize the design and implementation of example utilities that afford free motions and computational controls. Through various use scenarios of those exemplary utilities, we show how this new form of smart agents promises new ways of interacting with our physical environments. We also discuss design implications and technical details of our implementations.</p>", "people": ["harshit@media.mit.edu", "sangwon@media.mit.edu"], "title": "L'evolved", "modified": "2018-10-12T19:55:34.922Z", "visibility": "PUBLIC", "start_on": "2015-09-11", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "l-evolved"}, {"website": "", "description": "<p>Our hectic and increasingly digital lives can have a negative effect on our health and wellbeing. Some authors have argued that we socialize less frequently with other people in person and that people feel increasingly lonely. Loneliness has been shown to significantly affect health and wellbeing in a negative way. To combat this, we designed a game, SmileCatcher, which encourages players to engage in in-person, social interactions and get others to smile. Participants wear a device that takes regular pictures of what is in front of them and the system analyzes the pictures captured to detect the number of smiles.</p>", "people": ["nfarve@media.mit.edu", "pattie@media.mit.edu"], "title": "SmileCatcher", "modified": "2016-12-05T00:16:44.188Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "smilecatcher"}, {"website": "", "description": "<p>This project steps beyond data visualizations to create data experiences. It aims to engage not only the analytic mind, but also the artistic and emotional self. In this project, chemicals found in people's bodies and homes are turned into a series of fashions. Quantities, properties, and sources of chemicals are represented through various parameters of the fashion, such as fabric color, textures, and sizes. Wearing these outfits allows people to live the data\u2014to experience tangibly the findings from their homes and bodies. This is the first project in a series of works that seek to create aesthetic data experiences that prompt researchers and laypeople to engage with information in new ways.</p>", "people": ["perovich@media.mit.edu", "vmb@media.mit.edu"], "title": "Dressed in Data", "modified": "2016-12-05T00:16:39.161Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "dressed-in-data"}, {"website": "", "description": "<p>We believe that the narrative of only listening to experts or trusting the wisdom of the crowd blindly is flawed. Instead we have developed a system that weighs experts and lay-people differently and dynamically and show that a good balance is required. We show that our methodology leads to a 15 percent improvement in mean performance, 15 percent decrease in variance, and almost 30 percent increase in Sharpe-type ratio in a real online market.</p>", "people": ["sandy@media.mit.edu", "dhaval@media.mit.edu"], "title": "Leveraging leadership expertise more effectively in organizations", "modified": "2019-04-19T14:45:04.070Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["connection-science", "human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "leveraging-leadership-expertise-more-effectively-in-organizations"}, {"website": "", "description": "<p>DbDb (pronounced DubDub) is a collaborative, visually based analysis and simulation platform. We promote open distribution of experimental data by allowing researchers to present a graphical representation of their data and processing techniques that collaborators can build on and augment. This helps test the reproducibility of results and allows others to learn and apply their own techniques. Our intention is for the research community as a whole to benefit from a growing body of open, analytical techniques. DbDb provides an interface for archiving data, executing code, and visualizing a tree of forked analyses. It is part of the Viral initiative on open, author-driven publishing, collaboration, and analysis. It is intended to be linked to PubPub, the main project.</p>", "people": ["lip@media.mit.edu", "trich@media.mit.edu"], "title": "DbDb", "modified": "2016-12-05T00:16:20.776Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "viral-communications"], "published": true, "active": false, "end_on": null, "slug": "dbdb"}, {"website": "", "description": "<p>The LilyTiny is a small sewable breakout board for ATtiny85 microcontrollers\ufffddevices which may be integrated into circuits to enable pre-determined interactions such as lights that flash or areas that can sense touch. The circuit board can be pre-loaded with a program, enabling students to incorporate dynamic behaviors into e-textile projects without having to know how to program microcontrollers.</p>", "people": ["leah@media.mit.edu"], "title": "LilyTiny", "modified": "2016-12-05T00:16:34.346Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": null, "slug": "lilytiny"}, {"website": "", "description": "<p>French for \"single shell,\" Monocoque stands for a construction technique that supports structural load using an object's external skin. Contrary to the traditional design of building skins that distinguish between internal structural frameworks and non-bearing skin elements, this approach promotes heterogeneity and differentiation of material properties. The project demonstrates the notion of a structural skin using a Voronoi pattern, the density of which corresponds to multi-scalar loading conditions. The distribution of shear-stress lines and surface pressure is embodied in the allocation and relative thickness of the vein-like elements built into the skin. Its innovative 3D printing technology provides for the ability to print parts and assemblies made of multiple materials within a single build, as well as to create composite materials that present preset combinations of mechanical properties.</p>", "people": ["neri@media.mit.edu"], "title": "Monocoque", "modified": "2016-12-14T01:48:51.182Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "monocoque"}, {"website": "http://www.gendershades.org", "description": "<p>The Gender Shades project pilots an intersectional approach to inclusive product testing for AI.</p><h1><b>Algorithmic Bias Persists</b></h1><p>Gender Shades is a preliminary excavation of the inadvertent negligence that will cripple the age of automation and further exacerbate inequality if left to fester. The deeper we dig, the more remnants of bias we will find in our technology. We cannot afford to look away this time, because the stakes are simply too high. &nbsp;We risk losing the gains made with the civil rights movement and women's movement under the false assumption of machine neutrality. Automated systems are not inherently neutral. They reflect the priorities, preferences, and prejudices\u2014the coded gaze\u2014of those who have the power to mold artificial intelligence.</p>", "people": ["ethanz@media.mit.edu", "joyab@media.mit.edu"], "title": "Gender Shades", "modified": "2018-04-30T15:22:20.722Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["center-for-civic-media", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "gender-shades"}, {"website": "", "description": "<p>GI Mobile is a mobile companion to the Media Lab Glass Infrastructure system. It incorporates the MessageMe messaging system to deliver a suite of location-aware features that complement the Glass Infrastructure. These include locating others in the Lab, browsing projects physically near you, and sending location-based messages. In addition, GI Mobile will alert you when you pass by projects you may be interested in based on what projects you have \"liked.\"</p>", "people": ["havasi@media.mit.edu"], "title": "GI Mobile", "modified": "2016-12-05T00:16:26.473Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "", "groups": ["digital-intuition"], "published": true, "active": false, "end_on": null, "slug": "gi-mobile"}, {"website": "", "description": "..", "people": ["artemd@media.mit.edu"], "title": "artemd's untitled project", "modified": "2016-09-27T20:24:24.795Z", "visibility": "LAB", "start_on": "2014-01-01", "location": "--Choose Location", "groups": [], "published": false, "active": false, "end_on": null, "slug": "artemds-untitled-project-2"}, {"website": "http://www.eduardocastello.com", "description": "<p>Swarms of robots will revolutionize many applications, from targeted material delivery to farming. However, the characteristics that make them ideal for certain future applications, such as robot autonomy or decentralized control, can also be an obstacle when transferring this technology from academia to real-world problems. Blockchain, an emerging technology, demonstrates that by combining peer-to-peer networks with cryptographic algorithms, a group of agents can reach agreements without the need for a controlling authority. The combination of blockchain with other distributed systems, such as robotic swarm systems, can provide the necessary capabilities to make robotic swarm operations more secure, autonomous, flexible, and even profitable.</p>", "people": ["ecstll@media.mit.edu", "sandy@media.mit.edu"], "title": "Blockchain: A new framework for robotic swarm systems", "modified": "2019-04-19T14:39:26.259Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "blockchain-a-new-framework-for-swarm-robotic-systems"}, {"website": "", "description": "<p>Conventional 3-D printing processes are material-dependent and irreversible. We are working on an alternative approach based on 3-D assembly of mass-produced 2-D components of digital material. This significantly enlarges the available material set, allows reversible disassembly, and imposes constraints that reduce the accumulation of local positioning errors in constructing a global shape. Experimental work on material properties and dimensional scaling of the digital material leads to application in assembling functional structures. We propose that assembling digital material will be the future of 3-D free-form fabrication of functional materials.</p>", "people": ["neilg@media.mit.edu"], "title": "Digital Fabrication", "modified": "2016-12-05T00:16:21.199Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": null, "slug": "digital-fabrication"}, {"website": "", "description": "<p>The world uses an estimated 20 million mice in laboratory research experiments each year. These experiments are monitored and regulated to protect animal welfare whenever possible, including the use of painkillers where appropriate. However, analgesics cannot completely eliminate suffering, and many studies cannot use opiates or anti-inflammatory drugs because they would interfere with the biological process being studied. The benefits of animal research may outweigh the cost in animal suffering, but it would be better to perform these experiments without animal suffering. This project seeks to develop strains of mice that experience far less pain and suffering than current animals but are equally suited to laboratory and medical research. If successful, widespread adoption of these mice could drastically reduce the total amount of animal suffering in laboratories worldwide.</p>", "people": ["esvelt@media.mit.edu"], "title": "Reducing suffering in laboratory animals", "modified": "2019-04-17T18:55:43.303Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "reducing-suffering-in-laboratory-animals-3"}, {"website": "", "description": "<p>bandicoot provides a complete, easy-to-use environment for researchers using mobile phone metadata. It allows them to easily load their data, perform analysis, and export their results with a few lines of code. It computes 100+ standardized metrics in three categories: individual (number of calls, text response rate), spatial (radius of gyration, entropy of places), and social network (clustering coefficient, assortativity). The toolbox is easy to extend and contains extensive documentation with guides and examples.</p>", "people": ["sandy@media.mit.edu", "yva@media.mit.edu"], "title": "bandicoot: A Python toolbox for mobile phone metadata", "modified": "2019-04-19T14:40:08.264Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "bandicoot-a-python-toolbox-for-mobile-phone-metadata"}, {"website": "", "description": "<p>Wearables are being widely researched for monitoring individual's health and wellbeing. Current generation wearable devices sense an individual's physiological data such as heart rate, respiration, electrodermal activity, and EEG,  but lack in sensing their biological counterparts, which drive the majority of an individual's physiological signals. On the other hand, biosensors for detecting biochemical markers are currently limited to one-time use, are non-continuous, and don't provide flexibility in choosing which biomarker they sense. We present \"wearable lab on body,\" a platform for active continuous monitoring of human biomarkers from the biological fluid.&nbsp;<br></p><p><i>To appear in IEEE Engineering for Biology and Medicine Society (EMBC) - Pataranutaporn et. al., 2019</i></p>", "people": ["pratiks@media.mit.edu", "abyjain@media.mit.edu", "patpat@media.mit.edu", "pattie@media.mit.edu"], "title": "Wearable Lab on Body", "modified": "2019-05-10T04:27:17.379Z", "visibility": "PUBLIC", "start_on": "2018-08-30", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "wearable-lab-on-body"}, {"website": "", "description": "<p>The internet&nbsp; changed&nbsp; how we create, distribute, and consume music and media. Modern digital tools for creating music and media provide \"cloud enabled\" features like automatic backups, an asset marketplace, and real-time collaboration. Despite these features, current&nbsp; tools for creating music are still based on the personal computing paradigms of the 20th century. How will media production change in the 21st century? <br></p><p>Internet engineering introduced the concept of cloud-native  applications. What would it mean for end-user experience to be truly cloud-native? This project shows a very early prototype that illustrates some of the possibilities. In this approach, the assets that make up a music project are securely exposed to the Internet, where they can be accessed and manipulated by digital services&nbsp; and and human collaborators.</p><p>The long-term goal is to give individual content creators control of their data, and a share of the benefits provided by machine learning analytics. A longer description and technical blueprint can be found in&nbsp; <a href=\"https://medium.com/@charlesholbrow/turning-the-daw-inside-out-54834d0b674a\">Turning the Digital Audio Workstation Inside Out</a>.<br></p>", "people": ["holbrow@media.mit.edu"], "title": "The SciFi Audio Workstation", "modified": "2018-10-22T00:44:23.233Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "imagining-the-future-of"}, {"website": "", "description": "<p>A library of Python 2.7 code for facilitating commonsense reasoning with ConceptNet4. This makes high-level features of the ConceptNet4 code base more accessible to application developers. It does not require that users understand matrix math or many underlying concepts. The code is extensively documented, including example calls and lots of default arguments.</p>", "people": ["lieber@media.mit.edu", "cfry@media.mit.edu"], "title": "Divisi For Dummies (D4D)", "modified": "2016-12-05T00:16:21.673Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "divisi-for-dummies-d4d"}, {"website": "", "description": "<p>The MicroPsi project explores broad models of cognition, built on a motivational system that gives rise to autonomous social and cognitive behaviors. MicroPsi agents are grounded AI agents, with neuro-symbolic representations, affect, top-down/bottom-up perception, and autonomous decision making. We are interested in finding out how motivation informs social interaction (cooperation and competition, communication and deception), learning, and playing; shapes personality; and influences perception and creative problem-solving.</p>", "people": ["joscha@media.mit.edu", "slavin@media.mit.edu"], "title": "MicroPsi: An Architecture for Motivated Cognition", "modified": "2016-12-05T00:16:37.978Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "micropsi-an-architecture-for-motivated-cognition"}, {"website": "", "description": "<p>Medina is a social-networking site based around the idea of exchanging knowledge. The project explores new interfaces for visualizing connections between people and ideas. Knowledge and interests are valuable in and of themselves, but also provide useful structures for traversing the network. The site constantly measures the interactions between people and their interests in order to provide a more accurate picture of what relationships and information are important. The goal is to build an interface that more accurately represents the state of the network. </p>", "people": ["judith@media.mit.edu"], "title": "Medina", "modified": "2016-12-05T00:16:36.057Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "medina"}, {"website": "", "description": "<p>Building on the understanding of music and architecture as creators of spatial experience, this project aims to create a novel way of unfolding music\u2019s spatial qualities in the physical world.\u200b \u200bThe objective is to create a new type of architectural typology that morphs responsively with a musical piece.\u200b \u200bPresenting spatial and musical composition as one synchronous entity. </p><p>The goal is to create a multisensory environment where music\u2019s perpetually changing characteristics reconfigure the spatial organization of a\u200b \u200b\u200bspace. This space - a hanging cube - will perform a\u200b\u200b \u200bspatial choreography of sound, movement, light and color\u200b\u200b;\u200b p\u200bresenting a dynamic\u200b \u200broom that is alive and in constant flux\u200b.\u200b \u200bThis performance will construct\u200b an \u200baesthetic \u200bexperience that challenges models of thinking, presenting a post-humanistic phenomenological encounter of the world\u200b \u200bto\u200b\u200b \u200bstretching our cognition and malleable forms.\u200b&nbsp;</p>", "people": [], "title": "Spaces That Perform Themselves", "modified": "2018-05-07T22:54:15.752Z", "visibility": "PUBLIC", "start_on": "2017-04-03", "location": "", "groups": ["opera-of-the-future"], "published": false, "active": false, "end_on": null, "slug": "spaces-that-perform-themselves"}, {"website": "", "description": "<p>Since the release of <a href=\"https://scratch.mit.edu\">Scratch</a> in 2007, young people around the world have programmed and shared more than 15 million Scratch projects\u200a. The <a href=\"https://wiki.scratch.mit.edu/wiki/Scratch_1.4\">first generation of Scratch</a> was an application that kids downloaded to local machines. With <a href=\"https://wiki.scratch.mit.edu/wiki/Scratch_2.0\">Scratch 2.0</a>, the second and current generation of Scratch, kids create and share their interactive stories, games, and animations directly in web browsers.<br></p><p><span style=\"font-size: 18px; font-weight: normal;\">Scratch 3.0 is the next generation of Scratch which takes this experience further by empowering children to create with technology on their mobile devices. In addition, Scratch 3.0 puts a special emphasis on creating with a wide variety of mediums including sound, data, and even the physical world by seamlessly integrating with IoT and digitally enhanced construction kits.</span></p>", "people": ["tmickel@media.mit.edu", "kaschm@media.mit.edu", "cwillisf@media.mit.edu", "rschamp@media.mit.edu", "ericr@media.mit.edu", "ascii@media.mit.edu", "bowman@media.mit.edu"], "title": "Scratch 3.0", "modified": "2016-12-14T20:11:47.650Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratch-3-0"}, {"website": "", "description": "<p>We present an interactive shape-changing display\u2014Dancing Membrane\u2014using the deformation of fabric and airflow control. To explore the new way of rendering an organic and natural experience with the shape-changing display, we have been experimenting with different types of fabric that can give soft textures. </p><p>In order to create and control the local deformation of fabric, we developed a 6-DOF variable diameter nozzle platform which enables us to control the direction and pressure of airflow.&nbsp;By controlling those, we were able to create variable sizes of fabric deformation and vibration of fabric with the computed simulation results. The computational model we created allowed us to predict the responsive dynamic motion of fabric to the airflow. For the next step, we hope to exhibit it as an interactive art installation\u2014as well as a shape-changing display in general for games, projection mapping with organic textures, and create a library of different types of fabric interacting with airflow for a computational simulation model.</p>", "people": ["yun_choi@media.mit.edu", "vsumini@media.mit.edu"], "title": "Dancing Membrane: Controlling the local deformation of fabric for an interactive shape-changing display and its computational simulation", "modified": "2019-04-23T21:15:19.625Z", "visibility": "LAB-INSIDERS", "start_on": "2018-07-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "dancing-membrane"}, {"website": "", "description": "<p>Facilitating opportunities for &nbsp;non-dominant youth to learn how to use technology as a tool to engage civically, and create positive change in their communities is both powerful and revolutionary. Through this curriculum we are developing, we explore activities that allow underrepresented youth to understand computer science as an accessible field of exploration that can be used to develop learners\u2019 understandings of identity, race, and power. Additionally, we\u2019ll examine how allowing underrepresented youth to explore issues in their communities and propose solutions (or create awareness) while enhancing their abilities and understanding of computer science is life-changing, and leads to an increase in participation in STEM and helps students develop as creative thinkers. This curriculum builds off the work of Seymour Papert, Mitch Resnick, and Karen Brennan, emphasizing constructionism, creative learning, and developing computational creators. Additionally, this work builds off of the many, and often unnamed, community organizers and activists in marginalized communities that continue to fight for better lives for all people - emphasizing the right importance of civic engagement and creating positive change in our own communities. &nbsp;This curriculum uses an anti-racist framework, with the belief that everyone has the right to have access to computer science education.</p>", "people": [], "title": "Purpose-Based Creative Computing with Scratch", "modified": "2019-02-19T22:09:36.329Z", "visibility": "PUBLIC", "start_on": "2018-09-26", "location": "", "groups": ["lifelong-kindergarten"], "published": false, "active": false, "end_on": null, "slug": "purpose-based-creative-computing-with-scratch"}, {"website": "", "description": "<p>Shape displays can be used to render both 3D physical content and user interface elements. We propose to use shape displays in three different ways to mediate interaction: facilitate, providing dynamic physical affordances through shape change; restrict, guiding users through dynamic physical constraints; and manipulate, actuating passive physical objects on the interface surface. We demonstrate this on a new, high-resolution shape display.</p>", "people": ["daniell@media.mit.edu", "olwal@media.mit.edu", "ishii@media.mit.edu"], "title": "inFORM", "modified": "2018-04-27T17:57:19.788Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["terrestrial-sensing", "tangible-media"], "published": true, "active": false, "end_on": null, "slug": "inform"}, {"website": "", "description": "<p>The scourge of cyberbullying has assumed worrisome proportions, with an ever-increasing number of adolescents admitting to having dealt with it either as a victim or a bystander. Anonymity and the lack of meaningful supervision in the electronic medium are two factors that have exacerbated this social menace. This project explores computational methods from natural language processing and reflective user interfaces to alleviate this problem.</p>", "people": ["picard@media.mit.edu", "lieber@media.mit.edu", "kdinakar@media.mit.edu"], "title": "Ruminati: Tackling Cyberbullying with Computational Empathy", "modified": "2016-12-05T00:16:11.046Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "ruminati-tackling-cyberbullying-with-computational-empathy"}, {"website": "", "description": "", "people": [], "title": "Beijing Initiative", "modified": "2018-04-25T16:54:28.465Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["human-dynamics"], "published": false, "active": false, "end_on": null, "slug": "beijing-initiative"}, {"website": "", "description": "<p><i>Seasons Change Together&nbsp;</i>is a collaborative song construction experience for multiple simultaneous participants. It represents a first step towards the creation of a framework, designed for interactive multiplayer musical experiences, that explores the potential for technology-enabled systems to facilitate collaborative creativity through expression, the emotional affordances of musical storytelling, and the spatiotemporal boundaries of co-presence.</p><p>Participants are presented with multiple interfaces that determine the musical texture, rhythmic patterns, and lyrical content of an interactive song. Individual participants can freely move between and share interfaces as they wish, allowing the experience to accommodate play sessions with variable user counts as well as encouraging participants to actively engage with all aspects of the song's construction through collaborative composition.</p><p>Drawing inspiration from improvisational practice in addition to game design and interactive storytelling, <i>Seasons Change Together</i> strives to open up new possibilities for experiencing music, narrative, and creativity in a social environment.</p>", "people": ["davidsu@media.mit.edu"], "title": "Seasons Change Together", "modified": "2019-05-10T15:20:33.103Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "seasons-change-together"}, {"website": "", "description": "<p>This project aims to collect and reason over common-sense knowledge in languages other than English. We have collected large bodies of common-sense knowledge in Portuguese and Korean, and we are expanding to other languages such as Spanish, Dutch, and Italian. We can use techniques based on AnalogySpace to discover correlations between languages, enabling our knowledge bases in different languages to learn from each other.</p>", "people": ["lieber@media.mit.edu"], "title": "Multilingual Common Sense", "modified": "2016-12-05T00:16:39.712Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "multilingual-common-sense"}, {"website": "", "description": "<p>A ConceptNet in English is already established and working well. We are now attempting to expand it to other languages and cultures. This project is an extended ConceptNet with Korean common-sense language, which is fundamentally different from English. Through this project, we can learn how to expand the ConceptNet into other languages and how to connect them. By connecting English and Korean ConceptNets, we are hoping not only to see cultural or linguistic differences, but also to solve problems such as the ambiguity of multivocal words, which were difficult to solve with only one ConceptNet.</p>", "people": ["lieber@media.mit.edu"], "title": "Multi-Lingual ConceptNet", "modified": "2016-12-05T00:16:39.545Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-384", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "multi-lingual-conceptnet"}, {"website": "", "description": "<p>The control of living systems as part of design interfaces is of interest to both the scientific and design communities due to the ability of living organisms to sense and respond to their environments.  They may, for example, detect and break down harmful environmental agents, or create beneficial products when environmental levels dropped below a certain threshold.  However, it is also important for these systems to be reversible, so that the biological components are only active when their functionality is necessary, and the system can remain dormant otherwise.&nbsp;</p><p>The Living Material Library is an exploration of tunable hybrid systems. Our work in this area demonstrates the means through which intrinsic material properties may be functionally changed through environmental factors and, in turn, serve as dynamic substrates for living systems. Nearly all organisms have highly developed sensing capabilities, and have been shown to behaviorally respond to changes in substrate properties. By creating a tunable and reversible material system, we explore how cell behavior such as adhesion, patterning, and differentiation may be influenced via an active interface.\n                    \n                </p><p>In this iteration, we propose a reversible material system that allows for control of living interactions (much like a light switch). We are particularly interested in fluid material systems (such as electrorheological fluids) that transition from a liquid-like to a solid-like state when exposed to electric fields and currents.&nbsp;</p><p>This endeavor brings to light the complex relationship between dynamic materials and living systems. While other methods of cell intervention often rely on light, chemicals, or temperature, here we explore substrate material properties as inputs for organisms. &nbsp;Our library may allow for more directed inquiry into processes such as collective cell durotaxis, general mechanotaxis, and active sensing. This marks an initial foray into establishing candidate design methods for responsive applications.<br></p>", "people": ["ssunanda@media.mit.edu", "bdatta@media.mit.edu", "neri@media.mit.edu", "vmb@media.mit.edu"], "title": "Living Materials Library", "modified": "2017-08-04T19:50:13.221Z", "visibility": "PUBLIC", "start_on": "2017-03-01", "location": "", "groups": ["object-based-media", "mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "living-materials-library"}, {"website": "", "description": "<p>\"For Once In Your Life...\" is a site-specific interactive radio play that uses the various sensors in a smartphone to determine specific details, such as where the user walks within a space, to dynamically affect the story. It's a blend of experiential theatre, modern choice-based interactive fiction, and audio walks such as the work of Janet Cardiff.</p>", "people": ["mslw@media.mit.edu", "slavin@media.mit.edu"], "title": "For Once In Your Life...", "modified": "2016-12-05T00:17:12.536Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["future-storytelling", "playful-systems"], "published": true, "active": false, "end_on": null, "slug": "for-once-in-your-life"}, {"website": "", "description": "<p>The values endorsed by vernacular architecture have traditionally promoted designs constructed and informed by and for the environment, while using local knowledge and indigenous materials. Under the imperatives and growing recognition of sustainable design, Rapid Craft seeks integration between local construction techniques and globally available digital design technologies to preserve, revive, and reshape these cultural traditions. </p>", "people": ["neri@media.mit.edu"], "title": "Rapid Craft", "modified": "2016-12-14T01:49:35.711Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "rapid-craft"}, {"website": "", "description": "<p>How will gene drive systems evolve once released into the wild? Can they be reliably overwritten and blocked by immunizing reversal drives? Might they spread into related species? These are difficult questions because wild populations are so much larger than laboratory colonies, meaning critical evolutionary events would never be observed in the lab. We seek to develop nematode worms as a model system to help answer these questions. Nematodes are genetically tractable, reproduce twice each week, and are readily grown in populations numbering in the billions. This allows us to study drive systems intended for other organisms in nematodes. Synthetic site targeting, split drives, and ecological confinement will prevent spread into wild nematodes. Because nematodes are easy to culture and count using Foldscope microscopes, we intend to work with educators to enable students, museum-goers, and citizen scientists to participate in gene drive research.</p>", "people": ["jmin01@media.mit.edu", "codyg@media.mit.edu", "esvelt@media.mit.edu"], "title": "Studying the evolution of gene drive systems", "modified": "2019-04-17T18:56:53.592Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "studying-the-evolution-of-gene-drive-systems"}, {"website": "", "description": "<p>Songs released on music streaming services are static, never changing after their initial release.&nbsp;Evolving Media proposes a content production and publishing pipeline, enabling artists and content creators to release media that evolves and matures as it is consumed. To take advantage of this capability, we are re-thinking the tools and processes used to create and update media content.&nbsp;</p><p>The current implementation integrates our custom augmented reality stack to rapidly iterate and publish synchronized audio/video content to the web.&nbsp;</p>", "people": ["holbrow@media.mit.edu"], "title": "Evolving Media on the Internet", "modified": "2019-04-24T13:30:28.162Z", "visibility": "PUBLIC", "start_on": "2019-03-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "evolving-media"}, {"website": "", "description": "<p><i>Evergreen Blues</i> is a suite of interactive songs that together provide a collaborative musical narrative experience in the form of a multiplayer operatic game.</p><p><dfn class=\"dictionary-of-numbers\">Two players simultaneously control </dfn>the construction and direction of a piece of music through the use of a real-time lyrical conversation system, allowing for granular control of musical expression. Choices made in <dfn class=\"dictionary-of-numbers\">one song influence the </dfn>outcomes of the next, paving the way for multi-scene interactive experiences grounded in narrative principles of persistence and emotional consequence.</p><p>This work seeks to provide a novel means of creating and understanding multi-user, interactive music systems in which users participate in active and collaborative music-making in conjunction with narrative engagement. It is the goal that this work will open up new possibilities for experiencing music, narrative, and social interaction.&nbsp;</p>", "people": ["davidsu@media.mit.edu"], "title": "Evergreen Blues", "modified": "2019-05-10T15:21:17.020Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "evergreen-blues"}, {"website": "", "description": "<p>In biology, many quantitative tools for measuring varied data streams at single cell levels are largely nonexistent. We built a solid-phase hierarchical assembly method for micron-scale structures from nano-precise DNA origami bricks. Possible applications include single cell targeted sensors and novel nanoelectronic patterning methods with resolutions well below current capabilities and with tools far cheaper.</p>", "people": ["fracchia@media.mit.edu", "neilg@media.mit.edu"], "title": "NanoAssembler: Solid Phase Synthesis of Nanometrically Precise DNA Origami Bricks", "modified": "2016-12-05T00:17:18.504Z", "visibility": "LAB", "start_on": "2012-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "nanoassembler-solid-phase-synthesis-of-nanometrically-precise-dna-origami-bricks"}, {"website": "", "description": "<p>Participants in cryptocurrency markets are in constant communication with each other about the latest coins&nbsp;and news releases. Do these conversations build hype through the contagiousness of excitement, help the&nbsp;community process information, or play some other role? Using a novel dataset from a major cryptocurrency&nbsp;forum, we conduct an exploratory study of the characteristics of online discussion around cryptocurrencies. We find that coins with more information available and higher levels of&nbsp;technical innovation are associated with higher quality discussion. People who talk about serious coins tend&nbsp;to participate in discussion displaying signatures of collective intelligence and information processing, while&nbsp;people who talk about less serious coins tend to display signatures of hype and na\u00efvety. Interviews with&nbsp;experienced forum members also confirm these quantitative findings. These results highlight the varied roles&nbsp;of discussion in the cryptocurrency ecosystem and suggest that discussion of serious coins may be oriented&nbsp;towards earnest, perhaps more accurate, attempts at discovering which coins are likely to succeed.&nbsp;</p>", "people": ["suhara@media.mit.edu", "eaman@media.mit.edu", "pkrafft@media.mit.edu", "sandy@media.mit.edu", "emoro@media.mit.edu"], "title": "Collective sensemaking in cryptocurrency community", "modified": "2019-04-19T14:41:02.807Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "collective-sensemaking-in-cryptocurrency-community"}, {"website": "", "description": "<p><a href=\"https://medium.com/mit-media-lab/shifting-priorities-finding-places-9ad3bdbe38b8\">Read more about this project here</a></p><p>MIT City Science is working with HafenCity University to develop CityScope for the neighborhood of Rothenburgsort in Hamburg, Germany. The goal is to create an interactive stakeholder engagement tool that also serves as the platform for joint research of modules for city simulation. Researchers are developing modules for walkability, neighborhood connectivity, energy efficiency, and economic activity, among others.</p>", "people": ["mdchurch@media.mit.edu", "alonsolp@media.mit.edu", "agrignar@media.mit.edu", "kll@media.mit.edu", "noyman@media.mit.edu"], "title": "City Science Lab Hamburg", "modified": "2019-05-10T19:07:36.178Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "cityscope-hamburg"}, {"website": "", "description": "<p>S.C.A.L.E. is a system for detecting localization of an external object or agent, utilizing weight and pressure as a controlling constant for the detection of place and pressure. &nbsp;The system is designed for simple prototyping of interactive products. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>", "people": ["pewebb@media.mit.edu", "achituv@media.mit.edu", "ishii@media.mit.edu"], "title": "S.C.A.L.E.", "modified": "2016-12-05T00:16:27.706Z", "visibility": "LAB-INSIDERS", "start_on": "2016-08-20", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "s-c-a-l-e"}, {"website": "http://nv.mit.edu", "description": "", "people": ["anderton@media.mit.edu", "lip@media.mit.edu"], "title": "Nuestra Vista", "modified": "2019-04-17T19:53:30.162Z", "visibility": "PUBLIC", "start_on": "2018-01-01", "location": "", "groups": ["viral-communications"], "published": false, "active": false, "end_on": null, "slug": "nuestra-vista"}, {"website": "", "description": "<p>Internet 0 is an experiment at networking at the ultra-lightweight scale. Instead of relying on the architectural notions of Internet 1 with its routing, servers, and layered network stacks, we are toying with very small, cheap, and simple ways to bring Internet Protocols all the way to the physical interface.</p>", "people": ["neilg@media.mit.edu"], "title": "Internet 0", "modified": "2016-12-05T00:17:15.197Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": null, "slug": "internet-0"}, {"website": "", "description": "<p>Computation and fabrication in biology occur in aqueous environments. Through on-chip mixing, analysis, and fabrication, microfluidic chips have introduced new possibilities in biology for over two decades. Existing construction processes for microfluidics use complex, cumbersome, and expensive lithography methods that produce single-material, multi-layered 2D chips. Multi-material 3D printing presents a promising alternative to existing methods that would allow microfluidics to be fabricated in a single step with functionally graded material properties. We aim to create multi-material microfluidic devices using additive manufacturing to replicate current devices, such as valves and ring mixers, and to explore new possibilities enabled by 3D geometries and functionally graded materials. Applications range from medicine to genetic engineering to product design.</p>", "people": ["stevenk@media.mit.edu", "moonshot@media.mit.edu", "neri@media.mit.edu"], "title": "Printing Multi-Material 3D Microfluidics", "modified": "2016-12-14T01:49:52.124Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "printing-multi-material-3d-microfluidics"}, {"website": "https://www.scratchjr.org", "description": "<p>ScratchJr makes coding accessible to younger children (ages 5-7), enabling them to program their own interactive stories, games, and animations. To make ScratchJr developmentally appropriate for younger children, we revised the interface and provided new structures to help young children learn relevant math concepts and problem-solving strategies. ScratchJr is available as a free app for iPads, Android, and Chromebook. ScratchJr is a collaboration between the MIT Media Lab, Tufts University, and Playful Invention Company.</p>", "people": ["chrisg@media.mit.edu", "tmickel@media.mit.edu", "sdg1@media.mit.edu", "mres@media.mit.edu", "ascii@media.mit.edu"], "title": "ScratchJr", "modified": "2016-12-14T20:17:13.716Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratchjr"}, {"website": "", "description": "", "people": ["heun@media.mit.edu"], "title": "Thursday morning screenshare project creation", "modified": "2016-10-06T14:24:24.211Z", "visibility": "PUBLIC", "start_on": "2016-10-06", "location": "", "groups": ["fluid-interfaces"], "published": false, "active": false, "end_on": null, "slug": "thursday-morning-screenshare-project-creation"}, {"website": "", "description": "<p>Data for refugees is a big data challenge whereby Turk Telekom opens a large dataset of anonymized mobile phone records to research groups for the purpose of providing better living conditions to Syrian refugees in Turkey.&nbsp;&nbsp;</p><p>We introduce different measures extracted from mobile phone metadata to study the integration of refugees along three dimensions: (1) social integration, (2) spatial integration, and (3) economic integration through signatures of employment activity. We use these measures to compare integration across different regions in Turkey and find striking differences both in the distributions of these dimensions and the relations between them.&nbsp;<br></p><p>The paper is currently under review but will be shared soon.&nbsp;</p>", "people": ["singhv@media.mit.edu", "alfredom@media.mit.edu", "bakker@media.mit.edu", "jobalbar@media.mit.edu", "sandy@media.mit.edu", "yleng@media.mit.edu"], "title": "Data for Refugees", "modified": "2019-04-19T14:42:49.278Z", "visibility": "PUBLIC", "start_on": "2018-06-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "data-for-refugees"}, {"website": "", "description": "<p>If one considers true costs, bicycles are by far the most efficient form of transportation. We explore how individual actors in a city can collaborate asynchronously to create a city-wide bike-sharing network. At the core of this system is a bike that unlocks with your phone or RFID. The lock is designed as a kickstand, so that when users in the network shakes a bicycle in this network, the bicycle recognizes them, unlocks the bike for them, and allows them to ride, then charging for the time they spend before putting the kickstand down, thus re-engaging the lock. Such a system allows micro-entrepreneurs to keep adding bikes in the network in a peer-to-peer fashion, benefiting both the users and the providers.</p>", "people": ["sdkamvar@media.mit.edu"], "title": "Organic Bike Networks", "modified": "2016-12-05T00:16:42.700Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "organic-bike-networks"}, {"website": "", "description": "<p>Opus is an online tool exploring the work and trajectory of scholars. Through a suite of interactive visualizations, Opus help users explore the academic impact of a scholar's publications, discover her network of collaborators, and identify her peers.</p>", "people": ["hidalgo@media.mit.edu"], "title": "Opus", "modified": "2016-12-05T00:16:42.674Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "opus"}, {"website": "", "description": "<p>Newspaper front pages are a key source of data about our media ecology. Newsrooms spend massive time and effort deciding what stories make it to the front page. PageOneX makes coding and visualizing newspaper front page content much easier, democratizing access to newspaper attention data. Communication researchers have analyzed newspaper front pages for decades, using slow, laborious methods. PageOneX simplifies, digitizes, and distributes the process across the net and makes it available for researchers, citizens, and activists.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "elplatt@media.mit.edu"], "title": "PageOneX", "modified": "2016-12-05T00:16:43.038Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "pageonex"}, {"website": "", "description": "<p>The recent availability of quantitative behavioral data provides an opportunity to study human behavior at unprecedented scale. Using large-scale financial transaction data, we propose a novel deep learning framework for understanding human purchase patterns and testing the link between them and the existence of individual financial troubles. Our work opens new possibilities in studying human behavioral traits using state-of-the-art machine learning techniques, without the need for hand-engineered features.</p>", "people": ["suhara@media.mit.edu", "sandy@media.mit.edu", "xdong@media.mit.edu"], "title": "DeepShop: Understanding purchase patterns via deep learning", "modified": "2019-04-19T14:43:35.517Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "deepshop-understanding-purchase-patterns-via-deep-learning"}, {"website": "", "description": "<p>A crucial part of Montessori education is observation of the students, so teachers can assist individuals and structure the environment as needed. Our work aims to assist this observation by measuring proximity of students through Simblee COM sensors. We provide detailed visualizations in a dashboard-style interface to both teachers and parents. This dashboard helps teachers individualize their own methods to facilitate a child's growth in the classroom.</p>", "people": ["sdkamvar@media.mit.edu", "saquib@media.mit.edu", "aybose@media.mit.edu"], "title": "Proximity Networks", "modified": "2016-12-05T00:16:46.304Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "--Choose Location", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "proximity-networks"}, {"website": "", "description": "Test test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test testTest test test test test test test test test", "people": [], "title": "plain's untitled project", "modified": "2016-09-27T20:24:34.178Z", "visibility": "LAB", "start_on": "2014-01-01", "location": "--Choose Location", "groups": [], "published": false, "active": false, "end_on": null, "slug": "plains-untitled-project-2"}, {"website": "", "description": "<p>Exploring conductive inks as an expressive medium for narrative storytelling, StoryClip synthesizes electrical functionality, aesthetics, and creativity, to turn a drawing into a multimedia interface that promotes rich engagement with children.</p>", "people": ["leah@media.mit.edu"], "title": "StoryClip", "modified": "2016-12-05T00:16:52.783Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": null, "slug": "storyclip"}, {"website": "", "description": "<p>Raycounting is a method for generating customized light-shading constructions by registering the intensity and orientation of light rays within a given environment. 3D surfaces of double curvature are the result of assigning light parameters to flat planes. The algorithm calculates the intensity, position, and direction of one or multiple light sources placed in a given environment, and assigns local curvature values to each point in space corresponding to the reference plane and the light dimension. Light performance analysis tools are reconstructed programmatically to allow for morphological synthesis based on intensity, frequency, and polarization of light parameters as defined by the user.</p>", "people": ["neri@media.mit.edu"], "title": "Raycounting", "modified": "2016-12-14T01:50:24.480Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "raycounting"}, {"website": "", "description": "", "people": [], "title": "f;lsdfj;zlkcjf;z  tjxklxz lk", "modified": "2016-10-18T01:06:56.622Z", "visibility": "PUBLIC", "start_on": "2016-10-13", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "f-lsdfj-zlkcjf-z-tjxklxz-lk"}, {"website": "", "description": "<p>Cooperation in a large society of self-interested individuals is notoriously difficult to achieve when the externality of one individual's action is spread thin and wide. This leads to the \"tragedy of the commons,\" with rational action ultimately leaving everyone worse off. Traditional policies to promote cooperation involve Pigouvian taxation or subsidies that make individuals internalize the externality they incur. We introduce a new approach to achieving global cooperation by localizing externalities to one's peers in a social network, thus leveraging the power of peer pressure to regulate behavior. The mechanism relies on a joint model of externalities and peer-pressure. Surprisingly, this mechanism can require a lower budget to operate than the Pigouvian mechanism, even when accounting for the social cost of peer pressure. Even when the available budget is very low, the social mechanisms achieve greater improvement in the outcome.</p>", "people": ["sandy@media.mit.edu", "dhaval@media.mit.edu", "shrier@media.mit.edu"], "title": "Incentivizing cooperation using social pressure", "modified": "2019-04-19T14:44:23.786Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["connection-science", "human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "incentivizing-cooperation-using-social-pressure"}, {"website": "", "description": "<p>PostHistory visualizes users' email mailboxes and the activity patterns within them. The idea is not to create yet another email reader, but instead is an attempt at making activity patterns within one's email account visible and visually interesting. Some of the questions in the project are: how much email do you get on any given day?; how active are you in replying, deleting, or moving messages around?; and how different does your mailbox activity look from someone else's?</p>", "people": ["judith@media.mit.edu"], "title": "PostHistory", "modified": "2016-12-05T00:16:45.356Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "posthistory"}, {"website": "", "description": "<p>We are using signaling theory to explore how identity is presented and perceived in online environments. The fundamental idea is that many qualities we are interested in knowing about others are not directly observable. Instead, we rely on signals that are more or less reliably correlated with the quality. Sometimes signals are very reliable indicators of a particular quality, but sometimes they are not. The power of signaling theory is that it provides a means of evaluating the reliability of signals. We are using this approach to analyze social networking sites, reputation systems, graphical representations, and humanoid agents. </p>", "people": ["judith@media.mit.edu"], "title": "Identity Signals", "modified": "2016-12-05T00:16:31.118Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "identity-signals"}, {"website": "", "description": "<p>Most countries are projected to see the number of people ages 65 and older surpass the population under the age of 15 by 2050. The limitations of current solutions to assisting older adults, the increased social and emotional toll on caregivers, and the inability of institutions to create structural solutions in a timely manner calls for a paradigm shift in the way we approach aging.</p><p>As these new meanings of age, aged, and aging are re-negotiated at a personal and collective level, the <b>main goal of this research initiative is to&nbsp;study aging adults\u2019 daily living assistance, social and emotional needs, and intergenerational connection</b> while exploring the optimized modalities for embodied agents to successfully deliver these interactions.&nbsp;We see embodied agents as a method to enable older adults to age-in-place, supporting them in ways such as promoting social connectedness, tracking vitals, coaching in emotional wellness, and assisting with medical adherence.</p><p>Our work is rooted in partnering with the community through co-design and participatory design methods to inform robot design by empowering older adults to engage in our research. We prioritize developing robot interactions that can be tested long-term in older adults\u2019 homes to better inform how social robots can shape aging-in-place.<br></p><p>Currently, we are running a long-term codesign study with older adults. Over the course of the year, older adults will engage in interviews, interactive artwork, living with a robot, prototyping on a robot, and design guideline generation.&nbsp;</p><p>If you are 70 years of age or older and interested in participating in future study opportunities, please contact Anastasia Ostrowski (akostrow@media.mit.edu).</p>", "people": ["cynthiab@media.mit.edu", "akostrow@media.mit.edu", "nikhita@media.mit.edu", "haewon@media.mit.edu"], "title": "Designing social robots for older adults", "modified": "2019-05-10T19:51:29.310Z", "visibility": "PUBLIC", "start_on": "2017-06-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "designing-social-robots-for-older-adults"}, {"website": "", "description": "<p>The LilyPad Arduino is a set of tools that empowers people to build soft, flexible, fabric-based computers. A set of sewable electronic modules enables users to blend textile craft, electrical engineering, and programming in surprising and beautiful ways. A series of workshops that employed the LilyPad have demonstrated that tools such as these, which introduce engineering from new perspectives, are capable of involving unusual and diverse groups in technology development. Ongoing research will explore how the LilyPad and similar devices can engage under-represented groups in engineering, change popular assumptions about the look and feel of technology, and spark hybrid communities that combine rich crafting traditions with high-tech materials and processes. </p>", "people": ["leah@media.mit.edu"], "title": "LilyPad Arduino", "modified": "2016-12-05T00:16:34.398Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-368", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": null, "slug": "lilypad-arduino"}, {"website": "", "description": "<p>The PCB Origami project is an innovative concept for printing digital materials and creating 3D objects with Rigid-flex PCBs and pick-and-place machines. These machines allow printing of digital electronic materials, while controlling the location and property of each of the components printed. By combining this technology with Rigid-flex PCB and computational origami, it is possible to create from a single sheet of PCB almost any 3D shape that is already embedded with electronics, to produce a finished product with that will be both structural and functional. </p>", "people": ["neri@media.mit.edu"], "title": "PCB Origami", "modified": "2016-12-14T01:51:18.957Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "pcb-origami"}, {"website": "", "description": "<p>The world uses an estimated 20 million mice in laboratory research experiments each year. These experiments are monitored and regulated to protect animal welfare whenever possible, including the use of painkillers where appropriate. However, analgesics cannot completely eliminate suffering, and many studies cannot use opiates or anti-inflammatory drugs because they would interfere with the biological process being studied. The benefits of animal research may outweigh the cost in animal suffering, but it would be better to perform these experiments without animal suffering. This project seeks to develop strains of mice that experience far less pain and suffering than current animals but are equally suited to laboratory and medical research. If successful, widespread adoption of these mice could drastically reduce the total amount of animal suffering in laboratories worldwide.</p>", "people": ["esvelt@media.mit.edu"], "title": "Reducing Suffering in Laboratory Animals", "modified": "2016-12-05T00:17:21.835Z", "visibility": "LAB", "start_on": "2016-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "reducing-suffering-in-laboratory-animals-2"}, {"website": "http://www.adamjhh.com/", "description": "<p>Sleep is a forgotten country of the mind. A vast majority of our technologies are built for our waking state, even though a third of our lives are spent asleep. Current technological interfaces miss an opportunity to access the unique, imaginative, elastic cognition ongoing during dreams and semi-lucid states. In turn, each of us misses an opportunity to use interfaces to influence our own processes of memory consolidation, creative insight generation, gist extraction, and emotion regulation that are so deeply sleep-dependent.&nbsp;In this project, we explore ways to augment human creativity by extending, influencing, and capturing dreams in stage-1 sleep. It is currently impossible to force ourselves to be creative because so much creative idea association and creative incubation happens in the absence of executive control and directed attention. Sleep offers an opportunity for prompting creative thought in the absence of directed attention, if only dreams can be controlled.</p><p>During sleep onset, a window of opportunity arises in the form of hypnagogia, a semi-lucid sleep state where we all begin dreaming before we fall fully unconscious. Hypnagogia is characterized by phenomenological unpredictability, distorted perception of space and time, and spontaneous, fluid idea association. Edison, Tesla, Poe, and Dal\u00ed each accessed this state by napping with a steel ball in hand to capture creative ideas generated in hypnagogic microdreams when it dropped to the floor below.</p><p>In this project we modernize this technique, using an interactive social robot accompanied with an EEG system, muscular sleep stage tracking system, and auditory biofeedback. We are able to influence, extract information from, and extend hypnagogic microdreams for the first time: we found that active use of hypnagogia with the system can augment human creativity. This system enables future research into sleep, an underutilized and understudied state of mind vital for memory, learning, and creativity.</p><p>This work has been hugely collaborative. The following people, in alphabetical order by first name, have all made it possible: Abhinandan Jain, Eyal Perry, Ishaan Grover, Matthew Ha, Oscar Rosello, Pedro Reynolds-Cu\u00e9llar, Robert Stickgold, and Tom\u00e1s Vega. For an in depth dive, see the FAQ below and see more on <a href=\"http://adamjhh.com/dormio\">this website</a>.</p><br>", "people": ["igrover@media.mit.edu", "adamjhh@media.mit.edu", "tomasero@media.mit.edu", "abyjain@media.mit.edu", "rosello@media.mit.edu", "pcuellar@media.mit.edu"], "title": "Dormio: Interfacing with Dreams", "modified": "2019-03-26T14:01:38.378Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "sleep-creativity"}, {"website": "", "description": "<p>Within the last few years, cellphone subscriptions have spread widely and now cover even the remotest parts of the planet. Adequate access to healthcare, however, is not widely available, especially in developing countries. We propose a new approach to converting cellphones into low-cost scientific devices for microscopy. Cellphone microscopes have the potential to revolutionize health-related screening and analysis for a variety of applications, including blood and water tests. Our optical system is more flexible than previously proposed mobile microscopes, and allows for wide field-of-view panoramic imaging, the acquisition of parallax, and coded background illumination, which optically enhances the contrast of transparent and refractive specimens.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "Single Lens Off-Chip Cellphone Microscopy", "modified": "2016-12-05T00:17:02.653Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "single-lens-off-chip-cellphone-microscopy"}, {"website": "", "description": "<p>We are experimenting with technology for low-cost, large-area input devices, using change-source tomography on an electrically resistive medium.</p>", "people": ["neilg@media.mit.edu", "rehmi@media.mit.edu"], "title": "Resistive Sheets", "modified": "2016-12-05T00:16:47.870Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": null, "slug": "resistive-sheets"}, {"website": "", "description": "<p>We are using an online poker game as a platform for studying how people share and interpret non-verbal social information online.</p>", "people": ["judith@media.mit.edu"], "title": "PokerSpaces: Hiding and Revealing Social Information Online", "modified": "2016-12-05T00:17:20.266Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-391", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "pokerspaces-hiding-and-revealing-social-information-online"}, {"website": "http://nightmare.mit.edu/", "description": "<p>For centuries, across geographies, religions, and cultures, people try to innovate ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity. This challenge is especially important in a time when we wonder what the limits of Artificial Intelligence are: Can machines learn to scare us? Towards this goal, we present you Haunted Faces and Haunted Places: computer generated scary imagery powered by deep learning algorithms!\n                    \n                </p>", "people": ["cebrian@media.mit.edu", "pinary@media.mit.edu", "irahwan@media.mit.edu", "nobradov@media.mit.edu"], "title": "Nightmare Machine", "modified": "2017-01-31T04:41:35.401Z", "visibility": "PUBLIC", "start_on": "2016-10-20", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "nightmare-machine"}, {"website": "", "description": "<p>What an am;zing research project</p>", "people": ["kll@media.mit.edu"], "title": "Test project 265", "modified": "2019-03-27T20:19:27.918Z", "visibility": "GROUP", "start_on": "2019-03-26", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "test-project-265"}, {"website": "", "description": "<p>DataBasic is a suite of web-based tools that give people fun and relevant ways learn how to work with data. Existing tools focus on operating on data quickly to create some output, rather than focusing on helping learners understand how to work with data. This fails the huge population of data literacy learners, who are trying to build their capacity in various ways. Our tools focus on the user as learner. They provide introductory activities, connect to people with fun sample datasets, and connect to other tools and techniques for working with data. We strongly believe in building tools focused on learners, and are putting those ideas into practice on these tools and activities. Visit <a href=\"https://databasic.io\">databasic.io</a> today to try it out!</p>", "people": ["dignazio@media.mit.edu", "ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "DataBasic", "modified": "2018-04-30T15:30:39.371Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["center-for-civic-media", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "databasic"}, {"website": "", "description": "<p>Pasts and Presents is a visualization of activity in a space, both current and in the past. The visualization is an abstract, animated image in which the movements of the elements are shaped by the actions of the people passing by; it is an abstract visualization in much the same way that ripples on a pond are a visualization of activity on and near the surface of the water.</p>", "people": ["judith@media.mit.edu"], "title": "Pasts and Presents", "modified": "2016-12-05T00:16:43.325Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-391", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "pasts-and-presents"}, {"website": "", "description": "<p>Current unmotorized prostheses do not provide adequate energy return during late stance to improve level-ground locomotion. Robotic prostheses can provide power during late-stance to improve metabolic economy in an amputee during level-ground walking. This project seeks to improve the types of terrain a robotic ankle can successfully navigate by using command signals taken from the intact and residual limbs of an amputee. By combining these command signals with sensors attached to the robotic ankle, it might be possible to further understand the role of physiological signals in the terrain adaptation of robotic ankles.</p>", "people": ["hherr@media.mit.edu"], "title": "Sensor-fusions for an EMG controlled robotic prosthesis", "modified": "2019-04-26T19:12:04.371Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "sensor-fusions-for-an-emg-controlled-robotic-prosthesis"}, {"website": "", "description": "<p>What motivates people? What changes do people want in the world? We approach questions of this kind by mining goals and plans from text-based websites: wikiHow, eHow, 43things, to-do lists, and commonsense knowledge bases. 43things tells us about people's long term ambitions. How-to instructions and to-do lists tell us about everyday activities. We've analyzed the corpus to find out which goals are most popular, controversial, and concealed. The resulting goal network can be used for plan recognition, natural language understanding, and building intelligent interfaces that understand why they are being used. Come by and learn about how you can use this knowledge about actions/goals, their properties (cost, duration, location), and their relations in your own applications.</p>", "people": ["lieber@media.mit.edu"], "title": "Human Goal Network", "modified": "2016-12-05T00:16:30.052Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "human-goal-network"}, {"website": "", "description": "<p>We present a new method for scanning 3D objects through a single-shot, shadow-based method. We decouple 3D occluders from 4D illumination using shield fields: the 4D attenuation function which acts on any light field incident on an occluder. We then analyze occluder reconstruction from cast shadows, leading to a single-shot light-field camera for visual hull reconstruction.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "Shield Field Imaging", "modified": "2016-12-05T00:16:50.320Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-044A", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "shield-field-imaging"}, {"website": "", "description": "<p>To explore future mobility modes, the City Science group is working with Media Lab member company Panasonic to explore the use and potential adaptations of the popular&nbsp;<i>MamaChari</i>&nbsp;bikes. &nbsp;Like other mobility modes, the&nbsp;<i>MamaChari</i>&nbsp;bikes have developed and adapted over the past decades. &nbsp;Bikes for women first became popular during Japan\u2019s economic boom in the 1980s when many households benefited from one income, and women were encouraged to stay home and take care of their children. &nbsp;Women used bikes to quickly navigate their cities and make frequent trips to shops and schools, kids in tow. Even as women gradually entered the workforce in the 1990s and 2000s, the stereotype of the Japanese biking woman remained. &nbsp;By 2008, electric assist bikes were introduced to the market, and again they targeted women with children as the primary users. Today&nbsp;<i>MamaChari</i>&nbsp;bikes are stable, secure and ubiquitous in Japan, yet they have yet to enter other global markets.&nbsp;&nbsp;<br></p><p>The City Science group strives to understand current uses of the MamaChari and adapt the bike for new and future uses globally. Ideation workshops were completed in February and May 2018.</p><p>Learn more about the first workshop here:&nbsp;&nbsp;<a href=\"https://www.media.mit.edu/posts/mamachari/\">https://www.media.mit.edu/posts/mamachari/</a></p>", "people": ["yasushis@media.mit.edu"], "title": "Hackable Bike", "modified": "2018-06-19T18:01:03.323Z", "visibility": "PUBLIC", "start_on": "2018-02-28", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "hackable-bike"}, {"website": "http://20daystranger.net/", "description": "<p>20 Day Stranger is a mobile app that creates an intimate and anonymous connection between you and another person. For 20 days, you get continuous updates about where they are, what they are doing, and eventually even how they are feeling, and them likewise about you. But you will never know who this person is. Does this change the way you think about other people you see throughout your day, any one of which could be your stranger?</p>", "people": ["tjlevy@media.mit.edu", "slavin@media.mit.edu", "cwwang@media.mit.edu"], "title": "20 Day Stranger", "modified": "2016-12-05T00:17:00.445Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "20-day-stranger"}, {"website": "", "description": "<p>The ability to record images with extreme temporal resolution enables a diverse range of applications, such as time-of-flight depth imaging and characterization of ultrafast processes. Here we present a demonstration of the potential of single-photon detector arrays for visualization and rapid characterization of events evolving on picosecond time scales. The single-photon sensitivity, temporal resolution, and full-field imaging capability enables the observation of light-in-flight in air, as well as the measurement of laser-induced plasma formation and dynamics in its natural environment. The extreme sensitivity and short acquisition times pave the way for real-time imaging of ultrafast processes or visualization and tracking of objects hidden from view. </p>", "people": ["raskar@media.mit.edu", "barmak@media.mit.edu"], "title": "Single-Photon Sensitive Ultrafast Imaging", "modified": "2016-12-05T00:17:02.706Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "terrestrial-sensing", "camera-culture"], "published": true, "active": false, "end_on": null, "slug": "single-photon-sensitive-ultrafast-imaging"}, {"website": "", "description": "<p>How can additive fabrication technologies be scaled to building-sized construction? We introduce a novel method of mobile swarm printing that allows small robotic agents to construct large structures. The robotic agents extrude a fast-curing material which doubles as both a concrete mold for structural walls and as a thermal insulation layer. This technique offers many benefits over traditional construction methods, such as speed, custom geometry, and cost. As well, direct integration of building utilities such as wiring and plumbing can be incorporated into the printing process. This research was sponsored by the NSF EAGER award: Bio-Beams: FGM Digital Design &amp; Fabrication.</p>", "people": ["stevenk@media.mit.edu", "neri@media.mit.edu", "j_klein@media.mit.edu"], "title": "Building-Scale 3D Printing", "modified": "2016-12-14T01:51:39.690Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "building-scale-3d-printing"}, {"website": "", "description": "", "people": [], "title": "Test Project", "modified": "2016-10-26T15:50:09.564Z", "visibility": "LAB", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "test-project"}, {"website": "", "description": "..", "people": ["jon@media.mit.edu"], "title": "jon's untitled project", "modified": "2018-10-05T18:02:46.646Z", "visibility": "LAB", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["ultimate-media"], "published": true, "active": false, "end_on": null, "slug": "jons-untitled-project-2"}, {"website": "", "description": "<p>Patient adherence to physical therapy regimens is poor, and there is a lack of quantitative data about patient performance, particularly at home. This project is an end-to-end virtual rehabilitation system for supporting patient adherence to home exercise that addresses the multi-factorial nature of the problem. The physical therapist and patient make shared decisions about appropriate exercises and goals and patients use a sensor-enabled gaming interface at home to perform exercises. Quantitative data is then fed back to the therapist, who can properly adjust the regimen and give reinforcing feedback and support.</p>", "people": ["fmoss@media.mit.edu", "jom@media.mit.edu"], "title": "Oovit PT", "modified": "2016-12-05T00:16:42.147Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["new-media-medicine"], "published": true, "active": false, "end_on": null, "slug": "oovit-pt"}, {"website": "", "description": "<p>Introducing the&nbsp;newest edition of the gamma musical/medical instruments - the&nbsp;Gamma MOON (<b>M</b>usical <b>O</b>mnisensory <b>O</b>rbital <b>N</b>euroinstrument).&nbsp;This instrument features a capacitive interface which delivers multisensory gamma stimulation through audio, visual, haptic and tactile feedback. In collaboration with the Aging Brain Alzheimer\u2019s Initiative at MIT, the Gamma MOON pilots a novel treatment form-factor with the goal of device deployment in large-scale clinical trials. Research reveals that gamma instrument interaction can strengthen cognitive function and sensory perception while increasing focus even in neurocognitively healthy individuals.&nbsp;Contact arrangement allows both patients and performers to create high-level musical abstractions as well as follow traditional notational melodies.&nbsp;&nbsp;Gamma MOON's heightened sensorial engagement recruits increased cognitive entrainment, multimodal expression and creative freedom.&nbsp;</p>", "people": [], "title": "Gamma MOON: Musical Omnisensory Orbital Neuroinstrument", "modified": "2018-10-18T04:39:29.389Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "gamma-moon-musical-omnisensory-orbital-neuroinstrument"}, {"website": "", "description": "", "people": [], "title": "Networked Playscapes: Listentree", "modified": "2016-10-25T14:45:41.947Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "networked-playscapes-listentree"}, {"website": "", "description": "<p>We are designing new forms of situated displays and interfaces embedded in public spaces. Our current focus is on interfaces that can function as social sensors, capturing the dynamics of people in different locations. We design applications that engage with users with alternative models of input for content generation that foster collective behavior in public.</p>", "people": ["judith@media.mit.edu"], "title": "Sociomedia Garden", "modified": "2016-12-05T00:16:51.606Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-391", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "sociomedia-garden"}, {"website": "", "description": "<p>The major challenge in preventing blindness is identifying patients and bringing them to specialty care. Diseases that affect the retina, the image sensor in the human eye, are particularly challenging to address, because they require highly trained eye specialists (ophthalmologists) who use expensive equipment to visualize the inner parts of the eye. Diabetic retinopathy, HIV/AIDS-related retinitis, and age-related macular degeneration are three conditions that can be screened and diagnosed to prevent blindness caused by damage to retina. We exploit a combination of two novel ideas to simplify the constraints of traditional devices, with simplified optics and cleaver illumination in order to capture and visualize images of the retina in a standalone device easily operated by the user. Prototypes are conveniently embedded in either a mobile hand-held retinal camera, or wearable eyeglasses.</p>", "people": ["raskar@media.mit.edu", "elawson@media.mit.edu", "olwal@media.mit.edu", "gordonw@media.mit.edu", "naik@media.mit.edu"], "title": "Portable Retinal Imaging", "modified": "2016-12-05T00:16:45.247Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "portable-retinal-imaging"}, {"website": "", "description": "<p>Themail visualizes the content of a person's email archive over time. The project focuses on the unique content found in interactions between the owner of the email archive and each contact.</p>", "people": ["judith@media.mit.edu"], "title": "Themail", "modified": "2016-12-05T00:16:55.516Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "themail"}, {"website": "", "description": "<p>You Are Here is an experiment in microurbanism.  In this project, we are creating 100 maps each of 100 different cities. Each map gives a collective portrait of one aspect of life in the city, and is designed to give communities meaningful micro-suggestions of what they might do to improve their city. The interplay between the visualizations and the community work they induce creates a collective, dynamic, urban-scale project. </p>", "people": ["sdkamvar@media.mit.edu", "manassra@media.mit.edu", "zhangjia@media.mit.edu", "srife@media.mit.edu", "yonatanc@media.mit.edu", "pranavr@media.mit.edu"], "title": "You Are Here", "modified": "2017-04-03T16:06:20.214Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "you-are-here"}, {"website": "", "description": "<p>Vespers is a collection of masks exploring what it means to design (with) life. From the relic of the death mask to a contemporary living device, the collection embarks on a journey that begins with an ancient typology and culminates with a novel technology for the design and digital fabrication of adaptive and responsive interfaces. We begin with a conceptual piece and end with a tangible set of tools, techniques and technologies combining programmable matter and programmable life.</p><p>The project points towards an imminent future where wearable interfaces and building skins are customized not only to fit a particular shape, but also a specific material, chemical and even genetic make-up, tailoring the wearable to both the body and the environment which it inhabits.</p><p>Imagine, for example, a wearable interface designed to guide ad-hoc antibiotic formation customized to fit the genetic makeup of its user; or, consider smart packaging or surface coatings devices that can detect contamination; finally, consider environmentally responsive architectural skins that can respond to, and adapt\u2014in real time\u2014to environmental cues. Research at the core of this project offers a new design space for biological augmentation across a wide breadth of application domains, leveraging resolution and scale.</p><p>The collection includes three series. The first series features the death mask as a cultural artefact. The final series features a living mask as an enabling technology. The second series mediates between the two, marking the process of \u2018metamorphosis\u2019 between the ancient relic and its contemporaneous interpretation.The living masks in the final series embody habitats that guide, inform and \u2018template\u2019 gene expression of living microorganisms. Such microorganisms have been synthetically engineered to produce pigments and/or otherwise useful chemical substances for human augmentation such as vitamins, antibodies or antimicrobial drugs.Combined, the three series of the Vespers collection represent the transition from death to life, or from life to death, depending on one\u2019s reading of the collection.</p>", "people": ["ssunanda@media.mit.edu", "bader_ch@media.mit.edu", "rssmith@media.mit.edu", "jpcosta@media.mit.edu", "limulus@media.mit.edu", "neri@media.mit.edu"], "title": "Vespers III", "modified": "2018-10-23T15:19:03.760Z", "visibility": "PUBLIC", "start_on": "2018-04-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "vespers-iii"}, {"website": "", "description": "<p>Analogy Space, a previous project under the Digital Intuition group, developed a technique of plotting concepts in a many-dimensional semantic space in order to identify clusters of concepts that are similar to each other.  Story Space will apply this technique to human narrative in order to provide a measure of similarity between different stories.  It has had preliminary success using datasets that are easily broken up into discrete events, such as \"how-to\" articles from the Internet.  The next steps involve using automatic event taggers to determine the progression of a story.</p>", "people": ["havasi@media.mit.edu"], "title": "Story Space", "modified": "2016-12-05T00:16:52.703Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "", "groups": ["digital-intuition"], "published": true, "active": false, "end_on": null, "slug": "story-space"}, {"website": "", "description": "<p>In this project we present our exploration of several visualization techniques which use motion as the primary visual element for depicting online discussions. Visual attention appeals most strongly to motion. Since motion changes the immediate environment conditions, it is more likely than static visual cues to attract our attention. Motion is dynamic and vibrant; it is intuitive and suitable for representing large-scale online social data such as online discussions.  Our designs and findings shed light on choosing the mappings between motion and social characteristics for creating intuitive and legible visualizations.</p>", "people": ["judith@media.mit.edu"], "title": "Exploration of Motion to Visualize Large-Scale Online Discussions", "modified": "2016-12-05T00:16:08.250Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-390", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "exploration-of-motion-to-visualize-large-scale-online-discussions"}, {"website": "", "description": "<p>This concept gallery shows the chain of startups and ideas that will follow after the emergence of self-driving cars.</p>", "people": ["raskar@media.mit.edu", "barmak@media.mit.edu"], "title": "Beyond the Self-Driving Car ", "modified": "2016-12-05T00:16:14.777Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "beyond-the-self-driving-car"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: 400;\">This work explores a dynamic future in which the accessories we wear are no longer static, but are instead mobile, living objects on the body. Engineered with the functionality of miniaturized robotics, this \"living\" jewelry roams on unmodified clothing, changing location and reconfiguring appearance according to social context and enabling multiple presentations of self. With the addition of sensor devices, they can actively respond to environmental conditions. They can also be paired with existing mobile devices to become personalized on-body assistants to help complete tasks. Attached to garments, they generate shape-changing clothing and kinetic pattern designs\u2014creating a new, dynamic fashion.</span><br></p><p> </p><p><span style=\"font-size: 18px; font-weight: 400;\">It is our vision that in the future, these robots will be miniaturized to the extent that they can be seamlessly integrated into existing practices of body ornamentation. With the addition of kinetic capabilities, traditionally static jewelry and accessories will start displaying life-like qualities, learning, shifting, and reconfiguring to the needs and preferences of the wearer, also assisting in fluid presentation of self. With wearables that possess hybrid qualities of the living and the crafted, we explore a new on-body ecology for human-wearable symbiosis.</span><span style=\"font-size: 18px; font-weight: 400;\">&nbsp;</span></p>", "people": ["artemd@media.mit.edu", "dmajilo@media.mit.edu", "cindykao@media.mit.edu", "geek@media.mit.edu"], "title": "Kino", "modified": "2017-10-17T03:54:26.236Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "kino-kinetic-wearable"}, {"website": "", "description": "", "people": [], "title": "DataVRse", "modified": "2016-12-15T20:13:20.392Z", "visibility": "PUBLIC", "start_on": "2016-10-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "datavrse"}, {"website": "http://www.adamjhh.com/", "description": "<p>The Blank Canvas directs immersion inwards using virtual reality, augmenting awareness of the microscopic worlds inside each of us and the science that is changing them today. It has been shown at Cannes Film Festival, Vision Summit, VR Sci Fest and the World Economic Forum.</p><p><a href=\"https://www.youtube.com/watch?v=wo7iz8UgkYI\">This is the first episode of <i>The Blank Canvas</i></a>, a VR platform that showcases the future of science and scientific communication. So many of the brilliant contemporary innovations in science are lost to the general public because they happen at scales so small we can barely comprehend them. The Blank Canvas leverages the power of immersive technologies &nbsp;to make these ideas come to life in macro planetary scale, explaining themes like DNA editing, hacked viruses and CRISPR. We build collaborations between scientists and engineers for accurate, inspirational science storytelling that turns textbooks into experience.&nbsp;</p>", "people": ["adamjhh@media.mit.edu"], "title": "The Blank Canvas", "modified": "2018-10-20T17:52:17.331Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "the-blank-canvas"}, {"website": "", "description": "", "people": [], "title": "Tools to Investigate Societal Impact of Robots", "modified": "2018-07-11T18:20:53.033Z", "visibility": "LAB-INSIDERS", "start_on": "2018-07-01", "location": "", "groups": ["personal-robots"], "published": false, "active": false, "end_on": null, "slug": "tools-to-investigate-societal-impact-of-robots"}, {"website": "", "description": "<p>We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: What billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, Big Data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.</p>", "people": ["raskar@media.mit.edu", "ajdas@media.mit.edu"], "title": "Nashik Smart Citizen Collaboration with TCS", "modified": "2016-12-05T00:17:18.564Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["emerging-worlds", "camera-culture"], "published": true, "active": false, "end_on": null, "slug": "nashik-smart-citizen-collaboration-with-tcs"}, {"website": "", "description": "<p>This project builds a social, place-based information window into the Media Lab using 30 touch-sensitive screens strategically placed throughout the physical complex and at sponsor sites. The idea is get people to talk among themselves about the work that they jointly explore in a public place. We present Lab projects as dynamically connected sets of \"charms\" that visitors can save, trade, and explore. The GI demonstrates a framework for an open, integrated IT system and shows new uses for it.</p>", "people": ["ypod@media.mit.edu", "havasi@media.mit.edu", "borovoy@media.mit.edu", "holtzman@media.mit.edu", "mbletsas@media.mit.edu", "jon@media.mit.edu", "lip@media.mit.edu"], "title": "The Glass Infrastructure (GI)", "modified": "2018-10-12T22:19:06.789Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["necsys", "viral-communications"], "published": true, "active": false, "end_on": null, "slug": "the-glass-infrastructure-gi"}, {"website": "", "description": "<p>Complications of prosthetic leg use in persons with lower extremity amputation often occur at the prosthetic socket, and includes delayed wound healing, recurrent skin ulcerations, and pressure damage to soft tissues. Such complications can result in limited mobility, which further contributes to conditions such as obesity, musculoskeletal pathologies, and cardiovascular disease. Conventional prosthetic socket fabrication is an artisanal process requiring substantial human hours, financial cost and patient involvement for evaluation. Computer aided design (CAD) and computer aided manufacturing (CAM) methods have been explored as an alternative. However, these tools have not reached full clinical efficacy and do not inform the design in a data-driven sense since the actual design process remains a manual and experience-based procedure. The long-term goal of our research is to develop a fully-quantitative process for prosthetic socket design and production that requires minimal patient involvement and can be delivered at affordable price points. <br></p><p>A pre-print for our novel patient-specific and data-driven computational framework for the automated design of biomechanical interfaces is presented <a href=\"https://osf.io/preprints/engrxiv/g8h9n/\">here</a>. Optimization of the design of biomechanical interfaces is complex since it is affected by the interplay of the geometry and mechanical properties of both the tissue and the interface. The proposed framework is presented for the application of transtibial amputee prostheses where the interface is formed by a prosthetic liner and socket. Conventional socket design and manufacturing is largely artisan, non-standard, and insufficiently data-driven, leading to discrepancies between the quality of sockets produced by different prosthetists. Furthermore, current prosthetic liners are often not patient-specific. The proposed framework involves: A) non-invasive imaging to record patient geometry, B) indentation to assess tissue mechanical properties, C) data-driven and automated creation of patient-specific designs, D) patient-specific finite element analysis (FEA) and design evaluation, and finally E) computer aided manufacturing. Uniquely, the FEA procedure controls both the design and mechanical properties of the devices, and simulates, not only the loading during use, but also the pre-load induced by the donning of both the liner and the socket independently. Through FEA evaluation, detailed information on internal and external tissue loading, which are directly responsible for discomfort and injury, are available. Further, these provide quantitative evidence on the implications of design choices, e.g. : 1) alterations in the design can be used to locally enhance or reduce tissue loading, 2) compliant features can aid in relieving local surface pressure. The proposed methods form a patient-specific, data-driven and repeatable design framework for biomechanical interfaces, and by enabling FEA-based optimization reduces the requirement for repeated patient involvement in the currently manual and iterative design process.</p>", "people": ["branger@media.mit.edu", "kmoerman@media.mit.edu", "hherr@media.mit.edu", "danask@media.mit.edu", "lfreed@media.mit.edu"], "title": "Automated and data-driven computational design of subject-specific prosthetic sockets", "modified": "2019-04-17T19:03:36.507Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "variable-impedance-prosthetic-vipr-socket-design"}, {"website": "", "description": "<p>MessageMe is a location-based messaging infrastructure. It consists of a messaging server that delivers messages to recipients as they enter a designated physical space in the Lab. MessageMe builds on the Glass Infrastructure system, utilizing the RFID readers at each screen to determine users' locations.</p>", "people": ["havasi@media.mit.edu"], "title": "MessageMe", "modified": "2016-12-05T00:16:36.444Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "", "groups": ["digital-intuition"], "published": true, "active": false, "end_on": null, "slug": "messageme"}, {"website": "", "description": "", "people": [], "title": "Test", "modified": "2017-10-05T15:31:28.569Z", "visibility": "LAB", "start_on": null, "location": "", "groups": ["code-next"], "published": false, "active": false, "end_on": null, "slug": "test-100"}, {"website": "", "description": "<p>The goal of this project, building upon work begun by Stephen Benton and the Spatial Imaging group, is to enable consumer devices such as tablets, phones, or glasses to display holographic video images in real time, suitable for entertainment, engineering, telepresence, or medical imaging. Our research addresses real-time scene capture and transmission, computational strategies, display technologies, interaction models, and applications.</p>", "people": ["bdatta@media.mit.edu", "nsavidis@media.mit.edu", "sjolly@media.mit.edu", "vmb@media.mit.edu"], "title": "Consumer Holo-Video", "modified": "2019-04-11T13:16:03.063Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "Garden Conference Room", "groups": ["ultimate-media", "ce-20", "future-storytelling", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "consumer-holo-video"}, {"website": "", "description": "<p>Tasers are an electroshock weapon used by over 12,000 police agencies in the United States. The military recently completed testing on another less-lethal weapon which uses a 95GHz millimeter-wave transmitter, called the pain ray. The stated purpose of these less-lethal weapons is as an alternative to firearms, but in practice this hasn't been the case. Some police departments allow taser use in cases of passive resistance, refusing verbal commands, or civil disobedience. Their deployment is now routine and open to misuse. When a gun is fired, the shot is heard and the bullet can be found as evidence. Electronic weapons leave no such traces: they don't leave the telltale markings of traditional physical force, but their electronic signatures are evident in their electromagnetic frequencies and induced body currents. We are developing tools to sense and identify when these weapons are being used and document that evidence.</p>", "people": ["csik@media.mit.edu"], "title": "Watching the Watchers", "modified": "2016-12-05T00:17:25.982Z", "visibility": "LAB", "start_on": "2009-01-01", "location": "E15-001", "groups": [], "published": true, "active": false, "end_on": null, "slug": "watching-the-watchers"}, {"website": "", "description": "<p>Prolonged space travel plays a severe toll on the human body: microgravity impairs muscle and bone growth, and high doses of radiation cause irreversible mutations. As we seriously consider the human species becoming space-faring, a big question stands: even if we do break free from Earth\u2019s orbit, can we adapt to the extreme environments of space?<br></p>", "people": ["lisanip@media.mit.edu"], "title": "Limitations of and approaches to augmenting the human body in space", "modified": "2016-12-15T22:42:49.457Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "limitations-of-and-approaches-to-augmenting-the-human-body-in-space"}, {"website": "", "description": "<p>Scanner Grabber is a digital police scanner that enables reporters to record, playback, and export audio, as well as archive public safety radio (scanner) conversations. Like a TiVo for scanners, it's an update on technology that has been stuck in the last century. It's a great tool for newsrooms. For instance, a problem for reporters is missing the beginning of an important police incident because they have stepped away from their desk at the wrong time. Scanner Grabber solves this because conversations can be played back. Also, snippets of exciting audio, for instance a police chase, can be exported and embedded online. Reporters can listen to files while writing stories, or listen to older conversations to get a more nuanced grasp of police practices or long-term trouble spots. Editors and reporters can use the tool for collaborating, or crowdsourcing/public collaboration.</p>", "people": ["ethanz@media.mit.edu", "achituv@media.mit.edu", "pattie@media.mit.edu"], "title": "Scanner Grabber", "modified": "2016-12-05T00:16:48.984Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["future-of-news", "fluid-interfaces", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "scanner-grabber"}, {"website": "https://ninalutz.github.io", "description": "<p>This pedagogy of experiments seeks to accurately model and manufacture a new form of lighting fixtures.</p><p>These new fixtures are made of polyester resin and a particle distribution called a colloid. Unlike traditional light bulb models, rather than having illumination come from individual lighting fixtures that are wired to a common electric source, we are using one lighting source in the form of a laser to illuminate each fixture.</p><p>By using controlled resin casting alongside numerical simulation of structured colloids, we can program and evaluate different substances held in distribution with a 9 dimensionality lighting model. This allows us to understand the type of lighting output that can be achieved with various materials and shapes, as well as predict the aesthetic properties that these lighting sources may allow us. From various colors to different glittery reflects, this framework allows both technical and artistic framework.&nbsp;</p>", "people": ["vmb@media.mit.edu", "nlutz@media.mit.edu"], "title": "Pigments and Lumens", "modified": "2019-02-07T23:14:01.769Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "pigmented-lumens"}, {"website": "", "description": "<p>Our bodies continue beyond our flesh and bones. Humans have constantly augmented their bodies with tools like clothing or automobiles, and now our bodies also extend into virtual space. An identity includes online identity, which extends from cell phones and laptops into cyberspace. How do we regard our selves when the boundary between self and world is fading? Cach\ufffd is a project that aims to extend online gaze into real space. When a photograph of a body is viewed online, it manifests the gaze offline by means of sound localized on the body. Users know exactly when and where they are being seen. How does revealing online activity affect wearers? If data is neutral and equally accessible, how do we distinguish between personal space and neutral grounds?</p>", "people": ["csik@media.mit.edu", "peek@media.mit.edu"], "title": "Cach\ufffd", "modified": "2016-12-05T00:16:16.838Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-020D", "groups": ["computing-culture"], "published": true, "active": false, "end_on": null, "slug": "cach"}, {"website": "", "description": "<p>The Babbling Brook is an unnamed neighborhood creek in Waltham, MA, that winds its way to the Charles River. With the help of networked sensors and real-time processing, the brook constantly tweets about the status of its water quality, including thoughts and bad jokes about its own environmental and ontological condition. Currently, the Babbling Brook senses temperature and depth and cross-references that information with real-time weather data to come up with extremely bad comedy. Thanks to Brian Mayton, the Responsive Environments group, and Tidmarsh Farms Living Observatory for their support.</p>", "people": ["dignazio@media.mit.edu", "ethanz@media.mit.edu"], "title": "The Babbling Brook", "modified": "2016-12-05T00:16:54.870Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "the-babbling-brook"}, {"website": "", "description": "<p>A steptorial (\"step tutorial\") is a new interaction strategy for learning complex topics. Conventional tutorials\ufffdsuch as Khan Academy-style videos, or interactive guided tours\ufffdrequire the student to cede control to the tutorial. Many students prefer to dive right in to try to learn how to do something by exploration, but they are quite likely to get lost or confused if the topic is complex. Steptorials are unique because they allow varying the autonomy of the user at every step. A steptorial has a control structure of a reversible programming language stepper. The user may choose, at any time, to be shown how to do a step, be guided through it, to try to attempt the task by themselves, or to return to a previous step. Steptorials introduce a new paradigm of mixed-initiative learning.</p>", "people": ["lieber@media.mit.edu", "cfry@media.mit.edu"], "title": "Steptorials: A New Interaction Technique for Learning Complex Topics", "modified": "2016-12-05T00:16:41.787Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "steptorials-a-new-interaction-technique-for-learning-complex-topics"}, {"website": "", "description": "<p>Time Out is an experimental user interface system for addressing cyberbullying on social networks. A Reflective User Interface (RUI) is a novel concept to help users consider the possible consequences of their online behavior, and assist in intervention or mitigation of potentially negative/harmful actions.</p>", "people": ["lieber@media.mit.edu", "kdinakar@media.mit.edu"], "title": "Time Out: Reflective User Interface for Social Networks", "modified": "2016-12-05T00:17:24.140Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "time-out-reflective-user-interface-for-social-networks"}, {"website": "", "description": "<p>How would you browse a VERY large display space, such as a street map of the entire world? The traditional solution is zoom and pan, but these operations have drawbacks that have gone unchallenged for decades. Shifting attention loses the wider context, leading to that \"lost in hyperspace\" feeling. We are exploring alternative solutions, such as a new technique that allows zooming and panning in multiple translucent layers.</p>", "people": ["lieber@media.mit.edu"], "title": "Navigating in Very Large Display Spaces", "modified": "2016-12-05T00:17:18.592Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "navigating-in-very-large-display-spaces"}, {"website": "", "description": "<p>In this visual brainstorming, we present the next 30 years of VR in a set of concept designs.</p>", "people": ["raskar@media.mit.edu", "barmak@media.mit.edu"], "title": "The Next 30 Years of VR ", "modified": "2016-12-05T00:16:55.169Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "the-next-30-years-of-vr"}, {"website": "", "description": "<p>We are working on ways to detect sensitive themes in the online discussions between teenagers on social networks, and to match them to similar experiences shared publicly by other teenagers. For example, imagine a thread between two teenagers on a social networking website; if the theme of their discussion involves \"pressure and regret conditioned on sex,\" we can show them stories by other teenagers who have publicly talked about pressure and regret based on sex. We're trying to help teenagers in difficult situations, helping them to feel that they are not alone, and that there have been other teenagers in similar plights.</p>", "people": ["lieber@media.mit.edu", "kdinakar@media.mit.edu"], "title": "You Too!", "modified": "2016-12-05T00:16:59.751Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "you-too"}, {"website": "", "description": "<h2>Multimodal textile sensate media as an expressive and deformable musical interface</h2><p>In the area of intelligent textiles, we are exploring a multi-modal, fabric-based, stretchable sensate surface for physical interaction media, specifically as&nbsp;deformable musical interface.&nbsp;</p><p>The fabric keyboard consists of multi-layer textile sensors machine-sewn in a keyboard pattern, and it detects different stimuli such as touch, pressure, stretch, proximity, and electric field. This allows users to explore physical and non-contact gestures for expressive on-body and on-surface musical performance. We've also developed additional textile-based inputs such as ribbon controller, trackpad, and fur for more expressive control. This soft sensate surface contributes toward developing seamless, self-aware, and washable media.</p>", "people": ["irmandy@media.mit.edu", "joep@media.mit.edu"], "title": "FabricKeyboard", "modified": "2018-01-04T23:22:07.671Z", "visibility": "PUBLIC", "start_on": "2016-05-27", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "FabricKeyboard"}, {"website": "", "description": "<p>This Is How is a platform for connecting makers with small businesses through stories. Small businesses share their stories in the form of video bytes in which they explain what they do and why, what their requirements and constraints are, and what kinds of issues they have. Makers can then annotate the video, ask further questions, and propose solutions for issues. The video is passed through SuperGlue for annotation and to categorize and find commonalities among requests.</p>", "people": ["weller@media.mit.edu", "lip@media.mit.edu"], "title": "This Is How", "modified": "2016-12-05T00:16:55.625Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "--Choose Location", "groups": ["ultimate-media", "viral-communications"], "published": true, "active": false, "end_on": null, "slug": "this-is-how"}, {"website": "", "description": "<p>Recent years have witnessed a surge in online digital storytelling tools, enabling users to more easily create engaging multimedia narratives. Increasing Internet access and powerful in-browser functionality have laid the foundation for the proliferation of new online storytelling technologies, ranging from tools for creating interactive online videos to tools for data visualization. While these tools may contribute to diversification of online storytelling capacity, sifting through tools and understanding their respective limitations and affordances poses a challenge to storytellers. The NetStories research initiative explores emergent online storytelling tools and strategies through a combination of analyzing tools, facilitating story-hack days, and creating an online database of storytelling tools.</p>", "people": ["ethanz@media.mit.edu"], "title": "NetStories", "modified": "2016-12-05T00:16:40.787Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "netstories"}, {"website": "", "description": "<p>Holographic displays offer many advantages, including comfort and maximum realism. In this project we adapt our guided-wave light-modulator technology to see-through lenses to create a wearable 3D display suitable for augmented or virtual reality applications. As part of this work we also are developing a femtosecond-laser-based process that can fabricate the entire device by \"printing.\"</p>", "people": ["bdatta@media.mit.edu", "nsavidis@media.mit.edu", "sjolly@media.mit.edu", "vmb@media.mit.edu"], "title": "Printed Wearable Holographic Display", "modified": "2016-12-05T00:16:56.157Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["ultimate-media", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "printed-wearable-holographic-display"}, {"website": "", "description": "<p>The Transformative Copy Suite is a pair of applications that deal with the social history and evolution of media files. The first, Infinite Animation, is an exquisite-corpse style collaborative animation tool that allows users to collectively generate and revise an open-ended animation consisting of drawing and text; the second, Remote Whistles, is a mobile music project where users can record audio files with their cell phones and pass them on to friends. Each recipient can remix or rerecord parts of the piece.</p>", "people": ["judith@media.mit.edu"], "title": "Transformative Copy Suite", "modified": "2016-12-05T00:16:56.384Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-389", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "transformative-copy-suite"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: 400;\">Often, we neglect to see the city as living, complex, and dynamic. Shrouded by its masses of concrete and steel, however, lie unique ecosystems and uncharacterized phenomena awaiting exploration and inquiry. Now more than ever, as urban populations boom and city boundaries expand, there exists a pressing need to understand urban ecology, the environmental impact of cities and their development, and&nbsp; the importance of designing in concert with nature. Yet, in spite of this, curricula for youth focused on ecology canonically instruct solely on topics which apply exclusively to natural, undeveloped systems\u2014even in metropolitan schools where access to \u201cnature\u201d is difficult or a privilege. Our failure to use cities as educational resources must be addressed.&nbsp;</span><br></p><p><i>City as Classroom, City as Laboratory</i>&nbsp;is a series of six educational workshops for youth in the Greater Boston area, ages 8 to 14. Throughout the sessions, students make use of \"urban wilds\" in order to become enveloped in the hybrid ecology of the growing city. This curriculum utilizes hands-on approaches for culturing ecological identity such that students are able to recognize and appreciate the complex ecological processes ongoing in urban contexts, and thus understand cities as novel ecosystems.</p><p>The goal of this educational framework is to inspire urban youth to champion future endeavors related to the environmental and political spheres (in efforts related to conservation, wildlife protection, sustainability, infrastructure development) and to see the city as a forum for intervention.<br></p><p><b>The curriculum is designed to be neuroinclusive and sensory-friendly\u2014thus, we encourage participation by neurodivergent individuals.&nbsp;&nbsp;</b></p><p>Participation is of free of charge, and all materials are provided. This program is sponsored by the MIT Media Lab, MIT School of Architecture and Planning, Empowered Brain Institute, and National Geographic.&nbsp;</p><p>The first session of this series will be offered this summer on the following dates:<br></p><p><b><b>City as Classroom Summer 2018\u2014July 8, 15, 22, and 29; August 5 and 12.&nbsp;&nbsp;</b></b></p><p><b>To register, please visit <a href=\"http://bit.ly/ebi_mit_caccal\">this link</a>.&nbsp;&nbsp;</b></p><p>For questions, please contact Avery Normandin (ave@media.mit.edu)<br></p>", "people": ["devora@media.mit.edu", "ave@media.mit.edu"], "title": "City as Classroom, City as Laboratory", "modified": "2018-08-01T18:00:08.501Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "city-as-classroom"}, {"website": "", "description": "<p>We are adding an olfactory dimension to storytelling in order to create more immersive and evocative experiences. Smell Narratives allows the authoring of a \"smell track,\" involving individual or proportionally mixed fragrance components. </p>", "people": ["vmb@media.mit.edu"], "title": "Smell Narratives", "modified": "2016-12-05T00:16:51.129Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["ultimate-media", "ce-20", "future-storytelling", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "smell-narratives"}, {"website": "", "description": "<p>Variable Reality is an augmented reality system designed for reading digital and physical books more intuitively and efficiently. Through a head-worn display device such as Oculus Rift, the user is able to instantly access and display any desired book contents onto either a real book or a hand, depending on the need and affordability. Quick hand gestures integrated with the system further facilitate natural user interactions.</p>", "people": ["geek@media.mit.edu"], "title": "Variable Reality: Interaction with the Virtual Book", "modified": "2016-12-05T00:17:25.195Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "variable-reality-interaction-with-the-virtual-book"}, {"website": "", "description": "<p>We are developing statistical tools for understanding, modeling, and predicting self-harm by using advanced probabilistic graphical models and fail-soft machine learning in collaboration with Harvard University and Microsoft Research.</p>", "people": ["picard@media.mit.edu", "kdinakar@media.mit.edu"], "title": "Valinor: Mathematical models to understand and predict self-harm", "modified": "2019-04-19T17:37:54.819Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "valinor-mathematical-models-to-understand-and-predict-self-harm"}, {"website": "", "description": "<p>&nbsp;In this project we explore how to recognize and localize affect in images.</p>", "people": ["agata@media.mit.edu"], "title": "Image Sentiment Analysis", "modified": "2018-10-22T19:54:06.978Z", "visibility": "PUBLIC", "start_on": "2017-01-02", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "image-sentiment-anlysis"}, {"website": "", "description": "<p>We show that using thin slices (&lt; 1 minute) of facial expression and body language data, we can train a deep neural network to predict whether two people in a conversation will bond with each other. Bonding is measured using the Bonding subscale of the Working Alliance Inventory. We show that participants who experience bonding perceive their conversational partner as interesting, charming, and friendly, and do not perceive them as distant or annoying. </p><p>The data are collected from a user study of naturalistic conversations, in which participants were asked to interact for 20 minutes, and were recorded using cameras, microphones, and Microsoft Kinects. To ensure participants did not become self-conscious of their non-verbal cues, they were told the purpose of the study was to train machine learning algorithms to read lips. </p><p>We show that not only can we accurately predict bonding from participants' personality, disposition, and traits, but that we can predict whether the participant will experience bonding up to 20 minutes later, using only one-minute thin slices of facial expression and body language data. This ability could be extremely useful to an intelligent virtual agent, because if it could detect at one-minute intervals whether it was bonding with its user, it could make course corrections to promote enjoyment and foster bonding. We provide an analysis of the facial expression and body language cues associated with higher bonding, and show how this information could be used by an agent to synthesize the appropriate non-verbal cues during conversation.</p>", "people": ["jaquesn@media.mit.edu", "picard@media.mit.edu"], "title": "Predicting Bonding in Conversations", "modified": "2019-02-08T16:15:38.454Z", "visibility": "PUBLIC", "start_on": "2015-10-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "predicting-bonding"}, {"website": "", "description": "<p>Webbed Footnotes is a Web annotation system that enables users to have a discussion about, around, and within the space of a Web page. Webbed Footnotes' approval-based moderation system promotes contributions that other users find useful and interesting.</p>", "people": ["judith@media.mit.edu"], "title": "Webbed Footnotes", "modified": "2016-12-05T00:17:26.305Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-391", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "webbed-footnotes"}, {"website": "", "description": "<p>When a piece of work is created or performed, the digital rights to that piece are oftentimes complex and spread across many different organizations and entities. This makes it difficult for artists to get paid for their work and many large platforms, like Spotify, suffer from lawsuits because they don\u2019t do a good enough job of navigating the labyrinth. &nbsp;How might you build a system to help artists get paid for their work? In partnership with the Berklee College of Music, Harvard Berkman Center, and several industry partners in the Open Music Initiative, we are investigating the design of a blockchain-inspired open and interoperable digital rights management platform.\n                    \n                </p>", "people": ["dramsay@media.mit.edu", "moia@media.mit.edu", "narula@media.mit.edu"], "title": "Open Music Initiative", "modified": "2016-12-16T02:50:30.321Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["digital-currency-initiative-dci"], "published": true, "active": false, "end_on": null, "slug": "open-music-initiative"}, {"website": "", "description": "<p>CNSILK explores the design and fabrication potential of silk fibers\u2013inspired by silkworm cocoons\u2013for the construction of woven habitats. It explores a novel approach to the design and fabrication of silk-based building skins by controlling the mechanical and physical properties of spatial structures inherent in their microstructures using multi-axis fabrication. The method offers construction without assembly, such that material properties vary locally to accommodate for structural and environmental requirements. This approach stands in contrast to functional assemblies and kinetically actuated facades which require a great deal of energy to operate, and are typically maintained by global control. Such material architectures could simultaneously bear structural load, change their transparency so as to control light levels within a spatial compartment (building or vehicle), and open and close embedded pores so as to ventilate a space.</p>", "people": ["neri@media.mit.edu"], "title": "CNSILK: Computer Numerically Controlled Silk Cocoon Construction", "modified": "2016-12-05T00:16:27.548Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "cnsilk-computer-numerically-controlled-silk-cocoon-construction"}, {"website": "", "description": "<p>In honor of May being Mental Health Awareness month, the collaboration between Footwear x Mental Health needs YOUR participation in a call to action.</p><p>According to the National Alliance on Mental Illness (NAMI), approximately 46.6 million adults in the US live with a mental illness and half of all lifetime mental health conditions begin by age 14 and 75 percent begin by age 24 [<a href=\"https://www.nami.org/Learn-More/Mental-Health-By-the-Numbers\">1</a>]. Additionally, African Americans are \u201c20 percent more likely to experience serious mental health problems than the general population\u201d [<a href=\"https://www.nami.org/find-support/diverse-communities/african-americans\">2</a>].</p><p>Barriers to treatment and engagement for African Americans include lack of cultural competence by health professionals, and shame and stigma within the community surrounding the topic. Other barriers affecting engagement with mental health treatment include an inability or unwillingness to use creative and innovative approaches to engagement, and an inability to work effectively within and across diverse cultures [<a href=\"https://www.nami.org/About-NAMI/Publications-Reports/Public-Policy-Reports/Engagement-A-New-Standard-for-Mental-Health-Care/NAMI_Engagement_Web.pdf\">3</a>].</p><p>This is why we need your help in unleashing the power of footwear to prove it\u2019s more than just a shoe or a product. Sneakers impact communities.</p><p><i>[Embed video]</i></p><p>To participate:</p><ul><li>Post a sneaker (designed, already owned, or one you admire) on Twitter, along with one to two sentences related to how it links to mental health and wellness. You may answer one of the three prompts below or share thoughts of your own:</li></ul><ol><li>How does this sneaker inspire you or a friend in overcoming a difficult situation?</li><li>Describe how this sneaker makes you feel.</li><li>How can footwear provide an innovative approach to destigmatizing mental health?</li></ol><ul><li>Include in your post the hashtags: #FootwearxMentalHealth and #BlackMentalHealthMatters (or #MentalHealthAwareness ).</li></ul><ul><li>Tag two people in your post, and encourage them to also share how their kicks collaborate with mental health and wellness.</li></ul><p>The content you share will be reviewed to see how footwear may assist in destigmatizing the conversation surrounding mental health.</p><p>For more mental health resources, please visit:</p><ul><li><a href=\"http://www.nami.org/\">The National Alliance on Mental Illness</a></li><li><a href=\"https://www.nimh.nih.gov/index.shtml\">National Institute of Mental Health</a></li><li><a href=\"https://silencetheshame.com/\">Silence the Shame</a></li><li><a href=\"https://www.instagram.com/askdrjess/\">Ask Dr. Jess</a></li></ul>", "people": ["britneyj@media.mit.edu"], "title": "Footwear x Mental Health: A Collaboration", "modified": "2019-05-22T14:21:12.337Z", "visibility": "PUBLIC", "start_on": "2019-05-21", "location": "", "groups": ["viral-communications"], "published": false, "active": false, "end_on": null, "slug": "footwear-x-mentalhealth"}, {"website": "", "description": "<p>We are looking to better enable cooperation between individuals working in the same domain yet having differing goals. Tools for collaboration are mainly focused on groups with a single shared goal, however we observe cooperation between individuals occurring in much broader spheres. Our work will take the form of a networked programming environment where all code is shared; the goal is to develop tools and techniques that make it easy to leverage the work of others, as well as provide value (both social and technical) for making one's work available to the collective. In particular we want to make more visible the act of appropriation, and also make it easier for individuals to track the evolution of their contributions. We believe that by making appropriation more explicitly visible we can translate contributions into a form of social capital and thus encourage cooperation between otherwise unconnected individuals.</p>", "people": ["judith@media.mit.edu"], "title": "Share: Socio-Technical Tools for Loosely Bound Cooperation", "modified": "2016-12-05T00:17:00.557Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-390", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "share-socio-technical-tools-for-loosely-bound-cooperation"}, {"website": "", "description": "<p>It is increasingly hard for adults and children alike to be attentive given the increasing amounts of information and distractions surrounding us. </p><p>We have developed AttentivU: a device, in a socially acceptable form factor of a pair of glasses, that a person can put on in moments when he/she wants/needs to be attentive. </p><p>The AttentivU glasses use brain activity (electroencephalography - EEG) as well as eye movements (electrooculography - EOG) sensors to measure engagement of a person in real-time and provide either audio or haptic feedback to the user when their engagement is low, thereby nudging them to become engaged again. </p><p>We have tested the first generation prototype of the device in workplace and class- room settings with over 100 subjects. We have performed experiments with people studying or working by themselves, viewing online lectures as well as listening to classroom lectures. The obtained results show that our device makes a person more attentive and produces improved learning and work performance outcomes.&nbsp;</p><p>We have now finished the first tests of the glasses (second prototype) with more than 30 subjects who were performing driving task in the simulator or using the glasses during everyday activities like reading, watching videos or writing.&nbsp;</p><p>The novelty of our system is that it is meant to be used in the moment, that is, in the context where sustained attention is necessary. In order to make real-world use possible, we have developed a socially acceptable, inconspicuous form factor: a pair of glasses that contain EEG and EOG electrodes as well as a an amplifier, Bluetooth LTE module, and a speaker for bone-conduction auditory feedback. The user can optionally receive the feedback or nudges through a wireless vibration brooch that can be attached where desired and remains invisible.&nbsp;</p><p>We envision a future in which people can decide when they want to be more attentive and can in those moments put on their AttentivU glasses to help them be focused.&nbsp;</p><p>Project Lead: Nataliya Kosmyna, Ph.D</p><p>Project Team: Caitlin Morris,&nbsp;Thanh Nguyen and Pattie Maes.</p>", "people": ["pattie@media.mit.edu", "nkosmyna@media.mit.edu"], "title": "AttentivU", "modified": "2019-05-24T18:43:31.915Z", "visibility": "PUBLIC", "start_on": "2018-01-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "attentivu"}, {"website": "http://www.judithamores.com/essence", "description": "<h2><b>A wearable olfactory display that monitors cardio-respiratory information to support mental wellbeing.</b></h2><p>BioEssence is a novel wearable olfactory&nbsp;display that provides just-in-time release of scents based on&nbsp;the physiological state of the wearer. The device can release up&nbsp;to three scents and passively captures subtle chest vibrations&nbsp;associated with the beating of the heart and respiration through&nbsp;clothes.&nbsp;<br></p>", "people": ["artemd@media.mit.edu", "javierhr@media.mit.edu", "amores@media.mit.edu", "pattie@media.mit.edu"], "title": "BioEssence", "modified": "2019-05-11T00:16:22.469Z", "visibility": "PUBLIC", "start_on": "2017-12-01", "location": "", "groups": ["affective-computing", "responsive-environments", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "bioessence"}, {"website": "", "description": "<p>Comment Flow is a flexible tool for the content-driven exploration and visualization of a social network. Building upon a traditional force-directed network layout, our system shows the activity and the information exchange (postings in the comment box) between nodes, taking the sequence and age of messages into account. This project serves both as an illustration of one approach to the general problem of individuated network visualization and as an example of the practical uses of such representations. By going beyond the \ufffdskeleton\ufffd of network connectivity and looking at the flow of information between the individual actors, we can create a far more accurate portrait of online social life.</p>", "people": ["judith@media.mit.edu"], "title": "Comment Flow", "modified": "2016-12-05T00:16:19.469Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-390", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "comment-flow"}, {"website": "", "description": "<p>These wall drawings are generated by a series of predesigned instructions that dictate their form. The instructions give both constraint and flexibility, so that the piece that unfolds has a clear structure and at the same time expresses the individual aesthetic preferences of the participants who contribute. Since each step depends on the previous steps, the result is a dynamic, collaborative piece, authored collectively by the artist and the exhibit visitors.&nbsp;</p>", "people": ["sdkamvar@media.mit.edu", "smithkim@media.mit.edu"], "title": "Boundaries Drawings", "modified": "2016-12-05T00:17:27.546Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "boundaries-drawings"}, {"website": "", "description": "<p>A major problem for natural language interfaces is their inability to handle text whose meaning depends in part on context.  If a user asks his car radio to play \"a fast song,\" or his calendar to schedule \"a short meeting,\" the interpreter would have to accommodate vagueness and ambiguity to figure out what he meant based on what he said.  For it to understand what songs or events the speaker intended, it must make decisions that depend on assumed common knowledge about the world and language.  Our research presents two approaches for reducing uncertainty in natural language interfaces, by modeling interpretation as a plan recognition problem.</p>", "people": ["lieber@media.mit.edu"], "title": "AIGRE: Natural Language Interface that Accommodates Vague and Ambiguous Input", "modified": "2016-12-05T00:17:06.249Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "aigre-natural-language-interface-that-accommodates-vague-and-ambiguous-input"}, {"website": "", "description": "<p>This project asks, how can we transmit experiences across space and time? The launching point of the project is to create a drawing that could only have been made in space, and to capture an emotional aspect of space travel, through an art piece based on notions of telepresence.&nbsp;</p><p>For my payload, a mark making object (graphite) is placed in the nano lab, which is lined with paper.&nbsp; As the payload makes its journey, the mark making object&nbsp;the MMO will float about making a&nbsp;unique&nbsp;drawing of its experience.&nbsp;A sensor inside the box will&nbsp;simultaneously&nbsp;record its movements and position.&nbsp;</p><p>This data and footage will be used to recreate the flight paths and movements of the graphite in an identical \u201csister\u201d box with a CNC back on Earth, post flight.&nbsp;<br></p><p>This project occurs in two stages, and involves [1] the capture of an object\u2019s experience in zero gravity through a sensor and a drawing made in zero g, and [2] the recreation of that object\u2019s flight experience back on Earth through a robotic arm.</p><p>The resulting objects for exhibition would include the original box that went up for the launch, the drawings inside of it, and the \"sister\" box that shows the recreated movements of the graphite back on Earth.</p>", "people": ["wonder@media.mit.edu"], "title": "Telepresent Drawings in Space", "modified": "2019-05-06T14:20:29.475Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "telepresent-drawings-in-space"}, {"website": "", "description": "<p>The CityScope \"Scout\" prototype integrates augmented reality with real-time mathematical modeling of geospatial systems. In practice, the technology transforms any tabletop into a canvas for land-use planning and walkability optimization. Users perform rapid prototyping with LEGO bricks and receive real-time simulation and evaluation feedback.</p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu"], "title": "CityScope Mark II: Scout", "modified": "2017-10-16T03:13:05.718Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": null, "slug": "OLD_cityscope-mark-ii-scout"}, {"website": "", "description": "<p>Inspired by the simplicity and aesthetics of traditional Montessori education, these materials live within and extend this pedagogy to address new proficiencies and emerging fields, such as computational thinking.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">The foundation of computer science is not included in many early childhood and elementary curricula. When it is, however, the emphasis tends to be on the interface, rather than the concepts behind it. These materials break down the fundamentals of computation into a set of discrete and tangible concepts that are expressed in hands-on, tactile ways.&nbsp;</span></p>", "people": ["sdkamvar@media.mit.edu", "smithkim@media.mit.edu"], "title": "New Learning Materials for Computational Thinking", "modified": "2016-12-05T00:17:27.529Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "new-learning-materials-for-computational-thinking"}, {"website": "", "description": "<p><b>Bias by us</b>&nbsp;envisions a future of media diversity by understanding the bias of today.</p><p>Our work seeks to understand how the US media ecosystem reports on underrepresented minorities. By using natural language processing algorithms and data-intensive models, we aim to uncover underlying stereotypes, associations, and modes of narration that media produces and reproduces when covering minority related events. We perform a multi- and cross-platform analysis, capturing media dynamics on different social media platforms and traditional media outlets across the political spectrum. Besides understanding how media language portrays underrepresented minorities, we locate effects on and associations to political saliency and bias-motivated crime.<br></p><p>By understanding media bias and its effects on underrepresented minorities, we reflect on the conditions that can ensure a diverse and inclusive US media ecosystem.</p>", "people": ["ethanz@media.mit.edu", "orestis@media.mit.edu"], "title": "Bias by us", "modified": "2019-04-21T19:06:53.374Z", "visibility": "PUBLIC", "start_on": "2019-02-18", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2019-06-15", "slug": "bias-by-us"}, {"website": "", "description": "<p>The Dynamic 3D prototype allows users to edit a digital model by moving physical 3D abstractions of building typologies. Movements are automatically detected, scanned, and digitized so as to generate inputs for computational analysis. 3D information is also projected back onto the model to give the user feedback while edits are made.</p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu"], "title": "CityScope Mark III: Dynamic 3D", "modified": "2017-10-16T03:03:54.492Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": null, "slug": "OLD_cityscope-mark-iii-dynamic-3d"}, {"website": "", "description": "<p>RESTful services and the Web provide a framework and structure for content delivery that is scalable, not only in size but, more importantly, in use cases. As we in Responsive Environments build systems to collect, process, and deliver sensor data, this project serves as a research platform that can be shared between a variety of projects both inside and outside the group. By leveraging hyperlinks between sensor data clients can browse, explore, and discover their relationships and interactions in ways that can grow over time.</p>", "people": ["gershon@media.mit.edu", "bmayton@media.mit.edu", "joep@media.mit.edu", "sfr@media.mit.edu"], "title": "Chain API", "modified": "2016-12-05T00:16:17.588Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "chain-api"}, {"website": "", "description": "<p>An online language learning tool and game with a purpose (GWAP) designed to simultaneously gather annotated speech and text data, useful for improving natural language processing (NLP) applications and serve as an English-language learning resource.</p>", "people": ["havasi@media.mit.edu", "kmh@media.mit.edu"], "title": "Second-Language Learning Using Games with a Purpose", "modified": "2016-12-05T00:16:18.722Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "", "groups": ["digital-intuition"], "published": true, "active": false, "end_on": null, "slug": "second-language-learning-using-games-with-a-purpose"}, {"website": "", "description": "<p>With families living further apart, it has become increasingly difficult for people to stay connected\u2014particularly in the case of grandparents and children. The challenge lies in deciding when, how and what to engage on. <b>Can technology act as a proactive facilitator of human-human connection?</b></p><p>Social robots are uniquely positioned to act as active facilitators of human-human connection. However, in order to do so, they require the ability to be <b>proactive</b>. Proactivity demands that an agent not only respond to it's environment, but also exhibit goal-directed behavior by taking the initiative. In this work, an ecosystem for connected social robots to utilize the surfaces of the home as a canvas for expression in order to engage grandparents and grandkids in human-human interaction is proposed. </p><p><b>This work models proactivity in an agent as a function of understanding the context, proposing a goal, and taking initiative through an interaction.&nbsp;</b>Human studies will be conducted in order to understand and draw inspiration from human behavior to drive how a robot gets an individual's attention. Further interaction studies will serve to design and evaluate the form of expression (robot, environment, or both) most relevant for given contexts.</p>", "people": ["nikhita@media.mit.edu"], "title": "Proactive, Connected Spaces", "modified": "2018-04-25T03:28:31.115Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "proactive-connected-spaces"}, {"website": "", "description": "<p>Dog is a programming language that makes it easy and intuitive to create social applications. A key feature of Dog is built-in support for interacting with people. Dog provides a natural framework in which both people and computers can be sent requests and return results. It can perform a long-running computation while also displaying messages, requesting information, or sending operations to particular individuals or groups. By switching between machine and human computation, developers can create powerful workflows and model complex social processes without worrying about low-level technical details. </p>", "people": ["sdkamvar@media.mit.edu"], "title": "The Dog Programming Language", "modified": "2016-12-05T00:17:23.364Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "the-dog-programming-language"}, {"website": "", "description": "<p>ForgetAboutIT has become an integrated part of CollaboRhythm. Currently only 50 percent of patients with chronic diseases take their medications. The problem is not simple forgetfulness; it is a complex combination of lack of understanding, poor self-reflection, limited social support, and almost non-existent communication between provider and patient. ForgetAboutIT? is a system to support medication adherence which presupposes that patients engaged in tight, collaborative communication with their providers through interactive interfaces would think it preposterous not to take their medications. Technically, it is an awareness system that employs ubiquitous connectivity on the patient side through cell phones, televisions, and other interactive devices and a multi-modal collaborative workstation on the provider side.</p>", "people": ["fmoss@media.mit.edu", "jom@media.mit.edu"], "title": "ForgetAboutIT?", "modified": "2016-12-05T00:17:12.569Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-320", "groups": ["new-media-medicine"], "published": true, "active": false, "end_on": null, "slug": "forgetaboutit"}, {"website": "", "description": "<p>StreetScore is a machine learning algorithm that predicts the perceived safety of a streetscape. StreetScore was trained using 2,920 images of streetscapes from New York and Boston and their rankings for perceived safety obtained from a crowdsourced survey. To predict an image's score, StreetScore decomposes this image into features and assigns the image a score based on the associations between features and scores learned from the training dataset. We use StreetScore to create a collection of map visualizations of perceived safety of street views from cities in the United States. StreetScore allows us to scale up the evaluation of streetscapes by several orders of magnitude when compared to a crowdsourced survey. StreetScore can empower research groups working on connecting urban perception with social and economic outcomes by providing high-resolution data on urban perception.</p>", "people": ["raskar@media.mit.edu", "hidalgo@media.mit.edu", "naik@media.mit.edu"], "title": "StreetScore", "modified": "2016-12-05T00:17:03.647Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["camera-culture", "collective-learning"], "published": true, "active": false, "end_on": null, "slug": "streetscore"}, {"website": "", "description": "<p>An electroporator is&nbsp;&nbsp;a fundamental tool for biotechnology,&nbsp;used to transform bacteria with higher efficiency when compared to heat shock transformation. However, electroporators are expensive devices.&nbsp;This project involves a novel approach to develop a low-cost electroporator.&nbsp;</p>", "people": ["ninawang@media.mit.edu", "dkong@media.mit.edu"], "title": "Zap-Pore", "modified": "2019-05-06T14:38:29.578Z", "visibility": "LAB", "start_on": "2017-10-01", "location": "", "groups": ["community-bio"], "published": true, "active": false, "end_on": null, "slug": "zap-pore"}, {"website": "http://tangible.media.mit.edu/project/animastage", "description": "<p>We present AnimaStage: a hands-on animated craft platform based on an actuated stage. Utilizing a pin-based shape display, users can animate their crafts made from various materials. Through this system, we intend to lower the barrier for artists and designers to create actuated objects and to contribute to interaction design using shape-changing interfaces for inter-material interactions.</p><p>We introduce a three-phase design process for AnimaStage with examples of animated crafts. We implemented the system with several control modalities that allow users to manipulate the motion of the crafts so that they could easily explore their desired motion through an iterative process. Dynamic landscapes can also be rendered to complement the animated crafts. We conducted a user study to observe the subject and process by which people make crafts using AnimaStage. We invited participants with different backgrounds to design and create crafts using multiple materials and craft techniques. A variety of outcomes and application spaces were found in this study.</p><p><a href=\"http://tangible.media.mit.edu/project/animastage/\">Project Page</a></p>", "people": ["udayan@media.mit.edu", "daniell@media.mit.edu", "ishii@media.mit.edu", "ken_n@media.mit.edu"], "title": "AnimaStage", "modified": "2017-06-21T18:35:35.551Z", "visibility": "PUBLIC", "start_on": "2017-06-12", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "animastage"}, {"website": "", "description": "<p>Named for, and inspired by, the medieval practice of erecting barriers to prevent the spread of disease, Cordon Sanitaire is a collaborative, location-based mobile game in which players seek to isolate an infectious \"patient zero\" from the larger population. Every day, the game starts abruptly, synchronizing all players at once, and lasts for two minutes. In 60 seconds, players must choose either to help form the front line of a quarantine, or remain passive. Under pressure, the \"uninfected\" attempt to collaborate without communication, seeking to find the best solution for the group. When those 60 seconds end, a certain number of players are trapped inside with patient zero, and the score reflects the group's ability to cooperate under duress.</p>", "people": ["slavin@media.mit.edu"], "title": "Cordon Sanitaire", "modified": "2016-12-05T00:17:03.672Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "cordon-sanitaire"}, {"website": "", "description": "<p>We use high-resolution geospatial data collected from mobile phones to measure social segregation at an unprecedented resolution in cities across the United States. Social segregation happens when people of varying socioeconomic groups in a city have little opportunity to be exposed to people different than them.</p><p>To construct this measure, we aggregate high-resolution data from&nbsp;over 4.5 million users in the principal metro areas in the US to characterize places in the city by how mixed their visitors are by income. Using this measure, rather than traditional residential metrics, reveals that social exposure in third places is crucial to understanding economic segregation patterns in cities. In fact, the social segregation of different economic groups is dependent on an extremely small proportion of overall venues in a city.&nbsp;</p><p>We also look at how much individual citizens would need to change their behavior in order to make their patterns of exposure more integrated. Surprisingly, small changes in the amount of time people spend in different categories of places\u2014changes as low as 2-5%\u2014can reduce their social segregation by half.&nbsp;</p><p>We're currently working on finalizing these results and exploring how we might translate these findings into policy.</p>", "people": ["dcalacci@media.mit.edu", "emoro@media.mit.edu", "xdong@media.mit.edu"], "title": "Measuring and reducing social segregation in cities", "modified": "2019-04-19T14:46:25.026Z", "visibility": "PUBLIC", "start_on": "2017-10-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "measuring-and-reducing-social-segregation-in-cities"}, {"website": "", "description": "<h2><p><span style=\"font-size: 18px; font-weight: normal;\">Optically transparent and structurally sound, glass has played a significant role in the evolution of product and architectural design across scales and disciplines, and throughout the ages. Glass processing methods\u2014such as blowing, pressing, and forming\u2014have aimed at achieving increased glass performance and functionality. Nonetheless, techniques and technologies enabling controlled tunability of its optical and mechanical properties at high spatial manufacturing resolution have remained an end without a means.</span><br></p></h2>", "people": ["ked03@media.mit.edu", "inamura@media.mit.edu", "dlizardo@media.mit.edu", "weller@media.mit.edu", "giorgiaf@media.mit.edu", "mlstern@media.mit.edu", "achituv@media.mit.edu", "neri@media.mit.edu", "nassia@media.mit.edu"], "title": "Glass II", "modified": "2018-10-23T15:19:17.532Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "g3p-II"}, {"website": "", "description": "<p><a href=\"http://dl.acm.org/citation.cfm?id=2897442.2897469\">Humans need sleep</a>, along with food, water, and oxygen, to survive. With about one-third of our lives spent sleeping, there has been increased attention and interest in understanding sleep and the overall state of our \"sleep health.\" The rapid adoption of smartphones, along with a growing number of sleep tracking applications for these devices, presents an opportunity to use phones to encourage better sleep hygiene. Procrastinating going to bed and being unable to stick to a consistent bedtime can lead to inadequate sleep time, which in turn affects quality of life and overall wellbeing. To help address this problem, we developed two applications, Lights Out and Sleep Wallpaper, which provide a sensor-based bedtime alarm and a connected peripheral display on the wallpaper of the user's mobile phone to promote awareness with sleep data visualization.</p>", "people": ["sra@media.mit.edu", "picard@media.mit.edu", "cvx@media.mit.edu"], "title": "Improving Sleep-Wake Schedule Using Sleep Behavior Visualization and a Bedtime Alarm", "modified": "2016-12-05T17:35:08.047Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "improving-sleep-wake-schedule-using-sleep-behavior-visualization-and-a-bedtime-alarm"}, {"website": "https://learn.media.mit.edu", "description": "<h1>The <a href=\"https://www.media.mit.edu/groups/ml-learning/overview/\"><b>ML Learning Initiative</b></a> is built around&nbsp;a cohort of Learning Fellows&nbsp;from across the diverse research groups of the Media Lab.</h1><h2>Meet our current cohort of 2018-19 Learning Fellows [<a href=\"https://www.media.mit.edu/posts/introducing-the-2018-19-mit-media-lab-learning-fellows/\">link</a>]</h2><p>--------------------------</p><p>The ML Learning Fellows Program began in 2016. Since then, the ML Learning Initiative team has supported three cohorts of Learning Fellows, building a&nbsp;community of graduate student researchers at the Media Lab who are developing new technologies to cultivate creative learning in a variety of contexts.&nbsp;In addition to funding, the ML Learning Initiative team provides the students with additional mentorship, connections, and support for their research.</p><p>Our Learning Fellows are supported in two ways, through the&nbsp;<i>LEGO Papert Fellowship</i>&nbsp;and the&nbsp;<i>Learning Innovation Fellowship</i>.&nbsp;</p><ul><li><b>LEGO Papert Fellowship:&nbsp;</b>The&nbsp;<a href=\"http://www.legofoundation.com/en-us/\">LEGO Foundation</a>, a long-time member of the Media Lab, has endowed this fellowship program to honor the legacy of educational-technology pioneer Seymour Papert, a founding Media Lab faculty member. Each year, the LEGO Papert Fellowship will fund the work of three graduate students who are working at the intersection of creativity, play, learning, and new technologies.</li><li><b>Learning Innovation Fellowship:</b> With a focus on creative learning environments and tools, early-childhood learning, life success skills, other learning mindsets, and adult learning, the Learning Innovation Fellowship supports&nbsp; research that brings opportunities for <a href=\"https://learn.media.mit.edu/creative-learning\">creative learning</a> to more people.&nbsp;</li></ul>", "people": ["jakory@media.mit.edu", "smithkim@media.mit.edu", "anneli@media.mit.edu", "tarmelop@media.mit.edu", "shrutid@media.mit.edu", "khanning@media.mit.edu", "hchen25@media.mit.edu", "ave@media.mit.edu", "akito@media.mit.edu", "sdruga@media.mit.edu", "jaleesat@media.mit.edu", "reeddc@media.mit.edu", "jwilbert@media.mit.edu", "joyab@media.mit.edu", "ktj@media.mit.edu", "ps1@media.mit.edu", "cdvm@media.mit.edu", "kamcco@media.mit.edu", "minakhan@media.mit.edu"], "title": "ML Learning Fellows Program", "modified": "2019-03-28T18:50:57.766Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["ml-learning"], "published": true, "active": false, "end_on": null, "slug": "ml-learning-fellows-program"}, {"website": "", "description": "<p>Grassroots Mapping (grassrootsmapping.org) is a participatory mapping project involving communities in cartographic dispute. This January, we worked with a series of organizations and communities to produce maps with children and adults from several communities in Lima, Peru. Seeking to invert the traditional power structure of cartography, participants used helium balloons and kites to loft their own \ufffdcommunity satellites\ufffd made with inexpensive digicams. The resulting images, owned by the residents, are georeferenced and stitched into maps which are 100x higher resolution that those offered by Google, at extremely low cost. In some cases these maps may be used to support residents\ufffd claims to land title. By creating open-source tools to include everyday people in exploring and defining their own geography, Warren hopes to enable a diverse set of alternative agendas and practices, and to emphasize the fundamentally narrative and subjective aspects of mapping over its use as a medium of control.</p>", "people": ["dsmall@media.mit.edu"], "title": "Grassroots Mapping with Balloons and Kites", "modified": "2016-12-05T00:17:13.445Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "grassroots-mapping-with-balloons-and-kites"}, {"website": "", "description": "<p>Consumer electronics devices are becoming more complicated, intimidating users. These devices do not know anything about everyday life or human goals, and they show irrelevant menus and options. We are building Roadie, a system with knowledge about the user's intentions to help the device to display relevant information to reach the user's goal. For example, a DVD player might suggest a sound configuration based on the movie it is playing. This will lead to more human-like interactions with these devices. We have constructed a Roadie interface to real consumer electronics devices: a television, set top box, and smart phone. The devices communicate over Wi-Fi, and use the UPnP protocols.</p>", "people": ["lieber@media.mit.edu"], "title": "Goal-Oriented Interfaces for Consumer Electronics", "modified": "2016-12-05T00:17:13.220Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-384A", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "goal-oriented-interfaces-for-consumer-electronics"}, {"website": "", "description": "<p>Panoply is a crowdsourcing application for mental health and emotional wellbeing. The platform offers a novel approach to computer-based psychotherapy, targeting accessibility without stigma, engagement, and therapeutic efficacy. A three-week randomized-controlled trial with 166 participants showed Panoply conferred greater or equal benefits for nearly every therapeutic outcome measure compared to an active control task (online expressive writing). Panoply significantly outperformed the control task also on all measures of engagement, and is now being commercialized at itskoko.com.</p>", "people": ["rmorris@media.mit.edu", "picard@media.mit.edu"], "title": "Panoply", "modified": "2016-12-05T00:16:43.180Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "panoply"}, {"website": "", "description": "<p>Metafluidics is an open source, community-driven repository that hosts digital design files, assembly specifications, and the bill of materials necessary for users to make and operate a fluidic device. The site enables the global microfluidics community, from trained scientists and engineers to hobbyists, students, and amateur makers alike, the ability to submit designs and reproduce and remix devices with the ultimate goal of democratizing microfluidics.</p><p>This repository is the hardware portal for the National Science Foundation-supported&nbsp;<strong><a href=\"https://www.programmingbiology.org/\">Living Computing Project</a></strong>. Share your fluidic devices with the global community today!</p>", "people": ["dkong@media.mit.edu"], "title": "Metafluidics: Open repository for fluidic systems", "modified": "2019-05-06T14:41:03.684Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["community-bio"], "published": true, "active": false, "end_on": null, "slug": "metafluidics-openrepository"}, {"website": "", "description": "<p>Is it possible to create passive displays that respond to changes in viewpoint and incident light conditions? Holograms and 4D displays respond to changes in viewpoint. 6D displays respond to changes in viewpoint as well as surrounding light. We encode the 6D reflectance field into an ordinary 2D film. These displays are completely passive and do not require any power. Applications include novel instruction manuals and mood lights.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "6D Display", "modified": "2016-12-05T00:17:03.790Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "6d-display"}, {"website": "", "description": "<p>In order to extend the Digital Intuition group's ability to understand human language, a module that fills in the gaps of current technology must be developed to understand dialogue. This module will be based on data from the Switchboard human-human dialogue corpus, as well as a dataset of recorded dialogues between parents and children while reading an interactive e-book created by the Lab's Personal Robots group. The goal is for the module to be able to identify the emotion and mood of the dialogue in order to make inferences about what parents and children generally talk about when reading the book, and to make suggestions about additional conversation topics. Conversations between an adult and child while reading a book can greatly contribute to the learning and development of young children.</p>", "people": ["havasi@media.mit.edu"], "title": "Understanding Dialogue", "modified": "2016-12-05T00:16:56.808Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "", "groups": ["digital-intuition"], "published": true, "active": false, "end_on": null, "slug": "understanding-dialogue"}, {"website": "", "description": "<p>Professor Minsky continues to develop the theory of human thinking and learning called the \"Society of Mind,\" which tries to explain how various phenomena of mind emerge from the interactions among many different kinds of highly evolved brain mechanisms. In this way we can account for many aspects of common sense, imagination, and reasoning by analogy, as resulting from negotiations among systems that use different ways of representing knowledge. Similarly, it appears that we can explain many of the regularities found in natural languages as consequences of how those representations work, rather than as constraints that are externally imposed on interpersonal communications. This approach also suggests that some of what we call \"emotions\" are mechanisms required for managing conflicts among competing goals. We may need to construct similar systems when we begin to build smarter and more versatile machines.</p>", "people": ["minsky@media.mit.edu"], "title": "Society of Mind/The Emotion Machine", "modified": "2016-12-05T00:17:04.517Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-309", "groups": ["society-of-mind"], "published": true, "active": false, "end_on": null, "slug": "society-of-mindthe-emotion-machine"}, {"website": "", "description": "<p>Exploring robotic sound generation mixed with human movements on the guitar. Guitar Machine II is a robotic guitar that responds to human gestures, as well as other input means such as midi controllers or algorithmic composition.&nbsp;</p>", "people": ["sangwon@media.mit.edu"], "title": "Guitar Machine II", "modified": "2018-10-23T15:19:45.702Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "guitar-machine-ii"}, {"website": "", "description": "<p>Sound Cycles is a new interface for exploring, re-mixing, and composing with large volumes of audio content. The project presents a simple and intuitive interface for scanning through long audio files or pre-recorded music. Sound Cycles integrates with the existing Digital Audio Workstation for on-the-fly editing, audio analysis, and feature extraction.</p>", "people": ["rebklein@media.mit.edu", "tod@media.mit.edu", "holbrow@media.mit.edu"], "title": "Sound Cycles", "modified": "2016-12-05T00:17:04.683Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "sound-cycles"}, {"website": "http://bit.ly/2nJawNG", "description": "<p>The gram-positive cocci <i>Streptococcus pneumoniae</i> causes pneumonia, otitis media, meningitis, and bacteremia in pediatric, elderly, and immunocompromised populations. Pneumococcal infection is the leading cause of pneumonia in children worldwide. Pneumococcal infections also occur frequently in at-risk populations including individuals with diabetes, asthma, chronic obstructive pulmonary disease, cardiovascular disease, human immunodeficiency virus (HIV), and sickle cell disease. In developed countries, pneumococcal infection is responsible for approximately 30% of all adult pneumonia cases and has a mortality rate of 11% to 40%.&nbsp;Due to this organism's impact on both morbidity and mortality in adults and children, healthcare efforts have relied on vaccines to reduce the rate of pneumococcal disease over the past 30 years. Vaccine research has focused on using immunogenic proteins and carbohydrates found on the pneumococcal surface as antigens.</p><p>Previous efforts to use protein vaccines were not successful as they only stimulated the human immune system. &nbsp;New&nbsp;research from Dr. Pratik Shah reports the discovery of a new protein molecule to&nbsp;immunize children, currently utilized by government agencies in Brazil and China, and by the Gates&nbsp;foundation, to develop affordable vaccines for prevention of pneumococcal diseases.&nbsp;&nbsp;Shah's approach cripples the bacterial nutrient acquisition and&nbsp;virulence pathways &nbsp;in addition&nbsp;to promoting effective recognition by the host immune system .</p><p><b>Project 1: &nbsp;</b>Discovery ofnovel protein vaccine antigens protective against <i>Streptococcus pneumoniae</i>&nbsp; pneumonia and invasive infections</p><ul><li>Discovered a bacterial ABC transporter that results in significant protective immunity in mice against carriage, pneumonia and bacteremia<br></li><li>Protein antigen-PotD used by government of Brazil and China in vaccine development and awarded Raymond Sarber National Award for Discovery in Microbiology by American Society of Microbiology.<br></li></ul><p><b>Project 2: </b>Elucidate the role of host and bacterial polyamine metabolism in bacterial infections</p><ul><li>Discovered polyamine biosynthesis and transport mechanisms are required for pneumococcal infection and are targets for prophylactic and therapeutic interventions<br></li></ul>", "people": ["pratiks@media.mit.edu"], "title": "Next-Generation Protein Vaccines Against Infectious Diseases", "modified": "2018-05-02T18:40:23.029Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "next-generation-low-cost-protein-vaccines-against-infectious-diseases"}, {"website": "http://opera.media.mit.edu/projects/deathandthepowers/powers_live.php", "description": "<p>Death and the Powers: Global Interactive Simulcast</p>", "people": ["ejessop@media.mit.edu", "patorpey@media.mit.edu", "sovsey@media.mit.edu", "tod@media.mit.edu", "holbrow@media.mit.edu", "benb@media.mit.edu"], "title": "Powers Live", "modified": "2018-10-23T15:20:09.180Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "death-and-the-powers-global-interactive-simulcast"}, {"website": "", "description": "<p>Skin and tissue perfusion measurements are important parameters for diagnosis of wounds and burns, and for monitoring plastic and reconstructive surgeries. In this project, we use a standard camera and a laser source in order to image blood-flow speed in skin tissue. We show results of blood-flow maps of hands, arms, and fingers. We combine the complex scattering of laser light from blood with computational techniques found in computer science.</p>", "people": ["raskar@media.mit.edu", "guysatat@media.mit.edu"], "title": "Skin Perfusion Photography", "modified": "2016-12-14T20:46:59.692Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "camera-culture"], "published": true, "active": false, "end_on": null, "slug": "skin-perfusion-photography"}, {"website": "", "description": "<p>Fluidics are promising foundational tools for synthetic biology. Unfortunately, fluidics are not broadly used, because they are difficult to manufacture and operate, and designs are currently not shared in a systematic fashion. <a href=\"https://metafluidics.org/\"><b>Metafluidics</b></a> is an open repository of both device and hardware designs to enable communities of users from around the world to share and remix bio-hardware. This repository is the hardware portal for the National Science Foundation-supported&nbsp;<strong><a href=\"https://www.programmingbiology.org/\">Living Computing Project</a></strong>. Share your fluidic devices with the global community today!</p>", "people": ["ninawang@media.mit.edu", "dkong@media.mit.edu"], "title": "Metafluidics", "modified": "2019-05-06T14:41:38.233Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "metafluidics"}, {"website": "", "description": "<p>VisualCV is an iconic representation of a person's career, exposing spatio-temporal patterns such as relocations and changes of position, and context information such as a person's impact on her environment. Similar to Tufte's \"Sparklines,\" the visualization does not use an explicit metric, but instead  suggests trends and shows patterns. At an individual level, this graphic representation has few advantages over a traditional CV; instead, it is meant for overview and comparison of a large number of CVs\ufffdwith hundreds of applications, patterns become immediately visible.</p>", "people": ["judith@media.mit.edu"], "title": "VisualCV", "modified": "2016-12-05T00:16:57.797Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-390", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "visualcv"}, {"website": "", "description": "<p>We envision a machine-driven evolution of the human body's form and function, where the programmable nature of machines plays a crucial role. Robotic joints worn on the wrist turn into extra fingers so that a person acquires skills beyond what five fingers can offer, or performs \"tri-manual\" tasks with the machine joints. The robot runs on a neural network that learns life-like controls autonomously. Similar to the way a human simulates actions in his or her mind, the robot conducts simulations in a virtual environment to teach itself a variety of handling strategies.</p>", "people": ["sangwon@media.mit.edu", "pattie@media.mit.edu"], "title": "Robotic Symbionts", "modified": "2017-10-14T20:19:00.058Z", "visibility": "LAB-INSIDERS", "start_on": null, "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "robotic-symbionts"}, {"website": "", "description": "<p>Even when real names and other personal information are stripped from metadata datasets, it is often possible to use just a few pieces of information to identify a specific person. Here, we study three months of credit card records for 1.1 million people and show that four spatiotemporal points are enough to uniquely reidentify 90 percent of individuals. We show that knowing the price of a transaction increases the risk of reidentification by 22 percent, on average. Finally, we show that even data sets that provide coarse information at any or all of the dimensions provide little anonymity, and that women are more reidentifiable than men in credit card metadata.</p>", "people": ["sandy@media.mit.edu", "yva@media.mit.edu"], "title": "On the re-identifiability of credit card metadata", "modified": "2019-04-19T14:48:17.075Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "on-the-reidentifiability-of-credit-card-metadata"}, {"website": "", "description": "<p>radiO_o is a battery-powered speaker worn by hundreds of party guests, turning each person into a local mobile sound system. The radiO_o broadcast system allows the DJ to transmit sounds over several pirate radio channels to mix sounds between hundreds of speakers roaming around the space and the venue's existing sound system.</p>", "people": ["geppetto@media.mit.edu", "tjlevy@media.mit.edu", "slavin@media.mit.edu", "cwwang@media.mit.edu", "novysan@media.mit.edu"], "title": "radiO_o", "modified": "2016-12-05T00:17:21.437Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "radio_o"}, {"website": "", "description": "<p>Selectricity (formerly HyperChad) is a Web-based voting system that supports anonymous and voter-verifiable balloting, and includes an election-methods library that implements a variety of election techniques, including several preferential systems. Unlike most voting projects, Selectricity does not attempt to address the issues raised in mainstream political elections. Instead, it provides a simple set of tools that small groups and organizations can use to incorporate computationally complex decision-making into new areas, and for purposes where they ordinarily would find such decision-making prohibitively complex. By supporting a variety of election methods, it provides a way for users to explore and compare the effects of different voting systems and, ultimately, come to better decisions.</p>", "people": ["csik@media.mit.edu"], "title": "Selectricity", "modified": "2016-12-05T00:16:49.678Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": null, "slug": "selectricity"}, {"website": "", "description": "<p>4K/8K Comics applies the affordances of ultra-high-resolution screens to traditional print media such as comic books, graphic novels, and other sequential art forms. The comic panel becomes the entry point to the corresponding moment in the film adaptation, while scenes from the film indicate the source frames of the graphic novel. The relationships among comics, films, social media, parodies, and other support materials can be navigated using native touch screens, gestures, or novel wireless control devices. Big data techniques are used to sift, store, and explore vast catalogs of long-running titles, enabling sharing and remixing among friends, fans, and collectors.</p>", "people": ["novysan@media.mit.edu", "vmb@media.mit.edu"], "title": "4K/8K Comics", "modified": "2018-06-20T17:07:10.273Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "4k8k-comics"}, {"website": "", "description": "<p>A recent focus of our lab has been making use of Tangible Displays and Body Object Space to develop new assistive technologies. As a test case, we prototyped the Mario side-scrolling game for visually impaired users, using body movement analogies to control Mario in the game. Mario and 2D side scrollers present a particularly interesting case, as they keep the main character location in the center of the display and move the world around the character. The shape display itself provides spatial audio of enemy positions. We make use of the AUFLIP sensor platform to pick up body movements\u2014walking and jumping, causing Mario to do the same in-game. This enables users to keep their hands engaged to understand the game landscape, while using their body to control Mario at the same time.&nbsp;</p>", "people": ["dvlevine@media.mit.edu", "ishii@media.mit.edu", "kalli@media.mit.edu"], "title": "CONJURE", "modified": "2019-02-08T17:48:57.541Z", "visibility": "PUBLIC", "start_on": "2017-03-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "conjugate"}, {"website": "", "description": "<p>Engineering in a limit of thermodynamic complexity, by compiling global optimizations into local dynamics in systems including analog logic, paintable computing, and quantum optics</p>", "people": ["neilg@media.mit.edu"], "title": "Statistical-Mechanical Engineering", "modified": "2016-12-05T00:17:04.904Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": null, "slug": "statistical-mechanical-engineering"}, {"website": "", "description": "<p>In a world where sensors, data storage, and processing power are too cheap to meter, how do you ensure that users can realize the full value of their data while protecting their privacy? openPDS is a field-tested, personal metadata management framework that allows individuals to collect, store, and give fine-grained access to their metadata to third parties. SafeAnswers is a new and practical way of protecting the privacy of metadata at an individual level. SafeAnswers turns a hard anonymization problem into a more tractable security one. It allows services to ask questions whose answers are calculated against the metadata, instead of trying to anonymize individuals' metadata. Together, openPDS and SafeAnswers provide a new way of dynamically protecting personal metadata.</p>", "people": ["brian717@media.mit.edu", "shmueli@media.mit.edu", "sandy@media.mit.edu", "yva@media.mit.edu"], "title": "openPDS/ SaferAnswers: Protecting the privacy of metadata", "modified": "2019-04-19T14:49:52.126Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "openpds-saferanswers-protecting-the-privacy-of-metadata"}, {"website": "", "description": "<p>We are developing a very intuitive and interactive platform to make complex information--especially science, technology, engineering, and mathematics (STEM) material--truly accessible to blind and visually impaired students by using a tactile device with no loss of information compared with printed materials. A key goal of this project is to develop tactile information-mapping protocols through which the tactile interface can best convey educational and other graphical materials.</p>", "people": ["pattie@media.mit.edu", "namdev@media.mit.edu"], "title": "STEM Accessibility Tool", "modified": "2016-12-05T00:16:52.685Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "stem-accessibility-tool"}, {"website": "", "description": "<p>Prolonged space travel plays a severe toll on the human body: microgravity impairs muscle and bone growth, and high doses of radiation cause irreversible mutations. As we seriously consider the human species becoming space-faring, a big question stands: even if we do break free from Earth\u2019s orbit, can we adapt to the extreme environments of space? Lisa Nip examines our odds.\n                    \n                </p>", "people": [], "title": "Limitations and approaches to solving the human body in space", "modified": "2016-12-16T20:27:34.595Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "limitations-and-approaches-to-solving-the-human-body-in-space"}, {"website": "", "description": "<p>Today, people can tell stories by composing, manipulating, and sequencing individual media artifacts using digital technologies. However, these tools offer little help in developing a story's plot. Specifically, when a user tries to construct her story based on a collection of individual media elements (videos, audio samples), current technological tools do not provide helpful information about the possible narratives that these pieces can form. Storied Navigation is a novel approach to this problem; media sequences are tagged with free-text annotations and stored as a collection. To tell a story, the user inputs a free-text sentence and the system suggests possible segments for a storied succession. This process iterates progressively, helping the user to explore the domain of possible stories. The system achieves the association between the input and the segments' annotations using reasoning techniques that exploit the WordNet semantic network and common-sense reasoning technology.</p>", "people": ["barbara@media.mit.edu", "lieber@media.mit.edu", "gid@media.mit.edu"], "title": "Storied Navigation", "modified": "2016-12-05T00:16:29.416Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "storied-navigation"}, {"website": "", "description": "<p>The analysis of ubiquitous biometric data is fundamental to pervasive, human-centric computer technologies. A major technical focus of our research is the extraction of human perceptual signatures for recognition. Our work exploits novel machine learning techniques inspired by mathematical methods developed in fields such as psychometrics and econometrics. Not only is biometric computing relevant to the burgeoning security industry, but when directed at ourselves, biometric technologies can serve as reflectors that enhance our self-awareness, understanding, and health, and they can facilitate our interaction with each another and computers.</p>", "people": [], "title": "Biometric Reflections / Biometric Computing", "modified": "2016-12-05T00:17:07.690Z", "visibility": "LAB", "start_on": "2006-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": null, "slug": "biometric-reflections-biometric-computing"}, {"website": "", "description": "<p>Location prediction is a critical building block in many location-based services and transportation management. This project explores the issue of next-location prediction based on the longitudinal movements of the locations individuals have visited, as observed from call detail decords (CDR). In a nutshell, we apply recurrent neural network (RNN) to next-location prediction on CDR. RNN can take in sequential input with no restriction on the dimensions of the input. The method can infer the hidden similarities among locations and interpret the semantic meanings of the locations. We compare the proposed method with Markov and a Naive Model proving that RNN has better accuracy in location prediction. </p>", "people": ["sandy@media.mit.edu", "yleng@media.mit.edu"], "title": "Recurrent neural network in context-free next-location prediction", "modified": "2019-04-19T14:52:06.657Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "recurrent-neural-network-in-context-free-next-location-prediction"}, {"website": "", "description": "<p>Communication technologies, from printing to social media, affect our historical records by changing the way ideas are spread and recorded. Yet, finding statistical evidence of this fact has been challenging. Here we combine a common causal inference technique (instrumental variable estimation) with a dataset on nearly forty thousand biographies from Wikipedia (Pantheon 2.0) to study the effect of the introduction of printing in European cities on Wikipedia\u2019s digital biographical records. By using a city\u2019s distance to Mainz as an instrument for the adoption of the movable type press, we show that European cities that adopted printing earlier were more likely to become the birthplace of a famous scientist or artist during the years following the invention of printing. We bring these findings to recent communication technologies by showing that the number of radios and televisions in a country correlates with the number of globally famous performing artists and sports players born in that country, even after controlling for GDP, population, and including country and year fixed effects. These findings support the hypothesis that the introduction of communication technologies can bias historical records in the direction of the content that is best suited for each technology.&nbsp;</p>", "people": ["hidalgo@media.mit.edu", "crisjf@media.mit.edu", "amy_yu@media.mit.edu"], "title": "How the medium shapes the message: Printing and the rise of the arts and sciences", "modified": "2019-03-08T15:11:50.010Z", "visibility": "PUBLIC", "start_on": "2015-12-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "medium-shapes-message"}, {"website": "", "description": "<p>\"Fab Lab\" is an abbreviation for Fabrication Laboratory, a group of off-the-shelf, industrial-grade fabrication and electronics tools, wrapped in open-source software and programs. Fab Labs give users around the world the ability to locally conceptualize, design, develop, fabricate, and test almost anything. The engineering capability for design and fabrication at micron length and microsecond time-scales opens up numerous possibilities for innovative solutions to common problems. Since local communities foster this innovation, it can lead to sustainable solutions. As yet, high-end technological solutions have not been addressing problems faced on the local level; therefore, we believe Fab Labs will provide a thriving incubator for local micro-businesses.</p>", "people": ["millner@media.mit.edu", "neilg@media.mit.edu"], "title": "Fab Labs", "modified": "2016-12-05T00:16:24.596Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": null, "slug": "fab-labs"}, {"website": "", "description": "", "people": [], "title": "Staff Enrichment", "modified": "2016-11-22T20:50:51.466Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "staff-enrichment"}, {"website": "", "description": "<p>As part of human evolution and revolution, food is among the earliest forms of human interaction, but it has remained essentially unchanged from ancient to modern times. What if we introduced engineered and programmable food materials? With that change, food can change its role from passive to active. Food can \"communicate\" using its inherent behaviors combined with engineering accuracy. Food becomes media and interface. During an MIT winter course we initiated and taught, we encouraged students to design pneumatic food. Students successfully implemented inflatable sugar and cheese products. To inflate food, we use both an engineering approach and a biological approach; to solidify the inflated food, we introduce both heat via the oven, and coldness with liquid nitrogen.</p>", "people": ["wwen@media.mit.edu", "liningy@media.mit.edu", "ishii@media.mit.edu", "jifei@media.mit.edu"], "title": "Inflated Appetite", "modified": "2016-12-24T22:05:38.254Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "tangible-media"], "published": true, "active": false, "end_on": null, "slug": "inflated-appetite"}, {"website": "", "description": "<p>Common Consensus is a fun, self-sustaining web-based game that both collects and validates commonsense knowledge about everyday goals. Goals are a key element of commonsense knowledge; in many of our inferface agents, we need to recognize goals from user actions (plan recognition), and generate sequences of actions that implement goals (planning). We also often need to answer more general questions about the situations in which goals occur, such as when and where a particular goal might be likely, or how long it is likely to take to achieve.</p>", "people": ["lieber@media.mit.edu"], "title": "CommonConsensus: A Game for Collecting Commonsense Goals", "modified": "2016-12-05T00:17:09.021Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-320", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "commonconsensus-a-game-for-collecting-commonsense-goals"}, {"website": "", "description": "<p>The mission of the Media Lab's new Center for Terrestrial Sensing is to explore unconventional ways to sense and visualize inaccessible natural environments where it is impossible for humans to go physically such as underground, undersea oil fields and in the atmosphere. How people connect with, navigate, and interact with large amounts of geoscience information is an area with world-changing potential and deep challenges. The Center for Terrestrial Sensing aims to connect People to the Planet.</p>", "people": [], "title": "Center for Terrestrial Sensing", "modified": "2016-12-29T21:36:09.792Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "center-for-terrestrial-sensing"}, {"website": "", "description": "<p>WATCH is a system that attempts to measure the possible influence that a new time-management interface will have on improving the habits of a user. Users set goals for each of the activities detected by the app. Detected activities include physical activity and time spent in pre-defined locations. An Andriod app (WATCH) on their personal phones is able to track their activities (running, walking, and sitting) as well as their GPS location. Their progress in comparison to their goals is displayed on their home screens as a pie chart.</p>", "people": ["nfarve@media.mit.edu", "pattie@media.mit.edu"], "title": "WATCH", "modified": "2016-12-05T00:16:58.002Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "watch"}, {"website": "", "description": "<p>Developed as part of the Playful Words research project at the MIT Media Lab's Laboratory for Social Machines, Learning Loops aims to make literacy learning\u2014both on and off the screen\u2014a family experience. We create small-scale coach-family networks centered around children\u2019s play on custom-built, open-ended literacy learning apps. Building Coach-family networks helps to empower children as authors and facilitate their narrative development. </p><p>StoryBlocks, a Learning Loops app, aims to promote literacy and social-emotional development through storytelling for children ages 6-10. StoryBlocks allows children to create and customize their own comic-style stories. These stories are analyzed using tools developed by the Learning Loops team to document children\u2019s narrative development, and support Coaches as they provide personalized scaffolding for children\u2019s narratives.&nbsp;</p><p><b><i>How does the Learning Loop work?</i></b></p><p>Data captured from a child\u2019s use of StoryBlocks is streamed via the internet to cloud servers, and can immediately be accessed from a remote location by the child\u2019s Coach. We have developed a Coach\u2019s dashboard, called the Coach Console, powered by play analytics which enables a Coach to rapidly inspect play traces collected from a child\u2019s activity and pull out their salient achievements, or meaningful moments. The Coaches then translate these moments into short personalized messages for the caregiver to inform them on their child\u2019s narrative progress and provide suggestions for how to encourage new activities using StoryBlocks, together with background knowledge about their child\u2019s path to literacy. Caregivers communicate with Coaches via text messages. Coaches can also help the children expand their sphere of learning and exploration by providing feedback on children's stories and suggesting new story starters&nbsp; directly to the child\u2019s device that are based on trends in the child\u2019s play data.</p><p><a href=\"http://learningloops.org\">More info is available on the Learning Loops website.</a></p>", "people": ["anneli@media.mit.edu", "bonnerd@media.mit.edu", "snehapm@media.mit.edu", "sballing@media.mit.edu", "jnazare@media.mit.edu"], "title": "Learning Loops", "modified": "2019-04-11T16:00:08.728Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "family-learning-coach"}, {"website": "", "description": "<p>The vision of pervasive computing is now mainstream. These connected devices permeate every aspect of our lives. Yet, we remain tethered to arcane user interfaces. Unlike consumer devices, building appliances and utilities perpetuate this outdated vision. Lighting control is a prime example. Here, we show how a data-driven methodology\u2014using people and sensors\u2014enables an entirely new method of lighting control.<br></p><p>We are evaluating new methods of interacting and controlling solid-state lighting based on our findings of how participants experience and perceive architectural lighting in our new lighting laboratory (E14-548S). This work, aptly named \"Experiential Lighting,\" reduces the complexity of modern lighting controls (intensity/color/space) into a simple mapping, aided by both human input and sensor measurement. We believe our approach extends beyond general lighting control and is applicable in situations where human-based rankings and preference are critical requirements for control and actuation. We expect our foundational studies to guide future camera-based systems that will inevitably incorporate context in their operation (e.g., Google Glass).</p>", "people": ["joep@media.mit.edu", "maldrich@media.mit.edu", "nanzhao@media.mit.edu"], "title": "Experiential Lighting: New user interfaces for lighting control", "modified": "2019-04-19T14:25:05.542Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "experiential-lighting-new-user-interfaces-for-lighting-control"}, {"website": "", "description": "", "people": ["hbedri@media.mit.edu"], "title": "Holo basketball", "modified": "2018-10-14T16:56:18.594Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "holo-basketball"}, {"website": "", "description": "<p>A tool that would allow journalists to compare the proportion of coverage\ufffdbased on Media Cloud\ufffdon a given health subject to 1) the proportion of related research spending in a given year, and 2) the relative health impact of that subject on the population.</p>", "people": ["a_hashmi@media.mit.edu"], "title": "Media Attention and Health", "modified": "2016-12-05T00:16:35.669Z", "visibility": "LAB", "start_on": "2014-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "media-attention-and-health"}, {"website": "", "description": "<p>We are developing a printer that builds functional, 3-D structures by reversible assembly of a discrete set of \"digital materials.\" This approach uses the components rather than a control system to impose the spatial and functional constraints. Printing can be performed as a parallel rather than a linear process; the printing process is reversible for re-use of the pieces or error correction at any point in the object\ufffds life. Error detection, error-reduction, and error-tolerance during assembly allow for reliable, high-throughput printing. We are presenting development approaches to such a printing device.</p>", "people": ["neilg@media.mit.edu"], "title": "Digital Printing of Digital Materials", "modified": "2016-12-05T00:17:10.679Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": null, "slug": "digital-printing-of-digital-materials"}, {"website": "", "description": "<p>Imagine a future where lights are not fixed to the ceiling, but follow us wherever we are. In this colorful world we enjoy lighting that is designed to go along with the moment, the activity, our feelings, and our outfits. Halo is a wearable lighting device created to explore this scenario. Different from architectural lighting, this personal lighting device aims to illuminate and present its user. Halo changes the wearer's appearance with the ease of a button click, similar to adding a filter to a photograph. It can also change the user's view of the world, brightening up a rainy day or coloring a gray landscape. Halo can react to activities and adapt based on context. It is a responsive window between the wearer and his or her surroundings.</p>", "people": ["joep@media.mit.edu", "nanzhao@media.mit.edu"], "title": "Halo: Wearable Lighting", "modified": "2016-12-05T00:16:27.619Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "halo-wearable-lighting"}, {"website": "", "description": "<p>Wildflower is an open-source approach to Montessori learning. Its aim is to be an experiment in a new learning environment, blurring the boundaries between home-schooling and institutional schooling, between scientists and teachers, between schools and the neighborhoods around them. At the core of Wildflower are nine principles that define the approach.  The Wildflower approach has been implemented by several schools, which serve as a research platform for the development of Montessori materials that advance the Montessori Method, software tools that enable Montessori research, and social software that fosters the growth and connection of such schools.</p>", "people": ["sdkamvar@media.mit.edu"], "title": "Wildflower Montessori", "modified": "2016-12-05T00:16:59.173Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "wildflower-montessori"}, {"website": "", "description": "<p>A large-scale art installation that investigates the biological systems that represent and embody human life, and their relationship to the built environment. This synthetic organism, built from interconnected microbiological systems, will be sustained in part through its own feedback and feedforward loops, but also through interactions with the architectural systems (like HVAC). As the different systems react and exchange material inputs and outputs, they move towards homeostasis. In the process, Homeostasis creates a new landscape of the human body, in which we can experience the wonder and vulnerability of its interconnected systems.</p>", "people": ["slavin@media.mit.edu"], "title": "Homeostasis", "modified": "2016-12-05T00:17:13.990Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "homeostasis"}, {"website": "", "description": "<p>A tabletop set of cellular automata ready to exhibit complex systems through simple behaviors, AutomaTiles explores emergent behavior through tangible objects. Individually they live as simple organisms, imbued with a simple personality; together they exhibit something \"other\" than the sum of their parts. Through communication with their neighbors, complex interactions arise. What will you discover with AutomaTiles?</p>", "people": ["jbobrow@media.mit.edu", "slavin@media.mit.edu"], "title": "AutomaTiles", "modified": "2017-01-05T22:03:10.469Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "playful-systems"], "published": true, "active": false, "end_on": null, "slug": "automatiles"}, {"website": "", "description": "<p>This is a new platform to automate experiments in genetic engineering and bring large-scale moonshot projects within reach. Too often, lab experiments are limited in scale by human fatigue and costs associated with manual labor. In particular, the process of delivering genetic materials via manual microinjection remains a long-standing bottleneck. We are developing a computer-assisted microinjection platform to streamline the production of transgenic organisms. Briefly, organisms are immobilized in a gel and microinjections are performed using precision robotics using computer vision algorithms. This platform demonstrated high-throughput gene editing in an animal model (C. elegans) for the first time. We will use this technology to refine and create safeguards for our gene drive technology.</p>", "people": ["erikad@media.mit.edu", "jmin01@media.mit.edu", "codyg@media.mit.edu", "esvelt@media.mit.edu"], "title": "Computer-Assisted Transgenesis", "modified": "2016-12-05T00:17:09.401Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "computer-assisted-transgenesis"}, {"website": "", "description": "<p>Making optimal decisions can improve a wide array of situations. Humans often perform well on small, focused choices, but performance degrades as complexity increases. Justify leverages human fine-grained reasoning capabilities into a hierarchy that automatically aggregates and summarizes at each level. This flexible organization makes understanding complex arguments more manageable. A Justify discussion is comprised of points; each point has a type that conveys its domain-independent meaning and determines its \"summarization strategy.\" There are points for questions, answers, arithmetic, pro and con rationale, voting, and grouping that help to crystalize an issue. These point types represent a language to facilitate reasoning both for humans and the Justify program itself.</p>", "people": ["lieber@media.mit.edu", "cfry@media.mit.edu"], "title": "Justify", "modified": "2016-12-05T00:16:32.917Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "justify"}, {"website": "", "description": "<p>An astronaut's ability to leverage the sense of touch is substantially reduced when wearing a protective, pressurized spacesuit. We imagine reinstating the biological skin's sense of touch by mapping inputs on the protective suit's exterior layer to a haptic feedback system on the biological skin.&nbsp;</p><p>This concept is an illustrative application area for the <a href=\"https://www.media.mit.edu/projects/SpaceSkin/overview/\"><b>SpaceSkin</b></a>&nbsp;project, showcasing how aerospace-grade electronic textiles&nbsp; might improve astronaut situational awareness, as well as strengthen a sense of connection to fellow explorers.&nbsp;</p>", "people": ["cherston@media.mit.edu"], "title": "SpaceTouch", "modified": "2019-05-28T18:04:56.501Z", "visibility": "PUBLIC", "start_on": "2019-04-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "spaceTouch"}, {"website": "", "description": "<p>We are exploring mathematical modeling of time-of-flight imaging problems and solutions.</p>", "people": ["raskar@media.mit.edu", "achoo@media.mit.edu", "ayush@media.mit.edu"], "title": "Inverse problems in time-of-flight imaging", "modified": "2019-04-19T18:28:33.852Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "inverse-problems-in-time-of-flight-imaging"}, {"website": "", "description": "<p>Our architecture takes a hybrid approach to microwaves and treats them like waves of light. Most other work places antennas in a 2D arrangement to directly sample the RF reflections that return. Instead of placing antennas in a 2D arrangment, we use a single, passive, parabolic reflector (dish) as a lens. Think of every point on that dish as an antenna with a fixed phase-offset. This means that the lens acts as a fixed set of 2D antennas which are very dense and spaced across a large aperture. We then sample the focal-plane of that lens. This architecture makes it possible for us to capture higher resolution images at a lower cost.</p>", "people": ["raskar@media.mit.edu", "michaf@media.mit.edu", "naik@media.mit.edu"], "title": "Time-of-Flight Microwave Camera", "modified": "2016-12-05T00:16:55.816Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "time-of-flight-microwave-camera"}, {"website": "", "description": "<p>We have developed a camera system that captures movies at an effective rate of approximately one trillion frames per second. In one frame of our movie, light moves only about 0.6 mm. We can observe pulses of light as they propagate through a scene. We use this information to understand how light propagation affects image formation and to learn things about a scene that are invisible to a regular camera.</p>", "people": ["raskar@media.mit.edu"], "title": "Trillion Frames Per Second Camera", "modified": "2016-12-05T00:17:00.920Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "trillion-frames-per-second-camera"}, {"website": "", "description": "<p>This project aims to build a powerful system as a scientific tool for bridging the gap in the literature by determining the dynamic biomechanics of the lower-limb joints and metabolic effects of physical interventions during natural locomotion. This system is meant for use in applying forces to the human body and measuring force, displacement, and other physiological properties simultaneously, helping investigate controllability and efficacy of mechanical devices physically interacting with a human subject.</p>", "people": ["kenpasch@media.mit.edu", "kuan525@media.mit.edu", "hherr@media.mit.edu"], "title": "Tethered robotic system for understanding human movements", "modified": "2019-04-17T19:22:13.955Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "tethered-robotic-system-for-understanding-human-movements"}, {"website": "http://www.eduardocastello.com", "description": "<p> A learning framework for secure, decentralized, computationally efficient data and model sharing among multiple robot units installed at multiple sites.<br></p><p>Robots have potential to revolutionize the way we interact with the world around us. One of their greatest potentials is in the domain of mobile health, where they can be used to facilitate clinical interventions. However, to accomplish this, robots need to have access to our private data in order to learn from these data and improve their interaction capabilities. To enhance this learning process, knowledge sharing among multiple robot units is the natural step forward. However, to date, there is no well-established framework which allows for such data sharing while preserving the privacy of the users, such as hospital patients. To this end, we introduce RoboChain: the first learning framework for secure, decentralized, computationally efficient data and model sharing among multiple robot units installed at multiple sites such as hospitals. RoboChain builds upon and combines the latest advances in open data access, blockchain technologies, and machine learning. We illustrate this framework using the example of a clinical intervention conducted in a private network of hospitals. Specifically, we lay down the system architecture that allows multiple robot units, conducting the interventions at different hospitals, to perform efficient learning without compromising the data privacy.&nbsp;&nbsp;</p>", "people": ["ecstll@media.mit.edu", "orudovic@media.mit.edu", "sandy@media.mit.edu"], "title": "RoboChain: A secure data-sharing framework for human-robot interaction", "modified": "2019-04-19T14:53:57.705Z", "visibility": "PUBLIC", "start_on": "2018-03-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "robochain-a-secure-data-sharing-framework-for-human-robot-interaction"}, {"website": "", "description": "<p>A game-based activity for developing an understanding of the history of American schooling.</p>", "people": ["jhaas@media.mit.edu"], "title": "Committee of N", "modified": "2018-06-20T19:48:21.060Z", "visibility": "LAB", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ml-learning"], "published": true, "active": false, "end_on": null, "slug": "committee-of-n"}, {"website": "", "description": "", "people": [], "title": "Metaspace II", "modified": "2016-12-05T18:15:31.062Z", "visibility": "PUBLIC", "start_on": "2015-10-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "metaspace-ii"}, {"website": "", "description": "<p>As part of our larger effort to build out a suite of tools for community organizers, we are helping to build their capacity to do their own creative data visualization and presentation. New computer-based tools are lowering the barriers of entry for making engaging and creative presentations of data. Rather than encouraging partnerships with epidemiologists, statisticians, or programmers, we see an opportunity to build capacity within small community organizations by using these new tools. This work involves workshops, webinars, and writing about how to pick more creative ways to present their data stories.</p><p><a href=\"https://datatherapy.org\">datatherapy.org</a></p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "Data Therapy", "modified": "2016-12-05T00:16:57.978Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "data-therapy"}, {"website": "", "description": "", "people": [], "title": "*Dive", "modified": "2016-10-21T15:04:34.162Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["collective-learning"], "published": false, "active": false, "end_on": null, "slug": "dive-new"}, {"website": "https://gobo.social", "description": "<h1>Your social media. Your rules.</h1><p><a href=\"http://gobo.social\">Gobo</a> is an experiment, not a startup. We\u2019re building it to change the conversation on social media and imagine a better version of it. This is a technology-to-think-with\u2014a tool we want you to play with and push against. Gobo is being built by a small team at <a href=\"https://www.media.mit.edu/groups/civic-media/overview/\">MIT Media Lab's Center for Civic Media</a>, where we work on technologies for social change.</p><p>For questions, feedback, and musings, you can reach the Gobo team at <a href=\"mailto:gobo@media.mit.edu\">gobo@media.mit.edu</a>.</p><h2>Control your own feed</h2><p>Social media companies use algorithms to control what we see on our feeds, but we don\u2019t know how these algorithms work. &nbsp;As a result, we\u2019re often unaware why certain posts show up in our feed while others don\u2019t. Gobo allows you to control the algorithms, or a set of \u201crules,\u201d so you can decide what gets shown on your feed and know why.</p><h2>Connect multiple platforms</h2><p>We believe that multiple social media platforms should exist to serve different purposes. However, it\u2019s not easy to keep up with all these platforms, especially when your data can\u2019t be easily shared between them. Gobo allows you to connect up to three platforms, so you can view all of your feeds in one place. </p><h2>See what gets hidden</h2><p>We believe that transparency can help you better understand what you see on social media and keep platforms accountable for algorithmic bias. Gobo tells you why certain posts are hidden based on the rules you set. It also shows you how many posts are hidden, so you can understand the overall impact of the rules you set.</p><h2>Expand your perspective</h2><p>Social media companies make assumptions about what we want to see based on what we read and click on. They tend to show us content we\u2019re already engaging with, reinforcing our echo chambers. Instead of assuming what you want to see, Gobo allows you to add unfamiliar perspectives into your feed, so you can better understand the range of opinions that are shared online.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "ahope@media.mit.edu", "jasrub@media.mit.edu", "dsjen@media.mit.edu"], "title": "Gobo", "modified": "2019-04-10T14:21:45.965Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["center-for-civic-media", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "gobo"}, {"website": "", "description": "<p>App Inventor is an intuitive, visual programming environment that allows everyone, even those with no prior coding experience, to build fully functional applications for smartphones and tablets. Those new to App Inventor can have a simple first app up and running in under 30 minutes. The tool allows anyone to program more complex, impactful apps in significantly less time than with more traditional programming environments. The MIT App Inventor project seeks to democratize software development by empowering all people, especially young people, to transition from being consumers of technology to becoming creators of it. MIT students and staff, led by Professor Hal Abelson, form the nucleus of an international movement of inventors. In addition to leading educational outreach around MIT App Inventor and conducting research on its impacts, this core team maintains the free online app development environment that serves more than four million registered users.</p>", "people": ["mckinney@media.mit.edu", "hal@media.mit.edu"], "title": "App Inventor", "modified": "2016-12-05T00:16:57.950Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "app-inventor"}, {"website": "", "description": "<p>Computers usually require us to be precise about what we want them to do and how we want them to do it, but humans find it hard to be so formal. If we were to give computers formal examples of our informal instructions, maybe they could learn to relate the natural instructions of ordinary users with the specifications, code, and tests with which they are comfortable. Zones and ProcedureSpace are examples of this. Zones is a code search interface that connects code with comments about its purpose. Completed searches become annotations, so the system learns by example. The backend, ProcedureSpace, finds code for a purpose comment (or vice versa) by relating words and phrases to code characteristics and natural language background knowledge. Users of the system are able to describe what they want in their own words, and often find that the system gives them helpful code.</p>", "people": ["lieber@media.mit.edu"], "title": "ProcedureSpace: Managing Informality by Example", "modified": "2016-12-05T00:16:45.783Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-385", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "procedurespace-managing-informality-by-example"}, {"website": "", "description": "<p>The boundaries and fabric of human experience are continuously redefined by microorganisms, interacting at an imperceptible scale. Though hidden, these systems condition our bodies, environment, and even sensibilities and desires. The proposed works introduce a model of interaction in which the microbiome is an extension of the human sensory system, accessed through a series of biological interfaces that enable exchange. Biological Interfaces transfer discrete behaviors of microbes into information across scales, where it may be manipulated, even if unseen. In the same way the field of HCI has articulated our exchanges with electronic signals, Soft Exchange opens up the question of how to design for this other invisible, though present, and vital material.</p>", "people": ["slavin@media.mit.edu"], "title": "Soft Exchange: Interaction Design with Biological Interfaces", "modified": "2016-12-05T00:16:11.253Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "soft-exchange-interaction-design-with-biological-interfaces"}, {"website": "", "description": "\u200b<p>In the United States, there are an estimated 1.7 million people living with amputation, with that number expected to double by 2050. Complications of prosthetic leg use in persons with lower extremity amputation (LEA) include delayed wound healing, recurrent skin ulcerations, and pressure damage to soft tissues. This can result in limited mobility, which further contributes to conditions such as obesity, musculoskeletal pathologies (e.g., osteoarthritis, osteopenia, and osteoporosis), as well as cardiovascular disease. Traditionally, fabrication of prosthetic sockets remains a fundamentally artisanal process with limited input of quantitative data. Even with advances in computer-aided design and manufacturing (CAD/CAM), prosthetists often modify sockets using non-quantitative craft processes requiring substantial human hours and financial cost. The goal of this research is to develop and validate musculoskeletal ultrasound imaging techniques for creating predictive biomechanical models of residual limbs that will reduce the barrier for and cost of computer aided design (CAD)-driven prosthetic socket design in the US and in low-and middle-income countries.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>\u200b", "people": ["branger@media.mit.edu", "kmoerman@media.mit.edu", "hherr@media.mit.edu"], "title": "Ultrasound imaging for transtibial prosthetic interface design", "modified": "2019-04-17T19:24:01.508Z", "visibility": "PUBLIC", "start_on": "2016-10-18", "location": "", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "ultrasound-prosthetic-interface-design"}, {"website": "", "description": "", "people": [], "title": "Sample Changing Places Project", "modified": "2016-11-28T14:15:19.628Z", "visibility": "PUBLIC", "start_on": "2016-11-28", "location": "", "groups": ["changing-places"], "published": false, "active": false, "end_on": null, "slug": "sample-changing-places-project"}, {"website": "", "description": "<p>The well-known \"small-world\" phenomenon indicates that an individual can be connected with any other in the world through a limited number of personal acquaintances. Furthermore, Nicholas and Fowler show that not only are we connected to each other, but we could also shape the behavior of our friends' friends. In this project, we are interested in understanding how social influence propagates and triggers behavioral change in social networks. Specifically, we analyze a large-scale, one-month international event held in the European country of Andorra using country-wide mobile phone data, and investigate the change in the likelihood of attending the event for people that have been&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">influenced by and are of different social distances from the attendees.&nbsp;</span></p><p><span style=\"font-size: 18px; font-weight: normal;\">Our results suggest that social influence exhibits the ripple effect, decaying across social distances from the source but persisting up to six degrees of separation. We further show that influence decays as communication delay increases and intensity decreases. Such ripple effect in social communication can lead to important policy implications in applications where it is critical to trigger behavior change in the population.</span></p>", "people": ["sandy@media.mit.edu", "emoro@media.mit.edu", "yleng@media.mit.edu", "xdong@media.mit.edu"], "title": "The Ripple Effect: You are more influential than you think", "modified": "2019-04-19T14:55:02.488Z", "visibility": "PUBLIC", "start_on": "2016-08-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "the-ripple-effect-your-are-more-influential-than-you-think"}, {"website": "", "description": "<p>Emotionally Intelligent Music Playback opens possibilities to various emotional trajectories through a piece of music. The listener can navigate through emotional territories via a touchscreen interface. The system transitions seamlessly to corresponding emotional interpretations extracted from various existing renditions of the same composition.</p>", "people": ["hanelee@media.mit.edu", "davidsu@media.mit.edu"], "title": "Emotionally Intelligent Playback", "modified": "2018-04-27T21:28:46.680Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "emotionally-intelligent-playback"}, {"website": "", "description": "<p>The Gamma Instrument is a small-format interactive device hovering between the realms of musical instrument and medical instrument. A capacitive hand-following interface allows one to create&nbsp;abstract&nbsp;gamma sounds while surrounded by an orbit of gamma-frequency lights. Creating musical tones on the device could heighten gamma entrainment as it mimics higher-level cognition and gamma-band processing noted in musicians. This device is part of a larger exploration of 40 Hz frequencies and Alzheimer\u2019s prevention/reduction within the context of the Aging Brain Initiative at MIT. The tabletop multi-sensory experience brings aspects of the Cognitarium to an interactive and portable platform.</p>", "people": ["arieger@media.mit.edu"], "title": "Gamma Instrument", "modified": "2019-05-13T02:44:22.899Z", "visibility": "PUBLIC", "start_on": "2018-01-18", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "gamma-instrument"}, {"website": "", "description": "<p>In this project we investigate how the process of building a circuit can be made more organic, like sketching in a sketchbook. We integrate a rechargeable power supply into the spine of a traditional sketchbook, so that each page of the sketchbook has power connections. This enables users to begin creating functioning circuits directly onto the pages of the book and to annotate as they would in a regular notebook.  The sequential nature of the sketchbook allows creators to document their process for circuit design.  The book also serves as a single physical archive of various hardware designs.  Finally, the portable and rechargeable nature of the book allows users to take their electronic prototypes off of the lab bench and share their creations with people outside of the lab environment.</p>", "people": ["joep@media.mit.edu", "jieqi@media.mit.edu"], "title": "Hacking the Sketchbook", "modified": "2016-12-05T00:16:27.472Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "hacking-the-sketchbook"}, {"website": "", "description": "<p>In physics education, virtual simulations have given us the ability to show and explain phenomena that are otherwise invisible to the naked eye. However, experiments with analog devices still play an important role. They allow us to verify theories and discover ideas through experiments that are not constrained by software. What if we could combine the best of both worlds? We achieve that by building our applications on a projected augmented reality system. By projecting onto physical objects, we can paint the phenomena that are invisible. With our system, we have built \"physical playgrounds\": simulations that are projected onto the physical world and that respond to detected objects in the space. Thus, we can draw virtual field lines on real magnets, track and provide history on the location of a pendulum, or even build circuits with both physical and virtual components.</p>", "people": ["pattie@media.mit.edu", "kubat@media.mit.edu", "linder@media.mit.edu"], "title": "Enlight", "modified": "2016-12-05T00:16:23.642Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "enlight"}, {"website": "", "description": "<p>This project examines how the expression granted by new musical interfaces can be harnessed to create positive changes in health and wellbeing. We are conducting experiments to measure EEG dynamics and physical movements performed by participants who are using software designed to invite physical and musical expression of the basic emotions. The present demonstration of this system incorporates an expressive gesture sonification system using a Leap Motion device, paired with an ambient music engine controlled by EEG-based affective indices. Our intention is to better understand affective engagement, by creating both a new musical interface to invite it, and a method to measure and monitor it. We are exploring the use of this device and protocol in therapeutic settings in which mood recognition and regulation are a primary goal.</p>", "people": ["picard@media.mit.edu", "gleslie@media.mit.edu"], "title": "An EEG and motion-capture based expressive music interface for affective neurofeedback", "modified": "2019-04-19T14:56:44.273Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "an-eeg-and-motion-capture-based-expressive-music-interface-for-affective-neurofeedback"}, {"website": "", "description": "Terahertz time-domain spectroscopy (THz-TDS) is a leading method for inspection in the frequency range of 0.1- 10 THz. In contrast to IR-based time-of-flight cameras, coherent techniques, and X-ray, THz-TDS provides both fine time resolution and broadband spectral information. We exploit both of these to extract occluding content from layers whose thicknesses are wavelength comparable. The method uses the statistics of the THz E-field at subwavelength gaps to lock into each layer position and then uses a time-gated spectral kurtosis to tune to highest spectral contrast of the content on that specific layer. To demonstrate, occluding textual content was successfully extracted from a sample similar to a closed book down to 9 pages without human supervision. The study impacts inspection of structural defects in wooden objects, plastic components, composites; drugs, and specially cultural artifacts with subwavelength or wavelength comparable layers.", "people": ["raskar@media.mit.edu"], "title": "Terahertz Time-gated Spectroscopic Imaging for Content Extraction through Layered Structures", "modified": "2016-12-05T00:17:23.179Z", "visibility": "LAB", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["terrestrial-sensing", "camera-culture"], "published": true, "active": false, "end_on": null, "slug": "terahertz-time-gated-spectroscopic-imaging-for-content-extraction-through-layered-structures"}, {"website": "", "description": "<p>Echologue is a new kind of public space media for sensing and displaying socio-cultural characteristics of a place based on its sonic features. It can reflect its surroundings like a \"smart mirror,\" highlight the salient details and patterns in the environment, and contribute to our understanding of the perception of social places. The interface senses ambient sound and deliberate user input and displays a visualization of the activity in that space as its output. The design explores the utility of sound for envisioning new social, cultural, and entertainment uses of public places and helps us shape our relationships with each other with new social interfaces embedded in urban settings.</p>", "people": ["judith@media.mit.edu"], "title": "Echologue", "modified": "2016-12-05T00:16:22.436Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "echologue"}, {"website": "", "description": "<p>Since the killing of Michael Brown, the Black Lives Matter movement has organized on social media to draw attention to the deaths of unarmed black people killed by US police. Have news organizations responded to this demand, and have we seen a significant change over time in reporting about those deaths?</p><p>In this analysis of deaths from January 2013 through June 2016, we show that an unarmed black person killed by US police received 10.5x the incidence rate of news articles after Michael Brown\u2019s death than those killed before, but that the predicted number of articles is no longer significantly different from 2013 levels.</p>", "people": ["ngyenes@media.mit.edu", "ethanz@media.mit.edu", "rahulb@media.mit.edu", "jnmatias@media.mit.edu"], "title": "Whose Lives Matter in the News?", "modified": "2016-12-13T16:23:36.317Z", "visibility": "PUBLIC", "start_on": "2015-07-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "whose-lives-matter-in-the-news"}, {"website": "", "description": "<p>Fluxa is a compact wearable device that&nbsp;<span style=\"font-size: 18px;\">exploits body movements, as well as the visual effects of&nbsp;</span><span style=\"font-size: 18px;\">persistence of vision (POV), to generate mid-air displays on&nbsp;</span><span style=\"font-size: 18px;\">and around the body. When the user moves his or her limbs,&nbsp;</span><span style=\"font-size: 18px;\">Fluxa displays a pattern that, due to retinal afterimage, can&nbsp;</span><span style=\"font-size: 18px;\">be perceived by the surrounding people.</span></p>", "people": ["xxxxxxin@media.mit.edu", "pattie@media.mit.edu"], "title": "Fluxa: Body Movements as a Social Display", "modified": "2017-01-18T19:06:58.973Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "fluxa-body-movements-as-a-social-display"}, {"website": "", "description": "<p>Traditional medical ultrasound assumes that we are imaging ideal liquids. We are interested in imaging muscle and bone as well as measuring elastic properties of tissues, all of which are places where this assumption fails quite miserably. Interested in cancer detections, Duchenne muscular dystrophy, and prosthetic fitting, we use tomographic techniques as well as ideas from seismic imaging to deal with these issues.</p>", "people": ["raskar@media.mit.edu", "michaf@media.mit.edu", "naik@media.mit.edu"], "title": "Ultrasound Tomography", "modified": "2016-12-05T00:17:24.963Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "ultrasound-tomography"}, {"website": "", "description": "<p>The rise in popularity of the Weblog, and the development of its many variants such as photoblogs, vlogs, moblogs, and tumblelogs, demonstrate that people are increasingly willing to share what they are doing, seeing, and thinking. Micro-blogging has opened this space up even further to those who would not at all consider themselves authors; services like Twitter and the status updates common to social networking sites open up a form of publication that is well suited to this wide and fundamentally amateur audience. Mycrocosm is a Web service that uses the visual language of statistics to share even smaller chunks of personal information\ufffdindividual numbers and words that are full of meaning in our lives\ufffdand allows users to track a wide variety of the minutiae of their daily lives to build up a rich online picture of the tiny things they find meaningful.</p>", "people": ["judith@media.mit.edu"], "title": "Mycrocosm", "modified": "2016-12-05T00:16:40.397Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-390", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "mycrocosm"}, {"website": "", "description": "<p>ML Open creates online learning experiences that are engaging, social, and project based, in areas where the Media Lab has unique expertise, such as learning, innovation, or design. Rather than distribute content, we design activities, and let learners participate in the Media Lab's way of thinking and doing. Our first experiment was Learning Creative Learning, a course taught at the Media Lab, which attracted 24,000 participants. The tools and strategies developed by ML Open are widely applicable in educational and corporate settings. </p>", "people": ["srishti@media.mit.edu", "ps1@media.mit.edu"], "title": "ML Open", "modified": "2018-06-20T19:54:56.815Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["ml-learning"], "published": true, "active": false, "end_on": null, "slug": "ml-open"}, {"website": "", "description": "<p>Collective memory and attention are sustained by two channels: oral communication (communicative memory) and the physical recording of information (cultural memory). Here, we use data on the citation of academic articles and patents, and on the online attention received by songs, movies, and biographies, to describe the temporal decay of the attention received by cultural products. We show that, once we isolate the temporal dimension of the decay, the attention received by cultural products decays following a universal biexponential function. We explain this universality by proposing a mathematical model based on communicative and cultural memory, which fits the data better than previously proposed log-normal and exponential models. Our results reveal that biographies remain in our communicative memory the longest (20\u201330 years) and music the shortest (about 5.6 years). These findings show that the average attention received by cultural products decays following a universal biexponential function.</p>", "people": ["ccandiav@media.mit.edu", "hidalgo@media.mit.edu", "crisjf@media.mit.edu"], "title": "The universal decay of collective memory and attention", "modified": "2019-04-17T19:27:00.707Z", "visibility": "PUBLIC", "start_on": "2017-02-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "temporal-scales-in-human-collective-forgetting"}, {"website": "", "description": "<p>This brief excerpt video shows a glimpse of some of Tod Machover\u2019s innovative, unusual opera realized at\u2014and with the collaboration of\u2014the MIT Media Lab over the past 30 years.</p>", "people": ["tod@media.mit.edu"], "title": "Tod Machover: Operas 1987-2014", "modified": "2017-10-17T13:37:38.335Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "tod-machover-operas-1987-2014"}, {"website": "", "description": "<h2>Real-time collaborative self-expression in virtual reality&nbsp;</h2>", "people": ["swgreen@media.mit.edu", "erinhong@media.mit.edu", "wjlc@media.mit.edu", "hbedri@media.mit.edu", "pattie@media.mit.edu"], "title": "CocoVerse: A playground for co-creation and communication in virtual reality", "modified": "2019-04-17T20:08:49.312Z", "visibility": "PUBLIC", "start_on": "2016-10-03", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "cocoverse"}, {"website": "", "description": "<p>With the LEGO Group and Hasbro, we looked at the emotional experience of playing with games and LEGO bricks. We measured participants' skin conductance as they learned to play with these new toys. By marking the stressful moments, we were able to see what moments in learning should be redesigned. Our findings suggest that framing is key: how can we help children recognize their achievements? We also saw how children are excited to take on new responsibilities but are then quickly discouraged when they aren't given the resources to succeed. Our hope for this work is that by using skin conductance sensors, we can help companies better understand the unique perspective of children and build experiences fit for them.</p>", "people": ["picard@media.mit.edu", "hedman@media.mit.edu"], "title": "Building the Just-Right-Challenge in Games and Toys ", "modified": "2016-12-05T00:17:08.136Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "building-the-just-right-challenge-in-games-and-toys"}, {"website": "", "description": "<p>One of the eternal challenges of economic development is how to identify the economic activities that a country, city, or region should target. During recent years, a large body of research has shown that countries, regions, and cities, are more likely to enter economic activities that are related to the ones they already have. For instance, a region specialized in the exports of frozen fish and crustaceans can more easily start exporting fresh fish than heavy machinery. This research has illuminated a new chapter in the economic development literature, but has left an important question unanswered: what is the right strategy for countries wanting to diversify their economies?&nbsp;</p>", "people": ["flaviopp@media.mit.edu", "aamena@media.mit.edu", "hidalgo@media.mit.edu"], "title": "What is the optimal way to diversify an economy?", "modified": "2019-04-17T19:27:50.930Z", "visibility": "PUBLIC", "start_on": "2018-04-02", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "strategic-diffusion"}, {"website": "", "description": "<p>PAL (Personalized Active Learner) is a <i>wearable</i> system&nbsp;with on-device machine learning&nbsp;to help users with&nbsp;<i>real-time, personalized,</i> and <i>context-aware</i> <b>memory augmentation</b>,<b> language learning, </b>and<b> self-awareness</b>.</p>", "people": ["utkarshs@media.mit.edu", "minakhan@media.mit.edu"], "title": "PAL", "modified": "2019-01-31T17:17:43.024Z", "visibility": "PUBLIC", "start_on": "2018-06-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "pal"}, {"website": "", "description": "<p>Currently each app lives in its own little world, with its own interface. Apps are usually unable to communicate with each other and unable to cooperate to meet users' needs. This project intends to enable end-users to \"program\" their phones using natural language and speech recognition to perform complex tasks. A user, for example, could say: \"Send the song I play most often to Bill.\" The phone should realize that an MP3 player holds songs; that the MP3 app has a function to order songs by play frequency; how to send a file to another user; and how to look up the user's contact information. We use state-of-the art natural language understanding, common-sense reasoning, and a partial-order planner. </p>", "people": ["lieber@media.mit.edu", "cfry@media.mit.edu", "kdinakar@media.mit.edu"], "title": "Goal-Oriented Interfaces for Mobile Phones", "modified": "2016-12-05T00:16:08.590Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "goal-oriented-interfaces-for-mobile-phones"}, {"website": "", "description": "<p>More information coming soon.</p>", "people": ["dlmocdm@media.mit.edu", "picard@media.mit.edu"], "title": "Understanding emotions in multiple sclerosis patients", "modified": "2018-05-07T15:44:41.701Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "understanding-emotions-in-multiple-sclerosis-patients"}, {"website": "", "description": "<p>Microculture gardens are a network of small-scale permaculture gardens that are aimed at reimagining our urban food systems, remediating our air supply, and making our streets more amenable to human-scale mobility. Microculture combines micro-gardening with the principles of permaculture, creatively occupying viable space throughout our communities for small-scale self-sustaining food forests. Micro-gardens have proven to be successful for the production of a broad range of species, including leafy vegetables, fruit, root vegetables, herbs, and more. Traditionally, container-based micro-gardens occupy approximately one meter of space or less and are made from found, up-cycled materials. Our innovations involve the combining of permaculture and micro-gardening principles, developing materials and designs that allow for modularity, mobility, easy replicability, placement in parking spots, and software that supports the placement, creation, and maintenance of these gardens.</p>", "people": ["sdkamvar@media.mit.edu", "yonatanc@media.mit.edu"], "title": "Microculture", "modified": "2016-12-05T00:16:36.992Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "microculture"}, {"website": "http://www.jinjoolee.com", "description": "<p>Collecting real world data to understand social interactions!</p>", "people": ["cynthiab@media.mit.edu", "igrover@media.mit.edu", "jinjoo@media.mit.edu", "nikhita@media.mit.edu"], "title": "Data of Children Storytelling", "modified": "2018-01-22T06:31:46.892Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "data-of-children-storytelling"}, {"website": "", "description": "", "people": [], "title": "New Robots Project", "modified": "2016-10-14T19:15:25.000Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "new-robots-project"}, {"website": "", "description": "<p>Chat Circles is an abstract graphical interface for synchronous text conversation. Here, color and form are used to convey social presence and activity, and proximity-based filtering is used intuitively to break large groups into conversational clusters. The system also includes an integrated history interface, which visualizes archival Chat Circle logs. Our goal in this work is to create a richer environment for online discussions. We are currently revamping several of the chatroom's features to give our users more flexibility when setting up their own Chat Circles server.</p>", "people": ["judith@media.mit.edu"], "title": "Chat Circles", "modified": "2016-12-05T00:16:17.649Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "chat-circles"}, {"website": "", "description": "<p>The Joy Branch project explores different user interfaces to allow parrots to shape their sonic environment.&nbsp;&nbsp;Animal agency\u2014control of the environment\u2014is an important and underutilized element of captive care.&nbsp;&nbsp;Parrot species are vocal learners, and as such are highly attuned to their sonic environment. Much of their brains are involved in the production and analysis of sound, and yet their sonic environment in managed care does not provide a rich experience. In this project, we assess the efficacy of new enrichment techniques that have the potential to improve the lives of these birds through music.&nbsp;&nbsp;The project involves the placement of sonic enrichment elements into the birds\u2019 enclosures under controlled and supervised conditions.</p><p>The \"joystick branch\" element exposes only a standard wooden perch to the birds. The aim is to create naturalistic interactive methods for birds to generate sounds, and to assess their optional engagement with these new modes of control.&nbsp;</p>", "people": ["rebklein@media.mit.edu", "davidsu@media.mit.edu", "gabem@media.mit.edu"], "title": "Joy Branch", "modified": "2019-04-18T01:19:48.567Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2019-12-31", "slug": "joy-branch"}, {"website": "", "description": "<p>A web browser extension that reveals less well-known aspects of corporate public behavior such as environmental respect and political bias. When one engages in a search, we place an image next to the link to a corporate site that graphically reveals relevant information. It might be a donkey versus an elephant, or a measure of \"greenness.\"&nbsp; We seed the system with public information and allow users to contribute to the database.&nbsp;</p><p>Data collected can be further explored and mapped out using data visualizations, allowing&nbsp; perception of network distributions and polarizations.</p>", "people": ["anderton@media.mit.edu", "lip@media.mit.edu"], "title": "Boycott!", "modified": "2019-04-18T01:20:40.082Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2020-01-01", "slug": "boycott"}, {"website": "", "description": "<p class=\"\">With our Ubiquitous Sonic Overlay, we are working to place virtual sounds in the user's environment, fixing them in space even as the user moves. We are working toward creating a seamless auditory display, indistinguishable from the user's actual surroundings. Between bone-conduction headphones, small and cheap orientation sensors, and ubiquitous GPS, a confluence of fundamental technologies is in place. However, existing head-tracking systems either limit the motion space to a small area (e.g., Oculus Rift), or sacrifice precision for scale using technologies like GPS. We are seeking to bridge the gap to create large outdoor spaces of sonic objects.</p>", "people": ["gershon@media.mit.edu", "joep@media.mit.edu", "sfr@media.mit.edu"], "title": "HearThere: Ubiquitous Sonic Overlay", "modified": "2018-06-07T19:16:45.828Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "hearthere-ubiquitous-sonic-overlay"}, {"website": "", "description": "<p>The FingerSynth is a wearable musical instrument made up of a bracelet and set of rings that enables its players to produce sound by touching nearly any surface in their environments. Each ring contains a small, independently controlled audio exciter transducer. The rings sound loudly when they touch a hard object, and are silent otherwise. When a wearer touches their own (or someone else's) head, the contacted person hears sound through bone conduction, inaudible to others. A microcontroller generates a separate audio signal for each ring, and can take user input through an accelerometer in the form of taps, flicks, and other gestures. The player controls the envelope and timbre of the sound by varying the physical pressure and the angle of their finger on the surface, or by touching differently resonant surfaces. The FingerSynth encourages players to experiment with the materials around them and with one another.</p>", "people": ["gershon@media.mit.edu", "joep@media.mit.edu"], "title": "FingerSynth: Wearable transducers for exploring the environment through sound", "modified": "2019-04-19T14:26:25.030Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "fingersynth-wearable-transducers-for-exploring-the-environment-through-sound"}, {"website": "", "description": "<p>We present an augmented handheld airbrush that allows unskilled painters to experience the art of spray painting. Inspired by similar smart tools for fabrication, our handheld device uses 6DOF tracking, mechanical augmentation of the airbrush trigger, and a specialized algorithm to let the painter apply color only where indicated by a reference image. It acts both as a physical spraying device and as an intelligent digital guiding tool that provides manual and computerized control. Using an inverse rendering approach allows for a new augmented painting experience with unique results. We present our novel hardware design, control software, and a discussion of the implications of human-computer collaborative painting.</p>", "people": ["joep@media.mit.edu", "pattie@media.mit.edu"], "title": "Augmented Airbrush", "modified": "2016-12-05T00:17:07.027Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "augmented-airbrush"}, {"website": "", "description": "<p>FlickInk is a gesture-sensing pen to support collaborative work and to augment the environment. With a quick \"flick\" of the pen toward a desired destination, analog written content on paper instantly transfers onto the corresponding physical object in the environment. The FlickInk gesture sensing module allows for wireless communication and directional gesture sensing. If multiple surfaces are present, the direction of the pen swing determines which screen the information is transferred to. Furthermore, multiple users can flick their written content to multiple devices, creating a personalized collaborative environment.</p>", "people": ["aithpao@media.mit.edu", "kll@media.mit.edu"], "title": "FlickInk", "modified": "2016-12-08T17:29:16.429Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": null, "slug": "flickink"}, {"website": "", "description": "<p>The Mobile Territorial Lab (MTL) aims at creating a \"living\" laboratory integrated in the real life of the Trento territory in Italy, open to manifold kinds of experimentations. In particular, the MTL is focused on exploiting the sensing capabilities of mobile phones to track and understand human behaviors (e.g., families' spending behaviors, lifestyles, mood, and stress patterns); on designing and testing social strategies aimed at empowering individual and collective lifestyles through attitude and behavior change; and on investigating new paradigms in personal data management and sharing. This project is a collaboration with Telecom Italia SKIL Lab, Foundation Bruno Kessler, and Telefonica I+D.</p>", "people": ["sandy@media.mit.edu", "shrier@media.mit.edu"], "title": "Mobile Territorial Lab", "modified": "2016-12-05T00:17:01.066Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["connection-science", "human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "mobile-territorial-lab"}, {"website": "", "description": "", "people": [], "title": "How to See Through Tissue", "modified": "2016-10-18T15:00:30.433Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["camera-culture"], "published": false, "active": false, "end_on": null, "slug": "how-to-see-through-tissue"}, {"website": "", "description": "<h2>Integrating sensors and actuators using flexible electronics</h2><p>Currently, the manufacturing of self-actuating and self-sensing robots requires non-standard manufacturing techniques and assembly steps to integrate electrical and mechanical systems. In this work, we developed a novel manufacturing technique, where such robots can be produced at a flexible electronics factory. We developed the technique using standard industrial machines, processes, and materials. Using a lamination process, we were able to integrate air pouches or shape memory alloy (SMA) inside a polyamide-based flexible circuit to produce bending actuators. The bend angle of the actuators is sensed with a chain of inertial measurement units integrated on the actuator. Air-pouch actuators can produce a force of a 2.24N, and a maximum bend angle of 74 degrees. To demonstrate, we manufactured a five-legged robot with the developed actuators and bend sensors, with all the supporting electronics (e.g., microcontrollers, radio) directly integrated into the flexible printed circuit. Such robots are flat and lightweight (15 grams) and thus conveniently compact for transportation and storage. We believe that our technique can allow inexpensive and fast prototyping and deployment of self-actuating and self-sensing robots.<br></p>", "people": ["artemd@media.mit.edu", "joep@media.mit.edu", "jieqi@media.mit.edu", "jifei@media.mit.edu"], "title": "Circuit Robots: Mass manufacturing of self-actuating robots", "modified": "2019-04-17T19:28:42.580Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["tangible-media", "responsive-environments", "hacking-manufacturing"], "published": true, "active": false, "end_on": null, "slug": "circuit-robots"}, {"website": "", "description": "<p>..</p>", "people": ["jon@media.mit.edu"], "title": "jon's retitled project", "modified": "2018-10-05T18:02:46.628Z", "visibility": "LAB", "start_on": "2012-01-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "jons-retitled-project"}, {"website": "", "description": "<p>The design process is no longer limited to one group of individuals, as number, level, and cost make tools ever more accessible. As we move towards tools that allow us to create our own materials, having a set of rules with which to evaluate, interpret, and design them will become increasingly important. One way of approaching this problem is by unpacking the ways in which materials create meaning. This project explores the more emotive aspects of materials, such as haptic responses to, cognitive evaluation of, and emotive perception of materials to understand how materials communicate meaning.The development of an effective methodology aims to lower the barriers of fabrication of engaging objects. By incorporating qualities that were not previously quantifiable, we aim to encourage a more interactive design process that allows for the production of experiences tailored to individual preference, and a framework for conversations around material issues.</p>", "people": ["bdatta@media.mit.edu", "vmb@media.mit.edu"], "title": "Emotive Materials", "modified": "2016-12-05T00:17:01.184Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "emotive-materials"}, {"website": "", "description": "<p>We only perceive a tiny sliver of the world around us. We are constrained by what our senses can process. These senses evolved to react to what is immediately important for our survival. Our technological development has outstripped the pace at which our physical senses evolve. We do not have access to things such as the electromagnetic spectrum at 2.5 GHz, even though it is relevant to the day to day life of most of us. We are poor at perceiving things such as changes in the chemical composition of the air we breathe, even though it is critical for our long term survival as a species. &nbsp;</p><p>Can we augment a stroll through nature with sensory experiences usually outside the range of our perception? Haptic Footprints explore using vibrotactile rendering for this purpose.</p>", "people": ["paul_str@media.mit.edu"], "title": "Haptic Footprint", "modified": "2018-01-08T22:47:19.225Z", "visibility": "PUBLIC", "start_on": "2017-10-12", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "haptic-footprint"}, {"website": "", "description": "", "people": [], "title": "Alexa Personal ChatBot", "modified": "2018-02-02T12:42:50.822Z", "visibility": "PUBLIC", "start_on": "2017-12-01", "location": "", "groups": ["personal-robots"], "published": false, "active": false, "end_on": null, "slug": "alexa-personal-chatbot"}, {"website": "", "description": "<p>We want to build programming systems that can converse with their users to build computer programs. Such systems will enable users without programming expertise to write programs using natural language. The text-based, virtual-world environments called the MOO (multi-user, object-oriented Dungeons and Dragons) allow their users to build objects and give them simple, interactive, text-based behaviors. These behaviors allow other participants in the environment to interact with those objects by invoking actions and receiving text messages. Through our natural-language dialogue system, the beginning programmer will be able to describe objects and the messages in MOO environments.</p>", "people": ["lieber@media.mit.edu"], "title": "Programming in Natural Language", "modified": "2016-12-05T00:16:45.949Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "programming-in-natural-language"}, {"website": "", "description": "<p>Are electronic instruments that generate their own power better than those that don't?  Can acoustic and electronic musical instruments be successfully merged?  Can the movement of the sound generation be tightly coupled to the power generation, as opposed to merely modulating a large power reserve, as in traditional instruments?  What useful musical artifacts/affordances can be created through this technology? </p>", "people": ["csik@media.mit.edu"], "title": "Exertion Music", "modified": "2016-12-05T00:17:11.666Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "Lower Atrium", "groups": ["computing-culture"], "published": true, "active": false, "end_on": null, "slug": "exertion-music"}, {"website": "http://eda-explorer.media.mit.edu/", "description": "<p>Electrodermal Activity (EDA) is a physiological indicator of stress and strong emotion. While an increasing number of wearable devices can collect EDA, analyzing the data to obtain reliable estimates of stress and emotion remains a difficult problem. We have built a graphical tool that allows anyone to upload their EDA data and analyze it. Using a highly accurate machine learning algorithm, we can automatically detect noise within the data. We can also detect skin conductance responses, which are spikes in the signal indicating a \"fight or flight\" response. Users can visualize these results and download files containing features calculated on the data to be used in their own analysis. Those interested in machine learning can also view and label their data to train a machine learning classifier. We are currently adding active learning, so the site can intelligently select the fewest possible samples for the user to label. </p>", "people": ["jaquesn@media.mit.edu", "picard@media.mit.edu", "sfedor@media.mit.edu", "sataylor@media.mit.edu", "cvx@media.mit.edu", "akanes@media.mit.edu"], "title": "EDA Explorer", "modified": "2016-12-05T00:17:11.264Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "eda-explorer"}, {"website": "", "description": "<h2><span style=\"font-weight: normal;\">Locating and classifying florescent tags behind turbid layers using time-resovled inversion&nbsp;</span></h2><p>Using time resolved and sparse optimization framework to locate and classify fluorescent markers hidden behind turbid layer:&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">The use of fluorescent probes and the recovery of their lifetimes allow for significant advances in many imaging systems, in particular medical imaging systems. Here, we propose and experimentally demonstrate reconstructing the locations and lifetimes of fluorescent markers hidden behind a turbid layer. This opens the door to various applications for non-invasive diagnosis, analysis, flowmetry, and inspection. The method is based on a time-resolved measurement which captures information about both fluorescence lifetime and spatial position of the probes. To reconstruct the scene, the method relies on a sparse optimization framework to invert time-resolved measurements. This wide-angle technique does not rely on coherence, and does not require the probes to be directly in line of sight of the camera, making it potentially suitable for long-range imaging.</span></p><p>More details:<br>http://web.media.mit.edu/~guysatat/project_scattering.html&nbsp;<br>http://web.media.mit.edu/~guysatat/fl/<br></p>", "people": ["raskar@media.mit.edu", "barmak@media.mit.edu"], "title": "Imaging Behind Diffusive Layers", "modified": "2017-04-05T01:48:57.383Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "terrestrial-sensing", "camera-culture"], "published": true, "active": false, "end_on": null, "slug": "imaging-behind-diffusive-layers"}, {"website": "", "description": "<p>We explore advanced machine learning and reflective user interfaces to scale the national Crisis Text Line. We are using state-of-the-art probabilistic graphical topic models and visualizations to help a mental health counselor extract patterns of mental health issues experienced by participants, and bring large-scale data science to understanding the distribution of mental health issues in the United States. </p>", "people": ["picard@media.mit.edu", "lieber@media.mit.edu", "kdinakar@media.mit.edu"], "title": "Fathom: Probabilistic graphical models to help mental health counselors", "modified": "2019-04-19T15:05:14.920Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "fathom-probabilistic-graphical-models-to-help-mental-health-counselors"}, {"website": "", "description": "<p>The wide availability of low-cost, wearable, biophysiological sensors enables us to measure how the environment and our experiences impact our physiology. This creates a new challenge: in order to interpret the collected longitudinal data, we require the matching contextual information as well. Collecting weeks, months, and years of continuous biophysiological data makes it unfeasible to rely solely on our memory for providing the contextual information. Many view maintaining journals as burdensome, which may result in low compliance levels and unusable data. We present an architecture and implementation of a system for the acquisition, processing, and visualization of biophysiological signals and contextual information.</p>", "people": ["picard@media.mit.edu", "yadid@media.mit.edu"], "title": "FEEL: A cloud system for frequent event and biophysiological signal labeling", "modified": "2019-04-19T15:06:26.059Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "feel-a-cloud-system-for-frequent-event-and-biophysiological-signal-labeling"}, {"website": "", "description": "<p>This project introduces layer jamming as an enabling technology for designing deformable, stiffness-tunable, thin sheet interfaces. Interfaces that exhibit tunable stiffness properties can yield dynamic haptic feedback and shape deformation capabilities. In contrast to particle jamming, layer jamming allows for constructing thin and lightweight form factors of an interface. We propose five-layer structure designs and an approach that composites multiple materials to control the deformability of the interfaces. We also present methods to embed different types of sensing and pneumatic actuation layers on the layer-jamming unit. Through three application prototypes we demonstrate the benefits of using layer jamming in interface design. Finally, we provide a survey of materials that have proven successful for layer jamming.</p>", "people": ["liningy@media.mit.edu", "ishii@media.mit.edu", "jifei@media.mit.edu"], "title": "jamSheets: Interacting with thin stiffness-changing material", "modified": "2019-04-17T19:29:31.671Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "jamsheets-interacting-with-thin-stiffness-changing-material"}, {"website": "", "description": "<p>BigBarChart is an immersive, 3D bar chart that provides a new physical way for people to interact with data. It takes data beyond visualizations to map out a new area--data experiences--that are multisensory, embodied, and aesthetic interactions. BigBarChart is made up of a number of bars that extend up to 10 feet to create an immersive experience. Bars change height and color in response to interactions that are direct (a person entering the room), tangible (pushing down on a bar to get meta information), or digital (controlling bars and performing statistical analyses through a tablet). BigBarChart helps both scientists and the general public understand information from a new perspective. Early prototypes are available.</p>", "people": ["perovich@media.mit.edu", "vmb@media.mit.edu"], "title": "BigBarChart", "modified": "2016-12-05T00:16:14.935Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "bigbarchart"}, {"website": "", "description": "<h2><span style=\"font-weight: normal;\">How to see through tissue</span></h2><p>We demonstrate a new method to image through scattering materials like tissue and fog. The demonstration includes imaging an object hidden behind 1.5cm of tissue; it's like imaging through the palm of a hand. Our optical method is based on measuring and using all photons in the signal (as opposed to traditional methods, which use only part of the signal). Specifically, we use a time-resolved method that allows us to distinguish between photons that travel different paths in the tissue. Combining this unique measurement process with novel algorithms allows us to recover the hidden objects. This technique can be used in biomedical imaging, as well as imaging through fog and clouds.</p>", "people": ["raskar@media.mit.edu", "guysatat@media.mit.edu", "barmak@media.mit.edu"], "title": "Imaging with All Photons", "modified": "2017-04-05T01:49:57.109Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "imaging-with-all-photons"}, {"website": "", "description": "<p>One of the major barriers encountered by designers and artists when programming digital media is difficulty translating mental models of interactive creations into a format and language that can be interpreted by computers. This problem arises because current software-development environments demand a sequential format for code. In contrast, Kaleido proposes a new interface that enables a user-defined, conceptual, visiospatial representation of computation that complements the traditional text-based perspective. Kaleido is a tool designed to help visual thinkers program; users can use Kaleido to create personally meaningful visuals for their code. Kaleido allows individuals to plan, organize, and navigate code in the idiosyncratic way we each think.</p>", "people": ["dsmall@media.mit.edu"], "title": "Kaleido: Idiosyncractic Graphical Interfaces for Software Development", "modified": "2016-12-05T00:16:33.033Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["design-ecology"], "published": true, "active": false, "end_on": null, "slug": "kaleido-idiosyncractic-graphical-interfaces-for-software-development"}, {"website": "", "description": "<p>It's well known that living in a foreign country dramatically improves the effectiveness of learning a second language over classroom study alone. We are designing language-teaching sequences for a sensor-equipped residence that can detect user interaction with household objects. We use our common-sense knowledge base and reasoning tools to construct teaching sequences, wholly in the target language, of sentences and question-answering interactions that gradually improve the learner's language competence. For example, the first time the user sits in a chair, the system responds with the foreign-language word for \"chair,\" and later with statements and questions that use complete sentences, tenses, questions, materials, goals, and plans.</p>", "people": ["lieber@media.mit.edu"], "title": "Learning Common Sense in a Second Language", "modified": "2016-12-05T00:17:15.915Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "learning-common-sense-in-a-second-language"}, {"website": "", "description": "<p>The use of data-driven methods to examine dynamic spaces, relationships, and mechanisms within an urban environment frames the city as a complex system. This framing in turn casts residents as contributors and actors rather than passive subjects. This project uses narrative interactive visualization to integrate public data with urban planning concepts for communication to and by residents of a city.&nbsp;\n                    \n                </p>", "people": ["zhangjia@media.mit.edu"], "title": "Narrative Visualization for Distributed Urban Interventions", "modified": "2016-12-14T13:52:42.196Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "narrative-visualization-for"}, {"website": "", "description": "<p>Stories, language, and art are at the heart StoryScape. While StoryScape began as a tool to meet the challenging language learning needs of children diagnosed with autism, it has become much more. StoryScape was created to be the first truly open and customizable platform for creating animated, interactive storybooks that can interact with the physical world. <a href=\"https://play.google.com/store/apps/details?id=edu.mit.media.storyscape\">Download the android app</a>&nbsp;and make your own amazing stories at <a href=\"https://storyscape.io\">https://storyscape.io</a>.</p>", "people": ["micahrye@media.mit.edu", "fergusoc@media.mit.edu", "picard@media.mit.edu"], "title": "StoryScape", "modified": "2017-09-19T17:58:07.097Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "storyscape"}, {"website": "", "description": "<p>As part of our motivation to expand the classic Montessori curriculum and to address contemporary proficiencies, we are working closely with Montessori experts and computer scientists to develop a scope and sequence for computational thinking that will contribute to the Montessori classroom. This curriculum outlines the key concepts behind computer science, along with the corresponding materials and their lessons.&nbsp;</p>", "people": ["sdkamvar@media.mit.edu", "smithkim@media.mit.edu", "yonatanc@media.mit.edu"], "title": "Computational Scope and Sequence for a Montessori Learning Environment", "modified": "2016-12-05T00:16:46.370Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "computational-scope-and-sequence-for-a-montessori-learning-environment"}, {"website": "http://annafuste.com", "description": "<p>Abstract data visualizations for enhancing social&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">interactions through clothing and augmented reality.</span></p>", "people": ["afuste@media.mit.edu", "geek@media.mit.edu"], "title": "ARTextiles: Promoting Social Interactions Around Personal Interests", "modified": "2018-04-18T18:33:06.161Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "artextiles"}, {"website": "", "description": "<p>A Live Object is a small device that can stream media content wirelessly to nearby mobile devices without an Internet connection. Live Objects are associated with real objects in the environment, such as an art piece in a museum, a statue in a public space, or a product in a store. Users exploring a space can discover nearby Live Objects and view content associated with them, as well as leave comments for future visitors. The mobile device retains a record of the media viewed (and links to additional content), while the objects can retain a record of who viewed them. Future extensions will look into making the system more social, exploring game applications such as media \"scavenger hunts\" built on top of the platform, and incorporating other types of media such as live and historical data from sensors associated with the objects.</p>", "people": ["vpanzica@media.mit.edu", "vmb@media.mit.edu", "arata@media.mit.edu"], "title": "Live Objects", "modified": "2016-12-14T13:53:33.699Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "ce-20", "future-storytelling", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "live-objects"}, {"website": "", "description": "<p>+nudge helps people to become their imagined future self one&nbsp;nudge&nbsp;at a time.</p><p>Distracted by all of the demands on our time from urgent notifications, reminders, and advertising on our phones and laptops, it is increasingly challenging to align our day to day actions with what we believe matters most inside. +nudge&nbsp;creates specifically tuned, subtle reminders throughout your day to reflect and be mindful of the things in life that really matter to you, and as a consequence assist you in making better, more holistic decisions.</p>", "people": ["anderton@media.mit.edu", "lip@media.mit.edu"], "title": "+nudge", "modified": "2019-04-19T16:09:59.016Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "plusnudge"}, {"website": "", "description": "<p>Analogy is a powerful comparison mechanism, commonly thought to be central to human problem solving. Analogies like \"an atom is like the solar system\" enable people to effectively transfer knowledge to new domains. Can we enable computers to do similar comparisons? Prior work on analogy (structure mapping) provides guidance about the nature of analogies, but implementations of these theories are inefficient and brittle. We are working on a new analogy mechanism that uses instance learning to make robust, efficient comparisons.</p>", "people": ["lieber@media.mit.edu"], "title": "Relational Analogies in Semantic Networks", "modified": "2016-12-05T00:16:47.602Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "relational-analogies-in-semantic-networks"}, {"website": "", "description": "<p>The future of human life outside of Earth will heavily depend on the ability to fabricate and manufacture things. Yet fabrication in space poses numerous difficulties. Some of these challenges include storage space in vehicles, availability of raw materials, lack of machines, and shortage of manpower.&nbsp;</p><p>&nbsp;Other challenges in fabricating objects in space are simply a result of &nbsp;the &nbsp;different &nbsp;physical environment; &nbsp;the lack of gravity introduces unexpected material &nbsp;behaviour, as other forces aside from gravity become dominate. Surface tension, for example, becomes very dominant in determining the shape of liquid materials and adhesion between liquids and other materials also plays a more dominant role.&nbsp;</p><p>Because of the reasons stated above, 3D printing in space&nbsp;was conceptually limited to fused deposition modeling (FDM) technologies, which are less susceptible to problems resulting from the harsh conditions. Liquid- or powder-based printing technologies are assumed to be very problematic for space fabrication because of liquid behavior in microgravity conditions. On the other hand, FDM technologies have a lot of limitations such as the inability to create transparent &nbsp;structures or layerless shapes with defined smooth curvatures.</p><p>In this experiment, we would like to harness surface tension's dominance in liquid behavior under zero gravity conditions &nbsp;to create various controllable and accurate, layerless and transparent geometries using UV-curable resin. The resin will be hardened using a high-power UV light source.</p><p>We will focus on rapid fabrication (in under 17 seconds) of the following shapes:</p><ol><li>Shapes that are hard to make on Earth without special machinery, e.g., perfect lenses.</li><li>Shapes and materials that could be necessary in the space environment and are hard to make with existing methods available in space, such as ball bearings.&nbsp;</li></ol>", "people": ["amosg@media.mit.edu"], "title": "Liquid resin fabrication in microgravity", "modified": "2019-04-17T19:30:16.889Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["tangible-media", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "lift-liquid-resin-injection-fabrication-technology-in-micro-gravity"}, {"website": "", "description": "<p>Circuit Stickers is a toolkit for crafting electronics using flexible and sticky electronic pieces. These stickers are created by printing traces on flexible substrates and adding conductive adhesive. These lightweight, flexible, and sticky circuit boards allow us to begin sticking interactivity onto new spaces and interfaces such as clothing, instruments, buildings, and even our bodies.</p>", "people": ["nanwei@media.mit.edu", "leah@media.mit.edu", "joep@media.mit.edu", "jieqi@media.mit.edu"], "title": "Circuit Stickers", "modified": "2016-12-14T20:48:46.926Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "circuit-stickers"}, {"website": "", "description": "<p>Innovation and investment in Smart City infrastructure has led to an increasing number of sensors in our urban environment and greater arsenals of data, stored in the cloud and viewed primarily by experts (if at all). &nbsp;We are interested in shifting the current Smart City paradigm to one where sensor data is explored in the physical places where it is being collected, by the communities that inhabit the space. &nbsp;Building on existing sensor infrastructure deployed through Chicago\u2019s Array of Things, we are partnering with the School of the Art Institute of Chicago &nbsp;to explore tools and processes that integrate local residents in gathering and analyzing data &nbsp;in order to spark dialogue, learning, and collaborative revisioning of our cities. </p>", "people": ["emreiser@media.mit.edu", "ethanz@media.mit.edu"], "title": "Our 2 Sense", "modified": "2016-12-05T00:17:19.600Z", "visibility": "LAB", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "our-2-sense"}, {"website": "http://tangible.media.mit.edu/project/printflatables/", "description": "<p>Printflatables is a design and fabrication system for human-scale, functional and dynamic inflatable objects. The user begins with specifying an intended 3D model which is decomposed to two dimensional fabrication geometry. This forms the input for a numerically controlled contact iron that seals layers of thermoplastic fabric. </p><p>In this project, we showcase the system design in detail, the pneumatic primitives that this technique enables and merits of being able to make large, functional and dynamic pneumatic artifacts. We demonstrate the design output through multiple objects which could motivate fabrication of inflatable media and pressure-based interfaces.</p><p><a href=\"http://tangible.media.mit.edu/project/printflatables/\">Project Website</a></p>", "people": ["udayan@media.mit.edu", "sareen@media.mit.edu", "ishii@media.mit.edu", "kakehi@media.mit.edu", "jifei@media.mit.edu", "pattie@media.mit.edu"], "title": "Printflatables: Printing human-scale, functional, and dynamic inflatable objects", "modified": "2019-04-17T19:31:11.620Z", "visibility": "PUBLIC", "start_on": "2017-05-10", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "inflatables"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: normal;\">Some readers require greater context to understand complex stories.&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">FOLD (</span><a href=\"http://fold.cm\" style=\"font-size: 18px; font-weight: normal;\">fold.cm</a><span style=\"font-size: 18px; font-weight: normal;\">) is an open publishing platform with a unique structure that lets writers link media cards to the text of their stories.&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">Media cards can contain videos, maps, tweets, music, interactive visualizations, and more.&nbsp;</span></p><p>FOLD is used by journalists, educators, and storytellers around the world.&nbsp;<br></p>", "people": ["ethanz@media.mit.edu", "matt54@media.mit.edu", "hidalgo@media.mit.edu", "ahope@media.mit.edu", "kzh@media.mit.edu"], "title": "FOLD", "modified": "2016-12-15T02:27:31.751Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["future-of-news", "collective-learning", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "fold"}, {"website": "", "description": "<p>CivicLink is an online organization in a box. A leader or moderator plugs it in, invites members, and the tools needed to build community action are in place. It is grassroots mobilization recursed to the lower level: we envision extremely large networks of extremely local, single-issue orgs. Core elements are an events calendar and forum for each event. The link is a private server that retains all communications and personal information within it; there is no contribution to an online cloud. The architecture is extensible to add features such as mapping, canvassing, etc. It is designed to be used where groups physically meet and for access via a smartphone app.</p><p>Related work tests whether privacy is important to users, whether games can be used to promote actions, how web sites can be distributed offline via QR codes, and how this link can merge culturally unique resonances.</p>", "people": ["oceane@media.mit.edu", "britneyj@media.mit.edu", "smpsnr@media.mit.edu", "lip@media.mit.edu"], "title": "CivicLink", "modified": "2019-04-18T01:23:05.324Z", "visibility": "PUBLIC", "start_on": "2018-09-04", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "civic-link"}, {"website": "", "description": "<p>Between 1989 and 2015, the World Wide Web transformed from an esoteric system for publishing technical notes to a basic infrastructure of commerce, learning and social interaction. In the process, the Web has centralized around a few key points of control, owned by large, for-profit, publicly traded companies which have enormous influence on our online interactions. And because so many of our interactions - commercial, interpersonal and civic - are mediated online, we have inadvertently given these companies a great deal of control over our political lives and civic discourse. In collaboration with the Center for Civic Media, we will identify and evaluate the status of structurally decentralized projects in the fields of online publishing, online social networks, and discovery of online content (directory and search). From this work we will launch an experiment in building a structurally decentralized publication system designed to solve a real and relevant problem within academic computing, but more broadly, to offer a proof of concept for one approach to building decentralized social networks and publishing systems.</p>", "people": [], "title": "Decentralized Web", "modified": "2016-12-13T19:05:00.928Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["digital-currency-initiative-dci"], "published": true, "active": false, "end_on": null, "slug": "decentralized-web"}, {"website": "", "description": "<p>Raconteur is a story-editing system for conversational storytelling that provides intelligent assistance in illustrating a story with photos and videos from an annotated media library. It performs natural language processing on a text chat between two or more participants, and recommends appropriate items from a personal media library to illustrate a story. A large common-sense knowledge base and a novel common-sense inference technique are used to find relevant media materials to match the story intent in a way that goes beyond keyword matching or word co-occurrence based techniques. Commonsense inference can identify larger-scale story patterns such as expectation violation or conflict and resolution, and helps a storyteller to chat and brainstorm personal stories with a friend.</p>", "people": ["lieber@media.mit.edu"], "title": "Raconteur: From Chat to Stories", "modified": "2016-12-05T00:16:46.659Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-384", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "raconteur-from-chat-to-stories"}, {"website": "", "description": "<p>Modern web presentations such as Youtube feature videos with commentary appended at the bottom. In our new imagining of <b>Videotext,</b> we put the two together: comments appear as active bubbles along the playback time line. We thereby associate the commentary with the place in the video to which it refers. It gains context. This project is in the early test stage and is presented for discussion and further development in summer 2016.</p>", "people": ["weller@media.mit.edu", "lip@media.mit.edu"], "title": "Captions++", "modified": "2016-12-05T00:17:03.626Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "viral-communications"], "published": true, "active": false, "end_on": null, "slug": "captions"}, {"website": "", "description": "<p>AI programs perform inference -- they go beyond exactly what users tell them. They make assumptions, infer user goals, learn from experience, adapt according to context. But how do we tell what they're doing and why? We need to get \"inside the head\" of programs that use complex inference or machine learning techniques. This project works on visualizing the relationship between concepts, statements about those concepts, and inference which causes the program to learn new things. We also show how visualization  can be used to help \"debug\" inference. Does the program have sufficient knowledge to draw particular kinds of conclusions? How \"liberal\" (quick to jump to conclusions) or \"conservative\" (require quite a lot of evidence before concluding something) should the inference be?</p>", "people": [], "title": "Seeing Thought: Visualizing Inference", "modified": "2016-12-05T00:16:24.839Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "seeing-thought-visualizing-inference"}, {"website": "", "description": "<p>In a study of human perception of music in relation to different representations of video graphics, this project explores the automatic synchronization in real time between audio and image. This aims to make the relationship seem smaller and more consistent. The connection is made using techniques that rely on audio signal processing to automatically extract data from the music, which subsequently are mapped to the visual objects. The visual elements are influenced by data obtained from various Musical Information Retrieval (MIR) techniques. By visualizing music, one can stimulate the nervous system to recognize different musical patterns and extract new features.</p>", "people": ["thomassl@media.mit.edu", "tod@media.mit.edu"], "title": "Music Visualization via Musical Information Retrieval", "modified": "2016-12-05T00:17:18.424Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "music-visualization-via-musical-information-retrieval"}, {"website": "", "description": "<p>CollaboRhythm is a platform that enables patients to be at the center of every interaction in their healthcare with the goal of empowering them to be involved, reflective, and proactive. Care can be coordinated securely through cell phones, tablets, televisions, and computers so that support can be provided in real-time in the real world instead of through inconvenient doctor's office visits. We are currently developing and demonstrating applications for diabetes and hypertension management. A number of third parties have also developed exciting applications using CollaboRhythm. Please visit http://newmed.media.mit.edu to learn about how you can build a project with us using CollaboRhythm.</p>", "people": ["fmoss@media.mit.edu", "jom@media.mit.edu"], "title": "CollaboRhythm", "modified": "2016-12-05T00:16:19.314Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-320", "groups": ["new-media-medicine"], "published": true, "active": false, "end_on": null, "slug": "collaborhythm"}, {"website": "", "description": "<p>The Jibo Research Platform is an in-the-field deployable Social Robotics experimentation and data collection infrastructure. Built upon the world's first commercial social robot for the home, it extends Jibo's design, hardware, and data security for research purposes.</p>", "people": ["cynthiab@media.mit.edu", "samuelsp@media.mit.edu", "jon@media.mit.edu", "haewon@media.mit.edu"], "title": "Jibo Social Robotic Research Platform", "modified": "2018-10-15T01:36:16.436Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "jibo-research-platform"}, {"website": "", "description": "<p>TRANSFORM fuses technology and design to celebrate its transformation from still furniture to a dynamic machine driven by a stream of data and energy. TRANSFORM aims to inspire viewers with unexpected transformations and the aesthetics of the complex machine in motion. First exhibited at LEXUS DESIGN AMAZING MILAN (April 2014), the work comprises three dynamic shape displays that move over one thousand pins up and down in real time to transform the tabletop into a dynamic tangible display. The kinetic energy of the viewers, captured by a sensor, drives the wave motion represented by the dynamic pins. The motion design is inspired by dynamic interactions among wind, water, and sand in nature, Escher's representations of perpetual motion, and the attributes of sand castles built at the seashore. TRANSFORM tells of the conflict between nature and machine, and its reconciliation, through the ever-changing tabletop landscape.</p>", "people": ["daniell@media.mit.edu", "ishii@media.mit.edu"], "title": "TRANSFORM", "modified": "2019-04-17T19:34:52.006Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["terrestrial-sensing", "tangible-media"], "published": true, "active": false, "end_on": null, "slug": "transform"}, {"website": "", "description": "<p>MirrorFugue is an installation for a player piano that evokes the impression that the \"reflection\" of a disembodied pianist is playing the physically moving keys. Live music emanates from a grand piano, whose keys move under the supple touch of a pianist's hands reflected on the lacquered surface of the instrument. The pianist's face is displayed on the music stand, with subtle expressions projecting the emotions of the music. MirrorFugue recreates the feeling of a live performance, but no one is actually there. The pianist is an illusion of light and mirrors, a ghost both present and absent. Viewing MirrorFugue evokes the sense of walking into a memory, where the pianist plays without awareness of the viewer's presence; or, it is as if viewers were ghosts in another's dream, able to sit down in place of the performing pianist and play along.</p>", "people": ["x_x@media.mit.edu", "ishii@media.mit.edu"], "title": "MirrorFugue", "modified": "2016-12-05T00:16:38.037Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "tangible-media"], "published": true, "active": false, "end_on": null, "slug": "mirrorfugue"}, {"website": "", "description": "<p>Complex biological systems such as brain circuits are extended 3-D structures made out of nanoscale building blocks such as proteins, RNAs, and lipids, which are often organized with nanoscale precision. This presents a fundamental tension in biology\u2014to understand a biological system like a brain circuit, you might need to map a large diversity of nanoscale building blocks, across an extended spatial expanse. We are developing a new suite of tools that enable the mapping of the location and identity of the molecular building blocks of complex biological systems such as the brain, aiming to map out the architecture of such systems with enough precision to understand how the structures of biological systems lead to function and dysfunction. One of the technologies we are developing, expansion microscopy (ExM), enables large 3D objects to be imaged with nanoscale precision, by physically expanding preserved biological systems (in contrast to all previous microscopies, that magnify light from the sample via lenses). We are working to improve expansion microscopy further, and are working, often in interdisciplinary collaborations, on a suite of new labeling and analysis techniques that exploit the biochemical freedom enabled by the expanded state. We are also applying expansion microscopy to the scalable mapping of complex biological systems, including brain circuits. Such brain circuit maps may be detailed enough to enable detailed computer simulations of neural circuits. Finally, we are extending and applying such tools to the early detection and understanding of complex diseases such as cancers and autoimmune diseases, and to the analysis of aging.</p>", "people": ["esb@media.mit.edu"], "title": "Tools for mapping the molecular architecture and wiring of the brain", "modified": "2018-10-20T18:30:00.953Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": null, "slug": "tools-for-mapping-the-molecular-structure-of-the-brain"}, {"website": "", "description": "<p>Speech synthesis in tutor mode. Using phones for literacy learning is an empowering application of mobile technology, but there are elements of the human tutor that have yet to be replicated in current apps. Namely, when reading a story, a tutor is likely to be more expressive and colorful in tone. When encountering a new word, a tutor might emphasize the vowel phoneme or stress a consonant pair the child has yet to master. By modeling speech with deep neural networks, our speech synthesizer will be able to interpolate between speaking styles, switching from 'normal' mode to 'tutor' mode as needed.\n                    \n                </p>", "people": ["echu@media.mit.edu"], "title": "ChatterBox", "modified": "2016-12-05T00:17:02.173Z", "visibility": "PUBLIC", "start_on": "2016-04-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "chatterbox"}, {"website": "", "description": "<p>Today, algorithms drive our cars, our economy, what we read, and how we play. Modern-day computer games utilize weighted probabilities to make games more competitive, fun, and addicting. In casinos, slot machines--once a product of simple probability--employ similar algorithms to keep players playing. Dice++ takes the seemingly straight probability of rolling a die and determines an outcome with algorithms of its own.</p>", "people": ["jbobrow@media.mit.edu", "slavin@media.mit.edu"], "title": "Dice++", "modified": "2016-12-15T02:58:30.762Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "dice"}, {"website": "", "description": "<p>Introducing TRANSFORM, a shape-changing desk. TRANSFORM is an exploration of how shape display technology can be integrated into our everyday lives as interactive, transforming furniture. These interfaces not only serve as traditional computing devices, but also support a variety of physical activities. By creating shapes on demand or by moving objects around, TRANSFORM changes the ergonomics and aesthetic dimensions of furniture, supporting a variety of use cases at home and work: it holds and moves objects like fruit, game tokens, office supplies, and tablets, creates dividers on demand, and generates interactive sculptures to convey messages and audio.</p>", "people": ["lajv@media.mit.edu", "daniell@media.mit.edu", "ishii@media.mit.edu", "ken_n@media.mit.edu", "viirj@media.mit.edu"], "title": "TRANSFORM: Adaptive and dynamic furniture", "modified": "2019-04-17T19:36:00.162Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "transform-adaptive-and-dynamic-furniture"}, {"website": "", "description": "<p>We concentrate on media that informs, unifies, and defines society such as news, sports, and public events. These are characterized by simultaneity, synchronicity, immersion, multiple perspectives, and new ways of framing discussions as stories. We design structures that allow for civic participation in media creation and distribution yet retain the editorial imperative to create a shared reality based on trust and truth.</p><p>Our current emphasis is on validating broadcast television news and creating applications that make it easier to broaden ones views than it is to remain in a bubble.&nbsp; This is based on \"Unspoken News,\" an AI-driven engine for revealing cues such as emotions, peripheral text, set layout and other non-literal influencers. This extends our fully functional video recorder and analysis engine, Superglue. Related work also provides for adding confidence indications to messages that one passes on.</p>", "people": ["lip@media.mit.edu"], "title": "Theme | Ultimate Media", "modified": "2019-04-18T01:24:17.283Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "ultimate-media"}, {"website": "", "description": "<p>We have built a microfluidic logic family and architecture based on two-phase flow. In this scheme, presence or absence of a bubble or a droplet in a fluidic network represents a bit. Logic operations are executed based on direct or indirect bubble interactions. The system is sufficiently nonlinear to exhibit universal logic. The nonlinearity is derived from boundary conditions of two-phase flow. With the emergence  of very large-scale integrated microfluidic systems, there is an immediate need to build local flow control systems. Bubble Logic can be used for various applications including bubble display, combinatorial synthesis, and control architecture for lab-on-chip devices.</p>", "people": ["neilg@media.mit.edu"], "title": "Bubble Logic", "modified": "2016-12-05T00:16:16.537Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": null, "slug": "bubble-logic"}, {"website": "", "description": "<p>People use text to communicate in online spaces because of its directness and ease of use. However, we are unable to convey many social cues and communication nuances by text. We explore how to enhance the richness and expressiveness of text, so that we can have an expressive and intuitive online environment for textual communication. Cheiro is a chat space offering a novel form of gesture-enhanced textual communication over the Internet. It provides an intuitive interface for turning the form of text into an expressive visual medium; it is an experimental arena for exploring the relationships between gestures, emotions, and visuals.</p>", "people": ["judith@media.mit.edu"], "title": "Cheiro", "modified": "2016-12-05T00:16:17.850Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-390", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "cheiro"}, {"website": "", "description": "<p>The proliferation of smartphones and wearable sensors is creating very large data sets that may contain useful information. However, the magnitude of generated data creates new challenges as well. Processing and analyzing these large data sets in an efficient manner requires computational tools. Many of the traditional analytics tools are not optimized for dealing with large datasets. Tributary is a parallel engine for searching and analyzing sensor data. The system utilizes large clusters of commodity machines to enable in-memory processing of sensor time-series signals, making it possible to search through billions of samples in seconds. Users can access a rich library of statistics and digital signal processing functions or write their own in a variety of languages.</p>", "people": ["picard@media.mit.edu", "yadid@media.mit.edu"], "title": "Tributary", "modified": "2016-12-05T00:17:24.703Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "tributary"}, {"website": "", "description": "<p>Case and Molly is a prototype for a game inspired by (and in homage to) William Gibson's novel Neuromancer. It's about the coordination between virtual and physical, \"cyberspace\" and \"meat.\" We navigate the tension between our physical surroundings and our digital networks in a state of continuous partial attention; Case and Molly uses the mechanics and aesthetics of Neuromancer to explore this quintessential contemporary dynamic. The game is played by two people mediated by smartphones and an Oculus Rift VR headset. Together, and under time pressure, they must navigate Molly through physical space using information that is only available to Case. In the game, Case sees Molly's point of view in immersive 3D, but he can only communicate a single bit of information to her. Meanwhile, Molly traverses physical obstacles hoping Case can solve abstract puzzles in order to gain access to the information she needs.</p>", "people": ["gregab@media.mit.edu", "slavin@media.mit.edu"], "title": "Case and Molly ", "modified": "2016-12-05T00:16:17.244Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "case-and-molly"}, {"website": "", "description": "<p>We use time-resolved information in an iterative optimization algorithm to recover reflectance of a three-dimensional scene hidden behind a diffuser. We demonstrate reconstruction of large images without relying on knowledge of diffuser properties.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "Imaging through scattering media using femtophotography", "modified": "2019-04-19T18:27:07.846Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "imaging-through-scattering-media-using-femtophotography"}, {"website": "", "description": "<p>Embodied voice-based agents, such as Amazon\u2019s Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most, these agents represent their first experience of&nbsp;<i>living</i> with artificial intelligence in such private and personal spaces.&nbsp;</p><p>However, little is known about people\u2019s desires, preferences, and boundaries for these technologies. This projects seeks to answer questions surrounding this space:&nbsp;<b>How do we live with voice-based agents in the home? How do different generations interact with voice-based agents? How should these technologies be designed to incorporate people\u2019s preferences, desires, and boundaries? What tools can be used to understand this space?</b></p><p>This work presents insights from a long-term exploration with over 70 children, adults, and older adults over a one-year period to interact with, discover, experience, reflect upon, and design voice-based agents. In addition, design tools and learnings from the experience have been developed into an open-source design kit to enable designers and researchers to explore these ideas with the broader population.</p><p>For more information, please contact <b>Nikhita Singh (nikhita@media.mit.edu) </b>and <b>Anastasia Ostrowski (akostrow@media.mit.edu)</b>.</p>", "people": ["cynthiab@media.mit.edu", "akostrow@media.mit.edu", "nikhita@media.mit.edu", "haewon@media.mit.edu"], "title": "Talking Machines: Democratizing the design of voice-based agents for the home", "modified": "2018-07-15T03:34:47.660Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "talking-machines-democratizing-the-design-of-voice-based-agents-for-the-home"}, {"website": "", "description": "<p>In the quest towards general artificial intelligence (AI), researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards. This paper argues that such research has overlooked an important and useful intrinsic motivator: social interaction. We posit that making an AI agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations, and could potentially impact AI safety. We collect social feedback in the form of facial expression reactions to samples from Sketch RNN, an LSTM-based variational autoencoder (VAE) designed to produce sketch drawings. We use a Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small group of viewers, and then show in an independent evaluation with 76 users that this model produced sketches that lead to significantly more positive facial expressions. Thus, we establish that implicit social feedback can improve the output of a deep learning model.</p>", "people": ["jaquesn@media.mit.edu", "picard@media.mit.edu"], "title": "Learning via Social Awareness: Improving sketch representations with facial feedback", "modified": "2019-04-19T17:14:55.804Z", "visibility": "PUBLIC", "start_on": "2018-03-20", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "learning-via-social-awareness-improving-sketch-representations-with-facial-feedback"}, {"website": "", "description": "<p>One of the biggest challenges for the digital economy is what to do when things go wrong. Orders get misplaced, numbers mistyped, requests misunderstood: then what? Consumers are frustrated by long waits on hold, misplaced receipts, and delays to problem resolution; companies are frustrated by the cost of high-quality customer service. Online companies want customers\ufffd trust, and how a company handles problems directly impacts that. We are exploring how software agents and other technologies can help with this issue. Borrowing ideas from software debugging, we can have agents help to automate record-keeping and retrieval, track dependencies, and provide visualization of processes. Diagnostic problem-solving can generate hypotheses about causes of errors, and seek information that allows hypotheses to be tested. Agents act on behalf of both the consumer and the vendor to resolve problems more quickly and at lower cost.</p>", "people": ["lieber@media.mit.edu"], "title": "E-Commerce When Things Go Wrong", "modified": "2016-12-05T00:16:22.327Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "e-commerce-when-things-go-wrong"}, {"website": "", "description": "<p>CityScope MarkIVb is programmed to demonstrate and model the relationship between land use (live and work), population density, parking supply and demand, and traffic congestion. </p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu", "csmuts@media.mit.edu"], "title": "CityScope Mark IVb: Land Use/Transportation", "modified": "2017-10-16T03:05:09.935Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": null, "slug": "OLD_cityscope-mark-ivb-land-usetransportation"}, {"website": "", "description": "<p>Zappore is a low-cost ($10), portable electroporator that can be fabricated in any community or academic setting with readily available and common materials. It is a tool to explore and experiment the possibility of transformation for known and unknown strains of microbes.</p>", "people": ["ninawang@media.mit.edu", "dkong@media.mit.edu", "suryaj@media.mit.edu"], "title": "Zappore: A low-cost electroporator", "modified": "2019-04-24T20:16:00.368Z", "visibility": "LAB-INSIDERS", "start_on": "2018-01-01", "location": "", "groups": ["community-bio"], "published": true, "active": false, "end_on": null, "slug": "zappore"}, {"website": "", "description": "<p>&nbsp;</p>", "people": [], "title": "Purpose-Based Creative Computing with Scratch", "modified": "2019-02-18T20:28:11.752Z", "visibility": "PUBLIC", "start_on": "2018-02-05", "location": "", "groups": ["lifelong-kindergarten"], "published": false, "active": false, "end_on": null, "slug": "clubhouse-co-design"}, {"website": "", "description": "<p>NCCU: music is a strong and universally accepted aspect of black culture. It has given birth to the genres of blues, jazz, gospel, rap, house, disco, funk, soul, trap, ragga, ska, dub, grime, reggae, calypso, hip hop, r&amp;b, dubstep, soul, and neo-soul. We believe that there are new instruments and ideas to be discovered. We are beginning this paradigmatic journey at an innovation center we launched at North Carolina Central University.&nbsp;</p>", "people": ["tcarew@media.mit.edu", "bdunning@media.mit.edu", "fonuoha@media.mit.edu"], "title": "NCCU: The Design and Fabrication of New Black Musical Instruments", "modified": "2018-05-08T12:02:11.010Z", "visibility": "PUBLIC", "start_on": "2015-03-01", "location": "", "groups": ["code-next"], "published": true, "active": false, "end_on": null, "slug": "nccu-the-creation-of-new-black-musical-instruments-and-new-black-musical-idioms"}, {"website": "", "description": "<p>We aim to create an easy-to-use, scalable, widely accessible method to allow for the co-culturing of multiple organisms through the use of water droplets stabilized by surfactants in oil. This methodology will be used to explore and study the effects of various strains of biota living in the human microbiome on each other.&nbsp;</p>", "people": ["dkong@media.mit.edu", "suryaj@media.mit.edu", "desireed@media.mit.edu"], "title": "Co-Culture: Open source technique to study dynamics of the microbiome", "modified": "2019-04-24T18:45:39.803Z", "visibility": "LAB-INSIDERS", "start_on": "2019-02-04", "location": "", "groups": ["community-bio"], "published": true, "active": false, "end_on": null, "slug": "co-culture"}, {"website": "", "description": "<p>We are surrounded by displays and technologies whose mechanisms are hidden from view. This work is an exploration of revealing the underlying mechanisms of not only how we generate and parse visual information but how it can be delivered.&nbsp; The design process and its outcomes are derivatives, in part, of the mechanics of the eye which guide our estimation of a \u201cfair curve.\u201d&nbsp; Working within the parameters of 19th-century tools and techniques, I adopted the perspective of research modalities relevant for a time in which the eye and low-level visual mechanisms for discerning thresholds, edges, and shapes were the dominant tools in the creation of experimentation. Modern design and engineering tools\u2013while enabling increasingly complex and sophisticated research\u2013can also obfuscate the fundaments of form and function.&nbsp; By minimizing the influence of technological aides, we give our vision in the act of creation, the articulation of this machine intended to give form to the fundamental algorithms inherent in early biological visual processing.</p><p>Process album:<br><a href=\"https://photos.app.goo.gl/ybEyqeIWt0ugMXd73\">https://photos.app.goo.gl/ybEyqeIWt0ugMXd73</a><br></p>", "people": ["elawson@media.mit.edu"], "title": "Atmopragmascope", "modified": "2019-04-18T01:33:53.490Z", "visibility": "PUBLIC", "start_on": "2018-01-27", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "atmopragmascope"}, {"website": "", "description": "<p>Archived TV programs evoke earlier times. This application combines a video and music archive with an immersive screen and a simple user interface suitable for everyone, from children to the elderly, to create a \"Time Machine\" effect. The only key for exploring is the user's age. People can enjoy over 1,300 TV programs from the last seven decades without having to do tedious text searches. This  catalogue intuitively guides the user with an image array (64 different videos on one screen at the same time) that simplifies navigation and makes it immediate, rather than referencing it to previous screens.</p>", "people": ["vmb@media.mit.edu"], "title": "8K Time Machine", "modified": "2016-12-05T00:17:05.553Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "ce-20", "future-storytelling", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "8k-time-machine"}, {"website": "http://datausa.io", "description": "<p>DataUSA is the most comprehensive site visualizing public data for the United States. Through interactive profiles, DataUSA makes available data from a variety of public sources, including the American Community Survey, the Bureau of Economic Analysis, the Bureau of Labor and Statistics, the Department of Education (IPEDS), and the county health records from the University of Wisconsin in Madison<span style=\"font-size: 18px; font-weight: normal;\">.&nbsp;</span></p>", "people": ["hidalgo@media.mit.edu"], "title": "DataUSA", "modified": "2016-12-14T22:40:02.595Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "datausa"}, {"website": "", "description": "", "people": [], "title": "Smartphone dermatoscope", "modified": "2017-04-06T16:35:30.353Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "smartphone-dermatoscope"}, {"website": "", "description": "<p>Embodied voice-based agents, such as Amazon\u2019s Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most people, these agents represent their first experience <i>living</i> with artificial intelligence in such private and personal spaces. However, little is known about people\u2019s desires, preferences, and boundaries for these technologies. This project seeks to answer questions surrounding this space. How do we live with voice-based agents in the home? How do different generations interact with voice-based agents? How should these technologies be designed to incorporate people\u2019s preferences, desires, and boundaries? What tools can be used to understand this space?</p>", "people": ["akostrow@media.mit.edu", "nikhita@media.mit.edu"], "title": "Talking Machines: Democratizing the Design of Voice-Based Agents in the Home", "modified": "2018-07-15T03:35:18.476Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["personal-robots"], "published": false, "active": false, "end_on": null, "slug": "talking-machines-democratizing-the-design-of-voice-controlled-agents-in-the-home"}, {"website": "https://vikparthiban.wordpress.com/", "description": "<p>LUI is a scalable, multimodal web-interface that uses a custom framework of nondiscrete, free-handed gestures and voice to control modular applications with a single stereo-camera and voice assistant. The gestures and voice input are mapped to ReactJS web elements to provide a highly-responsive and accessible user experience. This interface can be deployed on an AR or VR system, heads-up displays for autonomous vehicles, and everyday large displays.</p><p>Integrated applications include media browsing for photos and YouTube videos. Viewing and manipulating 3D models for engineering visualization are also in progress, with more applications to be added by developers in the longer-term. The LUI menu consists of a list of applications which the user can \"swipe\" and \"airtap\" to select an option. Each application has its unique set of non-discrete gestures to view and change content. If the user wants to find a specific application, they can also say a voice command to search or directly go to that application. Developers will be able to easily add more applications because of the modularity and extensibility of this web platform.</p><p>For more information, contact graduate researcher Vik Parthiban at vparth@mit.edu.</p><p>Advisors:<br>V. Michael Bove, Director, Object-Based Media group<br>Zach Lieberman, openFrameworks&nbsp;<br>John Underkoffler, CEO, Oblong Industries; Scientific advisor, <i>Minority Report</i> and <i>Iron Man</i> interface</p>", "people": ["vparth@media.mit.edu"], "title": "LUI: Large User Interface with Gesture and Voice Feedback", "modified": "2019-04-21T20:40:17.351Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "large-user-interface-with-gesture-and-voice-feedback"}, {"website": "", "description": "<p>We are developing a mobile phone-based platform to assist people with chronic diseases, panic-anxiety disorders, or addictions. Making use of wearable, wireless biosensors, the mobile phone uses pattern analysis and machine learning algorithms to detect specific physiological states and perform automatic interventions in the form of text/images plus sound files and social networking elements.  We are currently working with the Veterans Administration drug rehabilitation program involving veterans with PTSD.</p>", "people": ["fletcher@media.mit.edu", "picard@media.mit.edu"], "title": "Mobile health interventions for drug addiction and PTSD", "modified": "2019-04-19T17:18:29.063Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "mobile-health-interventions-for-drug-addiction-and-ptsd"}, {"website": "", "description": "<p>Urban noise pollution has been a problem since the days of Buddha. Walkmans help, but issues of both social and accoustic isolation have become more urgent with the popularity of the iPod. Addressing these issues may require a look at how recorded music devices work at a fundamental level. Ambient Addition is a Walkman-like device, built on a DSP core, that synthesizes music by sampling the sound around the listener, creating harmony and rhythm from the chaos and noise of the environment. By simultaneously opening music to incorporate the environment, but also turning the environment into music, the sound stays fresh and the listener is encouraged to explore new territory.</p>", "people": ["csik@media.mit.edu"], "title": "Ambient Addition", "modified": "2016-12-05T00:16:12.255Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-001", "groups": ["computing-culture"], "published": true, "active": false, "end_on": null, "slug": "ambient-addition"}, {"website": "", "description": "<p>Technological advances in the past decade have allowed us to take a close look at the proteomes of living organisms. As a result, more than 120,000 solved protein structures are readily available, and we are still on an exponential growth curve. By looking at the proteomes of current living organisms, we are essentially taking snapshots of the successful results in this evolutionary process of continuous adaptation to the environment. Could we process the information available to us from nature to design new proteins, without the need for millions of years of Darwinian evolution?</p><p>To answer this question, we are developing an integrated Deep Learning framework for the evolutionary analysis, search, and design of proteins, which we call Evolutron. Evolutron is based on a hierarchical decomposition of proteins into a set of functional motif embeddings. Two of our strongest motivations for this work are gene therapy and drug discovery. In both cases, protein analysis and design play a fundamental role in the implementation of safe and effective therapeutics.</p>", "people": ["jacobson@media.mit.edu", "kfirs@media.mit.edu", "karydis@media.mit.edu"], "title": "Evolutron: Deep Learning for Protein Design", "modified": "2019-04-17T19:37:56.781Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "evolutron"}, {"website": "", "description": "<p>While people attend presentations, panels, and lectures to learn something from the people at the front of the room, there's a lot of potential for creating spaces where audience members can interact with each other and the people presenting. This project focuses on augmenting the physical space of the auditorium to provide a venue for the audience to ask (and filter) questions for presenters. backchan.nl is a simple, Web-based tool that allows audience members to identify themselves, post questions, and vote on other people's questions. The current top questions are projected at the front of the room so that the audience can see them, as well as shown on a monitor visible to presenters. Upcoming posts are shown to users participating on the Web, but not on the main screen.</p>", "people": ["judith@media.mit.edu"], "title": "backchan.nl", "modified": "2016-12-05T00:16:14.136Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": null, "slug": "backchannl"}, {"website": "", "description": "<h1>Measuring Cooperation at Scale</h1>", "people": ["mrfrank@media.mit.edu", "sunlijun@media.mit.edu", "irahwan@media.mit.edu", "nobradov@media.mit.edu"], "title": "Global Cooperation", "modified": "2018-01-10T16:18:38.348Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "global-cooperation"}, {"website": "", "description": "", "people": [], "title": "Personal Food Computer", "modified": "2017-09-20T23:28:42.201Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "personal-food-computer-1"}, {"website": "", "description": "<p>We are conducting EEG studies to identify the musical features and musical interaction patterns that universally impact measures of arousal. We hypothesize that we can induce states of high and low arousal using electrodermal activity (EDA) biofeedback, and that these states will produce correlated differences in concurrently recorded skin conductance and EEG data, establishing a connection between peripherally recorded physiological arousal and cortical arousal as revealed in EEG. We also hypothesize that manipulation of musical features of a computer-generated musical stimulus track will produce changes in peripheral and cortical arousal. These musical stimuli and programmed interactions may be incorporated into music technology therapy, designed to reduce arousal or increase learning capability by increasing attention. We aim to provide a framework for the neural basis of emotion-cognition integration of learning that may shed light on education and possible applications to improve learning by emotion regulation.</p>", "people": ["picard@media.mit.edu", "gleslie@media.mit.edu"], "title": "Modulating peripheral and cortical arousal using a musical motor response task", "modified": "2019-04-19T17:19:56.174Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "modulating-peripheral-and-cortical-arousal-using-a-musical-motor-response-task"}, {"website": "", "description": "<p>The Honest Crowds project addresses shortcomings of traditional survey techniques in the modern information and big data age. Web survey platforms, such as Amazon's Mechanical Turk and CrowdFlower, bring together millions of surveys and millions of survey participants, which means paying a flat rate for each completed survey may lead to survey responses that lack desirable care and forethought. Rather than allowing survey takers to maximize their reward by completing as many surveys as possible, we demonstrate how strategic incentives can be used to actually reward information and honesty rather than just participation. The incentive structures that we propose provide scalable solutions for the new paradigm of survey and active data collection.</p>", "people": ["mrfrank@media.mit.edu", "lorenzoc@media.mit.edu", "sunlijun@media.mit.edu", "irahwan@media.mit.edu"], "title": "Honest Crowds", "modified": "2018-01-10T16:32:52.208Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "honest-crowds"}, {"website": "", "description": "<p>The Boycott Toolkit provides tools for consumers to organize collective economic action. Users can learn about the politics behind everyday companies and products, pledge to join a campaign, and share information with their friends through social networks.</p>", "people": ["csik@media.mit.edu"], "title": "Boycott Toolkit", "modified": "2016-12-05T00:16:16.262Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["computing-culture"], "published": true, "active": false, "end_on": null, "slug": "boycott-toolkit"}, {"website": "", "description": "<p>Breathing Window is a tool for non-verbal dialogue that reflects on your own breathing while also offering a window on another person's respiration. This prototype is an example of shared human experiences (SHEs) crafted to improve the quality of human understanding and interactions. Our work on SHEs focuses on first encounters with strangers. We meet strangers every day, and without prior background knowledge of the individual we often form opinions based on prejudices and differences. In this work, we bring respiration to the foreground as one common experience of all living creatures.</p>", "people": ["rebklein@media.mit.edu", "tod@media.mit.edu"], "title": "Breathing Window", "modified": "2016-12-05T00:16:16.294Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "breathing-window"}, {"website": "", "description": "<p>\n                    Animated GIFs are widely used on the Internet to express emotions, but automatic analysis of their content is largely unexplored. To help with the search and recommendation of GIFs, we aim to predict &nbsp;how their emotions will be perceived by humans based on their content. Since previous solutions to this problem only utilize image-based features and lose all the motion information, we propose to use 3D convolutional neural networks (CNNs) to extract spatiotemporal features from GIFs. We evaluate our methodology on a crowdsourcing platform called GIFGIF with more than 6,000 animated GIFs, and achieve better accuracy than any previous approach in predicting crowdsourced intensity scores of 17 emotions. We have also found that our trained model can be used to distinguish and cluster emotions in terms of valence and risk perception.</p>", "people": ["picard@media.mit.edu", "cvx@media.mit.edu"], "title": "Predicting perceived emotions in animated GIFs with 3D convolutional neural networks", "modified": "2019-04-19T17:22:36.655Z", "visibility": "PUBLIC", "start_on": "2016-10-18", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "deep-gif"}, {"website": "", "description": "<p>Hyperproduction is a conceptual framework and a software toolkit that allows producers to specify a descriptive computational model and consequently an abstract state for a live experience through traditional operating paradigms, such as mixing audio or operation of lighting, sound, and video systems. The hyperproduction system is able to interpret this universal state and automatically utilize additional production systems, allowing for a small number of producers to cohesively guide the attention and perspective of an audience using many or very complex production systems simultaneously. The toolkit is under active development and has been used for new pieces such as Fensadense, and to recreate older systems such as those for the original Hyperstring Triolgy as part of the Lucerne Festival in 2015. Work continues to enable new structures and abstraction within the framework.</p>", "people": ["tod@media.mit.edu", "benb@media.mit.edu"], "title": "Hyperproduction: Advanced Production Systems", "modified": "2016-12-05T00:17:14.278Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "hyperproduction-advanced-production-systems"}, {"website": "", "description": "<p>ShapeBlocks is a play analytics observatory that tracks, remembers, and aids players in building traditional LEGO-style structures. As players build a structure using these blocks, an underlying geometry engine analyzes the players' moves and suggests next steps (if a target structure is provided). The players can see real-time updates of what they are building in 3D. Instead of only suggesting, the AI learns from the players' moves and corrects itself through reinforcement learning. This essentially gives an opportunity for children and machines to learn shapes and geometry together.</p><p>Other use cases include urban design, and interactive strategy games and/or storytelling experiences that fuse the physical and virtual world together.</p><p>This is a work in progress. The hardware is complete, and the AI tool and games are currently being built.</p>", "people": ["dkroy@media.mit.edu", "saquib@media.mit.edu"], "title": "ShapeBlocks", "modified": "2016-12-05T00:17:27.676Z", "visibility": "PUBLIC", "start_on": "2016-08-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "shapeblocks"}, {"website": "", "description": "<p>CharmMe is a mobile social discovery application that helps people meet each other during events. The application blends physical and digital proximity to help you connect with with other like-minded individuals. Armed with RFID sensors and a model of how the Lab works, CharmMe determines who you should talk to using information including checking in to conference talks or \ufffdliking\ufffd projects using QR codes. In addition, possible opening topics of conversation are suggested based on users' expressed similar interests. </p>", "people": ["havasi@media.mit.edu"], "title": "CharmMe", "modified": "2016-12-05T00:16:17.774Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["digital-intuition"], "published": true, "active": false, "end_on": null, "slug": "charmme"}, {"website": "", "description": "<p>The Circuit Sticker Activity Book is a primer for using circuit stickers to create expressive electronics. Inside are explanations of the stickers, and circuits and templates for building functional electronics directly on the pages of the book. The book covers five topics, from simple LED circuits to crafting switches and sensors. As users complete the circuits, they are also prompted with craft and drawing activities to ensure an expressive and artistic approach to learning and building circuits. Once completed, the book serves as an encyclopedia of techniques to apply to future projects.</p>", "people": ["leah@media.mit.edu", "joep@media.mit.edu", "jieqi@media.mit.edu"], "title": "Circuit Stickers Activity Book", "modified": "2016-12-05T00:16:18.006Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "circuit-stickers-activity-book"}, {"website": "", "description": "<p>An interactive picture book that explores storytelling techniques through paper-based circuitry. Sensors, lights, and microcontrollers embedded into the covers, spine, and pages of the book add electronic interactivity to the traditional physical picture book, allowing us to tell new stories in new ways. The current book, \"Ellie,\" tells the adventures of an LED light named Ellie who dreams of becoming a star, and of her journey up to the sky.</p>", "people": ["joep@media.mit.edu", "slavin@media.mit.edu", "jieqi@media.mit.edu"], "title": "Circuit Storybook", "modified": "2016-12-05T00:17:08.516Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["future-storytelling", "responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "circuit-storybook"}, {"website": "", "description": "<p>Our sensory mechanisms can be thought of as networked input controls responsible for encoding the natural world before us.  Acting as go-between from environment to our ultimate perception of it, sensory organs themselves exhibit deep learning potentials that, if acted upon, could radically challenge the way we develop and experience future technologies and extant realities. If we consider neural plasticity as a form of temporary memory which can be written and rewritten, consciously or unconsciously, externally or internally, there is potential to biologically encode new sensing parameters directly onto receptive fields, re-scripting and filtering input data from natural scenes into new and unique outcomes in our perception.  By generating tailored stimuli specific to adaptation mechanisms, there is an exciting opportunity to create novel environments of controlled perceptual phenomenon, programming inherent biological functions to facilitate more dynamic, immersive environments.<br></p>", "people": ["elawson@media.mit.edu"], "title": "Programming Perception: Emergent Potentials in Perceptual Mechanisms", "modified": "2016-12-05T00:16:27.941Z", "visibility": "LAB-INSIDERS", "start_on": "2015-10-14", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "programming-perception"}, {"website": "", "description": "<h2>Data Fusion&nbsp;for Dynamic Traffic Prediction</h2><p>Traffic congestion has huge negative impacts on the productivity, health and personal lives of city dwellers. To manage this problem&nbsp;effectively, transportation engineers need to predict traffic congestion throughout the road network at all hours of the day.&nbsp;Prediction of traffic typically involves travel surveys that are expensive, time consuming and do not capture temporal variation in travel demand.&nbsp;However,&nbsp;anonymised&nbsp;location data from mobile phones present an alternative source of data which is passively collected, widely available and naturally captures temporal trends.&nbsp;On the other hand, these data contain other biases and so if we use these data for transportation models, we must take care to correct for these biases using more reliable data. As part of the City Science collaboration with Andorra, we used&nbsp;a&nbsp;Bayesian network to build a calibrated transportation model for the country based on&nbsp;geolocated telecoms data and validated using a small sample of traffic counts.</p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "doorleyr@media.mit.edu", "kll@media.mit.edu"], "title": "Dynamic Traffic Prediction in Andorra: a Bayesian network approach", "modified": "2019-04-17T19:43:06.032Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "traffic-andorra"}, {"website": "", "description": "<p>We demonstrate a smartphone based spectrometer design that is standalone and supported on a wireless platform. The device is inherently low-cost and the power consumption is minimal making it portable to carry out a range of studies in the field. All essential components of the device like the light source, spectrometer, filters, microcontroller and wireless circuits have been assembled in a housing of dimensions 88\u2009mm\u2009\u00d7\u200937\u2009mm\u2009\u00d7\u200922\u2009mm and the entire device weighs 48\u2009g. The resolution of the spectrometer is 15\u2009nm, delivering accurate and repeatable measurements. The device has a dedicated app interface on the smartphone to communicate, receive, plot and analyze spectral data. The performance of the smartphone spectrometer is comparable to existing bench-top spectrometers in terms of stability and wavelength resolution. Validations of the device were carried out by demonstrating non-destructive ripeness testing in fruit samples. Ultra-Violet (UV) fluorescence from Chlorophyll present in the skin was measured across various apple varieties during the ripening process and correlated with destructive firmness tests. A satisfactory agreement was observed between ripeness and fluorescence signals. This demonstration is a step towards possible consumer, bio-sensing and diagnostic applications that can be carried out in a rapid manner.</p>", "people": [], "title": "Food sensing on smartphones", "modified": "2016-11-08T19:18:03.587Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "asdf"}, {"website": "", "description": "<p>While we have learned much about human behavior and neurobiology, there is arguably no field that studies the mind itself. We want to overcome the fragmentation of the cognitive sciences. We aim to create models and concepts that bridge between methodologies, and can support theory-driven research. Among the most interesting questions: How do our minds construct the dynamic simulation environment that we subjectively inhabit, and how can this be realized in a neural substrate? How can neuronal representations be compositional? What determines the experiential qualities of cognitive processes? What makes us human?</p>", "people": ["esb@media.mit.edu", "joscha@media.mit.edu", "slavin@media.mit.edu", "amarbles@media.mit.edu"], "title": "Cognitive Integration: The Nature of the Mind", "modified": "2016-12-05T00:17:08.864Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["synthetic-neurobiology", "playful-systems"], "published": true, "active": false, "end_on": null, "slug": "cognitive-integration-the-nature-of-the-mind"}, {"website": "", "description": "<p>A long-standing dream of artificial intelligence has been to put common-sense knowledge into computers\ufffdenabling machines to reason about everyday life. Some projects, such as Cyc, have begun to amass large collections of such knowledge. However, it is widely assumed that the use of common sense in interactive applications will remain impractical for years, until these collections can be considered sufficiently complete, and common-sense reasoning sufficiently robust. Recently we have had some success in applying common-sense knowledge in a number of intelligent interface agents, despite the admittedly spotty coverage and unreliable inference of today's common-sense knowledge systems.</p>", "people": ["lieber@media.mit.edu"], "title": "Common-Sense Reasoning for Interactive Applications", "modified": "2016-12-05T00:16:07.341Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": null, "slug": "common-sense-reasoning-for-interactive-applications"}, {"website": "", "description": "<p>The choices we make about diet, environment, medications, or alternative therapies constitute a massive collection of \"everyday experiments.\" These data remain largely unrecorded and are underutilized by traditional research institutions. Collective Discovery leverages the intuition and insight of patient communities to generate datasets about everyday experiments.  We support the patient's process by simplifying tracking and assessment of lifestyle changes in their bodies and lives.  This model is embodied in the free-for-the-public website Personal Experiments (http://personalexperiments.org) and is used to power a clinical \"N-of-1\" experiment platform called MyIBD at the Cincinnati Children's Hospital.</p>", "people": ["fmoss@media.mit.edu", "lieber@media.mit.edu", "eslick@media.mit.edu"], "title": "Collective Discovery", "modified": "2016-12-05T00:16:19.266Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-320", "groups": ["new-media-medicine"], "published": true, "active": false, "end_on": null, "slug": "collective-discovery"}, {"website": "", "description": "<h2><i>Forecasting the supply of fleets to meet emerging travel demands and service needs in cities</i></h2><p>The availability of vehicles is a critical factor behind successful shared-use mobility services. Proper management of supply-demand dynamics is paramount for achieving viability in a new mobility service, as achieving scale often requires a large capital investment. Under-supplying the fleet would result in low service availability and user dissatisfaction; over-supplying results in inefficient use of capital. In addition, as a new shared mobility platform diversifies its service across both passenger and freight delivery, its required scale of operation and investment becomes more difficult to estimate. &nbsp;</p><p>In this fleet deployment and optimization research, the City Science group aims to create an accessible simulation tool to enable cities to forecast the size of deployment of new shared mobility services using the Persuasive Electric Vehicle delivering passengers and packages as an initial test case. The simulation tool also provides a platform for testing fleet rebalancing and service-hub strategies.</p>", "people": ["mcllin@media.mit.edu", "ptinn@media.mit.edu"], "title": "Service deployment simulation and optimization for mixed-use delivery fleets", "modified": "2019-04-17T19:44:17.413Z", "visibility": "PUBLIC", "start_on": "2016-04-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "fleet-simulation"}, {"website": "", "description": "<p>The New England Aquarium was&nbsp;one of the world\u2019s first modern aquariums when it opened its doors in Boston in 1969.&nbsp; Throughout its history, the aquarium has been a leader in innovative ways to share the ocean with the public, including the creation of the&nbsp;&nbsp;Giant Ocean Tank, the largest circular saltwater tank in the world when it opened in 1970.&nbsp;</p><p>Approaching its 50th anniversary, the New England Aquarium is working with the Open Ocean initiative and&nbsp;MIT Design Lab to develop future scenarios depicting what the experience of the aquarium will be in the next 50 years.</p>", "people": ["katybell@media.mit.edu", "joep@media.mit.edu", "emilysa@media.mit.edu", "novysan@media.mit.edu", "vmb@media.mit.edu"], "title": "NEAQ 2069: Envisioning the Future Aquarium Experience", "modified": "2018-04-30T16:04:10.248Z", "visibility": "PUBLIC", "start_on": "2018-01-22", "location": "", "groups": ["open-ocean"], "published": true, "active": false, "end_on": null, "slug": "neaq-2069"}, {"website": "", "description": "<p>The physical world is increasingly coming online. We have things that measure, sense, and broadcast to the rest of the world. We call this the Internet of Things (IoT). But our cameras are blind to this new layer of metadata on reality. The IoT recorder is a camera that understands what IoT devices it sees and what data they are streaming, thus creating a rich information \"caption-track\" for the videos it records. Using this meta-data, we intend to explore how this enables new video applications, starting with cooking.</p>", "people": ["thariq@media.mit.edu", "lip@media.mit.edu"], "title": "IoT Recorder", "modified": "2016-12-05T00:16:32.661Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "viral-communications"], "published": true, "active": false, "end_on": null, "slug": "iot-recorder"}, {"website": "", "description": "<p>How can a knowledge base learn from the Internet, when you shouldn't trust everything you read on the Internet? CORONA is a system for building a knowledge base from a combination of reliable and unreliable sources, including crowd-sourced contributions, expert knowledge, games with a purpose (GWAP), automatic machine reading, and even knowledge that is imperfectly derived from other knowledge in the system. It confirms knowledge as reliable as more sources confirm it or unreliable when sources disagree, and then by running the system in reverse it can discover which knowledge sources are the most trustworthy.</p>", "people": [], "title": "Corona", "modified": "2016-12-05T00:16:20.075Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["digital-intuition"], "published": true, "active": false, "end_on": null, "slug": "corona"}, {"website": "", "description": "<p>Fablur explores the limit of the self in its relationship to others through the medium of clothing. The augmented gown uses a rear dome projection system on the surface of the fabric. The system comprises laser projectors and mirror structures talking wirelessly with a computer, within which is contained both content and warp projection mapping software. This novel technological interface presents both a performative element and a seamless integration in a woman's life experience. This wearable project questions the boundary between the self and others, the boundary between the individual and society, and the boundary between the body and nature.</p>", "people": ["rebklein@media.mit.edu", "tod@media.mit.edu"], "title": "Fablur", "modified": "2016-12-05T00:16:26.876Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "fablur"}, {"website": "", "description": "<p>VR Codes are dynamic data invisibly hidden in television and graphic displays.  They allow the display to present simultaneously visual information in an unimpeded way, and real-time data to a camera. Our intention is to make social displays that many can use at once; using VR codes, users can draw data from a display and control its use on a mobile device.  We think of VR Codes as analogous to QR codes for video, and envision a future where every display in the environment contains latent information embedded in VR codes.</p>", "people": ["lip@media.mit.edu"], "title": "VR Codes", "modified": "2018-05-01T15:29:34.350Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "vr-codes"}, {"website": "", "description": "<h2><i>Facilitating coexistence, trust-building, and collaboration among people and machines.</i></h2><p>New modes of 21st century urban transportation are becoming increasingly lightweight, electrified, connected, shared, and autonomous. Cohabitation of humans and machines is an increasingly important question, and one which requires careful attention and design.&nbsp; We strive to enable new forms of human-machine co-existence, trust, and collaboration.</p><h2>This work focuses on enabling:</h2><ol><li>Intuitive and effective two-way communication between vehicles and pedestrians;</li><li>Street safety and traffic-yielding mechanisms; and</li><li>Behavior change related to the adoption of active mobility mode, or electric assist.</li></ol>", "people": ["lukeji@media.mit.edu", "cq_zhang@media.mit.edu", "mcllin@media.mit.edu", "jerryao@media.mit.edu", "yagol@media.mit.edu", "cassiano@media.mit.edu", "ptinn@media.mit.edu"], "title": "Human-machine cooperation (HMC) for lightweight autonomous robots", "modified": "2019-04-17T19:49:25.786Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "hmi"}, {"website": "", "description": "<p>&nbsp;For 18 months, the Smithsonian has been collecting art, photos, films, and interviews about the New Thing Art and Architecture Center, a Washington, DC inner-city cultural center that Topper Carew founded immediately after college. The work will be installed in the Smithsonian's personal collection.<br></p>", "people": ["tcarew@media.mit.edu"], "title": "New Thing Art and Architecture Center at the Smithsonian", "modified": "2018-05-08T13:54:00.724Z", "visibility": "PUBLIC", "start_on": "2018-05-07", "location": "", "groups": ["code-next"], "published": true, "active": false, "end_on": null, "slug": "creating-new-jazz-mus"}, {"website": "", "description": "<p>Scratch Community Blocks is an NSF-funded project that extends the Scratch programming language to enable youth to analyze and visualize their own learning and participation in the Scratch online community. With Scratch Community Blocks, youth in the Scratch community can easily access, analyze, and represent data about the ways they program, share, and discuss Scratch projects.</p>", "people": ["sdg1@media.mit.edu", "mres@media.mit.edu", "ascii@media.mit.edu", "nrusk@media.mit.edu"], "title": "Scratch Community Blocks", "modified": "2016-12-05T00:17:22.736Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratch-data-blocks"}, {"website": "", "description": "<p>Programmable Paintings are a series of artworks that use electronic elements such as LED lights and microphone sensors as \"pigments\" in paintings. The goal is to blend traditional elements of painting\u2014color, texture, composition\u2014with these electronic components to create a new genre of time-based and interactive art.</p>", "people": ["leah@media.mit.edu", "joep@media.mit.edu", "jieqi@media.mit.edu"], "title": "Programmable Paintings", "modified": "2017-10-16T15:13:03.200Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "programmable-paintings"}, {"website": "", "description": "<p>Visualization recommender systems aim to lower the barrier to exploring basic visualizations by automatically generating results for analysts to search and select, rather than manually specify. Here, we demonstrate a novel machine learning-based approach to visualization recommendation that learns visualization design choices from a large corpus of datasets and associated visualizations. First, we identify five key design choices made by analysts while creating visualizations, such as selecting a visualization type and choosing to encode a column along the X- or Y-axis. We train models to predict these design choices using one million dataset-visualization pairs collected from a popular online visualization platform. Neural networks predict these design choices with high accuracy compared to baseline models. We report and interpret feature importances from one of these baseline models. To&nbsp;evaluate the generalizability and uncertainty of our approach, we benchmark with a crowdsourced test set, and show that the performance of our model is comparable to human performance when predicting consensus visualization type, and exceeds that of other visualization recommender systems.&nbsp;</p>", "people": ["bakker@media.mit.edu", "hidalgo@media.mit.edu"], "title": "VizML: A Machine Learning Approach to Visualization Recommendation", "modified": "2019-05-06T20:23:32.720Z", "visibility": "PUBLIC", "start_on": "2017-12-05", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "vizml"}, {"website": "http://www.code4rights.org", "description": "<p>Code4Rights promotes human rights through technology education. By facilitating the development of rights-focused mobile applications in workshops and an online course, Code4Rights enables participants to create meaningful technology for their communities in partnership with local organizations. For example, Code4Rights, in collaboration with It Happens Here, a grassroots organization focused on addressing sexual violence, created the First Response Oxford App to address sexual violence at Oxford University. Over 30 young women contributed to the creation of the app, which provides survivors of sexual violence and friends of survivors with information about optional ways to respond, essential knowledge about support resources, critical contact details, and answers to frequently asked questions.  </p>", "people": ["ethanz@media.mit.edu", "joyab@media.mit.edu"], "title": "Code4Rights", "modified": "2016-12-05T00:16:54.952Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "code4rights"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: normal;\">This collection of Tod Machover\u2019s music focuses on chamber and orchestral music composed during the last decade, both with and without electronic enhancement. Machover\u2019s music is a fascinating blend of expressive and lyrical melody combined with a sophisticated ear for textural complexity. The resulting music is always a treat for the ears\u2014colorful, vibrant, and rhythmically propulsive. The largest composition on this disc is the piano concerto Jeux Deux, scored for large orchestra (the work was commissioned and first performed by the Boston Symphony Orchestra), with the soloist performing on a \u201chyperpiano\u201d\u2014a concert grand piano which interacts with sensors and computer programs in order to expand its technical possibilities. Machover produces cutting-edge music with a heart!</span></p>", "people": ["tod@media.mit.edu"], "title": "\u2026but not simpler\u2026", "modified": "2019-04-17T19:56:25.695Z", "visibility": "PUBLIC", "start_on": "2011-11-08", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "but-not-simpler"}, {"website": "", "description": "<p>Does how you play reflect who you really are? The Media Lab and Tilburg University are bringing science into the game to figure out the connections between our play style and our cognitive traits. To do that, we are gathering data from League of Legends, World of Warcraft, and Battlefield 4, and Battlefield: Hardline players to gain insights across all the major online game genres (MOBA, MMORPG, and FPS). In return, every participant will get an in-depth GAMR profile that shows their personality, brain type, and gamer type.</p>", "people": ["slavin@media.mit.edu", "tekofsky@media.mit.edu"], "title": "GAMR", "modified": "2016-12-05T00:16:25.916Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "gamr"}, {"website": "http://aberke.com", "description": "<p>An environment of plants and mirrors that extends beyond the terrarium walls.&nbsp;</p>", "people": ["aberke@media.mit.edu"], "title": "AR Enhanced Wall Plants: Escape Pod", "modified": "2019-05-07T19:24:13.501Z", "visibility": "PUBLIC", "start_on": "2018-11-30", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "ar-enhanced-wall-plants-escape-pod"}, {"website": "", "description": "<p>The MIT Scratch Team is exploring ways to make it easier for newcomers to get started creating with coding. We are designing \"microworlds\"\u2014 customized versions of the Scratch editor that contain a small set of blocks for making projects based on a theme. </p><p>Microworlds offer a more creative entry point to coding. While many introductory coding experiences focus on engaging children in puzzles with one right answer, microworlds provide an open-ended experience, enabling children to explore, experiment, and create, while still providing a more simplified and scaffolded entry point into coding.</p><p>Each microworld includes subset of the Scratch programming blocks that are most relevant and useful for the particular interest area, along with specialized graphical assets related to the interest area. In addition to aligning with a particular interest area, each microworld highlights how coding can enable young people to create projects and express ideas with code. For example, by tinkering with the music microworld, young people can see how they can use code to make musical melodies and beats; by tinkering with the soccer microworld, young people can see how they can use coding to make objects move and start building their own game. </p><p>The project is part of the <a href=\"http://scratch.mit.edu/info/codingforall\">Coding for All project</a>. The Coding for All project brings together an interdisciplinary research team from the MIT Media Lab, the Digital Media and Learning Hub at University of California Irvine, and Harvard University\u2019s Berkman Center for Internet and Society to develop new online tools and activities to engage more young people in developing computational fluency, particularly youth from groups currently underrepresented in computing.&nbsp;</p>", "people": ["otts@media.mit.edu", "morant@media.mit.edu", "mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "Microworlds", "modified": "2017-04-05T01:53:28.281Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "microworlds"}, {"website": "", "description": "<p>Consider each of our individual voices as a flashlight to illuminate how we project ourselves in society and how much sonic space we give ourselves or others. Thus, turn-taking computation through speaker recognition systems has been used as a tool to understand social situations or work meetings. We present SIDR, a deep learning-based, real-time speaker recognition system designed to be used in real-world settings. The system is resilient to noise, and adapts to room acoustics, different languages, and overlapping dialogues. While existing systems require the use of several microphones for each speaker or the need to couple video and sound recordings for accurate recognition of a speaker, SIDR only requires a medium-quality microphone or computer-embedded microphone.</p>", "people": ["rebklein@media.mit.edu", "tod@media.mit.edu"], "title": "SIDR: Deep Learning-Based Real-Time Speaker Identification", "modified": "2016-12-05T00:17:02.465Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "sidr-deep-learning-based-real-time-speaker-identification"}, {"website": "", "description": "<p>The goal of this project is to apply machine learning methods to model the wellbeing of MIT undergraduate students. Extensive data is obtained from the SNAPSHOT study, which monitors participating students on a 24/7 basis, collecting data on their location, sleep schedule, phone and SMS communications, academics, social networks, and even physiological markers like skin conductance, skin temperature, and acceleration.&nbsp;</p><p>We extract features from this data and apply a variety of machine learning algorithms, including Gaussian mixture models and Multi-task Multi-Kernel Learning; we are currently working to apply Bayesian hierarchical multi-task learning and Deep Learning as well.</p><p>Interesting findings include: when participants visit novel locations they tend to be happier; when they use their phones or stay indoors for long periods they tend to be unhappy; and when several dimensions of wellbeing (including stress, happiness, health, and energy) are learned together, classification accuracy improves. The biggest classification accuracy improvements come when we use multi-tasking algorithms to leverage group data while customizing a model for each participant.</p>", "people": ["jaquesn@media.mit.edu", "ehinosa@media.mit.edu", "asma_gh@media.mit.edu", "picard@media.mit.edu", "sataylor@media.mit.edu", "azaria@media.mit.edu", "akanes@media.mit.edu"], "title": "Predicting students' wellbeing from physiology, phone, mobility, and behavioral data", "modified": "2019-04-19T17:23:56.302Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "predicting-students-wellbeing-from-physiology-phone-mobility-and-behavioral-data"}, {"website": "", "description": "<p>Hello, Operator! is a vintage telephone switchboard from 1927, refurbished and wired up to a modern computer. It currently runs a time-management game; other games being prototyped are exploring the narrative potential of the system. Overall, the project exists to explore what we gain when we are able to physically engage with the antiquated technology that made the past tick.</p>", "people": ["mslw@media.mit.edu", "slavin@media.mit.edu"], "title": "Hello, Operator!", "modified": "2016-12-05T00:16:37.619Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "--Choose Location", "groups": ["future-storytelling", "playful-systems"], "published": true, "active": false, "end_on": null, "slug": "hello-operator"}, {"website": "", "description": "<p>We introduce TRANS-DOCK, a passive docking system for pin-based shape displays that enhances the interaction capability for both the output and input. By simply switching the \"transducer\" module to be docked on a single shape display, users can selectively switch between different display sizes and resolutions, movement modalities, as well as pin alignments enabled by the transducers. We introduce a design space consisting of mechanical elements and enabled interaction capabilities. We utilized several mechanical elements to develop the docking hardware for customizable interactions. With this idea, we present potential application spaces, which include digital 3D model explorations, active tangible interface prototyping, gaming, and dynamic house models. TRANS-DOCK intends to expand what a single shape display can do for dynamic physical interactions, by converting arrays of linear motion to several types of dynamic motion in an adaptable and flexible manner.</p>", "people": ["ishii@media.mit.edu", "ken_n@media.mit.edu", "jifei@media.mit.edu"], "title": "TRANS-DOCK", "modified": "2018-10-05T18:57:23.901Z", "visibility": "LAB-INSIDERS", "start_on": "2018-08-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "trans-dock"}, {"website": "", "description": "<p>Increasing understanding of how to categorize patient symptoms for efficient diagnosis has led to structured patient interviews and diagnostic flowcharts that can provide diagnostic accuracy and save valuable physician time. But the rigidity of predefined questions and controlled vocabulary for answers can leave patients feeling over-constrained, as if the doctor (or computer system) is not really attending to them.  I\ufffdm Listening is a system for automatically conducting patient pre-visit interviews. It does not replace a human doctor, but can be used before an office visit to prepare the patient, deliver educational materials or triage care, and preorder appropriate tests, making better use of both doctor and patient time. It uses an on-screen avatar and natural language processing to (partially) understand the patient's response. Key is a common-sense reasoning system that lets patients express themselves in unconstrained natural language, even using metaphor, and that maps the language to medically relevant categories.</p>", "people": ["fmoss@media.mit.edu", "jom@media.mit.edu"], "title": "I'm Listening", "modified": "2016-12-05T00:16:30.784Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-320", "groups": ["new-media-medicine"], "published": true, "active": false, "end_on": null, "slug": "im-listening"}, {"website": "http://www.mit.edu/~ajdas", "description": "<p>We demonstrate a smartphone based spectrometer design that is standalone and supported on a wireless platform. The device is inherently low-cost and the power consumption is minimal making it portable to carry out a range of studies in the field. All essential components of the device like the light source, spectrometer, filters, microcontroller and wireless circuits have been assembled in a housing of dimensions 88 mm \u00d7 37 mm \u00d7 22 mm and the entire device weighs 48\u2009g. The resolution of the spectrometer is 15\u2009nm, delivering accurate and repeatable measurements. The device has a dedicated app interface on the smartphone to communicate, receive, plot and analyze spectral data. The performance of the smartphone spectrometer is comparable to existing bench-top spectrometers in terms of stability and wavelength resolution. Validations of the device were carried out by demonstrating non-destructive ripeness testing in fruit samples. Ultra-Violet (UV) fluorescence from Chlorophyll present in the skin was measured across various apple varieties during the ripening process and correlated with destructive firmness tests. A satisfactory agreement was observed between ripeness and fluorescence signals. This demonstration is a step towards possible consumer, bio-sensing and diagnostic applications that can be carried out in a rapid manner.</p>", "people": ["ajdas@media.mit.edu"], "title": "Food sensing on smartphones", "modified": "2018-10-22T20:30:10.303Z", "visibility": "PUBLIC", "start_on": "2016-11-08", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "food-sensing-on-smartphones"}, {"website": "", "description": "<p>Terahertz time-gated spectral imaging for content extraction through layered structures.<br></p>", "people": ["barmak@media.mit.edu"], "title": "Reading Through a Closed Book", "modified": "2017-10-10T19:20:00.582Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "reading-through-a-closed-book"}, {"website": "", "description": "<p>As AI and AR/VR become more prevalent, their impact on our daily lives grows. But, advances in these technologies come with a further distancing from nature. This research is exploring the possibilities of an integrated, personal companion AI to assist users with emotional and psychological health and balance. </p><p>An AI companion that grows with you, through its years of interaction, will become familiar with your moods, health, behavior, and habits. This knowledge will assist the AI companion in suggesting gentle interventions to improve your quality of life\u2014perhaps some time spent out of doors, in nature, would benefit your concentration and mood. And if your companion is, say, a turtle, its reminder to visit the ocean or a lake holds an extra layer of meaning related to common aspects of the animal. Finally, additional layers of meaning relating to specific indigenous communities are also available. Thus, these digital spirit companions, while reconnecting the user with the natural world, can also provide links to an unknown culture, and open doors to that culture\u2019s wisdom, history, and teachings.</p>", "people": ["elenack@media.mit.edu"], "title": "Digital Spiritual Companions: Enhancing our connection with nature", "modified": "2019-04-25T01:28:14.694Z", "visibility": "LAB-INSIDERS", "start_on": "2019-04-07", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "digital-spiritual-companions-enhancing-our-connection-with-nature"}, {"website": "", "description": "", "people": [], "title": "Test project 201y5123-04u", "modified": "2018-09-26T18:22:33.160Z", "visibility": "PUBLIC", "start_on": "2018-09-26", "location": "", "groups": ["pixel-factory"], "published": false, "active": false, "end_on": null, "slug": "test-project-201y5123-04u"}, {"website": "", "description": "<p>This project investigates urban metagenomics to reveal the invisible microbiological worlds within our cities. Using honeybees to gather samples and hives modified to capture \"bee debris,\" the project employs genetic sequencing to discern and visualize urban microbiological neighborhoods and render microbiological landscapes of the city. The Holobiont project was first displayed at the Palazzo Mora in the 2016 Venice Architecture Biennale, with an installation that includes a \"metagenomic beehive.\" Creative, scientific, development and production collaboration with: Ben Berman, Dr. Elizabeth Henaff, Regina Flores Mir, Dr. Chris Mason, Devora Najjar, Tri-Lox, and Chris Woebken, with contributions from Timo Arnall and Jack Schulze and local beekeepers in Brooklyn, Sydney, and Venice.</p>", "people": ["slavin@media.mit.edu", "mperez4@media.mit.edu"], "title": "Holobiont Urbanism: Revealing the Microbiological World of Cities", "modified": "2016-12-05T00:17:13.934Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "holobiont-urbanism-revealing-the-microbiological-world-of-cities"}, {"website": "", "description": "", "people": [], "title": "Robotic Symbionts (draft)", "modified": "2016-10-21T01:00:56.077Z", "visibility": "LAB-INSIDERS", "start_on": null, "location": "", "groups": ["fluid-interfaces"], "published": false, "active": false, "end_on": null, "slug": "robotic-symbionts-draft"}, {"website": "", "description": "<p>An animated GIF is a magical thing. It has the power to compactly convey emotion, empathy, and context in a subtle way that text or emoticons often miss. GIFGIF is a project to combine that magic with quantitative methods. Our goal is to create a tool that lets people explore the world of GIFs by the emotions they evoke, rather than by manually entered tags. A web site with 200,000 users maps the GIFs to an emotion space and lets you peruse them interactively.</p>", "people": ["hidalgo@media.mit.edu", "lip@media.mit.edu", "kzh@media.mit.edu", "trich@media.mit.edu"], "title": "GIFGIF", "modified": "2016-12-14T14:00:48.619Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["viral-communications", "collective-learning"], "published": true, "active": false, "end_on": null, "slug": "gifgif"}, {"website": "", "description": "<p>Since Alan Turing envisioned Artificial Intelligence (AI), a major driving force behind technical progress has been competition with human cognition (e.g. beating humans in Chess or Jeopardy!). Less attention has been given to developing autonomous machines that learn to cooperate with humans. Cooperation does not require sheer computational power, but relies on intuition, and pre-evolved dispositions toward cooperation, common-sense mechanisms that are difficult to encode in machines. We develop state-of-the-art machine-learning algorithms that cooperate with people and other machines at levels that rival human cooperation in two-player repeated games.</p><p>Scientific writings:&nbsp;<br><span style=\"font-size: 18px; font-weight: normal;\">Jacob Crandall, Mayada Oudah, Tennom, Fatimah Ishowo-Oloko, Sherief Abdallah, Jean-Fran\u00e7ois Bonnefon, Manuel Cebrian, Azim Shariff, Michael A. Goodrich, Iyad Rahwan. </span><a style=\"font-size: 18px; font-weight: normal;\" href=\"https://arxiv.org/abs/1703.06207\">Cooperating with Machines</a><span style=\"font-size: 18px; font-weight: normal;\">.&nbsp;\tarXiv:1703.06207</span></p>", "people": ["irahwan@media.mit.edu"], "title": "Human-Machine Cooperation", "modified": "2018-01-16T19:29:48.704Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "human-machine-cooperation"}, {"website": "", "description": "", "people": [], "title": "A Flying Pantograph (draft)", "modified": "2017-03-20T19:40:57.664Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["fluid-interfaces"], "published": false, "active": false, "end_on": null, "slug": "flying-pantograph-draft"}, {"website": "", "description": "<p>Depression correlated with anxiety is one of the key factors leading to suicidal behavior, and is among the leading causes of death worldwide. Despite the scope and seriousness of suicidal thoughts and behaviors, we know surprisingly little about what suicidal thoughts look like in nature (e.g., How frequent, intense, and persistent are they among those who have them? What cognitive, affective/physiological, behavioral, and social factors trigger their occurrence?). The reason for this lack of information is that historically researchers have used retrospective self-report to measure suicidal thoughts, and have lacked the tools to measure them as they naturally occur. In this work we explore use of wearable devices and smartphones to identify behavioral, affective, and physiological predictors of suicidal thoughts and behaviors.</p>", "people": ["picard@media.mit.edu", "sfedor@media.mit.edu"], "title": "Real-time assessment of suicidal thoughts and behaviors", "modified": "2019-04-19T17:25:15.168Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "real-time-assessment-of-suicidal-thoughts-and-behaviors"}, {"website": "", "description": "<p>The Digital Construction Platform (DCP) is an in-progress research project consisting of a compound robotic arm system. The system comprises a 6-axis KUKA robotic arm attached to the endpoint of a 3-axis Altec hydraulic boom arm, which is mounted on a mobile platform. Akin to the biological model of the human shoulder and hand, this compound system utilizes the large boom arm for gross positioning and the small robotic arm for fine positioning and oscillation correction, respectively. Potential applications include fabrication of non-standard architectural forms, integration of real-time on-site sensing data, improvements in construction efficiency, enhanced resolution, lower error rates, and increased safety.</p>", "people": ["cail@media.mit.edu", "jleland@media.mit.edu", "stevenk@media.mit.edu", "neri@media.mit.edu", "j_klein@media.mit.edu"], "title": "Digital Construction Platform v.1", "modified": "2016-12-14T01:42:36.452Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "mobile-digital-construction-platform-mdcp"}, {"website": "http://maggic.ooo/Estrofem-Lab-2016", "description": "<p><span style=\"font-weight: normal;\">Geeking, workshoplogy, and freak science on the microcolonization of estrogen biomolecules Estrofem! Lab is dedicated to the development of a mobile estrogen lab: a set of tools, protocols, and wetware for low-cost, participatory biohacking necessitated by its genesis project, Open Source Estrogen. Regarded sometimes as hobo science, freak science, and public amateurism, the Estrofem Lab and its workshopologies aim to detect and extract estrogen from bodies and environmental sources, providing the contextual framework for why we hack estrogen, and why we perform science as citizens and hacktivists. This ongoing artistic investigation has led to creation of yeast estrogen sensors (YES-HER yeast) containing human estrogen receptor for detection, vacuum pump solid phase extraction (SPE) using cigarette filters, and DIY column chromatography using broken glass bottles, smashed silica gel, and methanol.</span></p>", "people": ["maggic@media.mit.edu"], "title": "Estrofem! Lab", "modified": "2016-12-05T00:17:04.078Z", "visibility": "PUBLIC", "start_on": "2015-09-07", "location": "", "groups": ["design-fiction"], "published": true, "active": false, "end_on": null, "slug": "estrofem-lab"}, {"website": "", "description": "<p>The Hyperinstruments project creates expanded musical instruments and uses technology to give extra power and finesse to virtuosic performers. They were designed to augment a wide range of traditional musical instruments and have been used by some of the world's foremost performers (Yo-Yo Ma, the Los Angeles Philharmonic, Peter Gabriel, and Penn & Teller). Research focuses on designing computer systems that measure and interpret human expression and feeling, exploring appropriate modalities and content of interactive art and entertainment environments, and building sophisticated interactive musical instruments for non-professional musicians, students, music lovers, and the general public. Recent projects involve the production a new version of the \"classic\" Hyperstring Trilogy for the Lucerne Festival, and the design of a new generation of Hyperinstruments, for Fensadense and other projects, that emphasizes measurement and interpretation of inter-player expression and communication, rather than simply the enhancement of solo performance.</p>", "people": ["rebklein@media.mit.edu", "tristan@media.mit.edu", "tod@media.mit.edu"], "title": "Hyperinstruments", "modified": "2016-12-05T00:16:30.636Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-483", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "hyperinstruments"}, {"website": "http://www.computerclubhouse.org/", "description": "<p>The Clubhouse provides a creative and safe out-of-school learning environment where young people from underserved communities around the world work with adult mentors to explore their own ideas, develop new skills, and build confidence in themselves through the use of technology.&nbsp;</p><p>The first Clubhouse was established in 1993, as a collaboration between the Lifelong Kindergarten group and The Computer Museum (now part of the Boston Museum of Science). Four guiding principles were created&nbsp;to empower youth from all backgrounds to become more capable, creative, and confident learners. The four principles are: learning by designing, following one's interests, building a community, and fostering respect and trust.&nbsp;&nbsp;Since then the network has expanded to more than 100 centers in 19 countries, serving more than 25,000 young people annually.</p><p>The Lifelong Kindergarten group continues to develop new technologies, introduce new educational approaches, and lead professional-development workshops for Clubhouses around the world.&nbsp;</p>", "people": ["chrisg@media.mit.edu", "calla@media.mit.edu", "jaleesat@media.mit.edu", "hisean@media.mit.edu", "sylvan@media.mit.edu", "mres@media.mit.edu", "nrusk@media.mit.edu", "leob@media.mit.edu"], "title": "The Clubhouse Network", "modified": "2018-05-08T15:28:19.244Z", "visibility": "PUBLIC", "start_on": "1993-01-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "computer-clubhouse"}, {"website": "", "description": "<p>The Computational Textiles Curriculum is a collection of projects that leverage the creativity and beauty inherent in e-textiles to create an introductory computer-science curriculum for middle- and high-school students. The curriculum is taught through a sequence of hands-on project explorations of increasing difficulty, with each new project introducing new concepts in computer science, ranging from basic control flow and abstraction to more complex ideas such as networking, data processing, and algorithms. Additionally, the curriculum introduces unique methods of working with the LilyPad Arduino, creating non-traditional projects such as a game controller, a networked fabric piano, an activity monitor, and a gesture recognition glove. The projects are validated, calibrated, and evaluated through a series of workshops with middle- and high-school youth in the Boston area. </p>", "people": ["kanjun@media.mit.edu", "leah@media.mit.edu"], "title": "Computational Textiles Curriculum", "modified": "2016-12-05T00:17:09.288Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": null, "slug": "computational-textiles-curriculum"}, {"website": "", "description": "<p>During the gait cycle, the human ankle complex serves as a primary power generator while simultaneously stabilizing the entire limb. These actions are controlled by an intricate interplay of several lower leg muscles that cannot be fully uncovered using experimental methods alone. A combination of experiments and mathematical modeling may be used to estimate aspects of neuromusculoskeletal functions that control human gait. In this research, a three-dimensional neuromuscular model of the human ankle-foot complex based on biplanar fluoroscopy gait analysis is presented.&nbsp;Driven by kinematics, kinetics, and electromyography (EMG), the model seeks to solve the redundancy problem, individual muscle-tendon contributions to net joint torque, in ankle and subtalar joint actuation during overground gait. An optimization approach was employed to calculate sets of morphological parameters that simultaneously maximize the neuromuscular model\u2019s metabolic efficiency and fit to experimental joint torques. Optimal morphological parameter sets produce estimates of force contributions and states for individual muscles.</p>", "people": ["dhill24@media.mit.edu", "hherr@media.mit.edu"], "title": "3D neuromuscular model of the human ankle-foot complex", "modified": "2019-04-26T18:59:57.781Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "3d-neuromuscular-model-of-the-human-ankle-foot-complex"}, {"website": "", "description": "<p>Imaging fluorescent disease biomarkers in tissues and skin is a non-invasive method to screen for health conditions. We report an automated process that combines intraoral fluorescent porphyrin biomarker imaging, clinical examinations, and machine learning for correlation of systemic health conditions with periodontal disease. 1,215 intraoral fluorescent images, from 284 consenting adults aged 18-90, were analyzed using a machine learning classifier that can segment periodontal inflammation. The classifier achieved an AUC of 0.677 with precision and recall of 0.271 and 0.429, respectively, indicating a learned association between disease signatures in collected images. Periodontal diseases were more prevalent among males (p=0.0012) and older subjects (p=0.0224) in the screened population. Physicians independently examined the collected images, assigning localized modified gingival indices (MGIs). MGIs and periodontal disease were then cross-correlated with responses to a medical history questionnaire, blood pressure and body mass index measurements, and optic nerve, tympanic membrane, neurological, and cardiac rhythm imaging examinations. Gingivitis and early periodontal disease were associated with subjects diagnosed with optic nerve abnormalities (p &lt;0.0001) in their retinal scans. We also report significant co-occurrences of periodontal disease in subjects reporting swollen joints (p=0.0422) and a family history of eye disease (p=0.0337). These results indicate cross-correlation of poor periodontal health with systemic health outcomes and stress the importance of oral health screenings at the primary care level. Our screening process and analysis method, using images and machine learning, can be generalized for automated diagnoses and systemic health screenings for other diseases.</p><p><strong>Why is this work important?</strong></p><p>Standard practices like visual assessment and diagnosis of oral diseases using bleeding with a probe do not account for patient-to-patient variation or identify disease progression risk. This study uses a machine learning model to segment oral porphyrin biomarker levels from intraoral photographs and find correlations with and prognoses of systemic health conditions.</p><p><strong>What has been done before?</strong></p><p>Current methods to diagnose oral diseases include visual inspection by doctors and probing the gums. Positive correlations have been found between oral health and heart diseases, diabetes, tobacco use, and smoking, but all depend on visual examination by doctors.</p><p><strong>What are our contributions?</strong></p><p>We report a novel process for automated machine learning oral health examinations using images of fluorescent biomarkers and cross-correlations between oral and systemic health. We collect a novel dataset for the study and find correlations between oral health and systemic conditions like swollen joints, optical nerve abnormalities in retinal scans, and a family history of eye disease. Our approach can be generalized for predicting systemic health by analyzing other biomarker images.</p><p><strong>What are the next steps?</strong></p><p>We are actively expanding the work to a larger population to discover novel cross-correlations between other biomarkers and systemic health outcomes.</p><p><strong>Related projects</strong></p><ol><li><a href=\"https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/\">Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care</a></li><li><a href=\"https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/\">Machine Learning and Automated Segmentation of Oral Diseases using Biomarker Images</a></li></ol>", "people": ["pratiks@media.mit.edu", "gyauney@media.mit.edu", "pjavia@media.mit.edu", "arana@media.mit.edu"], "title": "Machine Learning from Biomarker Signatures and Correlation to Systemic Health Conditions", "modified": "2018-11-14T19:32:31.569Z", "visibility": "PUBLIC", "start_on": "2017-09-18", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "machine-learning-from-biomarker-signatures-and-correlation-to-systemic-health-conditions"}, {"website": "", "description": "<p>Following upon work begun in the Graspables project, we are exploring what happens when a wide range of everyday consumer products can sense, interpret into human terms (using pattern recognition methods), and retain memories, such that users can construct a narrative with the aid of the recollections of the \"diaries\" of their sporting equipment, luggage, furniture, toys, and other items.</p>", "people": ["vmb@media.mit.edu"], "title": "Everything Tells a Story", "modified": "2016-12-05T00:16:28.691Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "Garden Conference Room", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "everything-tells-a-story"}, {"website": "", "description": "<p>Living Observatory is an initiative for documenting and interpreting ecological change that will allow people, individually and collectively, to better understand relationships between ecological processes, human lifestyle choices, and climate change adaptation. As part of this initiative, we are developing sensor networks that document ecological processes and allow people to experience the data at different spatial and temporal scales. Low-power sensor nodes capture climate and other data at a high spatiotemporal resolution, while others stream audio. Sensors on trees measure transpiration and other cycles, while fiber-optic cables in streams capture high-resolution temperature data. At the same time, we are developing tools that allow people to explore this data, both remotely and onsite. The remote interface allows for immersive 3D exploration of the terrain, while visitors to the site will be able to access data from the network around them directly from wearable devices.&nbsp;</p>", "people": ["gershon@media.mit.edu", "bmayton@media.mit.edu", "joep@media.mit.edu", "ddh@media.mit.edu", "sfr@media.mit.edu", "gid@media.mit.edu"], "title": "Living Observatory: Sensor networks for documenting and experiencing ecology", "modified": "2019-04-19T14:28:48.028Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["terrestrial-sensing", "responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "living-observatory-sensor-networks-for-documenting-and-experiencing-ecology"}, {"website": "", "description": "<p>Big Data for Small Places is a quantitative study of the qualities that define our neighborhoods and our collective role in the production of local places over time. We are translating the potentials of big data from the scale of the city to the scale of the urban block, the scale at which we physically experience urban space, to gain a better understanding of the local patterns and social spaces that aggregate to form metropolitan identity. We hope that this study will improve our collective understanding of the urban environments we shape and the stories they generate, that it will allow us to more sensitively test and implement real change in our shared public realm and support the invisible narratives it generates.</p>", "people": ["sdkamvar@media.mit.edu", "zhangjia@media.mit.edu", "saquib@media.mit.edu", "cjaffe@media.mit.edu", "echristo@media.mit.edu", "srife@media.mit.edu"], "title": "Big Data for Small Places", "modified": "2016-12-05T00:16:14.672Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "big-data-for-small-places"}, {"website": "", "description": "<p>Computational photography is an emerging multi-disciplinary field at the intersection of optics, signal processing, computer graphics and vision, electronics, art, and online sharing in social networks. The first phase of computational photography was about building a super-camera that has enhanced performance in terms of the traditional parameters, such as dynamic range, field of view, or depth of field. We call this Epsilon Photography. The next phase of computational photography is building tools that go beyond the capabilities of this super-camera. We call this Coded Photography. We can code exposure, aperture, motion, wavelength, and illumination. By blocking light over time or space, we can preserve more details about the scene in the recorded single photograph.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "Coded Computational Photography", "modified": "2016-12-05T00:17:08.841Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-320", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "coded-computational-photography"}, {"website": "", "description": "<p>Mental wellbeing is intimately tied to both social support and physical activity. The Challenge is a tool aimed at promoting social connections and decreasing sedentary activity in a workplace environment. Our system asks participants to sign up for short physical challenges and pairs them with a partner to perform the activity. Social obligation and social consensus are leveraged to promote participation. Two experiments were conducted in which participants' overall activity levels were monitored with a fitness tracker. In the first study, we show that the system can improve users' physical activity, decrease sedentary time, and promote social connection. As part of the second study, we provide a detailed social network analysis of the participants, demonstrating that users' physical activity and participation depends strongly on their social community.</p>", "people": ["jaquesn@media.mit.edu", "picard@media.mit.edu", "nfarve@media.mit.edu", "pattie@media.mit.edu"], "title": "The Challenge", "modified": "2016-12-05T00:16:31.390Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "the-challenge"}, {"website": "", "description": "<p>The Biblical story of the Tower of Babel involved a deliberate plan hatched by mankind to construct a platform from which man could fight God. The tower represented the first documented attempt at constructing a vertical city. The divine response to the master plan was to sever communication by instilling a different language in each builder. Tragically, the building's ultimate destruction came about through the breakdown of communications between its fabricators. In this installation we redeem the Tower of Babel by creating its antithesis. We will construct a virtuous, decentralized, yet highly communicative building environment of cable-suspended fabrication bots that together build structures bigger than themselves. We explore themes of asynchronous motion, multi-nodal fabrication, lightweight additive manufacturing, and the emergence of form through fabrication. (With contributions from Carlos Gonzalez Uribe and Dr. James Weaver (WYSS Institute and Harvard University))</p>", "people": ["m_kayser@media.mit.edu", "dumo@media.mit.edu", "jlaucks@media.mit.edu", "neri@media.mit.edu", "j_duro@media.mit.edu"], "title": "Bots of Babel", "modified": "2016-12-14T01:44:01.959Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "bots-of-babel"}, {"website": "", "description": "<p>A 3D-printed dress was debuted during Paris Fashion Week Spring 2013 as part of collaboration with fashion designer Iris Van Herpen for her show \"Voltage.\" The 3D-printed skirt and cape were produced using Stratasys' unique Objet Connex multi-material 3D printing technology, which allows a variety of material properties to be printed in a single build. This allowed both hard and soft materials to be incorporated within the design, crucial to the movement and texture of the piece. Core contributers include: Iris Van Herpen, fashion designer (Amsterdam); Keren Oxman, artist and designer (NY); and W. Craig Carter (Department of Materials Science and Engineering, MIT). Fabricated by Stratasys.</p>", "people": ["stevenk@media.mit.edu", "neri@media.mit.edu"], "title": "Anthozoa", "modified": "2016-12-14T01:44:18.853Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "anthozoa"}, {"website": "", "description": "<p>Light enables our visual perception. It is the most common medium for displaying digital information. Light regulates our circadian rhythms, affects productivity and social interaction, and makes people feel safe. Yet despite the significance of light in structuring human relationships with their environments on all these levels, we communicate very little with our artificial lighting systems. Occupancy, ambient illuminance, intensity, and color preferences are the only input signals currently provided to these systems. With advanced sensing technology, we can establish better communication with our devices. This effort is often described as context-awareness. Context has typically been divided into properties such as location, identity, affective state, and activity. Using wearable and infrastructure sensors, we are interested in detecting these properties and using them to control lighting. The Mindful Photons Project aims to close the loop and allow our light sources to \"see\" us.</p>", "people": ["joep@media.mit.edu", "maldrich@media.mit.edu", "nanzhao@media.mit.edu"], "title": "Mindful Photons: Context-Aware Lighting", "modified": "2017-06-14T21:42:10.971Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "mindful-photons-context-aware-lighting"}, {"website": "", "description": "<p>&nbsp;Biplanar fluoroscopy (BiFlo) enables three-dimensional bone kinematics analysis using x-ray videos and bone geometry from segmented CT. Hindered by a small capture volume relative to traditional optical motion capture (MOCAP), BiFlo applications to human movement are generally limited to single-joint motions with constrained range. Here, a hybrid procedure is developed for multi-joint gait analysis using BiFlo and MOCAP in tandem. Kinematic analysis of bones surrounding the knee, ankle, and foot was performed. Results show that this hybrid protocol effectively measures knee and ankle kinematics in all three body planes. Additionally, sagittal plane kinematics for select foot bone segments (proximal phalanges, metatarsals, and midfoot) was realized. The proposed procedure offers a novel approach to human gait analysis that eliminates errors originated by soft tissue artifacts, and is especially useful for ankle joint analysis, whose complexities are often simplified in MOCAP studies.</p>", "people": ["dhill24@media.mit.edu", "kmoerman@media.mit.edu", "hherr@media.mit.edu", "danask@media.mit.edu"], "title": "Biplanar Fluoroscopy Gait Analysis", "modified": "2019-04-26T19:00:25.238Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "biplanar-fluoroscopy-gait-analysis"}, {"website": "", "description": "", "people": [], "title": "Studying the Spread of False News Online", "modified": "2017-06-15T00:46:04.503Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "studying-the-spread-of-false-news-online"}, {"website": "", "description": "", "people": [], "title": "test sk proj", "modified": "2016-11-14T13:57:17.191Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "test-sk-proj"}, {"website": "", "description": "", "people": [], "title": "freal", "modified": "2016-11-14T14:19:45.744Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "freal"}, {"website": "", "description": "<p>The understanding and modeling of social influence on human economic behavior in city environments can have important implications. In this project, we study human purchase behavior at a community level and argue that people who live in different communities but work at similar locations could act as \"social bridges\" that link their respective communities and make the community purchase behavior similar through the possibility of social learning through face-to-face interactions.</p>", "people": ["suhara@media.mit.edu", "singhv@media.mit.edu", "sandy@media.mit.edu", "xdong@media.mit.edu"], "title": "Social Bridges in Community Purchase Behavior", "modified": "2016-12-05T00:17:04.354Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "social-bridges-in-community-purchase-behavior"}, {"website": "", "description": "<p>Data mining of email has provided important insights into how organizations function and what management practices lead to greater productivity. But important communications are almost always face-to-face, so we are missing the greater part of the picture. Today, however, people carry cell phones and wear RFID badges. These body-worn sensor networks mean that we can potentially know who talks to whom, and even how they talk to each other. Sensible Organizations investigates how these new technologies for sensing human interaction can be used to reinvent organizations and management.</p>", "people": ["amohan@media.mit.edu", "sandy@media.mit.edu", "orenled@media.mit.edu"], "title": "Sensible Organizations", "modified": "2016-12-05T00:17:02.975Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-383", "groups": [], "published": true, "active": false, "end_on": null, "slug": "sensible-organizations"}, {"website": "", "description": "<p>Generating 3D Lichtenberg structures in sintered media (i.e. glass) using electricity offers a new approach to digital fabrication. By robotically controlling the electrodes, a digital form can be rapidly fabricated with the benefits of a fine fractal structure. There are numerous applications, ranging from chemical catalysts, to fractal antennas, to product design.</p>", "people": ["stevenk@media.mit.edu", "neri@media.mit.edu"], "title": "Lichtenberg 3D Printing", "modified": "2016-12-14T01:44:50.091Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "lichtenberg-3d-printing"}, {"website": "", "description": "<p>Algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, yet scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of <a href=\"http://www.gendershades.org\">Gender Shades</a>, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the <a href=\"http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf\">Gender Shades study</a>;&nbsp;2) presents new performance metrics from targeted companies IBM, Microsoft, and Megvii(Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018; 3) provides performance results on PPB by non-target companies Amazon and Kairos; and 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within seven months of the original audit, we find that all three targets released new API versions.&nbsp;</p><p>All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of&nbsp;<b>31.37%&nbsp;</b>and&nbsp;&nbsp;<b>22.50%&nbsp;</b>&nbsp;for the darker female subgroup, respectively.&nbsp;</p><p>While algorithmic fairness may be approximated through reductions in subgroup error rates or other performance metrics, algorithmic justice necessitates a transformation in the development, deployment, oversight,&nbsp;and regulation of facial analysis technology. Consequently, the potential for weaponization and abuse of facial analysis technologies cannot be ignored, nor the threats to privacy or breaches of civil liberties diminished even as accuracy disparities decrease. More extensive explorations of policy, corporate practice, and ethical guidelines are thus needed to ensure vulnerable and marginalized populations are protected and not harmed as this technology evolves.&nbsp;</p>", "people": ["joyab@media.mit.edu"], "title": "Actionable Auditing: Coordinated bias disclosure study", "modified": "2019-02-11T18:35:25.732Z", "visibility": "PUBLIC", "start_on": "2019-01-24", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "actionable-auditing-coordinated-bias-disclosure-study"}, {"website": "", "description": "<p>&nbsp;Three-dimensional Digital Image Correlation (3D-DIC) is a non-contact optical-numerical&nbsp;technique for evaluating the dynamic mechanical behavior at the surface of structures and materials,&nbsp;including biological tissues. 3D-DIC can be used to extract shape and full-field displacements and strains&nbsp;with high resolution, at various length scales. While various commercial and academic 3D-DIC software&nbsp;exist, the field lacks 3D-DIC packages which offer straightforward calibration and data-merging solutions for&nbsp;multi-view analysis, which is particularly desirable in biomedical applications. To address these limitations,&nbsp;we present MultiDIC, an open-source MATLAB toolbox, featuring the first 3D-DIC software specifically&nbsp;dedicated to multi-view setups. MultiDIC integrates robust two-dimensional subset-based DIC software with&nbsp;specially tailored calibration procedures, to reconstruct the dynamic behavior of surfaces from multiple&nbsp;stereo-pairs. MultiDIC contains novel algorithms to automatically merge meshes from multiple stereo-pairs,&nbsp;and to compute and visualize 3D shape and full-field motion, deformation, and strain. User interfaces&nbsp;provide capabilities to perform 3D-DIC analyses without interacting with MATLAB syntax, while standalone&nbsp;functions also allow proficient MATLAB users to write custom scripts for specific experimental&nbsp;requirements. This paper discusses the challenges underlying multi-view 3D-DIC, details the proposed&nbsp;solutions, and describes the algorithms implemented in MultiDIC. The performance of MultiDIC is tested&nbsp;using a low-cost experimental system featuring a 360-deg 12-camera setup. The software and system are&nbsp;evaluated using measurement of a cylindrical object with known geometry subjected to rigid body motion&nbsp;and measurement of the lower limb of a human subject. The findings confirm that shape, motion, and full-field deformations and strains can be accurately measured, and demonstrate the feasibility of MultiDIC in&nbsp;multi-view in-vivo biomedical applications.</p>", "people": ["kmoerman@media.mit.edu", "hherr@media.mit.edu", "danask@media.mit.edu"], "title": "MultiDIC: a MATLAB Toolbox for Multi-View 3D Digital Image  Correlation", "modified": "2019-04-26T19:00:50.702Z", "visibility": "PUBLIC", "start_on": "2017-05-15", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "multidic-a-matlab-toolbox-for-multi-view-3d-digital-image-correlation"}, {"website": "", "description": "<p>The SpiderBot is a suspended robotic gantry system that provides an easily deployable platform from which to print large structures. The body is composed of a deposition nozzle, a reservoir of material, and parallel linear actuators. The robot is connected to stable points high in the environment, such as large trees or buildings. This arrangement is capable of moving large distances without the need for more conventional linear guides, much like a spider does. The system is easy to set up for mobile projects, and will afford sufficient printing resolution and build volume. Expanding foam can be deposited to create a building-scale printed object rapidly. Another material type of interest is the extrusion or spinning of tension elements, like rope or cable. With tension elements, unique structures such as bridges or webs can be wrapped, woven, or strung around environmental features or previously printed materials.</p>", "people": ["neri@media.mit.edu"], "title": "SpiderBot", "modified": "2016-12-14T01:44:57.660Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "spiderbot"}, {"website": "", "description": "<p>An enabling technology to build shape-changing interfaces through pneumatically driven, soft-composite materials. The composite materials integrate the capabilities of both input sensing and active shape output. We explore four applications: a multi-shape mobile device, table-top shape-changing tangibles, dynamically programmable texture for gaming, and a shape-shifting lighting apparatus.</p>", "people": ["liningy@media.mit.edu", "ishii@media.mit.edu", "jifei@media.mit.edu"], "title": "Pneumatic Shape-Changing Interfaces", "modified": "2016-12-05T00:16:44.984Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "pneumatic-shape-changing-interfaces"}, {"website": "", "description": "<p>Individuals with autism are known to have difficulties connecting with other people, reciprocating social interactions, and being emotionally regulated by others. Yet, until recently, very little attention has been given to the way people interact together, in a system, rather than by themselves. We propose a new way to collect data on how caregivers and their children, with and without autism, affect and are affected by each other (i.e., how they \"sync up\" with one another), both in their behavior and in their physiology. We also introduce a customizable digital-physical smart toy platform that will allow us to test hypotheses and collect data about patterns of caregiver-child synchrony in a naturalistic and engaging environment. MIT and Northeastern are forging a new collaboration between smart toy technology and autism research that will help uncover how the social brain develops.</p>", "people": ["picard@media.mit.edu", "ktj@media.mit.edu"], "title": "The enTRAIN Study: Physiological synchrony in children with autism", "modified": "2019-04-19T17:28:26.918Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "the-entrain-study"}, {"website": "", "description": "<p>Media scores provide a means to orchestrate multiple modalities in the creation of expressive works of art and performance. New technologies afford numerous opportunities to tell stories and create expressive artworks through a variety of media. Media scores extend the concept of a musical score to other modalities in order to facilitate the process of authoring and performing multimedia compositions, providing a medium through which to realize a modern-day Gesamtkunstwerk. Through research into the representation and the encoding of expressive intent, systems for composing with media scores are being developed. Using such a tool, the composer will be able to shape an artistic work that may be performed through human and technological means in a variety of media and utilizing various modalities of expression. Media scores offer the potential for authoring content considering live performance data and the potential for audience participation and interaction. This paradigm bridges the extremes of the continuum from composition to performance, allowing for improvisatory compositional acts at performance-time. The media score also provides a common point of reference in collaborative productions as well as the infrastructure for the real-time control of any technologies used during a live performance.</p>", "people": ["patorpey@media.mit.edu", "tod@media.mit.edu"], "title": "Media Scores", "modified": "2018-10-23T15:21:21.829Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "media-scores"}, {"website": "", "description": "<p>We demonstrate an always-available, on-body gestural interface. Using an array of pressure sensors worn around the wrist, it can distinguish subtle finger pinch gestures with high accuracy (&gt;80?). We demonstrate that it is a complete system that works wirelessly in real time. The device is simple and light-weight in terms of power consumption and computational overhead. Prototype's sensor power consumption is 89uW, allowing the prototype to last more then a week on a small lithium polymer battery. Also, device is small and non-obtrusive, and can be integrated into a wristwatch or a bracelet. Custom pressure sensors can be printed with off-the-shelf conductive ink-jet technology. We demonstrate that number of gestures can be greatly extended by adding orientation data from an accelerometer. Also, we explore various usage scenarios with the device.</p>", "people": ["artemd@media.mit.edu", "joep@media.mit.edu"], "title": "Low-power gesture input with wrist-worn pressure sensors", "modified": "2019-04-19T14:29:45.955Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "low-power-gesture-input-with-wrist-worn-pressure-sensors"}, {"website": "", "description": "<p>This project aims to develop embedded neural interface electronics for integration on mobile, external prosthetic devices. By pushing the technological limits of every layer within the stack, from semiconductor components to high-level signal processing and classification, we hope to provide practical benefit to people's daily lives in the near future.</p><p>As a first step, we are actively developing a high-fidelity electromyography (EMG) device used to observe human muscle activity. Several prosthetic ankles have already been demonstrated using this portable EMG device as a control system input.&nbsp;</p>", "people": ["thhsieh@media.mit.edu", "emrogers@media.mit.edu", "tonyshu@media.mit.edu", "jfduval@media.mit.edu", "mcarney@media.mit.edu", "hherr@media.mit.edu", "clites@media.mit.edu", "syeon@media.mit.edu", "lfreed@media.mit.edu"], "title": "Embedded Systems for Neural Interfaces", "modified": "2019-04-26T19:01:33.105Z", "visibility": "LAB-INSIDERS", "start_on": "2017-09-01", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "embedded-neural-interface-electronics-for-advanced-prosthesis-research"}, {"website": "", "description": "<p>The digitally reconfigurable surface is a pin matrix apparatus for directly creating rigid 3D surfaces from a computer-aided design (CAD) input. A digital design is uploaded into the device, and a grid of thousands of tiny pins, much like the popular pin-art toy, are actuated to form the desired surface. A rubber sheet is held by vacuum pressure onto the tops of the pins to smooth out the surface they form; this strong surface can then be used for industrial forming operations, simple resin casting, and many other applications. The novel phase-changing electronic clutch array allows the device to have independent position control over thousands of discrete pins with only a single motorized \"push plate,\" lowering the complexity and manufacturing cost of this type of device. Research is ongoing into new actuation techniques to further lower the cost and increase the surface resolution of this technology.</p>", "people": ["neri@media.mit.edu"], "title": "Digitally Reconfigurable Surface", "modified": "2016-12-14T01:45:05.951Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "digitally-reconfigurable-surface"}, {"website": "", "description": "<p>Sneak is a hybrid digital tabletop game for two-to-four players about deception, stealth, and social intuition. Each player secretly controls one agent in a procedurally generated supervillain lair. Their mission is to find the secret plans and escape without getting discovered, shot, or poisoned by another player. To accomplish this, players must interact and blend in with a series of computer-controlled henchmen while keeping a close eye on their human opponents for any social cues that might reveal their identity. Sneak introduces a number of systems that are common in video games, but were impractical in tabletop games that did not deeply integrate a smartphone app. These include procedural map generation, NPC pathfinding, dynamic game balancing, and the use of sound.</p>", "people": ["gregab@media.mit.edu", "slavin@media.mit.edu"], "title": "Sneak: A Hybrid Digital-Physical Tabletop Game", "modified": "2016-12-05T00:16:11.193Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": null, "slug": "sneak-a-hybrid-digital-physical-tabletop-game"}, {"website": "", "description": "<p>Radical Atoms is our vision of interactions with future materials. Radical Atoms goes beyond Tangible Bits by assuming a hypothetical generation of materials that can change form and appearance dynamically, becoming as reconfigurable as pixels on a screen. Radical Atoms is a computationally transformable and reconfigurable material that is bidirectionally coupled with an underlying digital model (bits) so that dynamic changes of physical form can be reflected in digital states in real time, and vice versa.</p>", "people": ["daniell@media.mit.edu", "x_x@media.mit.edu", "ishii@media.mit.edu", "labrune@media.mit.edu", "amerigo@media.mit.edu"], "title": "Radical Atoms", "modified": "2016-12-10T00:22:48.098Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "radical-atoms"}, {"website": "", "description": "<p>This project explores the effects of hardware intervention on human gait. Our current system works in parallel with a subject's biological legs to provide an unprecedented level of gait enhancement, without causing discomfort or inhibiting natural motion. Multiple controller designs are being developed to explore the effects of intervention on the metabolic cost of transport, as well as gait pathologies and adaptation. This system provides a powerful tool in the analysis of human locomotion that will &nbsp;lead to potential innovations in mobility, rehabilitation, and athletics.</p>", "people": ["kenpasch@media.mit.edu", "mnawrot@media.mit.edu", "kuan525@media.mit.edu", "hherr@media.mit.edu"], "title": "Tethered exoskeleton system for understanding and augmenting human movements", "modified": "2019-04-26T19:02:02.669Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "tethered-robotic-system-for-understanding-human-movements-1"}, {"website": "http://www.deepstream.tv", "description": "<p>Citizens and journalists are increasingly choosing to live stream civic events. But live streams are currently hard to find and lack in-depth information about the events being documented. DeepStream seeks to increase participation in this emergent form of media by creating tools for live stream curation. Users can add relevant news stories, images, tweets, and other media to almost any live or on-demand video to create more informative and engaging viewing experiences. To help find relevant videos, Deepstream includes a search engine that lets you find live streams across multiple platforms with a single search query.</p><p>By lowering the technical barriers to creating enhanced live and on-demand videos, Deepstream makes it possible for newsrooms or individuals to curate the chaos of live streams from major global events, add media to video in real-time like fact-checking live political debates, or create enhanced version of documentaries with extra footage and related stories that appear at specific times. Our goal is to connect viewers to global events in a way that emphasizes local perspectives and deeper engagement, while maintaining the experience of immediacy and authenticity that is an essential part of live streaming.</p>", "people": ["wgmangum@media.mit.edu", "ethanz@media.mit.edu"], "title": "DeepStream", "modified": "2016-12-05T00:16:35.962Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "deepstream"}, {"website": "https://tidmarsh.media.mit.edu", "description": "<p>Tidmarsh is a 600-acre former cranberry farm near Plymouth, MA that has undergone a restoration to wetland. We have instrumented the site with an extensive network of custom low-power environmental sensor nodes, microphones, and cameras. The data from the network is made available in real time and has enabled a number of explorations into the ways that people can experience and learn from large-scale, long-term sensor installations.</p><p><a href=\"https://tidmarsh.media.mit.edu\">See sensor data, listen to live audio, and watch live camera feeds on the Tidmarsh website.</a><br></p>", "people": ["bmayton@media.mit.edu", "joep@media.mit.edu"], "title": "Low-power wireless environmental sensor network", "modified": "2019-04-19T14:30:37.248Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "low-power-wireless-environmental-sensor-node"}, {"website": "", "description": "<p>OPAL is a project to allow for private data to be used in privacy-conscientious ways for good. Collaborating companies can use OPAL's open platform and algorithms behind their own firewalls to extract key development indicators. OPAL grew out of the recognition that accessing big data sources for research and policy purposes has been a conundrum. To date, data held by private companies, such as large-scale mobile phone data, have been accessed and analyzed externally, either through data challenges, or through bilateral agreements. While these types of engagements offered evidence of big data's promise and demand, these modalities limit the full realization of its potential. By \"sending the code to the data\" rather than the other way around, OPAL seeks to address these challenges and develop data services on the basis of greater trust between all parties involved.</p>", "people": ["sandy@media.mit.edu", "yva@media.mit.edu"], "title": "OPAL: Privacy-Conscientious Use of Mobile Phone Data", "modified": "2016-10-24T19:44:56.655Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": false, "active": false, "end_on": null, "slug": "yvas-untitled-project-2"}, {"website": "", "description": "<p>In December of 1968, the first human voyage to the moon catapulted the population of Earth into a new era of space exploration and self-reflection. It was during this voyage that astronauts Bill Anders and Jim Lovell recognized a familiar pale blue dot in the distance and snapped a photo, providing us with the first view of Earth from this distant vantage point. Since its release, this image has been the subject of various works of art and literature. After having seen the Earth from space, some astronauts reported a cognitive shift in awareness about the planet. This shift helped them recognize the fragility of Earth and has inspired feelings of global citizenship. Thanks to the writings of author Frank White, we now call this shift, \u201cThe Overview Effect.\u201d<br></p><p>The Media Lab\u2019s Space Enabled research group asks if it is possible to create a similar cognitive shift in Earthlings through an experiential installation piece meant to inspire global citizenship as well as universal citizenship. \u201cEarthrise: A 50 Year Contemplation\u201d will celebrate the original Earthrise photo by creating a meditative space of reflection where participants are transported to the surface of Earth\u2019s moon to reflect on themselves, Earth, and the solar system. The viewer will be immersed in the sensory experience that surrounds them. Through artificially creating \u201cThe Overview Effect,\u201d and altering our perspective, might we also inspire a more sustainable approach in our exploration of the solar system?</p><p>Frank White, author of <i>The Overview Effect: Space Exploration and Human Evolution</i>, is launching \u201cThe Human Space Program\" through his new book, <i>The Cosma Hypothesis: Implications of the Overview Effect</i> (Emergent Media; February 2019). The goal of the Human Space Program is to create a comprehensive, sustainable, and inclusive plan for exploring and developing the solar system. It is built around White\u2019s \u201cCosma Hypothesis,\u201d which addresses the question, \u201cWhat is the purpose of human space exploration? &nbsp;Why has the evolutionary process brought humanity to the brink of becoming a spacefaring species?\u201d White\u2019s surprising conclusion: <i>Homo sapiens</i> have a very significant role to play in the evolution of the universe (Cosma). Space Enabled appreciates the opportunity to dialogue with Frank about these fundamental questions shaping the moral compass of human exploration beyond earth.</p><p><i>By Lizbeth B. De La Torre,&nbsp; Rachael Petersen, Frank White&nbsp; and&nbsp; Danielle Wood</i></p>", "people": ["rachaelp@media.mit.edu", "drwood@media.mit.edu", "lizbethb@media.mit.edu"], "title": "Earthrise | A 50 Year Contemplation", "modified": "2019-01-08T17:44:59.020Z", "visibility": "PUBLIC", "start_on": "2018-12-23", "location": "", "groups": ["space-enabled"], "published": true, "active": false, "end_on": null, "slug": "earthrise-a-50-year-contemplation"}, {"website": "", "description": "<p>The Open Water Project aims to develop and curate a set of low-cost, open source tools enabling communities everywhere to collect, interpret, and share their water quality data. Traditional water monitoring uses expensive, proprietary technology, severely limiting the scope and accessibility of water quality data. Homeowners interested in testing well water, watershed managers concerned about fish migration and health, and other groups could benefit from an open source, inexpensive, accessible approach to water quality monitoring. We're developing low-cost, open source hardware devices that will measure some of the most common water quality parameters, using designs that makes it possible for anyone to build, modify, and deploy water quality sensors in their own neighborhood.</p>", "people": ["dignazio@media.mit.edu", "ethanz@media.mit.edu"], "title": "Open Water Project", "modified": "2016-12-05T00:16:38.427Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "open-water-project"}, {"website": "", "description": "<p>Traditional music production and studio engineering depends on dynamic range compression audio signal processors that precisely and dynamically control the gain of an audio signal in the time domain. This project expands on the traditional dynamic range compression model by adding a spatial dimension. Ambisonic Compression allows audio engineers to dynamically control the spatial properties of a three-dimensional sound field, opening new possibilities for surround-sound design and spatial music performance.</p>", "people": ["tod@media.mit.edu", "holbrow@media.mit.edu"], "title": "Ambisonic surround-sound audio compression", "modified": "2019-04-17T19:58:29.909Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "ambisonic-surround-sound-audio-compression"}, {"website": "", "description": "Nostalgic Touch proposes a new ritual for remembering the deceased in the digital and multicultural age. It is an apparatus that captures hand motions and attempts to replicate the sensation of intimacy or affection by playing back the comforting gestures. It stores gesture data of the people you cared about, then plays them back after they are gone. Similar to rituals in all religions, it gives us a sense of comfort in coping with the death. People in Japan, Singapore, and China live with high standards of technology, but many embrace religious rituals and superstitions as an important part of their wellbeing and decision-making. Nostalgic Touch explores how emerging technologies could be used to enrich the experience of these rituals. How could we augment these rituals to give an even better sense of comfort and intimacy? ", "people": ["sputniko@media.mit.edu", "dkc@media.mit.edu"], "title": "Nostalgic Touch", "modified": "2016-12-05T00:16:41.544Z", "visibility": "LAB", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["design-fiction"], "published": true, "active": false, "end_on": null, "slug": "nostalgic-touch"}, {"website": "", "description": "<p>Lower extremity amputation leads to limitations of biological function of individuals, which leads to challenges remaining physically active and participating in athletic activities. Physical activity is very important for cardiovascular health, weight management, and mental health. Designing devices aimed at increasing accessibility to sports will encourage individuals with amputations to continue or begin participating in athletic pursuits such as rock climbing.&nbsp;</p><p>This research presents the design and evaluation of a 2-degree-of-freedom powered ankle-foot prosthesis for rock climbing. The aim of this device is to restore function of the ankle and subtalar joints for trans-tibial amputees during rock climbing, providing the user with myoelectric position control of the foot. Precise positional control of the foot is especially important while climbing, as the climber\u2019s ability to successfully scale a route requires them to reliably reorient the foot to various shapes and orientations of holds. Passive prostheses do not allow the user to reposition the foot, and current powered prostheses are too bulky and heavy to provide benefit during rock climbing.&nbsp;</p><p>The design requirements for this device are that it must be lightweight (&lt; 1.5 kg), low profile, robust, with 2 degrees of freedom of electromyographically controlled movement. The custom designed device consists of 2 non-backdrivable linear actuators in a differential pair, allowing for powered motion in plantarflexion/dorsiflexion and inversion/eversion. Load cells aligned axially with each actuator are used to provide force feedback to the device, allowing for position control during free-space motion, and powering off the actuators when the device is loaded, relying on the non-backdrivable transmission to maintain ankle and foot position while loaded. This control scheme reduces the power requirements of the device, allowing for lighter batteries as well as smaller motors and transmission.&nbsp;</p>", "people": ["emrogers@media.mit.edu", "mcarney@media.mit.edu", "hherr@media.mit.edu", "syeon@media.mit.edu"], "title": "Design of a 2-Degree-of-Freedom Powered Ankle-Foot Prosthesis for Rock Climbing", "modified": "2019-04-26T19:02:57.902Z", "visibility": "LAB-INSIDERS", "start_on": "2017-09-15", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "ankle-foot-prosthesis-for-rock-climbing"}, {"website": "http://biomech.media.mit.edu", "description": "<p>Mechanical, electrical, and dynamic control systems recreate biological behavior with synthetic hardware. <br></p>", "people": ["kenpasch@media.mit.edu", "emrogers@media.mit.edu", "tonyshu@media.mit.edu", "mbweber@media.mit.edu", "kuan525@media.mit.edu", "mcarney@media.mit.edu", "romka@media.mit.edu", "syeon@media.mit.edu"], "title": "Mechatronic Systems", "modified": "2019-04-26T19:04:10.662Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "dynamic-interfaces"}, {"website": "http://promisetracker.org", "description": "<p>Promise Tracker is a citizen-monitoring platform designed to help communities track issues they care about and use that information to advocate for change with local government, institutions or the press. Using a simple web application, community groups can design a mobile phone-based survey, distribute the survey to community members\u2019 phones, collect data using a mobile app, visualize it on a map, and use the resulting data to advocate for change. We are currently partnering with civil society groups, universities, and government oversight agencies in Brazil who are implementing &nbsp;Promise Tracker as part of multi-sector alliances to monitor public spending and services. &nbsp;Key collaborators include:&nbsp;</p><ul><li>University of S\u00e3o Paulo's <a href=\"http://colab.each.usp.br/\" style=\"font-size: 18px; font-weight: normal;\">CoLaboratory for Development and Participation</a></li><li><a href=\"http://www.cgu.gov.br/\" style=\"font-size: 18px; font-weight: normal;\">Ministry of Transparency, Oversight and the Comptroller-General</a></li><li><a href=\"https://www.portal.ufpa.br/index.php\" style=\"font-size: 18px; font-weight: normal;\">Federal University of Par\u00e1</a><span style=\"font-size: 18px; font-weight: normal;\">'s Laboratory for Innovation and Oversight in the Public Sector</span></li><li><a href=\"https://www.facebook.com/pg/observatoriosocialdebelem/about/?ref=page_internal\" style=\"font-size: 18px; font-weight: normal;\">Social Observatory of Bel\u00e9m</a><span style=\"font-size: 18px; font-weight: normal;\">&nbsp;</span></li><li><a href=\"http://www.projetosolaaps.com.br/\" style=\"font-size: 18px; font-weight: normal;\">Project SOL</a></li><li><a href=\"http://humanitas360.org/?page_id=118\">Humanitas360</a></li></ul><p></p>", "people": ["emreiser@media.mit.edu", "jmwenda@media.mit.edu", "ethanz@media.mit.edu", "joyab@media.mit.edu", "rahulb@media.mit.edu", "ahope@media.mit.edu"], "title": "Promise Tracker", "modified": "2017-04-05T02:08:23.117Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "promise-tracker"}, {"website": "", "description": "<p>Cells\u2019 biomechanical responses to external stimuli have been intensively studied but rarely implemented into devices&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">that interact with the human body. We demonstrate that the hygroscopic and biofluorescent behaviors of&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">living cells can be engineered to design biohybrid wearables, which give multifunctional responsiveness to hu</span><span style=\"font-size: 18px; font-weight: 400;\">man sweat. By depositing genetically tractable microbes on a humidity-inert material to form a heterogeneous&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">multilayered structure, we obtained biohybrid films that can reversibly change shape and biofluorescence intensity&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">within a few seconds in response to environmental humidity gradients. Experimental characterization and&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">mechanical modeling of the film were performed to guide the design of a wearable running suit and a fluorescent&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">shoe prototype with bio-flaps that dynamically modulates ventilation in synergy with the body\u2019s need for cooling.</span></p>", "people": ["wwen@media.mit.edu", "guanyun@media.mit.edu", "oksana@media.mit.edu", "steinerh@media.mit.edu", "liningy@media.mit.edu", "ishii@media.mit.edu", "jifei@media.mit.edu", "chinyich@media.mit.edu"], "title": "bioLogic\u2014Science Advances", "modified": "2018-05-04T15:32:39.880Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "biologic"}, {"website": "http://udayan-u.com", "description": "<p>DropletIO proposes aqueous droplets as a programmable material for biology, art, and design. The DropletIO system can actuate and sense macro-scale droplets&nbsp;(nano-liter to micro-liter)&nbsp;on planar surfaces. The system can precisely move, merge, split, oscillate, and change the shape of droplets. We built custom printed circuit boards that integrate actuation and sensing, which act as building blocks for droplet control on devices of various form factors. We show how DropletIO boards can be integrated into a range of tools for biology, everyday objects as ubiquitous information displays and as an interaction medium for art and entertainment.&nbsp;&nbsp;</p><h2>Droplets in Biology&nbsp;&nbsp;</h2><p>Droplet-based microfluidics is extensively used in biology and chemistry.&nbsp;With DropletIO as the core technology, we are building a desktop machine to automate small volume liquid handling. The programmable system is capable of manipulating tiny droplets of biological samples/reagents with precise volume control. Our desktop machine will reduce lab equipment cost, eliminate human errors, and allow for the scaling of complex biological experiments from lab to production with ease. With the machine we want to bring down the cost of running assays from $10,000 to $10 to bring healthcare to billions of people. </p><p>Our solution replaces current liquid handling built on leaky tubes and unreliable pumping mechanisms with a solid-state device. Our digital device is entirely electronic and compact, a system that inexpensively scales to address complex experiments with small volume liquids. Due to its digital nature, a biologist using our system could define biological protocols by programming, executing, and sharing them. Thus, operations would scale digitally from lab to production. The system furthers cost savings by producing significantly less disposable waste such as pipette tips.</p>", "people": ["udayan@media.mit.edu", "ishii@media.mit.edu"], "title": "DropletIO", "modified": "2018-01-16T21:51:37.104Z", "visibility": "LAB-INSIDERS", "start_on": "2017-01-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "droplets"}, {"website": "", "description": "<p>Pneuduino is a hardware platform for kids, students, artists, designers, and researchers who are interested in controlling air flow and pressure for their projects. The Pneuduino toolkit is currently used in workshops with high school or college students. While each workshop has a different focus, they all introduce concepts of air as actuator and sensor as well as different fabrication methods to create transforming artifacts. Air is one the most abundant resources on earth. By adding computation ability to air, we can create new types of materials that enable us to design robots that are soft, furniture that is adaptive, clothing that is intelligent, and art pieces that are breathing.</p>", "people": ["heibeck@media.mit.edu", "liningy@media.mit.edu", "ishii@media.mit.edu", "jifei@media.mit.edu"], "title": "Pneuduino", "modified": "2016-12-05T00:17:03.712Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "pneuduino"}, {"website": "", "description": "<p>One of the fundamental goals of Montessori education is to create productive, stress-free educational environments for children. In addition to traditional methods of observation, we argue that teachers would benefit from tools that could provide supplemental data identifying stress responses in students using psychophysiological data. The child-suited wearable device we have designed incorporates sensors that track signs linked to emotional and sympathetic responses, such as heart rate variability and electro-dermal activity. Through these data points, teachers and parents can better understand the child's emotional responses to activities and social interactions at school, and tailor programs to support wellbeing and stress reduction.</p>", "people": ["sdkamvar@media.mit.edu", "koren@media.mit.edu"], "title": "A Multi-Sensor Wearable Device for Analyzing Stress Response in Preschool Classrooms", "modified": "2016-12-05T00:17:05.611Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "--Choose Location", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "a-multi-sensor-wearable-device-for-analyzing-stress-response-in-preschool-classrooms"}, {"website": "", "description": "<p>Beast is an organic-like entity created synthetically by the incorporation of physical parameters into digital form-generation protocols. A single continuous surface, acting both as structure and as skin, is locally modulated for both structural support and corporeal aid. Beast combines structural, environmental, and corporeal performance by adapting its thickness, pattern density, stiffness, flexibility, and translucency to load, curvature, and skin-pressured areas respectively.</p>", "people": ["neri@media.mit.edu"], "title": "Beast", "modified": "2016-12-14T01:47:26.954Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "beast"}, {"website": "", "description": "<p>This work explores the design of techniques based on the cognitive illusion of \"inattentional blindness\" which is a failure to notice something happening in front of you when you are focused on something else. The aim is to direct a user's attention and manage their perception to create VR experiences with high levels of presence. The&nbsp; techniques were tested through a VR implementation of a disruption-free natural walking experience.&nbsp; Video and paper available here:&nbsp;<a href=\"http://web.media.mit.edu/~sra/vmotion.html\">http://web.media.mit.edu/~sra/vmotion.html</a></p>", "people": ["sra@media.mit.edu"], "title": "Cognitive Illusions and VR", "modified": "2018-08-01T21:12:56.068Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "inattentional-blindness-in-vr"}, {"website": "", "description": "<p>This study attempts to examine humans' affective responses to superimposed sinusoidal signals. These signals can be perceived either through sound, in the case of electronically synthesized musical notes, or through vibro-tactile stimulation, in the case of vibrations produced by vibrotactile actuators. This study is concerned with the perception of superimposed vibrations, whereby two or more sinusoisal signals are perceived simultaneously, producing a perceptual impression that is substantially different than of each signal alone, owing to the interactions between perceived sinusoidal vibrations that give rise to a unified percept of a sinusoidal chord. The theory of interval affect was derived from systematic analyses of Indian, Chinese, Greek, and Arabic music theory and tradition, and proposes a universal organization of affective response to intervals organized using a multidimensional system. We hypothesize that this interval affect system is multi-modal and will transfer to the vibrotactile domain.</p>", "people": ["picard@media.mit.edu", "gleslie@media.mit.edu"], "title": "Affective Response to Haptic Signals", "modified": "2016-12-05T00:17:06.039Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "affective-response-to-haptic-signals"}, {"website": "", "description": "<p>Imparting common-sense knowledge to computers enables a new class of intelligent applications better equipped to make sense of the everyday world and assist people with everyday tasks. Our approach to this problem is ConceptNet, a freely available commonsense knowledge base that possesses a great breadth of general knowledge that computers should already know, ready to be incorporated into applications. ConceptNet 5 is a semantic network with millions of nodes and edges, built from a variety of interlinked resources, both crowd-sourced and expert-created, including the Open Mind Common Sense corpus, WordNet, Wikipedia, and OpenCyc. It contains information in many languages including English, Chinese, Japanese, Dutch, and Portuguese, resulting from a collaboration of research projects around the world. In this newest version of ConceptNet, we aim to automatically assess the reliability of its data when it is collected from variously reliable sources and processes.</p>", "people": ["havasi@media.mit.edu", "eslick@media.mit.edu"], "title": "ConceptNet", "modified": "2016-12-05T00:17:09.493Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-383", "groups": [], "published": true, "active": false, "end_on": null, "slug": "conceptnet"}, {"website": "", "description": "<p>ChainFORM is a modular hardware system for designing linear shape-changing interfaces. Each module is developed based on a servo motor with added flexible circuit board, and is capable of touch detection, visual output, angular sensing, and motor actuation. Moreover, because each module can communicate with other modules linearly, it allows users and designers to adjust and customize the length of the interface. Using the functionality of the hardware system, we propose a wide range of applications, including line-based shape changing display, reconfigurable stylus, rapid prototyping tool for actuated crafts, and customizable haptic glove. We conducted a technical evaluation and a user study to explore capabilities and potential requirements for future improvement.</p>", "people": ["artemd@media.mit.edu", "joep@media.mit.edu", "ishii@media.mit.edu", "ken_n@media.mit.edu"], "title": "ChainFORM", "modified": "2018-05-04T15:33:24.390Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["tangible-media", "responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "chainform"}, {"website": "", "description": "<p>Carpal Skin is a prototype for a protective glove to protect against Carpal Tunnel Syndrome, a medical condition in which the median nerve is compressed at the wrist, leading to numbness, muscle atrophy, and weakness in the hand. Night-time wrist splinting is the recommended treatment for most patients before going into carpal tunnel release surgery. Carpal Skin is a process by which to map the pain-profile of a particular patient \u2013 its intensity and duration \u2013 and to distribute hard and soft materials to fit the patient's anatomical and physiological requirements, limiting movement in a customized fashion. The form-generation process is inspired by animal coating patterns in the control of stiffness variation.</p>", "people": ["neri@media.mit.edu"], "title": "Carpal Skin", "modified": "2016-12-14T01:47:50.313Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "carpal-skin"}, {"website": "", "description": "<p>We are interested in investigating the motivations at play when an individual makes choices about mobility. How do factors like money, time, comfort, and habit impact how people choose to get around cities? And is it possible to disrupt these practices by nudging people in new directions?</p><p>In the first part of this study, we targeted individuals who live within five miles of their main workplace, but primarily use a car to get around. We lent these people a bicycle with the stipulation that they actually use it several times a week. This study is ongoing; we are investigating whether the act of giving somebody a bicycle will prompt them to change established commuting and transportation habits. We are assessing travel behavior through surveys and mobile location tracking app. </p><p>In the second part of this study,  we are developing a Blockchain-based marketplace that allows cyclists to anonymously share their location data and receive financial compensation from organizations that would like to sponsor cycling activity. For example, an insurance company may want to reward its customers with lower premiums for partaking in healthy commuting behavior. An advertising company may wish to understand cycling activity in order to improve their advertisement targeting. A local business may sponsor bicycling activity in its vicinity to increase sales.</p><p>This marketplace uses GPS data from sensors embedded in bicycles frames and powered by the cyclists themselves. The use of Blockchain technology makes transactions in the marketplace secure, seamless, trustworthy, and transparent. Users are able to reveal \u201cjust enough\u201d information about themselves to participate in the decentralized marketplace, instead of exposing their entire profile to a central entity. This market-driven system facilitates better incentive-matching, and in turn produces a scalable and stable solution for increasing the use of sustainable transportation in cities.</p><p>Through both these studies, we hope to develop a better understanding of individuals' travel behavior, the psychology of urban cyclists, and the benefits and challenges of city cycling.</p>", "people": ["sdkamvar@media.mit.edu", "cjaffe@media.mit.edu"], "title": "Motivation and Mobility: Bicycles and Sensors in the City", "modified": "2017-04-03T18:16:29.852Z", "visibility": "LAB-INSIDERS", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["social-computing"], "published": true, "active": false, "end_on": null, "slug": "bicycle-study"}, {"website": "", "description": "<h2>Nerve-Muscle Graft Chamber and micro-channel arrays tor interface to peripheral nerves for prosthesis control.&nbsp;</h2><p>This research effort consists of two sub-projects with the goal to develop a small implantable device for achieving bi-directional communication with the amputated nerves in a prosthesis user\u2019s residuum. The nerve-muscle graft chamber (NMGC) is a small implanted device which contains one or more electrically isolated chambers (ca. 20mm&nbsp;<i>l&nbsp;</i>x 4mm&nbsp;<i>h&nbsp;</i>x 4mm&nbsp;<i>w )</i>&nbsp;that can be filled with muscle or cutaneous tissue. The electrical activities of the components of a compound peripheral nerve that in the intact limb sub-served different motor functions can be separated by mechanically dividing the nerve and placing each isolated nerve segment into apposition with a small piece of muscle tissue in each of the separate chambers of the NMGC.&nbsp; For example, the muscle filled chambers can be ganged together in a modular design so that a single implanted device containing three chambers would interface to motor nerve fascicles that provide prosthesis command signals for three different motor functions. For a mixed peripheral nerve that is known to contain cutaneous fascicles as well as motor fascicles, an additional compartment could be added that contains cutaneous tissue. This would be done to provide an appropriate target for regenerating cutaneous nerve fibers to prevent the cutaneous axons from competing with regenerating motor nerve fibers and errantly taking up residence in the muscle tissues. Also, by provide cutaneous&nbsp; target tissue, regenerating sensory afferent nerve fiber are less likely to result in the formation of potentially painful &nbsp;neuromas.</p><p>The second sub-project aims to develop a micro-channel array into which peripheral nerve fibers will grow into. Because the micro-channels are on the order of 100 to 200 um I.D., only a small number of nerve fibers will be present in an individual micro-channel. This can potentially provide greater separation of axons by their functionality. Such separation by function is important when seeking to provide cutaneous and proprioceptive feedback by means of direct electrical activation of the sensory components of the interfaced peripheral nerves.&nbsp;&nbsp;</p>", "people": ["crtaylor@media.mit.edu", "hherr@media.mit.edu", "clites@media.mit.edu", "shriyas@media.mit.edu", "rriso@media.mit.edu", "bmaimon@media.mit.edu", "syeon@media.mit.edu", "lfreed@media.mit.edu"], "title": "Neural Interfaces", "modified": "2019-04-26T19:04:39.951Z", "visibility": "PUBLIC", "start_on": "2016-01-04", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "electrical-interfaces"}, {"website": "http://opera.media.mit.edu/projects/city_symphonies/", "description": "<p>The <a href=\"http://opera.media.mit.edu/projects/city_symphonies/\" style=\"font-size: 18px; font-weight: normal;\">City Symphony</a> project by the Opera of the Future group brings creative musical participation to everyone while encouraging collaboration between artists and amateurs, with symphony orchestras (and many other organizations) as the principal galvanizers. City Symphonies invite the citizens of a particular place to listen to the world around them, to discover the \"music\" in that place, and to work together to create a sonic portrait of that city that reveals its essential qualities and most important issues and questions to audiences locally and around the world. Going beyond crowd-sourcing, City Symphonies propose a new model of collaboration, where people of all ages and backgrounds work together to make beautiful, meaningful music that none of them\u2014including the highest-level professionals\u2014could have made alone.<br></p><p>Tod Machover and Opera of the Future launched the City Symphony project in 2012, and since then have created collaborative symphonies with the cities of <a href=\"http://toronto.media.mit.edu/\" style=\"font-size: 18px; font-weight: normal;\">Toronto </a>(Toronto Symphony Orchestra, 2013), <a href=\"http://edinburgh.media.mit.edu/\" style=\"font-size: 18px; font-weight: normal;\">Edinburgh</a> (Edinburgh International Festival, Royal Scottish National Orchestra, 2013), <a href=\"http://perth.media.mit.edu/\" style=\"font-size: 18px; font-weight: normal;\">Perth </a>(Perth International Festival, West Australian Symphony Orchestra, 2014), <a href=\"http://sinfoniefuerluzern.ch/de/\" style=\"font-size: 18px; font-weight: normal;\">Lucerne </a>(Lucerne Festival, Lucerne Festival Academy Orchestra, 2015), and <a href=\"http://detroit.media.mit.edu/\" style=\"font-size: 18px; font-weight: normal;\">Detroit</a> (Detroit Symphony Orchestra with Knight Foundation, 2015). Machover and his research group collaborated with these cities to explore new relationships between author/audience, composition/improvisation, music/noise, and online/onsite while emphasizing the potential of each locale to inspire its citizens to engage with their community through music in a profound way. One of the most rewarding aspects of the City Symphonies project is that the processes employed to achieve the final work are designed to grow naturally out of each particular city and context. For this reason, the five City Symphonies realized to date by Tod Machover and the MIT Media Lab have differed widely in terms of use of imagery/video, interactive performance elements, incorporation of local musicians, and the balance of acoustic/electronic sounds.<br></p>", "people": ["patorpey@media.mit.edu", "rebklein@media.mit.edu", "sovsey@media.mit.edu", "tod@media.mit.edu", "akito@media.mit.edu", "holbrow@media.mit.edu", "benb@media.mit.edu", "dnunez@media.mit.edu", "platte@media.mit.edu"], "title": "City Symphonies: Massive musical collaboration", "modified": "2019-04-17T19:59:05.709Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "city-symphonies-massive-musical-collaboration"}, {"website": "", "description": "<p>Analyze and visualize urban interaction with computer vision and deep neural net.&nbsp;</p>", "people": ["ryanz@media.mit.edu"], "title": "Deep Urban Interaction", "modified": "2018-05-03T15:51:35.403Z", "visibility": "PUBLIC", "start_on": "2018-02-14", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "DUI"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: normal;\">Chronosonogy was born through extending the neuroscience research of Teki Et Al, which reveals \"<i>Distinct Neural Substrates of Duration-Based and Beat-Based Auditory Timing</i>\u201d and Fassnidge Et Al\u2019s work examining \"</span><i style=\"font-size: 18px; font-weight: normal;\">Visual Interference of Auditory Signal Detection.\"&nbsp;</i><br></p><p><span style=\"font-size: 18px; font-weight: normal;\">Our perception of time is impacted by combining factors of visual-auditory override and imaginary notes sensations. Chronosonogy is both an experience and a newly discovered time-shifting phenomenon that activates a neurological quirk situated in&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">fronto-temporal-parietal regions of our brains. &nbsp;</span></p>", "people": ["arieger@media.mit.edu"], "title": "Chronosonogy: Sonic sensory time shifting", "modified": "2019-04-17T19:57:53.118Z", "visibility": "PUBLIC", "start_on": "2017-04-04", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "chronosonogy-sonic-sensory-time-shifting"}, {"website": "http://spirits.media.mit.edu", "description": "<p>Media manipulation technologies have the power to vanish people from photographs. Yet their souls live on in the deep memory of these algorithms of omission.</p>", "people": ["groh@media.mit.edu"], "title": "AI Spirits", "modified": "2019-02-14T19:49:25.678Z", "visibility": "PUBLIC", "start_on": "2018-10-22", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "ai_spirits"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: 400;\">Peoples' mindsets, meaning their beliefs about their own intellectual abilities, affect their effort and thereby their performance on tasks. The goal of this project is to investigate if we can change peoples' mindsets using a technological intervention.&nbsp;</span></p><p>The Thinking Cap is a wearable system that communicates praise for effort and ability in order to improve the resilience&nbsp;and self-esteem of the student wearing it and thus&nbsp; positively influence their motivation and academic achievements.&nbsp;</p><p><span style=\"font-size: 18px; font-weight: 400;\">The Thinking Cap is built into a \"Sorting Hat\" from the Harry Potter franchise, which we equipped with an embedded electroencephalography (EEG) headset and a Bluetooth speaker. We chose this \u201cmagical\u201d object from the well-known film/book franchise because popular press articles&nbsp;have&nbsp;suggested that people are likely to believe they possess the traits the Sorting Hat tells them they have, and consequently behave in related ways. One goal of this study is to investigate these findings in more depth. In our study we measure the self-esteem of children before and after the \u201cintervention of the hat\u201d to determine whether we observe any changes in their self-perception. The Sorting Hat could be replaced by any other object that a child may believe has \"magical\" powers. The hat uses established state-of-the-art Brain-Computer Interface (BCI) algorithms to recognize several mental processes like motor, auditory, or visual imagery as well as cognitive load and engagement level of the child (see also a related&nbsp; project from our group called </span><a href=\"https://www.media.mit.edu/projects/attentivu/overview/\" style=\"font-size: 18px; font-weight: 400;\">AttentivU</a><span style=\"font-size: 18px; font-weight: 400;\">). In an initial phase, the hat is used to recognize and report on the brain patterns of the child. We use supervised and unsupervised ML algorithms to train the system by asking the user explicitly to imagine/visualize either a simple movement or an object in their head (binary classification in most of the cases). The hat \"tells\" the child, via the Bluetooth speaker embedded in the hat, which of the two things he/she is thinking about.&nbsp;We hypothesize that, by demonstrating this basic capability of the hat to recognize their brain activity, the child will develop trust in the hat\u2019s abilities to know him or her. Thus, when the hat in a later phase praises the child for their ability or effort on a task (e.g., a math test), the child is likely to be affected by its suggestions in their future performance (\"You are doing well on this test now, let's do one more!\"). We hypothesize that using the hat can thus lead to improved academic performance.&nbsp;</span><br></p><p>If you are interested in participating in this study (your kid should be at least eight years old), please contact us at nkosmyna@media.mit.edu.&nbsp; &nbsp; &nbsp;</p>", "people": ["pattie@media.mit.edu", "nkosmyna@media.mit.edu"], "title": "Thinking Cap", "modified": "2019-04-16T17:38:17.218Z", "visibility": "PUBLIC", "start_on": "2017-12-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "thinking-cap"}, {"website": "", "description": "<p>New work:&nbsp; Fisheries in the Pacific Islands operate in an opaque market where an auditing tool built on a distributed ledger would be beneficial to the local economy by providing an immutable \"stamp\"&nbsp; that could leverage semi-trusted, third party auditors and vet fisheries for:</p><ul>\n\n<li>labor conditions on-board</li>\n<li>types of fish caught (Skipjack, Yellowfin, Bigeye)</li>\n<li>sustainability of fishing method (fish aggregating device (FAD), seine net fishing, longline)</li>\n</ul>", "people": ["oceane@media.mit.edu"], "title": "Distributed ledgers for ocean conservation and fisheries", "modified": "2019-04-16T19:18:03.919Z", "visibility": "PUBLIC", "start_on": "2019-02-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "provenance-in-pacific-island-fisheries"}, {"website": "", "description": "<p>This study aims to bring objective measurement to the multiple \"pulse\" and \"pulse-like\" measures made by practitioners of traditional Chinese medicine (TCM). The measurements are traditionally made by manually palpitating the patient's inner wrist in multiple places, and relating the sensed responses to various medical conditions. Our project brings several new kinds of objective measurement to this practice, compares their efficacy, and examines the connection of the measured data to various other measures of health and stress. Our approach includes the possibility of building a smartwatch application that can analyze stress and health information from the point of view of TCM.</p>", "people": ["picard@media.mit.edu", "javierhr@media.mit.edu", "cvx@media.mit.edu", "akanes@media.mit.edu"], "title": "Traditional Chinese medicine-inspired pulse analysis", "modified": "2019-04-19T17:36:58.241Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "large-scale-pulse-analysis"}, {"website": "https://github.com/aberke/moral-machine-simulation", "description": "<p>ABSTRACT</p><p>The impending introduction of autonomous vehicles (AVs) has posed regulatory and ethical questions regarding how they should operate. Much of the previous literature on this subject has explored these questions with an underlying model of streets based on the present.</p><p>This paper takes a different approach by putting forward a future vision for streets where privately owned and operated vehicles are no longer dominant and shared transit is more pervasive. In doing so, this paper expands the current discussions around individual AVs to the system of streets they will occupy. &nbsp;It views the topology of streets and the rules that govern them, coupled with the vehicles that move through the streets, as an autonomous system, or machine.  This project proposes updates to this autonomous system in order to build a more equitable system for a future where AVs will be ubiquitous.  The paper presents a design of two parts in order to ensure that AVs operate in the public\u2019s best interests:</p><ol><li>An update to the laws that govern the use of roads, vehicle regulations and safety standards.&nbsp;&nbsp;</li><li>A requirement that AV decision making code be open sourced.</li></ol>", "people": ["kll@media.mit.edu", "aberke@media.mit.edu"], "title": "A Future-Forward Proposal for a System of Streets and Autonomous Vehicles", "modified": "2019-01-08T20:15:32.709Z", "visibility": "PUBLIC", "start_on": "2018-12-01", "location": "", "groups": ["city-science"], "published": false, "active": false, "end_on": "2020-09-01", "slug": "a-future-forward-proposal-for-a-system-of-streets-and-autonomous-vehicles"}, {"website": "", "description": "<p>Developed by Ira Winder with the MIT Centre for Transportation and Logistics, the model seeks to use real population data and create a simulation to optimize delivery cost and coverage. This could be modified and applied to many disciplines, industries, and population types. The platform has the user place stores on a Tactile Matrix, a type of tangible interface, and displays the output of their potential delivery coverage and cost. This optimization game of sorts is a whole new approach to maximizing delivery potential. The interactive interface and layers of finely granulated and detailed data allow the user to make meaningful interventions and see the intertwining of many rich data sets. </p><p>Photos by James Li. Video by Nina Lutz.&nbsp;</p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu"], "title": "Last Mile Logistics", "modified": "2017-10-16T15:43:32.494Z", "visibility": "PUBLIC", "start_on": "2016-01-04", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "last-mile-logistics"}, {"website": "", "description": "<p>This is where you would type or paste in a project description. Please remember that a lay audience will be reading this, so consider rephrasing language from your abstract to be more accessible.</p>", "people": ["slotnick@media.mit.edu", "jliberty@media.mit.edu"], "title": "Sample Project", "modified": "2017-06-28T18:48:50.435Z", "visibility": "LAB", "start_on": "2017-06-28", "location": "", "groups": ["communications"], "published": true, "active": false, "end_on": null, "slug": "sample-project"}, {"website": "http://harpreetsareen.com", "description": "<p>Elowan is a cybernetic lifeform, a plant in direct dialogue with a machine. Using its own internal electrical signals, the plant is interfaced with a robotic extension that drives it toward light.</p>", "people": ["sareen@media.mit.edu", "pattie@media.mit.edu"], "title": "Elowan: A plant-robot hybrid", "modified": "2018-12-06T19:07:07.895Z", "visibility": "PUBLIC", "start_on": "2017-04-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "elowan-a-plant-robot-hybrid"}, {"website": "", "description": "<p>Shape-changing interfaces give physical shape to digital data so that users can feel and manipulate data with their hands and body. Combining techniques from haptics with the field of shape-changing interfaces, we propose a technique to build a perceptive model of material properties by taking advantage of the shape display's ability to dynamically render flexibility, elasticity, and viscosity in response to the direct manipulation of any computationally rendered physical shape. Using a computer-generated relationship between the manipulated pins and nearby pins in the shape display, we can create human proprioception of various material properties. Our results show that users can identify varying material properties in our simulations through direct manipulation, and that this perception is gathered mainly from their physical relationship (touch) with the shape display and its dynamic movements.</p>", "people": ["lajv@media.mit.edu", "daniell@media.mit.edu", "ishii@media.mit.edu", "ken_n@media.mit.edu"], "title": "Materiable", "modified": "2016-12-16T20:08:16.037Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "materiable-rendering-dynamic-material-properties-with-shape-changing-interfaces"}, {"website": "", "description": "<p>We built a low-cost and open source 405 nm imaging device to capture red fluorescence signatures associated with the oral biomarker porphyrin, demonstrating comparable performance to an expensive commercially available device.&nbsp; We also provide a miniaturized mobile-adaptable version of the device. A step-by-step guide for device assembly and the&nbsp;associated computer vision algorithm are shared on the project website&nbsp;to facilitate open-source access to imaging technologies.</p><p><strong>Related projects</strong></p><ol><li><a href=\"https://www.media.mit.edu/projects/machine-learning-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images/overview/\">Machine Learning for Combined Classification of Fluorescent Biomarkers and Expert Annotations Using White Light Images</a></li></ol>", "people": ["pratiks@media.mit.edu", "gyauney@media.mit.edu", "kla11@media.mit.edu"], "title": "Biomarker Imaging with Mobile Phones", "modified": "2018-05-04T20:56:15.953Z", "visibility": "PUBLIC", "start_on": "2015-06-01", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "porphyrin-imaging"}, {"website": "", "description": "<p>Nothing is more important in today's troubled world than the process of eliminating prejudice and misunderstanding, and replacing them with communication and empathy. We explore the possibility of creating public experiences to dramatically increase individual and community awareness of the power of empathy on an unprecedented scale. We draw on numerous precedents from the Opera of the Future group that have proposed concepts and technologies to inspire and intensify human connectedness (such as Sleep No More, Death and the Powers, Vocal Vibrations, City Symphonies, and Hyperinstruments) and from worldwide instances of transformative shared human experience (such as the Overview Effect, Human Libraries, Immersive Theatre, and non-sectarian spiritual traditions). The objective is to create a model of a multisensory, participatory, spatially radical installation that will break down barriers between people of immensely different backgrounds, providing instantaneous understanding of\u2013as well as long-term commitment to\u2013empathic communication.</p>", "people": ["patorpey@media.mit.edu", "rebklein@media.mit.edu", "sovsey@media.mit.edu", "tod@media.mit.edu", "akito@media.mit.edu", "holbrow@media.mit.edu", "benb@media.mit.edu", "dnunez@media.mit.edu", "platte@media.mit.edu"], "title": "Empathy and the future of experience", "modified": "2019-04-17T19:59:42.795Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "empathy-and-the-future-of-experience"}, {"website": "", "description": "<p>Temperature influences our perception and cognition both consciously and subconsciously. These effects are rooted in our bodily experiences and interactions with the environment, and are even embedded as metaphors in our language. By learning how temperature affects us in different contexts, we can make use of that knowledge to create interventions that help us with personal growth.</p><p>This project seeks to apply thermal interfaces to assist with emotion and attention regulation. Stress and attention levels can be inferred using implicit user inputs such as electrodermal activity, heart rate variability, and relative facial temperature. This information can then be used to determine appropriate thermal feedback to implicitly modify the user\u2019s perception and aid with emotional and attention regulation in a minimally disruptive fashion.</p>", "people": ["tomasero@media.mit.edu", "pattie@media.mit.edu"], "title": "Chill.out", "modified": "2018-04-25T20:05:40.570Z", "visibility": "PUBLIC", "start_on": "2018-02-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "chill-out"}, {"website": "", "description": "<p>We present CAPS, wearable on-body capsules which produce repulsive odor to deter sexual&nbsp;abuse. The capsules can be triggered by self-actuation,&nbsp;i.e. by pressure or when an act of forceful removal of clothing is identified. &nbsp;The formulation of odor involves compounds&nbsp;like civet reconstruction, 1-4, butadiene and more.</p>", "people": ["manisham@media.mit.edu", "geek@media.mit.edu"], "title": "CAPS: Curbing Assault to Protect Society", "modified": "2018-04-20T16:49:41.304Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "caps-curbing-assault-to-protect-society"}, {"website": "", "description": "<p>Computer vision uncovers predictors of physical urban change\n                    \n                </p>", "people": ["raskar@media.mit.edu", "hidalgo@media.mit.edu", "naik@media.mit.edu"], "title": "Streetchange", "modified": "2017-07-07T18:51:23.160Z", "visibility": "PUBLIC", "start_on": "2015-06-01", "location": "", "groups": ["camera-culture", "collective-learning"], "published": true, "active": false, "end_on": null, "slug": "streetchange"}, {"website": "", "description": "<p>Expansion microscopy (ExM) is a new imaging modality developed by MIT Media Lab's Synthetic Neurobiology group. ExM allows biomolecules to be imaged at nanoscale resolution on conventional, high-speed, diffraction limited optics by synthesizing a swellable polymer network within the sample and thereby physically separating objects of interest isotropically. In the past, we were able to expand the brain tissue 4-20 times in the lateral dimension, achieving 20-90 nm resolution with affordable optics, including a webcam \u2013 achieving ~90nm resolution. Considering that existing optics needed for such high resolutions require bulky, expensive parts subject to misalignment and irreversible damage in a physically demanding condition like a spaceship, ExM has the potential to be the most affordable solution for nanoscale imaging of biomolecules in the reduced gravity environment. We aim to demonstrate its claimed feasibility through the project.</p>", "people": ["jskang@media.mit.edu"], "title": "Nanoscale Imaging of Biomolecules in Zero G", "modified": "2017-11-21T21:01:47.515Z", "visibility": "PUBLIC", "start_on": "2017-06-01", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "nanoscale-imaging-of-biomolecules-in-zero-g"}, {"website": "", "description": "<p>MoveU is a wearable vestibular stimulation device for providing proprioceptive haptic feedback in virtual reality (VR).&nbsp; The device induces sensations of motion corresponding to virtual motion, thereby increasing immersion in VR and reducing cybersickness.&nbsp;</p><p>MoveU non-invasively stimulates the vestibular system using a technique called galvanic vestibular stimulation (GVS).&nbsp;GVS is a specific way to elicit vestibular reflexes&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">using electrical current&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">that has been used for over a century to study the function of the vestibular system. In addition to GVS, the device supports physiological sensing by connecting heart rate, electrodermal activity, and other sensors&nbsp; using a plug and play mechanism.&nbsp;MoveU supports multiple categories of virtual reality applications with different types of virtual motions such as driving, navigating by flying, teleporting, or riding.&nbsp;</span></p>", "people": ["sra@media.mit.edu", "abyjain@media.mit.edu", "pattie@media.mit.edu"], "title": "MoveU", "modified": "2019-05-06T20:24:10.520Z", "visibility": "PUBLIC", "start_on": "2018-07-15", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "move-u"}, {"website": "", "description": "<p>Nanoelectronics has the potential to enable radical tools for in-vivo interrogation of our biological systems in order to answer fundamental questions in biology as well as to provide novel technologies by combining diagnostics with automated, therapeutic effects at cellular precision. Realization of this promise, however, will require severe dimensional and power scaling of electronics, which is beyond the physical limitations of conventional nanoelectronics, dealing a hard blow to this dream. Our aim is to develop extremely <b>energy-efficient and ultra-scalable, next-generation nano-machines</b> that overcome these fundamental limitations and can make this dream come true, opening up entirely new avenues that were unthinkable earlier.&nbsp;These devices will possess the capabilities of&nbsp;<strong>energy harvesting</strong>,&nbsp;<strong>wireless communication with systems outside the body</strong>,&nbsp;and can be&nbsp;<strong>remotely controlled</strong>.&nbsp;&nbsp;They will be coated with biomolecules such that they can effectively camouflage and trick the body into thinking that it is a part of its own biological system. Such devices can cause a paradigm shift in life-machine synergism.</p><p>The possibilities with such bioelectronic devices are endless, and we are exploring, among other opportunities, brain activity recording at a large scale with a precision of single neuron, activity recording in spinal cord and peripheral nervous system, monitoring tumor microenvironment, observing response to pathology development or external stimulus at a single cell level, along with integrated functionalities such as stimulation and drug delivery. </p><p>The versatility of electronics is that they are inherently very fast and can be designed according to an engineer\u2019s dream to perform unique functions, which are beyond the capabilities of biology. While our immediate aims are to develop electronic devices for probing and controlling/modulating (for therapeutics) the body and brain, our long-term goal is to achieve seamless integration of nanoelectronics-bio hybrid structures into biological systems to incorporate functionalities not otherwise enabled by biology\u2014thus helping us transcend our biological constraints.&nbsp;</p><p>D. Sarkar, <a href=\"https://www.youtube.com/watch?v=GZLKFDWtNX8\">\"Could We Soon Augment Our Brains?\", TEDx</a> 2016</p>", "people": ["deblina@media.mit.edu"], "title": "Life-Nanomachine Synergism", "modified": "2018-11-26T20:39:22.417Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["nano-cybernetic-biotrek"], "published": true, "active": false, "end_on": null, "slug": "life-nanomachine-synergism"}, {"website": "", "description": "<p>We are exploring how soft, stretchable rubbers can be combined with functional electronic components to make skin-like wearable devices for healthcare and HCI applications. Our approach could not only augment user comfort, but also bring sensors into better contact with the human body to enhance their performance.</p>", "people": [], "title": "SkinBand", "modified": "2019-04-19T18:51:34.242Z", "visibility": "LAB-INSIDERS", "start_on": "2018-10-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "skinband"}, {"website": "https://unhangout.media.mit.edu", "description": "<p>Unhangout is an open source platform for running large-scale, participant-driven events online.</p><p>Each event has a landing page, which we call the lobby. When participants arrive, they can see who else is there and chat with each other. Hosts can welcome their community and do introductions in a video window that gets streamed into the lobby. Participants can then join breakouts, which are small group video chats, for in-depth conversations, peer-to-peer learning, and collaboration on projects.</p><p><b>Unhangout facilitates participant-driven, community-based learning, rather than top-down information transfer.</b></p><p>For more information visit <a href=\"https://unhangout.media.mit.edu\">https://unhangout.media.mit.edu</a>.</p>", "people": ["srishti@media.mit.edu", "ps1@media.mit.edu", "kamcco@media.mit.edu", "yumikom@media.mit.edu"], "title": "Unhangout", "modified": "2017-02-10T14:52:25.536Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["ml-learning"], "published": true, "active": false, "end_on": null, "slug": "unhangout"}, {"website": "", "description": "<p>We explore an art form where machines take on an essential role in the aesthetics and processes of the creation. Our main theme can be summarized as \"body, hybrid, and evolve,\" as we study an artistic medium that incorporates mechanical machines that institutes a hybrid creation process as well as an expressive capacity beyond body limits.</p><p>Flying Pantograph transposes human-scale drawing acts to a physically remote output canvas in different scales and aesthetics. A drone becomes an \"expression agent,\" modified to carry a pen and be controlled by human motions, then carries out the actual process of drawing on a vertical wall. Not only mechanically extending a human artist, the drone plays a crucial part of the expression as its own motion dynamics and software intelligence add new visual language to the art. This agency forms a strong link between a human artist and the canvas; however, at the same time, it is a deliberate programmatic disconnect that offers space for exploiting machine aesthetics as a core expression medium.</p><p>This seemingly straightforward technical realization is in fact a combination of non-trivial mechanical and algorithmic solutions. The drone, a floating machine, is relying on a slim chance of stabilization acquired by battling the vortex of air, the pressure and friction on the canvas surface, and the capricious mind of the human artist. This suspense, the vulnerability to instability, and the aftermath of crashing, poses a contrast with the optimistic idea of technologically evolved capability of a human artist.</p><p>At this critical point of balance, we embody an instance of evolution in form of an artistic medium. The interaction between people and our installation itself is one message, where the outcome drawing of the interaction offers another. This pushes forth the idea of collective and technological evolution across scale.</p>", "people": ["harshit@media.mit.edu", "sangwon@media.mit.edu", "pattie@media.mit.edu"], "title": "A Flying Pantograph", "modified": "2018-05-08T19:49:03.751Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "flying-pantograph"}, {"website": "", "description": "<p>Local changes in the volume, shape, and mechanical properties of the residual limb can be caused by adjacent joint motion, muscle activation, hydration, atrophy, and more. These changes affect socket fit quality and might cause inefficient load distribution, discomfort, and dermatological problems. Analyzing these effects is an important step in considering their influence on socket fit, and in accounting for their contribution within the socket design process. </p><p>In this study, a 360\u00b0 3D digital image correlation (3D-DIC) system was developed for the full-field deformation measurements of the residuum. A multi-camera rig was designed for capturing synchronized image sets as well as force measurements from a hand-held indenter. Custom camera calibration and data-processing procedures were specifically designed to transform image data into 3D point clouds, and automatically merge data obtained from multiple views into continuous surfaces. Moreover, a specially developed data-analysis procedure was applied for correlating pairs of largely deformed images of speckled surfaces, from which displacements, deformation gradients, and strains were calculated.&nbsp;Characterization of the full-field deformations using 3D-DIC provides insight into the patterns and sources of the phenomena.&nbsp;</p><p>In addition, local and subject-specific soft tissue mechanical properties were obtained by analyzing surface deformation and force measurement during indentation using inverse FE analysis. These data can be used to accurately describe the residuum\u2019s biomechanical behavior. Consequently, prosthetic socket designs that take into account these effects can be considered.</p>", "people": ["kmoerman@media.mit.edu", "hherr@media.mit.edu", "danask@media.mit.edu"], "title": "Analysis of residual limb changes using digital image correlation and finite element modeling", "modified": "2019-04-26T19:04:59.543Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "analysis-of-residual-limb-changes-using-digital-image-correlation-and-finite-element-modelling"}, {"website": "", "description": "<p><b>Bubble </b>is a pneumatically actuated wearable system that enables people with hand<br>disabilities to use their own hands to grasp objects without fully bending their fingers. Bubble offers<br>a novel approach to grasping, where slim, ultra-lightweight silicone actuators are attached to the<br>fingers. When the user wishes to grasp an object, the silicone units inflate pneumatically to fill the<br>available space around the object. The inflatable units are interchangeable, can be independently<br>inflated, and can be positioned anywhere on the fingers and in any orientation, thereby enabling a wide&nbsp;variety of grasping gestures.</p>", "people": ["alims@media.mit.edu"], "title": "Bubble: Wearable assistive grasping augmentation based on soft inflatables", "modified": "2019-05-06T20:24:23.803Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "robotics"}, {"website": "", "description": "<p>HeartBit is an interface designed for haptic heart rate biofeedback. A handheld heart beats alongside your own, mirroring the size, weight, and movement of a hidden internal organ, now external and tangible in real-time. HeartBit offers a medium for users to self-regulate in moments of stress, anxiety or exertion: Control your heart to control your breath and body\u2014for relaxation, performance enhancement, or augmented self-awareness.</p>", "people": ["rosello@media.mit.edu"], "title": "HeartBit", "modified": "2019-04-18T17:08:14.374Z", "visibility": "PUBLIC", "start_on": "2018-05-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "heartbit"}, {"website": "", "description": "<p>MM-RT is a tabletop tangible musical interface that employs electromagnetic actuators and small permanent magnets to physically induce sounds on objects. When, for example, a box with permanent magnets inside is placed on top of a pad, an electromagnet installed below the pad actuates the permanent magnets, causing them to bounce and hit the walls of a box. Timbre generation on each box is physically and digitally constrained: Each object comprises different materials and size, and a granular synthesis technique (a digital form of time domain additive synthesis) is used to create the sound producing mechanism.\n                    \n                </p>", "people": ["akito@media.mit.edu"], "title": "MM-RT", "modified": "2017-04-05T18:11:08.812Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "mm-rt"}, {"website": "", "description": "<p><a href=\"https://www.media.mit.edu/projects/city-science-andorra/overview/\">View the main City Science Andorra project profile.</a></p><p>The MIT Media Lab's City Science research group, the University of Andorra, and national and international companies are collaborating in order to bring an innovative ecosystem into the capital of Andorra. This innovation district aims to engage local citizens, researchers, and R&amp;D from the companies in order to build together an Andorran living lab, an \"innovation district\" where national and international companies can test and deploy their products and ideas and cultivate human capital.</p><p><b>Current Projects</b></p><ul><li>Andorra Innovation Space</li><li>Andorra Cultural Heritage</li><li>Drones patterns and flows, collaboration living lab<br></li><li>Young Future</li></ul>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "doorleyr@media.mit.edu", "devisj@media.mit.edu", "kll@media.mit.edu", "noyman@media.mit.edu", "csmuts@media.mit.edu"], "title": "Andorra | Innovation", "modified": "2018-07-09T18:49:41.844Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "andorra-innovation"}, {"website": "", "description": "<p>The Lightning Network is the leading solution for extending digital currencies to global scale, starting with Bitcoin. &nbsp;Lit is an open-source, minimal lightning network node which makes it easy for users to safely send high volume micropayments.\n                    \n                </p>", "people": ["tdryja@media.mit.edu"], "title": "Lit: A Lightning Network Node", "modified": "2017-05-12T18:44:24.371Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["digital-currency-initiative-dci"], "published": true, "active": false, "end_on": null, "slug": "lit"}, {"website": "", "description": "<p><b>The Launch Team: </b>Christian McBride (6 Grammys), David Gage, Mas Hino and Topper Carew.</p>", "people": [], "title": "Creating New Jazz Musical Instruments and New Jazz Idioms 2", "modified": "2018-05-08T01:55:43.662Z", "visibility": "PUBLIC", "start_on": "2018-05-07", "location": "", "groups": ["code-next"], "published": false, "active": false, "end_on": null, "slug": "creating-new-jazz-musical-instruments-and-new-jazz-idioms-the-launch-team-christian-mcbride-6-grammys-david-gage-mas-hino-and-topper-carew"}, {"website": "", "description": "<p>For understanding the brain structure, it is necessary to decipher the nanoscale organization of the biomolecular building blocks of the brain in 3D. However, the resolution of optical microscopes is limited by the diffraction of light while conventional super-resolution technologies, require specialized expensive equipment and have challenges in scaling to 3D [K. R. Porter et. al., <i>J. Exp. Med.</i><i>,</i> 81, 233\u2013246 (1945)] [E. Betzig, E. <i>et al.</i>, <i>Science</i><i>,</i> 313, 1642\u20131645 (2006)] [M. J. Rust et. al., <i>Nat Methods</i><i>,</i> 3, 793\u2013795 (2006)]. Recently, it has been shown that it is possible to break the diffraction limited resolution of optical microscopes by physically expanding the biological samples using electrostatic forces in a hydrogel: a methodology named expansion microscopy (ExM) [F. Chen et. al., <i>Science</i>, 347 (6221), 543\u2013548 (2015)]. We have <b>developed the technology to achieve highest expansion factor, reported till date</b> (100-fold linear expansion), of tissue-polymer hybrids [D. Sarkar et. al., <i>Society for Neuroscience</i> (2016)] [manuscript in preparation]. Such high physical expansion factors allow <b>imaging of biological specimens at sub-10 nm resolution</b> (i.e., 300 nm (diffraction limit) / 100 (expansion factor)), using conventional diffraction limited microscopes. This technology, which we termed <b>iterated direct E</b>xpansion Microscopy (idExM), utilizes both electrostatic and mechanical forces to achieve extremely high expansion factors and is fundamentally different from expansion microscopy (ExM) or iterated-ExM, which involves solely the electrostatic repulsive forces in the polymer for expansion. Moreover, idExM is much simpler to implement, provides better yield and retention of biomolecules, allowing post processing. IdExM enables precise mapping of the biomolecular building blocks of cells (proteins, transcriptomes (RNA), DNA) as well as the cellular interconnections that form large scale, 3D circuits, using hardware and reagents easily available in research laboratories. Thus, it is highly advantageous compared to conventional super-resolution imaging techniques, which are difficult to scale to 3D thick tissues and require forbiddingly expensive hardware and expert handling. </p><p>We are now working on applying this technology for mapping the biomolecular biomolecular building blocks of brain and provide in-depth insights into the pathology mechanisms, that could lead to the discovery of new targets for treating neurological diseases.</p>", "people": ["deblina@media.mit.edu"], "title": "Nanoscale mapping of bio-molecular building blocks of brain", "modified": "2018-09-10T21:05:13.392Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["nano-cybernetic-biotrek"], "published": true, "active": false, "end_on": null, "slug": "nanoscale-mapping-of-bio-molecular-building-blocks-of-brian"}, {"website": "", "description": "<p>Electrodermal activity (EDA) recording is a powerful, widely used tool for monitoring psychological or physiological arousal. However, analysis of EDA is hampered by its sensitivity to motion artifacts. We propose a method for removing motion artifacts from EDA, measured as skin conductance (SC), using a stationary wavelet transform (SWT). We modeled the wavelet coefficients as a Gaussian mixture distribution corresponding to the underlying skin conductance level (SCL) and skin conductance responses (SCRs). The goodness-of-fit of the model was validated on ambulatory SC data. We evaluated the proposed method in comparison with three previous approaches. Our method achieved a greater reduction of artifacts while retaining motion-artifact-free data.</p>", "people": ["jaquesn@media.mit.edu", "picard@media.mit.edu", "sfedor@media.mit.edu", "sataylor@media.mit.edu", "cvx@media.mit.edu", "akanes@media.mit.edu"], "title": "Wavelet-based motion artifact removal for electrodermal activity", "modified": "2019-04-19T17:39:31.969Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "wavelet-based-motion-artifact-removal-for-electrodermal-activity"}, {"website": "", "description": "<p>The Duct Tape Network (DTN) is a series of fun, hands-on maker clubs that encourage young children (ages 7-10) to use cardboard, tape, wood, fabric, LED lights, motors, and more to bring their stories and inventions to life. We are designing an educational framework and toolkit to engage kids in the creation of things that they care about before they lose their curiosity or get pulled in by more consumer-oriented technology. Work on DTN started in 2014 as part of a collaboration with Autodesk and is now expanding to communities all around the world.</p>", "people": ["alishap@media.mit.edu", "leob@media.mit.edu"], "title": "Duct Tape Network", "modified": "2017-02-10T02:45:19.416Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "duct-tape-network"}, {"website": "https://www.p2pu.org", "description": "<p><a href=\"https://www.p2pu.org/en/\">Peer 2 Peer University (P2PU)</a> has developed \"learning circles,\" a model for facilitating in-person study groups at community libraries. Aimed at adult learners, learning circles take advantage of libraries as public community spaces for learning. We curate open, online courses and pair learners up with their peers to foster deeper, more meaningful adult basic educational experiences.</p>", "people": ["ps1@media.mit.edu", "kamcco@media.mit.edu"], "title": "Peer 2 Peer University", "modified": "2017-02-10T17:12:23.457Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ml-learning"], "published": true, "active": false, "end_on": null, "slug": "peer-2-peer-university"}, {"website": "", "description": "<p>iCenters: We work with a select group of Historically Black Colleges and Universities (HBCUs) to encourage and support the development of iCenters (Innovation Centers). The model for the iCenters is greatly influenced by the South End Technology Center (SETC). SETC was the first community located Maker Space to spin out of the Media Lab\u2019s Bits and Atoms Lab. It is helmed by MIT Professor Emeritus Mel King. iCenters emphasize experiential learning by making and doing. By learning to code and learning to make and do with fabrication tools, we capture and retain interest in technology and facilitate closure of the Technology gap.</p>", "people": ["tcarew@media.mit.edu", "bdunning@media.mit.edu"], "title": "iCenters : The Design of a Methodology to Encourage  a New Generation of Computer Scientists, Inventors, and Innovators", "modified": "2017-04-04T21:48:44.601Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "", "groups": ["code-next"], "published": true, "active": false, "end_on": null, "slug": "icenters-the-design-of-a-methodology-to-encourage-a-new-generation-of-computer-scientists-inventors-and-innovators"}, {"website": "", "description": "", "people": [], "title": "Dynamics", "modified": "2017-04-02T17:31:57.025Z", "visibility": "LAB", "start_on": "2017-04-02", "location": "", "groups": ["biomechatronics"], "published": false, "active": false, "end_on": null, "slug": "dynamics"}, {"website": "", "description": "<p><b>L2D:D2L:</b> Learn to Develop:Develop to Learn. Much of our work is with young people from underserved communities. To better understand the age group as end users, we added a youth cohort to our Content Development staff. Their responsibility is to learn the nuances of Content Development, recommend Best Practices, create Prototypes, and develop Peer Appropriate Learning Activities.</p>", "people": ["tcarew@media.mit.edu", "bdunning@media.mit.edu"], "title": "L2D:D2L : High School Students Develop Coding and Fabrication Learning Activities For Their Peers", "modified": "2017-04-04T21:27:36.398Z", "visibility": "PUBLIC", "start_on": "2016-10-17", "location": "", "groups": ["code-next"], "published": true, "active": false, "end_on": null, "slug": "l2d-d2l-high-school-students-develop-coding-and-fabrication-learning-activities-for-their-peers"}, {"website": "", "description": "<p>Adoption of self-driving, Autonomous Vehicles (AVs) promises to dramatically reduce the number of traffic accidents, but some inevitable accidents will require AVs to choose the lesser of two evils, such as running over a pedestrian on the road or the sidewalk. Defining the algorithms to guide AVs confronted with such moral dilemmas is a challenge, and manufacturers and regulators will need psychologists to apply methods of experimental ethics to these situations.</p>", "people": ["awad@media.mit.edu", "dsouza@media.mit.edu", "irahwan@media.mit.edu"], "title": "Ethics of Autonomous Vehicles", "modified": "2018-05-01T19:43:34.342Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ethics", "scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "ethics-of-autonomous-vehicles"}, {"website": "", "description": "<p>We present Smell Camera: an innovative way of capturing memories in the form of smell that can be recorded, stored, and played in the future.</p><p><span style=\"font-size: 18px; font-weight: normal;\">The device consists of a hand-held pneumatic pump which is controlled by the user's phone, through which the user can record memories. The smells are encapsulated in a gelatin capsule which can be preserved in an air-tight personalized accessory. Whenever the user wants to experience the same environment or feeling, or evoke the same emotions, the user can play the smell and relive the &nbsp;moments.&nbsp;</span><br></p>", "people": ["manisham@media.mit.edu"], "title": "Smell Camera: Record, Play, Rewind", "modified": "2018-03-24T23:06:36.139Z", "visibility": "PUBLIC", "start_on": "2016-03-30", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "smell-camera2"}, {"website": "http://www.media.mit.edu/~mrfrank", "description": "<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>", "people": ["cebrian@media.mit.edu", "mrfrank@media.mit.edu", "hyoun@media.mit.edu", "groh@media.mit.edu", "emoro@media.mit.edu", "irahwan@media.mit.edu"], "title": "Towards Understanding the Impact of AI on Labor", "modified": "2019-03-22T16:27:19.712Z", "visibility": "PUBLIC", "start_on": "2018-03-01", "location": "", "groups": ["scalable-cooperation"], "published": false, "active": false, "end_on": null, "slug": "towards-understanding-the-impact-of-ai-on-labor"}, {"website": "", "description": "<p>Each new technology forces us to confront who we are as humans.&nbsp; As technological innovations in deep space exploration keep advancing, and space travel becomes more accessible to the broader population, what might become of our memories of Earth?&nbsp;</p><p>Speculating on a future where some of us might embark on a one-way trip into space, this project investigates the sensory modalities of memory beyond the digital.&nbsp; In addition to the terabytes of data that we are sure to bring with us on this long journey, what other forms of communication and connection might we invent for an extraterrestrial future? Olfaction has been shown to have strong ties to emotion and memory.&nbsp;</p><p>This project consists of a sensory token for astronauts that contains the unique scent of three memories of Earth: that of a loved one, that of a home, and that of a natural resource. Chemically, the fragrances are embedded in a special polymer designed to contain and release the scents over a long period of time. Through a dial, the user can choose to program and re-live one of three Earth experiences through an immersive olfactive experience. As an emotional time capsule, this project is akin to the Voyager Golden Record, but for precious smells. An exploration in the use of science for emotional ends, this project investigates alternative biological and perceptual modalities of communication and memory through olfaction.</p>", "people": ["wonder@media.mit.edu"], "title": "Smells for Space: Olfactory Timecapsule for Earthly Memories", "modified": "2017-11-21T21:16:11.380Z", "visibility": "PUBLIC", "start_on": "2017-11-06", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "memories-of-earth"}, {"website": "http://www.ghandeharioun.com", "description": "<p>Can we modulate the way we hear the world around us to make it more calming or to induce focus? </p><p>While technology is usually associated with causing stress, technology also&nbsp;has the potential to bring about calm. In particular, breathing usually&nbsp;speeds up with higher stress, but it can be slowed through a manipulation,&nbsp;and in so doing, it can help the person calm down. We are exploring a range&nbsp;of interventions to influence breathing without requiring any focused&nbsp;attention in order to be effective. In multiple projects, we have looked at&nbsp;dynamic composition of music, modulation of screen brightness, and headphone volume to create a seamless pulsating behavior, similar to breathing biofeedback, to indirectly influence breathing. Our preliminary analyses show promising results that such seamless modulation indeed have an influence on breathing rate and pattern.</p><p>In this project, we explore modulating insertion gain on a headphone in harmony with affective signals, particularly breathing rate. We study the influence of this dynamic change between \u201cinside\u201d and \u201coutside\u201d sources of sound to induce a sense of calmness. We experiment in simulated environments that resemble different situations such as a library, a busy street, and a fireplace.</p><p>We would like to thank Dan Gauger for giving us equipment and his thoughtful suggestions, including the project name. We would also like to thank Bose for making this project happen.&nbsp;</p>", "people": ["asma_gh@media.mit.edu", "picard@media.mit.edu"], "title": "WorldBeat: Hearing the world differently", "modified": "2019-04-19T17:40:48.922Z", "visibility": "PUBLIC", "start_on": "2017-12-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "worldbeat"}, {"website": "", "description": "<p>\n                    Optogenetic techniques have recently been applied to peripheral nerves as a scientific tool with the translatable goal of alleviating a variety of disorders, including chronic pain, muscle fatigue, glucose-related pathologies, and others.  When compared to the electrical stimulation of peripheral nerves, there are numerous advantages: the ability to target molecularly defined subtypes, access to opsins engendering neural inhibition, and optical recruitment of motor axons in a fashion that mimics natural recruitment, which eliminates the fatigue roadblock inherent to functional electrical stimulation. The ability to control peripheral nerves situated under deep tissue structures with transdermal, optical signals would be of enormous benefit, integrating all of the advantages conferred by optogenetics while averting the drawbacks associated with implantable devices, such as mechanical failure, device tissue heating, and a chronic foreign body response.&nbsp;</p><p>We work to develop novel molecular and optical methods in an effort to enable this transdermal optogenetic peripheral nerve control. A further example of a potential clinical application involves optogenetically targeting the vagus nerve, a peripheral cranial nerve implicated in numerous ailments, including epilepsy, migraines, obesity, hypertension, fibromyalgia, Crohn\u2019s disease, asthma, depression, and obsessive-compulsive disorder.  An efficient method of stimulating the vagus nerve with minimal side-effects and high target specificity, such as described here, may have profound implications to the study of various illnesses and disabilities.</p>", "people": ["esb@media.mit.edu", "hherr@media.mit.edu", "shriyas@media.mit.edu", "bmaimon@media.mit.edu", "lfreed@media.mit.edu"], "title": "Transdermal Optogenetic Peripheral Nerve Stimulation", "modified": "2019-04-26T19:05:38.186Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "transdermal-optogenetic-peripheral-nerve-stimulation"}, {"website": "", "description": "<p>Excessive stress can decrease office workers' productivity and negatively impact overall health. This project aims to predict office workers' stress levels using physiological and behavioral markers based on heart rate, skin conductance, skin temperature, and acceleration. Building on knowledge and models developed for student populations in the <a href=\"https://www.media.mit.edu/projects/snapshot-study/overview/\">SNAPSHOT study</a> and collecting new data from worker populations, we plan to improve stress-level prediction performance for office workers. Furthermore, we will also study how to improve productivity by decreasing stress. In order to accomplish this, we will look for causal factors that increase stress and possible interventions that can be deployed to office workers to decrease these factors.</p>", "people": ["picard@media.mit.edu", "sataylor@media.mit.edu", "akanes@media.mit.edu", "terumi@media.mit.edu"], "title": "Improving Wellbeing for Office Workers", "modified": "2018-04-25T20:10:56.203Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "improving-well-being-for-office-workers"}, {"website": "", "description": "<p>The relationship between breathing and self-reported stress is bidirectional. Respiration pattern is an indicator of stress, but it can also be manipulated to induce calmness.&nbsp;</p><p>In this project we explore this relationship via novel means of interaction. BrightBeat is a set of seamless visual, auditory, and tactile interventions that mimic a calming breathing oscillation, with the aim of influencing physiological syncing and consequently bringing a sense of focus and calmness.&nbsp;</p><p>The animation above shows an exaggerated version of BrightBeat. These interventions are designed to run easily on commonplace personal electronic devices, respect the user's privacy, and not to require constant focus or attention in order to be effective.&nbsp;<br></p>", "people": ["asma_gh@media.mit.edu", "picard@media.mit.edu"], "title": "BrightBeat: Effortlessly influencing breathing for cultivating calmness and focus", "modified": "2019-04-19T14:59:31.588Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "brightbeat"}, {"website": "", "description": "<p>A common practice in Traditional Chinese Medicine (TCM) is visual examination of the patient's tongue. This study will examine ways to make this process more objective and to test its efficacy for understanding stress- and health-related changes in people over time. We start by developing an app that makes it comfortable and easy for people to collect tongue data in daily life together with other stress- and health-related information. We will obtain assessment from expert practitioners of TCM, and also use pattern analysis and machine learning to attempt to create state-of-the-art algorithms able to help provide better insights for health and prevention of sickness.</p>", "people": ["fergusoc@media.mit.edu", "picard@media.mit.edu", "javierhr@media.mit.edu", "cvx@media.mit.edu", "akanes@media.mit.edu"], "title": "Automated Tongue Analysis", "modified": "2017-09-21T15:02:57.578Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "automated-tongue-analysis"}, {"website": "http://www.WildlifeData.org", "description": "<h2>Leveraging the power of platforms, big data, and advanced analytics for species protection and the public good in a privacy-preserving, scalable, and sustainable manner</h2><p>Modern tracking technology enables new ways of mining data in the wild. It allows wildlife monitoring centers to permanently collect geospatial data in a non-intrusive manner and in real time. Unfortunately, such sensible data is exposed to fraud and misuse and there is already a first reported case of \"cyber-poaching.\" Based on stolen geospatial data, poachers can easily track and kill animals. Meanwhile, cautious monitoring centers limited data access for research and public use. We propose a novel privacy-preserving system to allow these monitoring centers to securely answer questions from the research community and the public while the raw data is protected against unauthorized third parties. Based on the core system, several new applications are conceivable, such as a mobile app for preventing conflicts between human and wildlife or for engaging people in wildlife donation. Besides providing a solution and working on specific use cases, the intention of this project is to start a discussion about the need for data protection in the animal world. </p><p>\n                    \n                </p>", "people": ["rfrey@media.mit.edu", "sandy@media.mit.edu"], "title": "Secure Sharing of Wildlife Data", "modified": "2017-05-24T15:32:00.205Z", "visibility": "PUBLIC", "start_on": "2017-02-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "secure-sharing-of-wildlife-data"}, {"website": "", "description": "<p>We have described validation of novel machine learning architectures for designing faster, safer, and more efficacious digital medicines in our&nbsp;published research findings. This work has&nbsp; significant impact on the ethical decisions facing patients and their families, and regulatory decisions for the United States Food and Drug Administration (FDA) and European Medical Agencies (EMA). For example: Phase 3 clinical outcome trials evaluating new therapies, and vaccines are among the most complex experiments performed in medicine. Around 50% of Phase 3 trials fail. The US FDA states that a common theme is the difficulty of predicting clinical results in a wide patient base. More importantly, the barriers to this cost healthcare industries, government, and academic research hospitals millions of dollars each year, as well as drive up costs, delay life-saving treatments to patients, and in some cases lead to adverse events . We invent ethical, secure and explainable AI and machine learning systems which learn from diverse and inclusive datasets. Our research classifies, predicts and enriches novel digital endpoints to benefit patient health, eliminate adverse events, and improve outcomes while managing diseases and pioneers a regulatory path for AI and ML in medical care.</p>", "people": ["gyauney@media.mit.edu"], "title": "Research Area | Novel Ethical, Secure and Explainable Artificial Intelligence based Digital Medicines and Treatments", "modified": "2019-03-26T19:44:08.171Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "research-area-novel-ethical-and-explainable-AI-and-digital-medicines"}, {"website": "", "description": "<p>In collaboration with Massachusetts General Hospital, we are conducting a clinical trial exploring objective methods for assessing depression and its severity.&nbsp;</p><p>We are challenging the assessment methods that were created decades ago and which rely mostly on self-reported measures. We are including information from wearable sensors and regular sensors in mobile phones to collect information about sleep, social interaction, and location changes to find behavioral patterns that are associated with depressive symptoms. \n                    \n                </p>", "people": ["asma_gh@media.mit.edu", "picard@media.mit.edu", "sfedor@media.mit.edu"], "title": "Behavioral Indications of Depression Severity", "modified": "2018-04-09T01:37:18.568Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "behavioral-indications-of-depression-severity"}, {"website": "", "description": "<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Cone beam computed tomography (CBCT) is also widely used for diagnosis but is expensive and relatively cumbersome. Near-infrared imaging (NIR) offers a non-ionizing alternative for dental analysis. We examine and compare features in multiple extracted teeth using conventional radiographic, CBCT, and NIR transillumination imaging modes. NIR imaging can provide unique diagnostic value, primarily in its ability to reveal the extent of surface demineralization. We also provide examples where NIR illumination indicated underlying problem sites in need of further clinical attention and propose the use of NIR imaging to guide targeted and rational use of ionizing radiation in patients.</p><p><strong>Why is this work important?</strong></p><p>Two-dimensional radiographs and cone beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong><br></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. Much previous work has focused on light at 1310 nm, which strikes a balance between enamel and water attenuation, but such a wavelength often requires expensive sensors to image. NIR light at 850 nm has similar dental imaging properties, but it has not been studied as thoroughly as NIR at 1310 nm. It is not well understood what clinical features, if any, are present in NIR dental images, especially at 850 nm. Our previous work has examined the sensitivity of 850 nm NIR images to early caries lesions, but if NIR is to synergistically augment X-rays and CBCT as the standard of care, we must evaluate how well such images represent other clinical features.</p><p><strong>What are our contributions?</strong></p><p>We examine and compare features in multiple extracted teeth using conventional radiographic, CBCT, and NIR transillumination modes. NIR imaging can provide unique diagnostic value, primarily in its ability to reveal the extent of surface demineralization. We also provide examples where NIR illumination indicated underlying problem sites in need of further clinical attention and propose the use of NIR imaging to guide targeted and rational use of ionizing radiation in patients. We also show that NIR imaging identifies clinical features associated with early dimineralization and enamel caries that are not apparent upon expert visual examination.</p><p><strong>What are the next steps?</strong></p><p>Ongoing work is being done to model the interaction of light inside the tooth in order to provide even more diagnostic power.</p><p><strong>Related projects</strong></p><ol><li><a href=\"https://www.media.mit.edu/projects/near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth/overview/\">Near-Infrared Imaging for Detecting Dental Caries</a></li><li><a href=\"https://www.media.mit.edu/projects/replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis/overview/\">Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography</a></li></ol>", "people": ["pratiks@media.mit.edu", "ggbhatia@media.mit.edu", "kla11@media.mit.edu"], "title": "Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging", "modified": "2018-10-21T18:35:38.823Z", "visibility": "PUBLIC", "start_on": "2017-05-08", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging"}, {"website": "http://www.youplural.org", "description": "<p>Canadian artist Emily Carr once said, \u201cYou come into the world alone and you go out of the world alone yet it seems to me you are more alone while living than even going and coming.\u201d This observation seems more piercingly accurate now, in our 21st century reality, than it was in Carr\u2019s early 20th century reality. If we consider how individuals connect and how their connections evolve into community, we may extend that thinking toward how to facilitate those connections\u2014those sparks\u2014through which individuals who know very little about each other realize they indeed share something in common, in sometimes unexpected or surprising ways. </p><p>Through the medium of photography, YOU:PLURAL aims to create that spark of connection in an all-inclusive way. Beginning with everyone in our neighborhood\u2014E14 and E15, including the Media Lab, CBA, CMS, and ACT\u2014and hopefully beyond, we invite you to explore connections to people you may often see but don\u2019t know well. (And we will help you!) We will then document those nascent connections as photographic images and associated metadata. These collected portraits will be displayed as part of our first show on the Civic Media ArtCube in early 2019.</p><p>If you\u2019re interested in building and enhancing community here in E14 and E15 and want to have fun in the process, let us know. We\u2019d love to hear your ideas for making creative, compelling images of our community.</p><p>\u2014Cindy Bishop + Lorrie LeJeune&nbsp;</p>", "people": ["csbishop@media.mit.edu", "lorrie@media.mit.edu"], "title": "You:Plural", "modified": "2019-04-08T15:15:24.252Z", "visibility": "PUBLIC", "start_on": "2019-04-04", "location": "", "groups": ["center-for-civic-media"], "published": true, "active": false, "end_on": null, "slug": "youplural"}, {"website": "", "description": "<p>Lower-extremity amputation surgery has not seen significant change since the Civil War. This research is focused on the development of novel amputation paradigms that leverage native biological end organs to interpret efferent motor commands and to provide meaningful neural feedback from an artificial limb. Surgical replication of natural agonist-antagonist muscle pairings within the residuum allow us to use biomimetic constructs to communicate joint state and torque from the prosthesis directly to the peripheral nervous system. We hypothesize that these architectures will facilitate control of advanced prosthetic systems to improve gait and reduce metabolic cost of transport.\n                    \n                </p>", "people": ["hherr@media.mit.edu", "clites@media.mit.edu", "shriyas@media.mit.edu", "lfreed@media.mit.edu"], "title": "Revolutionizing amputation surgery for the restoration of natural neural sensation and mobility", "modified": "2019-04-26T19:05:58.968Z", "visibility": "PUBLIC", "start_on": "2016-08-15", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "amputation"}, {"website": "", "description": "", "people": [], "title": "test", "modified": "2017-02-15T16:48:43.566Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "test"}, {"website": "", "description": "", "people": [], "title": "Hakoniwa: Tabletop-scale Networked Sensory Landscape", "modified": "2017-04-05T15:31:18.578Z", "visibility": "PUBLIC", "start_on": "2016-10-01", "location": "", "groups": ["responsive-environments"], "published": false, "active": false, "end_on": null, "slug": "hakoniwa"}, {"website": "", "description": "<p>The Dermal&nbsp;<span style=\"font-size: 18px;\">Abyss (d-abyss) is a novel approach to biointerfaces in which&nbsp;</span><span style=\"font-size: 18px;\">the body surface is rendered an interactive display by patterning&nbsp;</span><span style=\"font-size: 18px;\">into the skin biosensors whose colors change in response&nbsp;</span><span style=\"font-size: 18px;\">to variations in the interstitial fluid. This modern interpretation o</span><span style=\"font-size: 18px;\">f self-expression blends recent advances in biotechnology&nbsp;</span><span style=\"font-size: 18px;\">with traditional methods in tattoo artistry.</span></p>", "people": ["xxxxxxin@media.mit.edu"], "title": "Dermal Abyss: Interfacing with the Skin by Tattooing Biosensors", "modified": "2017-06-01T17:19:34.712Z", "visibility": "LAB-INSIDERS", "start_on": null, "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "dermal-abyss"}, {"website": "", "description": "<p>The Scratch Team invited young people from around the world to create <a href=\"http://www.scratch.mit.edu\">Scratch</a>&nbsp;projects designed specifically to be played in zero gravity. Scratch members submitted over two hundred projects to this special initiative. &nbsp;Eric Schilling from the Scratch Team deployed a diverse collection of these projects on the Space Exploration initiative's inaugural research flight in zero gravity.&nbsp;</p>", "people": ["christan@media.mit.edu", "eschill@media.mit.edu"], "title": "Scratch in Space", "modified": "2018-01-25T21:35:26.771Z", "visibility": "PUBLIC", "start_on": "2017-10-01", "location": "", "groups": ["lifelong-kindergarten", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "scratch-in-space"}, {"website": "", "description": "<p>Opinion aggregation on social media uses various mechanisms,  such as \"Likes\" or thumbs-up/-down, which handle a single item at a time. In many domains (e.g., political discussion), we need to consider the relationships between different claims, and how they rebut one another through complex webs of arguments and counter-arguments. We study methods for aggregating opinions about such complex argument networks, the quality of the outcomes of different methods of opinion aggregation, and whether strategic agents can manipulate those outcomes.\n                    \n                </p>", "people": ["awad@media.mit.edu", "irahwan@media.mit.edu"], "title": "Opinion Aggregation", "modified": "2017-06-24T15:24:58.283Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "opinion-aggregation"}, {"website": "", "description": "<h2>Creating long-term interpersonal interaction and shared experiences with social robots&nbsp;<br></h2><p>Many of our current projects explore the use of social robots as a technology to support young children's early language development. In this project, instead of focusing on <i>how</i> to make social robots effective as an educational tools, we ask <i>why</i> they are effective. Based on our prior work, we hypothesize that a key aspect of why social robots can benefit children's learning is their nature as a<i> relational technology</i>\u2014that is, a technology that can build long-term, social-emotional relationships with users. </p><p>Thus, in this project, our goals are twofold. First, we aim to understand how children conceptualize social robots as relational agents in learning contexts, and how children relate to these robots through time. Second, we explore the core nature of autonomous relational technologies, that is, relational AI. We will examine how adding features of relational AI to a social robot impacts longitudinal child-robot learning interactions, including children's learning, engagement, and relationships.</p><p>As part of this project, we are taking a second look at work we have done so far, this time through the lens of children's relationships. We are creating assessments for measuring young children's relationships. We are developing a computational relational AI model, which we will test during a longitudinal study with a social robot.</p><p><a href=\"https://www.media.mit.edu/posts/making-new-robot-friends/\">Read more about children's relationships with robots here!</a><br></p>", "people": ["cynthiab@media.mit.edu", "jakory@media.mit.edu", "picard@media.mit.edu"], "title": "Relational AI", "modified": "2018-10-19T15:23:25.170Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["ml-learning", "personal-robots"], "published": true, "active": false, "end_on": null, "slug": "relational-ai"}, {"website": "", "description": "<p>Given the cross-border impact of AI and related technologies, what are appropriate and workable governance mechanisms that can operate at a global scale? Leveraging insights from research into innovative governance models in the Internet realm, this project examines the application of governance models to AI systems through a series of case studies and working meetings in the US, Europe, and Asia. Outputs include a policy report, recommendations, and an international working group.</p>", "people": [], "title": "AI and Global Governance", "modified": "2017-07-11T00:57:29.055Z", "visibility": "PUBLIC", "start_on": "2017-07-07", "location": "", "groups": ["ethics-and-governance"], "published": true, "active": false, "end_on": null, "slug": "test-project-friday-pm"}, {"website": "", "description": "<p>Recent rapid advances in Artificial Intelligence (AI) and Machine Learning have raised many questions about the regulatory and governance mechanisms for autonomous machines. This is not about individual gadgets, but about complex, networked systems of humans and algorithms making decisions in business, government, and the media. We need conceptual frameworks for designing new governance architectures for these human-machine social systems. In doing so, it is helpful to learn lessons about human cooperation and governance from political philosophy and cultural anthropology. Read more <a href=\"https://arxiv.org/abs/1707.07232\">here</a>.</p>", "people": ["irahwan@media.mit.edu"], "title": "Society-in-the-Loop", "modified": "2017-07-30T19:19:31.110Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["scalable-cooperation", "ethics-and-governance"], "published": true, "active": false, "end_on": null, "slug": "governance-artificial-intelligence-social-media"}, {"website": "http://shelley.ai", "description": "<h2>Project website:&nbsp;<a href=\"http://shelley.ai\">shelley.ai&nbsp;<br></a>Human-AI collaborated stories:&nbsp;<a href=\"http://stories.shelley.ai\">stories.shelley.ai&nbsp;<br></a>Follow&nbsp;<a href=\"http://twitter.com/shelley_ai\">@shelley_ai</a> to collaborate with Shelley!&nbsp;</h2><br><p>For centuries, across geographies, religions, and cultures, people have innovated ways of scaring each other. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity.&nbsp;This challenge is especially important at a time when we are exploring the limits of artificial intelligence: Can machines learn to scare us?&nbsp;</p><p>In Halloween 2016 we presented the&nbsp;<a href=\"http://nightmare.mit.edu/\">Nightmare Machine</a>\u2014computer-generated scary imagery powered by deep learning algorithms.&nbsp;</p><p>This Halloween, we present <b>Shelley:&nbsp;Human-AI Collaborated Horror Stories</b>!&nbsp;</p><p>Shelley is a deep-learning powered AI who was raised reading eerie stories coming from&nbsp;<a href=\"http://reddit.com/r/nosleep\">r/nosleep</a>. Now, as an adult\u2014and not unlike Mary Shelley, her Victorian idol\u2014she takes a bit of inspiration in the form of a random seed, or a short snippet of text, and starts creating stories emanating from her creepy creative mind. But what Shelley truly enjoys is working collaboratively with humans, learning from their nightmarish ideas, creating the best scary tales ever. If you want to work with her, respond to the stories she'll start every hour on her Twitter <a href=\"http://twitter.com/shelley_ai\">account</a>, and she will write with you the first AI-human horror anthology ever put together!</p>", "people": ["cebrian@media.mit.edu", "pinary@media.mit.edu", "irahwan@media.mit.edu"], "title": "Shelley: Human-AI Collaborated Horror Stories", "modified": "2017-12-12T21:51:49.213Z", "visibility": "PUBLIC", "start_on": "2017-10-15", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "shelley"}, {"website": "", "description": "", "people": [], "title": "Test project to look at project associations", "modified": "2018-04-18T15:50:28.082Z", "visibility": "PUBLIC", "start_on": "2018-04-18", "location": "", "groups": ["responsive-environments"], "published": false, "active": false, "end_on": null, "slug": "test-project-to-look-at-project-associations"}, {"website": "", "description": "", "people": [], "title": "Sculpting the Composition of the Gut Microbiota", "modified": "2017-02-17T05:15:33.074Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "gutmicrobiota"}, {"website": "http://turingbox.mit.edu", "description": "<p>TuringBox is a platform&nbsp;that makes it easier for social and behavioral scientists to study Artificial Intelligence algorithms.&nbsp;It is a two-sided marketplace. On one side, <i>AI contributors</i> upload existing and novel algorithms to be studied scientifically by others, gaining reputation in their community as a result. They can also upload software that interacts with deployed AI systems that are already on the Internet.&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">On the other side, </span><i style=\"font-size: 18px; font-weight: 400;\">AI examiners</i><span style=\"font-size: 18px; font-weight: 400;\">&nbsp;develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.</span></p>", "people": ["cebrian@media.mit.edu", "dubeya@media.mit.edu", "judyshen@media.mit.edu", "zive@media.mit.edu", "groh@media.mit.edu", "irahwan@media.mit.edu", "nobradov@media.mit.edu", "felbo@media.mit.edu", "blakeley@media.mit.edu"], "title": "TuringBox: Democratizing the study of AI", "modified": "2018-04-05T19:00:33.916Z", "visibility": "PUBLIC", "start_on": "2018-03-21", "location": "", "groups": ["ethics", "scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "turingbox"}, {"website": "", "description": "<p>ImmerSound is a virtual reality experience wherein one can compose music by drawing in 3D. The resulting composition is a sculpted soundscape to be experienced both visually and in 3D audio.&nbsp;</p><p>The user starts by choosing an instrument in the system and testing the sound that this instrument would produce at different locations. Then the user can \"paint\" a melody in space, where the elevation of the \"sound brush\" defines the pitch of the instrument, and the speed of the hand corresponds to the tempo of the melody created. &nbsp;<span style=\"font-size: 18px; font-weight: normal;\">A wide range of instruments enables the creation of rich compositions with percussion, bass, classical instruments, and ambient sounds.&nbsp;</span></p><p><span style=\"font-size: 18px; font-weight: normal;\">This project associates sounds and space in a new way by offering an intuitive and natural way to interact with music. One can also imagine the same type of visual compositional space used as a neutral zone for collaboration between two or more people in different geographical locations and from different cultural backgrounds, using the universal language of music to connect in less-biased ways. This system is a first example of the potential of virtual reality for music and experiences of connection.</span></p>", "people": ["rebklein@media.mit.edu"], "title": "ImmerSound VR", "modified": "2017-04-05T18:49:37.476Z", "visibility": "PUBLIC", "start_on": "2016-09-15", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "immersound-vr"}, {"website": "", "description": "<p>ScratchBit is an effort to enable children to create more seamlessly in both the physical and digital world by creating a dedicated physical interface for the&nbsp;<a href=\"https://scratch.mit.edu\">Scratch</a>&nbsp;programming language and environment. Designed to be rugged, low cost, and highly composable, the ScratchBit allows children to take the materials around them\u2014such as cardboard, clothes, skateboards, and trees\u2014and &nbsp;transform them into inputs to their digital creations on Scratch. Unlike the <a href=\"http://makeymakey.com/\">Makey Makey</a> which was designed to make these connections electronically, the ScratchBit is designed to create these connections through motion and mechanism.</p>", "people": ["khanning@media.mit.edu", "jieqi@media.mit.edu", "mres@media.mit.edu", "ericr@media.mit.edu", "ascii@media.mit.edu", "nrusk@media.mit.edu"], "title": "ScratchBit", "modified": "2018-11-03T16:11:24.635Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratch-pad"}, {"website": "", "description": "<p>The Open Leadership Camp (OLC) is a new type of professional development program for senior leaders of nonprofit and public sector organizations. It aims to &nbsp;<span style=\"font-size: 18px; font-weight: 400;\">apply the principles of open source, open innovation, and the decentralized nature of the web to the way some of our most crucial social sector organizations work.&nbsp;</span></p><p>This project is a collaboration between <a href=\"https://www.mozilla.org\">Mozilla</a> and the <a href=\"https://www.media.mit.edu/groups/ml-learning/overview/\">ML Learning Initiative</a>, and is&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">hosted by MIT Media Lab Director <a href=\"https://www.media.mit.edu/people/joi/overview/\">Joi Ito</a> and <a href=\"https://www.mozilla.org/en-US/about/leadership/\">Mitchell Baker</a>, co-founder and Executive Chairwoman of the Mozilla Corporation. In March 2017, we brought together our first cohort of 14 participants, including the CEO of Consumer Reports, the CIO of the City of Detroit, and the CEO of WGBH.&nbsp;</span></p>", "people": ["ps1@media.mit.edu", "kamcco@media.mit.edu", "yumikom@media.mit.edu"], "title": "Open Leadership Camp", "modified": "2017-06-19T14:42:03.293Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["ml-learning"], "published": true, "active": false, "end_on": null, "slug": "open-leadership-camp"}, {"website": "", "description": "<p>As we generally experience on earth, there is no space without sound and there is no sound without space. Building on the understanding of music and architecture as creators of spatial experience, this project presents a novel way of unfolding music\u2019s spatial qualities in the physical world.&nbsp;<i>Spaces that Perform Themselves&nbsp;</i>exposes an innovative response to the current relationship between sound and space: where we build static spaces to contain dynamic sounds. What if we change the static parameter of the spaces and start building dynamic spaces to contain dynamic sounds?&nbsp;</p><p>A multi-sensory kinetic architectural system is built in order to augment our sonic perception through a cross-modal spatial choreography that combines sound, movement, light, color, and vibration. By breaking down boundaries between music and architecture, possibilities of a new typology that morphs responsively with a musical piece can be explored. As a result,&nbsp;spatial&nbsp;and musical composition can exist as one synchronous entity. These spatial choreographies build up the scenario to study the possible relationships between a human body and a robotic architectural body, throughout a dance of perception and matter.&nbsp;</p><p>This project seeks to contribute a novel perspective on leveraging technology, art, science, and design to provide a setting to enrich and augment the way we relate to the built environment. The objective is to enhance our perception and challenge models of thinking by presenting a post-humanistic phenomenological encounter of the world.</p>", "people": ["nicolelh@media.mit.edu"], "title": "Spaces that Perform Themselves", "modified": "2019-04-17T20:06:12.678Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "spaces-that-perform-themselves-1"}, {"website": "", "description": "<p>Recent advancements in orthopedic implants have made way for a new generation of bionic limbs that attach directly to the skeleton. Leveraging these \"osseointegrated\" implants to pass wires out of the body enables robust, long-term communication with residual muscles and the nervous system. We are exploring the ways in which the improved neural communication afforded by osseointegration can impact the experience of controlling a limb prosthesis.\n                    \n                </p>", "people": ["thhsieh@media.mit.edu", "tonyshu@media.mit.edu", "mcarney@media.mit.edu", "hherr@media.mit.edu", "clites@media.mit.edu", "syeon@media.mit.edu", "lfreed@media.mit.edu"], "title": "An osseointegrated prosthesis with bi-directional neural communication", "modified": "2019-04-26T19:06:19.611Z", "visibility": "PUBLIC", "start_on": "2016-10-19", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "osseo"}, {"website": "", "description": "<h1>Young Learner's Companion&nbsp;</h1><h2>Developing robots' growth mindset and pro-curious behavior and fostering the same in young learners via long-term interaction</h2><p>A growth mindset and curiosity have significant impact on children's academic and social achievements. We are developing and evaluating a novel expressive cognitive-affective architecture that synergistically integrates models of curiosity, understanding of mindsets, and expressive social behaviors to advance the state-of the-art of robot companions. In doing so, we aim to contribute major advancements in the design of AI algorithms for artificial curiosity, artificial mindset, and their verbal and non-verbal expressiveness in a social robot companion for children. In our longitudinal study, we aim to evaluate the robot companion's ability to sustain engagement and promote children's curiosity and growth mindset for improved learning outcomes in an educational play context.<br></p>", "people": ["cynthiab@media.mit.edu", "akostrow@media.mit.edu", "safinah@media.mit.edu", "haewon@media.mit.edu"], "title": "Robot Mindset and Curiosity", "modified": "2018-05-09T04:35:35.760Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "robot-mindset-and-curiosity"}, {"website": "https://www.responsivescience.org/", "description": "<p>Responsive Science is a way of conducting research that invites openness and community involvement from the earliest stages of each project. Real-time interaction between scientists, citizens, and broader communities allows questions and concerns to be identified before experiments are performed, fosters open discussion, and encourages research studies and new technologies to be redesigned in response to societal feedback.</p>", "people": ["devora@media.mit.edu", "ave@media.mit.edu"], "title": "Responsive Science", "modified": "2017-09-07T00:11:35.521Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "responsive-science"}, {"website": "", "description": "", "people": [], "title": "Sequence Tutor", "modified": "2017-03-31T16:27:47.445Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["affective-computing"], "published": false, "active": false, "end_on": null, "slug": "sequence-tutor"}, {"website": "", "description": "<p>Digital machine knitting is a highly programmable manufacturing process that has been utilized to produce apparel, accessories, and footwear.&nbsp;Our research presents three classes of textile sensors exploiting the resistive, piezoresistive, and capacitive&nbsp;properties of various textile structures enabled by machine knitting with conductive yarn.&nbsp;</p>", "people": ["joep@media.mit.edu", "ishii@media.mit.edu", "ddh@media.mit.edu", "danoran@media.mit.edu", "jifei@media.mit.edu"], "title": "SensorKnits: Architecting textile sensors with machine knitting", "modified": "2019-04-09T13:53:02.936Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["tangible-media", "responsive-environments", "synthetic-neurobiology", "hacking-manufacturing"], "published": true, "active": false, "end_on": null, "slug": "sensorknits"}, {"website": "", "description": "<p>NeverMind is an interface and application designed to support human memory. We combine the memory palace memorization method with augmented reality technology to create a tool to help anyone memorize more effectively. Early experiments conducted with a prototype of NeverMind suggest that the long-term memory recall accuracy of sequences of items is nearly tripled compared to paper-based memorization tasks. With this project, we hope to make the memory palace method accessible to novices and demonstrate one way augmented reality can support learning.</p>", "people": ["exposito@media.mit.edu", "rosello@media.mit.edu", "pattie@media.mit.edu"], "title": "NeverMind: Using AR for memorization", "modified": "2019-04-18T17:07:28.479Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "nevermind"}, {"website": "", "description": "<p>The explosion of mHealth in both abundant and resource-constrained countries is both a cause for celebration and for concern.&nbsp;While mHealth clearly has the potential to deliver information and diagnostic decision support to the poorly trained, it is not appropriate to simply translate the technologies which the trained clinician uses into the hands of non-experts. In particular, it is important that the explosion of access does not lead to a flooding of the medical system with low quality data and false negatives. Clearly for mHealth to expand, a paradigm shift in how data is analysed must occur. Data must be vetted at the front end, using automated algorithms, to provide robust filtering of low quality data.</p><p>This project addresses the specific problem of vetting the quality of electrocardiograms (ECGs) collected by an untrained user in ambulatory scenarios using smartphone devices.</p>", "people": ["dlmocdm@media.mit.edu"], "title": "Electrocardiogram collection in noisy ambulatory environments with Android smartphone devices", "modified": "2018-01-18T06:42:50.538Z", "visibility": "PUBLIC", "start_on": "2017-12-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "ecg-ambulatory"}, {"website": "", "description": "<p>We are exploring physiology and nonverbal gestures in real-life contexts, particularly for children with Autism Spectrum Disorders (ASD). Our long-term goal is to enhance understanding and communication by pairing knowledge from wearable devices with context and individualized information. Our focus is to understand longitudinal patterns of these signals in an individual's day-to-day life, and then develop personalized algorithms to interpret signals.</p>", "people": ["jnarain@media.mit.edu", "ktj@media.mit.edu"], "title": "ECHOS: Enhancing Communication using Holistic Observations and Sensing", "modified": "2019-04-17T14:57:48.343Z", "visibility": "PUBLIC", "start_on": "2019-03-16", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "comm-phys-gest"}, {"website": "", "description": "<p>Our voice is an important part of our individuality. From the voices of others, we understand a wealth of non-linguistic information, such as identity, social-cultural clues, and emotional state. But the relationship we have with our own voice is less obvious. We don't hear it the way others do, and our brain treats it differently from any other sound. Yet its sonority is deeply connected with how we are perceived by society and how we see ourselves, body and mind. This project is composed of software, devices, installations, and thoughts used to challenge us to gain new insights on our voices. To increase self-awareness, we propose different ways to extend, project, and visualize the voice. We show how our voices sometimes escape our control, and we explore the consequences in terms of self-reflection, cognitive processes, therapy, affective features visualization, and communication improvement.</p>", "people": ["rebklein@media.mit.edu", "tod@media.mit.edu"], "title": "Using the voice as a tool for self-reflection", "modified": "2019-04-17T20:07:10.276Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "using-the-voice-as-a-tool-for-self-reflection"}, {"website": "", "description": "<p>We present an 8K (7680 x 4320 pixels) visualization system for terabyte-scale, three-dimensional microscopy images of a brain slice that can facilitate neuroscience research. High resolution, large format (85\u201d or 188 cm x 106 cm) rendering allows the viewer to dive into the massive dataset of 700 billion voxels capturing thousands of neurons and to investigate nanoscale and macroscale structures of the neurons simultaneously.</p>", "people": ["esb@media.mit.edu", "bandy@media.mit.edu", "shoh@media.mit.edu", "itot@media.mit.edu", "vmb@media.mit.edu", "kanaya@media.mit.edu"], "title": "8K Brain Tour", "modified": "2019-04-17T18:31:38.208Z", "visibility": "PUBLIC", "start_on": "2017-02-23", "location": "", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": null, "slug": "8k-brain-tour"}, {"website": "", "description": "<p>The problem of ethical decision making presents&nbsp; a grand challenge for modern AI research. Arguably the main obstacle to automating ethical decisions is the lack of a formal specification of ground-truth ethical principles, which have been the subject of debate for centuries among philosophers (e.g., trolley problem).&nbsp;We present an algorithm to automate ethical decisions; using machine learning and computational social choice (new theory of&nbsp;swap-dominance efficient voting rules), we propose to learn a model of societal preferences, and, when faced with a specific ethical dilemma at runtime, efficiently aggregate those preferences to identify a desirable choice. Finally, we implement and evaluate a system for ethical decision making in the autonomous vehicle domain, using preference data collected from 1.3 million voters through the Moral Machine website.&nbsp;Our proof of concept shows that the decision the system takes is likely to be the same as if we could go to each of the 1.3 million voters, ask for their opinions, and then aggregate their opinions into a choice that satisfies mathematical notions of social justice.&nbsp;</p>", "people": ["awad@media.mit.edu", "gaikwad@media.mit.edu", "dsouza@media.mit.edu", "irahwan@media.mit.edu"], "title": "A voting-based system for ethical decision making", "modified": "2019-04-19T17:41:51.688Z", "visibility": "PUBLIC", "start_on": "2017-01-20", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "a-voting-based-system-for-ethical-decision-making"}, {"website": "", "description": "<p>We share the vision of cities as places where people live and work, connected by better mobility systems that are sustainable and enable a high quality of living. As a result of rapid urbanization, many existing transportation systems and particularly car depended ones can neither provide sufficient capacity nor help reaching that vision of the future city. Besides a rethinking in urban planning and public investments in urban transport systems, changes in the citizens\u2019 mobility choices are necessary to realize a vision of a high quality of life and sustainable city.</p><p>The project goal of Persuasive Urban Mobility is to examine how persuasive technologies can be utilized to shift the mobility behavior of citizens. We are particularly interested in studying the key persuasive strategies to enable, motivate and trigger users to shift from high energy to low energy modes of transportation, with the current research focusing on promoting cycling as an alternative to single passenger car use.</p><p><span style=\"font-size: 18px; font-weight: 400;\">This project is a research collaboration of AIT Austrian Institute of Technology and Changing Places at MIT Media Lab</span><br></p><p>by Agnis Stibe, Matthias Wunsch, Alexandra Millonig, Chengzhen Dai, Stefan Seer, Katja Schechtner, Ryan C.C. Chin, and Kent Larson.</p><p>Former affiliates: Felipe Lozano-Landinez, Francesco Pilla</p>", "people": ["kll@media.mit.edu", "rchin@media.mit.edu", "agnis@media.mit.edu", "katjas@media.mit.edu"], "title": "Persuasive Urban Mobility", "modified": "2017-08-11T17:26:57.370Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": null, "slug": "persuasive-urban-mobility"}, {"website": "", "description": "", "people": [], "title": "ActSim", "modified": "2019-04-17T20:09:05.591Z", "visibility": "LAB-INSIDERS", "start_on": null, "location": "", "groups": ["tangible-media"], "published": false, "active": false, "end_on": null, "slug": "actsim"}, {"website": "", "description": "<p>The Internet has unleashed the capacity for planetary-scale collective problem solving (also known as crowdsourcing). However, the very openness of crowdsourcing makes it vulnerable to sabotage by rogue or competitive actors. To explore the effect of errors and sabotage on the performance of crowdsourcing, we analyze data from the DARPA Shredder Challenge, a prize competition for exploring methods to reconstruct documents shredded by a variety of paper shredding techniques.</p>", "people": ["irahwan@media.mit.edu"], "title": "DARPA Shredder Challenge: Crowdsourcing under attack", "modified": "2019-04-19T17:43:17.995Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "crowdsourcing-under-attack"}, {"website": "", "description": "<p>Room-scale virtual reality opens up exciting new possibilities for exploratory learning. Phenomena that otherwise cannot be experienced directly (e.g. subjects that are microscopic, remote, or dangerous) can be transformed into environments that are immersive, interactive and social. Electrostatic Playground is a VR physics lab where multiple users can explore and discover principles of electrostatics through experimentation.&nbsp;<span style=\"font-size: 18px;\">It also concretizes abstract notions of electrostatics in the form of tangible, interactive objects. Users can learn by directly manipulating physics objects while receiving real-time feedback from the environment. We've incorporated the ability to record these interactions in order to provide a means of authoring content, reviewing one's notes, and teaching others. Electrostatic Playground is a multi-user lab where users can explore and discover principles in electrostatics.</span><br></p>", "people": ["swgreen@media.mit.edu", "erinhong@media.mit.edu", "wjlc@media.mit.edu", "hbedri@media.mit.edu"], "title": "Electrostatic Playground: A multi-user virtual reality physics learning experience", "modified": "2019-04-17T20:09:56.460Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "vr-physics-lab"}, {"website": "", "description": "<p>Changing Places researchers  are developing scalable strategies for creating hyper-efficient, technology-enabled spaces that can help make living more affordable, productive, enjoyable, and creative for urban dwellers.<br></p>", "people": ["hlarrea@media.mit.edu", "oarino@media.mit.edu", "alonsolp@media.mit.edu", "kll@media.mit.edu", "maitanei@media.mit.edu", "aizpurua@media.mit.edu", "mkh@media.mit.edu"], "title": "Theme | Changing Places", "modified": "2019-05-24T21:06:01.300Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "places"}, {"website": "", "description": "<p><i>M\u0101ori</i>&nbsp;have a long and deep connection to their island and ocean ecosystem. The <i>M\u0101ori</i> concept of <i>Rahui</i> focuses on traditional methods of ocean protection that long predate marine protected areas.&nbsp;Ocean Cultures hopes to support the younger generation of islanders to understand and monitor their own ocean surroundings at a time when it is critical. &nbsp;</p><p>We hope to do this through a dual set of tools, monitoring through science as well as culture. &nbsp;Monitoring through science will consist of educational hands-on workshops teaching participants fundamental concepts within the marine microbiome,  the invisible but critical foundation of the ocean\u2019s ecosystem, which governs the health, biodiversity, and innumerable processes that occur on our planet. Additional workshops on low cost sensors and remotely operated vehicles (ROVs)&nbsp; may be explored as well. To monitor through culture, we hope to collaborate with <i>M\u0101ori</i>&nbsp;<i>kaum\u0101tua</i>&nbsp;to teach participants traditional and cultural knowledge about their ocean ecosystems and how best to preserve that though generations.</p>", "people": ["devora@media.mit.edu", "ave@media.mit.edu"], "title": "Ocean Cultures", "modified": "2019-01-25T04:49:18.572Z", "visibility": "PUBLIC", "start_on": "2018-04-02", "location": "", "groups": ["open-ocean"], "published": true, "active": false, "end_on": null, "slug": "oceancultures"}, {"website": "", "description": "<h2>Idea</h2><p>A lab is where unique ideas are generated and early prototypes are synthesized. A factory is where designs are mass produced and quality is validated. They seem so far away from each other, that we usually separate research from reality, prototype from production. How can we demolish this separation for a future where design, technology and production are tightly coupled together? In this project, we are exploring the idea of \"researcher in residency\" in the context of manufacturing. As the city of manufacturing, Shenzhen has the most&nbsp;concentrated&nbsp;factories across industry in the world. By bringing researchers to the factory floor for 4 weeks, we are experimenting methods to fuse the lab and factory as a future venue for innovation.</p><h2>Projects Outcome</h2><p>Towards the end of the workshop, 8 projects were generated among 10 researchers. Three projects involved collaborations between 2 or 3 people, and the rest were individual projects. They range from art to sensors, from textiles to robotics.</p><p><b>Jie Qi, Donald Derek - Sound Spirals</b></p><p>This project explored blending natural materials and flexible printed circuit boards to create working speaker elements. &nbsp;We experimented with a number of unusual PCB composite materials including knitted textile, paper and pressed leaves. &nbsp;We created speaker coils by etching spiral shapes into flexible PCB, following Perner-Wilson's example handcrafted speakers, and laminated these directly onto the experimental materials using the industrial heat press. </p><p><b>Artem Dementyev - Circuit Robotics</b></p><p>The main idea of this project was to create actuators and sensors using only flexible circuit board technology, rather than separating the electrical components from the mechanical and structural components. Using the coverlay lamination process we were able to embed channels between the copper layer and the coverlay. The channels were used as air pockets to create pneumatic actuators. With a different channel geometry, we were able to embed shape memory alloy wires into the flexible printed circuit board to create an alternative actuator.</p><p><b>Laya Anasu - Self-Disassembly Knit</b></p><p>In a knitting process, waste can be produced by the automatically generated supporting roles of knit. This project asks a question: if we knit many small pieces, how can we reduce the waste? The student utilizes a yarn that dissolves in water to stitch all small pieces together into one big knit. The result is a large knitted fabric that can disassemble into many designed, smaller pieces.</p><p><b>Ani Liu - Psyche in the Age of Mechanical Reproduction</b></p><p>This is an art project that explores the interplay of factory workers, knitted fabric and data visualization. The student took the EEG data from a factory worker, translated the measured stress level to the program parameter, which controls the tension of a knit. The result is the individual knitted carpet that reflects the emotional signature of a factory worker.</p><p><b>Amos Golan - Knitting with Unusual Yarns</b></p><p>Usually a digital knitting machine takes soft yarn below 1400 denier. This project developed an auxiliary device that can be attached to the machine to control the pulling and feeding force of a yarn. By doing so, we can now knit much harder materials such as copper wire, shape memory alloy, etc. The student produced material samples that smoothly inlays a shape memory alloy inside, to create shape-change textile. </p><p><b>Miguel Perez - Structural </b><b>Textile</b></p><p>The project explores the possibility of integrating plastic filament into the knitting process to create fabric that has tunable stiffness. The result is a new process that involves knitting, heat press and forming. The student also created material samples that are produced with this process. </p><p><b>Guillermo Bernal - Neuroknit</b></p><p>This is a design exploration of how to create a three dimensional VR helmet with knitting process. The helmet contains also small pocket inside to hold a electro circuit that was developed in the flexible PCB factory. The result is a interplay between designing the shape of the Flex PCB and fine tuning the shape of the knitted helmet.</p><p><b>Jifei Ou,&nbsp;</b><b>Daniel Oran, and Donald Derek - Sensitive Textile</b><b>s</b></p><p>This project explores the design space of making textile-based sensor at the stitch level. The factory has created hundreds of knitting patterns that are mostly used for visual aesthetics. The students looked at their microstructure and developed stretch sensor and potentiometer by incorporating conductive yarn. The result is a series of production-ready textile sensors. Those sensors are connected to a control board that was developed in the flexible PCB factory.</p><h2>Participants</h2><p>Jifei Ou | Instructor | Tangible Media</p><p>Artem Dementyev | Instructor | Responsive Environments</p><p>Jie Qi | Instructor | Lifelong Kindergarten</p><p>Amos Golan | Student | Tangible Media</p><p>Ani Liu | Student | Design Fiction</p><p>Daniel Oran | Student | Synthetic Neurobiology</p><p>Donald Derek | Student | Responsive Environments</p><p>Guillermo Bernal | Student | Fluid Interfaces</p><p>Laya Anasu | Student | City Science</p><p>Miguel Perez | Student | Playful Systems</p>", "people": ["artemd@media.mit.edu", "gbernal@media.mit.edu", "amosg@media.mit.edu", "layanasu@media.mit.edu", "ddh@media.mit.edu", "mperez4@media.mit.edu", "jieqi@media.mit.edu", "danoran@media.mit.edu", "jifei@media.mit.edu", "wonder@media.mit.edu"], "title": "Hacking Manufacturing 2017", "modified": "2019-03-25T15:20:47.166Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "hacking-manufacturing-2017"}, {"website": "", "description": "<p>Every day, young people around the world use the Scratch programming language to create and share thousands of interactive projects on the <a href=\"https://scratch.mit.edu/\">Scratch website</a>. Yet many students aren\u2019t sure how to get started coding their own projects.  </p><p>To address this, we have launched a new set of free resources to help students learn to create with code. The <a href=\"https://scratch.mit.edu/go\">Things to Try</a> page offers a variety of project ideas, such as creating an animated story, making a pong game, or designing a virtual pet. For each theme, students can use step-by-step tutorials or printable activity cards. In addition, the site offers educator guides you can use to organize a class or workshop based on the theme.</p><p>The <a href=\"https://scratch.mit.edu/info/cards/\">Scratch Activity Cards</a> is a collection of more than 80 colorful cards with 11 project themes. The front of each card illustrates an activity students can do with Scratch, such as animating a character or keeping score in a game. The back of the card shows how to snap together blocks of code to make their projects come to life.  </p><p>These resources are designed to let students learn at their own pace and personalize their projects. Students can work individually or pair up to make projects together.&nbsp;</p>", "people": ["shrutid@media.mit.edu", "nrusk@media.mit.edu"], "title": "Getting Started with Scratch", "modified": "2017-04-19T17:58:02.568Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "getting-started-with-scratch"}, {"website": "", "description": "<p>2D screens, even stereoscopic ones, limit our ability to interact with and collaborate on 3D data. We believe that an augmented reality solution, where 3D data is seamlessly integrated in the real world, is promising. We are exploring a collaborative augmented reality system for visualizing and manipulating 3D data using a head-mounted, see-through display, that allows for communication and data manipulation using simple hand gestures.</p>", "people": ["kevinw@media.mit.edu", "pattie@media.mit.edu"], "title": "HandsOn: A gestural system for remote collaboration using augmented reality", "modified": "2019-04-17T20:10:59.187Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["terrestrial-sensing", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "handson-a-gestural-system-for-remote-collaboration-using-augmented-reality"}, {"website": "", "description": "<p>Conversations between two individuals\u2014whether between doctor and patient, mental health therapist and client, or between two people romantically involved with each other\u2014are complex. Each participant contributes to the conversation using her or his own \"lens.\" This project involves advanced probabilistic graphical models to statistically extract and model these dual lenses across large datasets of real-world conversations, with applications that can improve crisis and psychotherapy counseling and patient-cardiologist consultations. We're working with top psychologists, cardiologists, and crisis counseling centers in the United States.</p>", "people": ["picard@media.mit.edu", "kdinakar@media.mit.edu"], "title": "Lensing: Cardiolinguistics for Atypical Angina", "modified": "2019-04-18T03:29:15.605Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "ethics-and-governance"], "published": true, "active": false, "end_on": null, "slug": "lensing-cardiolinguistics-for-atypical-angina"}, {"website": "http://ira.mit.edu/blog/gui3d", "description": "<p><a href=\"https://github.com/irawinder/GUI3D/\">GUI3D</a> Template is a generic implementation of the GUI components that anyone might want for a 3D simulation in Processing 3.&nbsp;Components include navigation, zooming,&nbsp;rotation, control sliders, radio buttons, and the means to select objects in 3D space with a mouse.&nbsp;This template can speed up implementation of 3D simulations in Processing.</p><p>More information:&nbsp;<a href=\"https://ira.mit.edu/blog/gui3d\">https://ira.mit.edu/blog/gui3d</a></p><p>Jump directly to the GitHub Repository:&nbsp;<a href=\"https://github.com/irawinder/GUI3D\">https://github.com/irawinder/GUI3D</a></p>", "people": ["jiw@media.mit.edu"], "title": "GUI3D", "modified": "2018-04-27T14:47:52.452Z", "visibility": "PUBLIC", "start_on": "2017-12-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "gui3d"}, {"website": "", "description": "<p>Cooperation in a large society of self-interested individuals is notoriously difficult to achieve when the externality of one individual's action is spread thin and wide on the whole society (e.g., in the case of pollution). We introduce a new approach to achieving global cooperation by localizing externalities to one's peers in a social network, thus leveraging the power of peer-pressure to regulate behavior. Global cooperation becomes more like local cooperation.</p>", "people": ["irahwan@media.mit.edu"], "title": "Promoting cooperation through peer pressure", "modified": "2019-04-19T17:44:53.911Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "promoting-cooperation-through-peer-pressure"}, {"website": "", "description": "<p>Although there have been great advances in the control of lower extremity prostheses, transitioning between terrains such as ramps or stairs remains a major challenge for the field. The mobility of leg amputees is thus limited, impacting their quality of life and independence. This projects aims to solve this problem by designing, implementing, and integrating a combined terrain-adaptive and volitional controller for powered lower limb prostheses. The controller will be able to predict terrain changes using data from both intrinsic sensors and electromyography (EMG) signals from the user; adapt the ankle position before footfall in a biologically accurate manner; and provide a torque profile consistent with biological ankle kinetics during stance. The result will allow amputees to traverse and transition among flat ground, stairs, and slopes of varying grade with lower energy and pain, greater balance, and without manually changing the walking mode of their prosthesis.</p>", "people": ["romka@media.mit.edu", "hherr@media.mit.edu"], "title": "Terrain-Adaptive Lower Limb Prosthesis", "modified": "2019-04-26T19:07:36.576Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "terrain-adaptive-lower-limb-prosthesis"}, {"website": "", "description": "<p>We have developed a new process to screen patients at the point-of-care with FDA-approved technology-enabled mobile health screenings (TES) and compare the results with routine health screenings. A study of nearly 500 patients was conducted to test the effectiveness of this new screening process. This is one of the first studies to investigate using TES to augment routine health examinations. We recommend using TES in synergy with routine health screenings to identify missing sick patients who might otherwise lack comprehensive primary care.<br></p><p><strong>Why is this work important?</strong></p><p>Providing good healthcare in low- and middle-income countries (LMIC) paradoxically requires expensive equipment,&nbsp;<span style=\"font-family: &quot;Neue Haas Grotesk Display&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 18px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400;\">which may not be easily available because of resource limitations,</span><span style=\"font-size: 18px; font-weight: 400;\">&nbsp;for health monitoring and assessment. There is high variation in the degrees of healthcare access in LMIC, but such access is important because cardiovascular diseases, preventable blindness, oral cancer, and treatable neurological conditions constitute more than half of the disease burden in LMIC. Comprehensive TES may allow for more patients to be screened for more conditions in resource-limited settings, improving their access to primary healthcare. A lack of consensus exists about the usefulness of TES in augmenting primary health screenings in LMIC.</span></p><p><strong>What has been done before?</strong><br></p><p>Devices that allow TES have typically been evaluated in isolated silos, concentrating on individual devices or specific anatomical sites. They have additionally not been comprehensively evaluated alongside routine health screenings.</p><p><strong>What are our contributions?</strong><br></p><p>This is one of the first studies to investigate using multiple TES to augment routine health examinations. To facilitate this large-scale study, we developed and successfully used web examination platforms that enabled multiple physicians to diagnose health conditions remotely. We identified patients who would not have received the care they need in the absence of TES, and link TES to primary health outcomes.</p><p>This study led to significant insights about strategies to develop technologies at MIT that are ready for deployment for effective and scalable primary care in the real world.</p><p><strong>What technology-enabled examinations were performed?</strong></p><p>Single-lead ECG: AliveCor Mobile ECG</p><p>Blood oxygen saturation: Contec Medical Systems 50-DL Pulse Oximeter</p><p>Oral imaging: ACTEON Soprocare</p><p>Retinal scan: D-EYE direct ophthalmoscopy adapter attached to iPhone5s camera</p><p>Tympanic membrane imaging: CellScope Oto with iPhone5 LEDs and camera</p><p>Neurological examinations: Microsoft Kinect</p><p><strong>What are the next steps?</strong></p><p>We are actively working on automated diagnoses, analyses of disease co-occurrence, and patient risk stratification.</p><p>Future studies that build on our technology-enabled screening process can evaluate the process for larger numbers of patients. A future longitudinal study may allow for additional insights into time-varying conditions.</p><p><strong>Related projects</strong></p><ol><li><a href=\"https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/\">Machine Learning and Automated Segmentation of Oral Diseases using Biomarker Images</a></li></ol>", "people": [" otkrist@media.mit.edu", "pratiks@media.mit.edu", "gyauney@media.mit.edu", "mrinal@media.mit.edu"], "title": "Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care", "modified": "2018-10-21T18:32:18.676Z", "visibility": "PUBLIC", "start_on": "2015-05-15", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "technology-enabled-mobile-phone-screenings-augment-routine-primary-care"}, {"website": "", "description": "<p>Participie was a design experiment on direct participation for constrained choices (like budgets).</p>", "people": ["sanjayg@media.mit.edu"], "title": "Participie", "modified": "2018-05-03T20:08:11.478Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "participie-new"}, {"website": "", "description": "<p>Early in the opera \"Death and the Powers,\" the main character, Simon Powers, is subsumed into a technological environment of his own creation. The set comes alive through robotic, visual, and sonic elements that allow the actor to extend his range and influence across the stage in unique and dynamic ways. This environment assumes the behavior and expression of the absent Simon; to distill the essence of this character, we recover performance parameters in real time from physiological sensors, voice, and vision systems. Gesture and performance parameters are then mapped to a visual language that allows the off-stage actor to express emotion and interact with others on stage. To accomplish this, we developed a suite of innovative analysis, mapping, and rendering software systems.</p>", "people": ["ejessop@media.mit.edu", "patorpey@media.mit.edu", "tod@media.mit.edu"], "title": "Disembodied Performance", "modified": "2017-03-31T20:27:08.548Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "Swatch Lab", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "disembodied-performance"}, {"website": "", "description": "<p>We introduce Rovables, a miniature robot that can move freely on unmodified clothing. The robots are held in place by magnetic wheels, and can climb vertically. The robots are untethered and have an onboard battery, microcontroller, and wireless communications. They also contain a low-power localization system that uses wheel encoders and IMU, allowing Rovables to perform limited autonomous navigation on the body. In the technical evaluations, we found that Rovables can operate continuously for 45 minutes and can carry up to 1.5N. We propose an interaction space for mobile on-body devices spanning sensing, actuation, and interfaces, and develop application scenarios in that space. Our applications include on-body sensing, modular displays, tactile feedback and interactive clothing and jewelry.\n                    \n                </p>", "people": ["artemd@media.mit.edu", "dmajilo@media.mit.edu", "joep@media.mit.edu", "cindykao@media.mit.edu", "geek@media.mit.edu"], "title": "Rovables", "modified": "2017-03-03T18:38:10.500Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["living-mobile", "responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "rovables"}, {"website": "", "description": "<p>We are currently developing novel DNA editing technologies to broaden the scope of genome engineering. Our strategy is based on identifying and engineering endonucleases from diverse living systems, along with targeting with synthetic molecules. Together these components confer greater stability, minimize off-target DNA cleavage, and eliminate sequence restrictions for precision genetic manipulations within cells.</p>", "people": ["jacobson@media.mit.edu", "pranam@media.mit.edu", "njakimo@media.mit.edu"], "title": "Synthetic Genome Engineering", "modified": "2017-09-23T06:18:07.043Z", "visibility": "PUBLIC", "start_on": "2016-06-06", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "synthetic-genome-engineering"}, {"website": "", "description": "<p>We have designed a novel system to promote kindness and gratitude. We leverage pervasive technologies to naturally embed gratitude inspiration in everyday life. Mobile sensor data is utilized to infer optimal moments for stimulating contextually relevant thankfulness and appreciation. We analyze the interplay between mood, contextual cues, and gratitude expressions.</p>", "people": ["asma_gh@media.mit.edu", "picard@media.mit.edu", "sataylor@media.mit.edu", "azaria@media.mit.edu"], "title": "\"Kind and Grateful\": Promoting kindness and gratitude with pervasive technology", "modified": "2018-10-22T19:48:44.018Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "kind-and-grateful-promoting-kindness-and-gratitude-with-pervasive-technology"}, {"website": "", "description": "<p>Concerned about your privacy online? Worried with whom and how photos of you and/or your family might be shared on social media? You probably should be. Revelations about social networks (like Facebook) sharing your personal data with unethical actors (like Cambridge Analytica) are a major cause for concern. According to a recent Pew research study, 91 percent of Americans worry that social networks might misuse or resell their sensitive personal data.</p><br><p>That's why we built RockStar, a fully-featured social network in which everyone\u2019s identity is correlated to a rock. RockStar most literally ensures rock solid privacy. </p><p><a href=\"http://rockstar.cmprod.media.mit.edu\">Get My RockStar right now!!!</a></p><br><p>Because humans are excellent at recognizing patterns, we can see faces in non-human forms\u2014 something that machine learning algorithms have difficulty replicating. As such, our most secure Rockelgangers come from samples from inorganic material\u2014a collection of rocks at The Japanese Museum of Rocks that <a href=\"http://www006.upp.so-net.ne.jp/chinseki/index-ex.html\">Look like Faces</a>.&nbsp;We captured 1,500 unique rock face images that we estimate resemble 85 percent of the human population. Using the innovative Nimoy-Brenner Algorithm developed at MIT Center for Civic Media, we match your face to that of a celebrity and from that to one of these 1,500 rocks. By cloaking your identity via your Rockelganger, you can use social media freely without worry that your or your family\u2019s data is being resold or compromised.</p><br><p>Worried that you might be part of the 15 percent whose face doesn't map neatly to a rock? Using cutting-edge generative adversarial neural network <a href=\"https://junyanz.github.io/CycleGAN/\">techniques</a>,&nbsp;we can generate a custom Rockelganger just for you. These rocks look recognizably like you, but are sufficiently inorganic, enough to fool even the most advanced facial recognition systems.</p><br><p>ADDITIONAL BENEFITS!!!</p><p><i>Escape your echo chamber!</i></p><p>Enhanced privacy is not the only benefit of RockStar. We know that social networks can lead toward echo chambers of information where you hear only from your friends and people that you spend time with offline. RockStar offers a whole new way to make friends and meet people online.</p><br><p><i>Make new friends!</i></p><p>With only 1,500 default rocks, it's almost certain that you have the same Rockelganger as someone else online. Why not meet that person and find out what interests you share in common?</p><br><p><i>Learn about geology!</i> </p><p>Meet &nbsp;RockStar members who share your same type of rock. Is your rock face an igneous rock? Meet other igneous rocks or branch out and search for sedimentary or metamorphic rocks. </p><br><p>Rockstar\u2014smashing privacy violations and the filter bubble, one rock at a time.</p>", "people": ["csbishop@media.mit.edu"], "title": "RockStar-ai", "modified": "2019-05-20T14:17:56.011Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["center-for-civic-media"], "published": true, "active": false, "end_on": null, "slug": "rock-and-roll-spirit"}, {"website": "http://flipfeed.media.mit.edu", "description": "<p class=\"\"><a href=\"https://chrome.google.com/webstore/detail/flipfeed/glfjakcglibkihmaaekjcaefcbebgcfg?hl=en-US&amp;gl=US\">FlipFeed is a Google Chrome Extension</a> that enables Twitter users to replace their own feed with that of another real Twitter user. Powered by deep learning and social network analysis, feeds are selected based on inferred political ideology (\"left\" or \"right\") and served to users of the extension. For example, a right-leaning user who uses FlipFeed may load and navigate a left-leaning user's feed to observe the news stories, commentary, and other content they consume. The user can then decide to flip back to their own feed or repeat the process with another feed. We hope tools like FlipFeed will enable us to explore how social media platforms can be used to mitigate, rather than exacerbate, ideological polarization by helping people explore and empathize with different perspectives.<br></p>", "people": ["dkroy@media.mit.edu", "msaveski@media.mit.edu", "annyuan@media.mit.edu", "ngillani@media.mit.edu", "pralav@media.mit.edu"], "title": "FlipFeed", "modified": "2017-03-08T16:10:50.709Z", "visibility": "PUBLIC", "start_on": "2016-11-11", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "flipfeed"}, {"website": "", "description": "<p>Today, the environments that humans occupy in space are designed for survival. Humans are carefully shuttled to and from space, and during their relatively short stays, they are provided with minimum supplies to remain alive and able to perform experiments. As we begin to plan less for short visits and more for life in space (such as a six to eight month trip to Mars and beyond) the question becomes: What does human culture look like in space?</p><p><a href=\"https://www.instagram.com/nico_lh/\">Nicole L'Huillier</a> and <a href=\"https://www.instagram.com/sandsfish\">Sands Fish</a> decided to explore how design and creativity might evolve as we begin to do more than merely survive in space. <b>The Telemetron</b> is a unique mode of musical performance that takes advantage of the poetics of zero gravity, and opens a new field of musical creativity. The project attempts to expand expression beyond the limits of earth-based instruments and performers. Leveraging sensors, data transmission and capture (for performance after flight), as well as their experience as composers and performers, Sands and Nicole explore a new body language for music. </p><p>The Telemetron was played for the first time during the inaugural Media Lab Space Exploration Initiative's Zero G flight. This instrument is a clear dodecahedron chamber that contains customized \"chimes\" containing gyroscopes. The chimes emit their telemetry as they spin and collide. Sensors record the position, direction, and spin of each chime. These elements create the composition. The performers play the instrument by moving it in space, shaking it, colliding it. The performance can be recorded to be experienced on earth or used as a live instrument during future space flights. The instrument can be played inside space craft or in the vacuum of space without the benefit of sound waves.</p><p>Recorded as a beautiful audio-visual experience, this experiment opens the doors for new forms of creative expression, and brings the magic of space to musicians. We hope to reach beyond the utilitarian, and toward the inspiring.</p>", "people": ["sands@media.mit.edu", "thomassl@media.mit.edu", "nicolelh@media.mit.edu"], "title": "The Telemetron", "modified": "2018-08-20T20:13:42.164Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["opera-of-the-future", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "the-telemetron"}, {"website": "", "description": "<p>This project will support social scientists, philosophers, and policy and legal scholars who undertake research that aims to impact how artificial intelligence technologies are designed, implemented, understood, and held accountable. It will also provide a platform to create, convene, and support a diverse and powerful network of people and institutions who are working to steer AI in ethically conscious directions, both in fields of specialized AI as well as general AI. &nbsp;The project will investigate the social implications of the maturation and proliferation of AI. It will help catalyze and support research that advances AI in the public interest and fund engineers who want to help define public interest in AI through the code they write and machines they build. </p><p>\tThe initiative also organized a high-level symposium at the Media Lab on the topic that took place in April 2016 between the academic community and industry leaders working on AI.</p>", "people": ["tenzin@media.mit.edu", "irahwan@media.mit.edu"], "title": "AI Ethics and Governance", "modified": "2018-02-21T21:42:35.499Z", "visibility": "PUBLIC", "start_on": "2016-04-01", "location": "", "groups": ["ethics", "scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "ai-ethics-and-governance"}, {"website": "", "description": "<p>Eye exams via a slit lamp are critical in early diagnosis of diseases such as cataracts, corneal injury, and pterygia, in order to avert vision loss. The slit lamp is one of the most versatile tools in an ophthalmologist's clinic, but is big, expensive, and is designed with specialized ophthalmic clinics in mind. AnEye is a suite of portable, computationally driven solutions that leverage modern optics and commercially available consumer electronics to extend the reach of examinations of the anterior segment of the eye well beyond large hospitals and clinics, into resource-constrained settings such as rural mass-screening camps, mobile ophthalmology clinics, and even primary care.</p>", "people": ["raskar@media.mit.edu", "sssinha@media.mit.edu"], "title": "AnEye: Extending the reach of anterior segment ophthalmic imaging", "modified": "2019-04-19T17:46:56.865Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["emerging-worlds", "camera-culture"], "published": true, "active": false, "end_on": null, "slug": "aneye-extending-the-reach-of-anterior-segment-ophthalmic-imaging"}, {"website": "", "description": "<p><a href=\"http://dl.acm.org/citation.cfm?id=2984779&amp;CFID=856493995&amp;CFTOKEN=37757229\">Multiplayer virtual reality</a> games introduce the problem of variations in the physical size and shape of each user\u2019s space for mapping into a shared virtual space. We designed an asymmetric approach to solve the spatial variation problem, by allowing people to choose roles based on the size of their space. We demonstrate this design through the implementation of a virtual snowball fight where players can choose from multiple roles, namely the shooter, the target, or an onlooker depending on whether the game is played remotely or together in one large space. In the co-located version, the target stands behind an actuated cardboard fort that responds to events in VR, providing non-VR spectators a way to participate in the experience.\n                    \n                </p>", "people": ["sra@media.mit.edu", "andresc@media.mit.edu"], "title": "SnowballVR", "modified": "2017-04-20T08:59:00.349Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "snowballvr"}, {"website": "", "description": "<p>Mathematical experiences are intrinsic to our everyday lives, yet mathematics education is mostly confined to textbooks. Seymour Papert used the term \"Mathland\" to propose a world where one would learn mathematics as naturally as one learns French while growing up in France. We built a mixed reality application that augments the physical world with interactive mathematical concepts and annotations to create a real-life Mathland. Using Mathland, people can collaboratively explore, experience, and experiment with mathematical phenomena in their real, physical environments using tangible objects. Mathland opens up new opportunities for mathematical learning using Papert's constructionist principles in an immersive environment that affords situated learning, embodied interaction and playful constructionism.</p>", "people": ["trujano@media.mit.edu", "ashris@media.mit.edu", "jsirera@media.mit.edu", "minakhan@media.mit.edu"], "title": "Mathland: Play with math in mixed reality", "modified": "2019-04-17T20:11:43.769Z", "visibility": "PUBLIC", "start_on": "2017-03-08", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "mathland"}, {"website": "http://deep-resenv.media.mit.edu:8080", "description": "<p>Tid'Zam is an ambient sound analysis system for outdoor environments. It is a component of the Tidmarsh Farms project which monitors the environmental evolution of an industrial cranberry farm during its ecological restoration of wetland. Tid'Zam analyzes the audio streams generated by the deployed microphones in the wild in order to detect the sonic events happening on the site, such as bird calls, insects, frogs, rain, storms, car noise, human voices, and more.&nbsp;</p><p>This system is used to cross-validate other sensors for weather monitoring to identify, geolocalize, and track present wildlife and bird specimens over time. It also controls the audio mixers in order to mute or change the gain on noisy microphones.</p>", "people": ["gershon@media.mit.edu", "joep@media.mit.edu", "ddh@media.mit.edu", "sfr@media.mit.edu", "duhart@media.mit.edu"], "title": "Tid'Zam", "modified": "2017-10-02T15:14:53.312Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "tidzam"}, {"website": "", "description": "<p>Fensadense is a new work for 10-piece ensemble composed by Tod Machover, commissioned for the Lucerne Festival in summer 2015. The project represents the next generation of hyperinstruments, involving the measurement of relative qualities of many performers where previous systems only looked at a single performer. Off-the-shelf components were used to collect data about movement and muscle tension of each musician. The data was analyzed using the Hyperproduction platform to create meaningful production control for lighting and sound systems based on the connection of the performers, with a focus on qualities such as momentum, connection, and tension of the ensemble as a whole. The project premiered at the Lucerne Festival, and a spring European tour just concluded this May 2016.</p><p class=\"\"><a style=\"font-size: 18px; font-weight: normal;\" href=\"http://garrettparrish.com/about/fensadense/\">Fensadense site</a><span style=\"font-size: 18px; font-weight: normal;\"> created by our former UROPer, Garrett Parrish.</span><br></p><p class=\"\">Listen to a complete recording of the Lucerne performance <a href=\"http://www.wqxr.org/#!/story/listen-tod-machovers-fensadense-hyperinstruments-and-interactive-electronics/\">here</a>.<br></p>", "people": ["patorpey@media.mit.edu", "tod@media.mit.edu", "benb@media.mit.edu"], "title": "Fensadense", "modified": "2017-04-03T19:36:35.564Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "fensadense"}, {"website": "", "description": "<p>This project aims to enable fast prototyping of a multi-axis and multi-joint active prosthesis by developing a new modular electronics system. This system provides the required hardware and software to do precise motion control, data acquisition, and networking. Scalability is achieved through the use of a fast industrial communication protocol between the modules, and by a standardization of the peripherals' interfaces: it is possible to add functionalities to the system simply by plugging in additional cards. Hardware and software encapsulation are used to provide high-performance, real-time control of the actuators, while keeping the high-level algorithmic development and prototyping simple, fast, and easy.</p>", "people": ["jfduval@media.mit.edu", "hherr@media.mit.edu"], "title": "FlexSEA: Flexible, scalable electronics architecture for wearable robotics applications", "modified": "2019-04-26T19:08:03.939Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "flexsea-flexible-scalable-electronics-architecture-for-wearable-robotics-applications"}, {"website": "http://www.jinjoolee.com", "description": "<p>Tega is a new robot platform designed to support long-term, in-home interactions with children, with applications in early-literacy education from vocabulary to storytelling.<br></p>", "people": ["cynthiab@media.mit.edu", "jinjoo@media.mit.edu", "lukulele@media.mit.edu", "sooyeon6@media.mit.edu", "haewon@media.mit.edu"], "title": "Tega: A New Social Robot Platform", "modified": "2018-02-02T12:45:47.861Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "tega-a-new-robot-platform-for-long-term-interaction"}, {"website": "http://termites.synthetic.space/", "description": "<p>TerMITes are wireless environmental sensors that capture data to help us better understand our environments and human behavior. The sensor data is time-stamped and place-tagged, but otherwise hardware agnostic. TerMITes support multi-modal sensor attachments using common protocols and can be attached to objects in the home such as doors, windows, drawers, cabinets, tables, and chairs to register object usage. TerMITes directly log on to the Internet via low-power Wi-Fi for ease of connection and automatically upload&nbsp;to a centralized database. TerMITes bridge existing methods for qualitative inquiry about our experiences in various planes to quantitative recording based on sensor input. TerMITes are currently used to gather data on humidity, presence detection, ambient light, motion, carbon dioxide, and temperature.&nbsp;</p>", "people": ["cassiano@media.mit.edu", "csmuts@media.mit.edu"], "title": "TerMITes", "modified": "2019-04-08T16:56:32.037Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "termites"}, {"website": "", "description": "<p>The world uses an estimated 20 million mice in laboratory research experiments each year. These experiments are monitored and regulated to protect animal welfare whenever possible. However, analgesics cannot completely eliminate suffering, and many studies cannot use opiates or anti-inflammatory drugs because they would interfere with the biological process being studied. The benefits of animal research may outweigh the cost in animal suffering, but it would be better to perform these experiments without animal suffering. This project seeks to develop strains of mice that experience far less pain and suffering than current animals, but that are equally suited to laboratory and medical research. If successful, widespread adoption of these mice could drastically reduce animal suffering in laboratories worldwide.</p>", "people": ["devora@media.mit.edu", "esvelt@media.mit.edu"], "title": "Reducing Suffering in Laboratory Animals", "modified": "2017-10-03T18:17:03.994Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "reducing-suffering-in-laboratory-animals"}, {"website": "", "description": "<p>Gemini\u2014an acoustical \u201ctwin chaise\"\u2014spans multiple scales of the  human existence extending from the warmth of the womb to the stretches  of the Gemini zodiac in deep space. It recapitulates a human cosmos: our  body\u2014like the Gemini constellation\u2014drifting in space. <br></p>", "people": ["neri@media.mit.edu"], "title": "Gemini", "modified": "2018-10-19T20:01:10.378Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "gemini"}, {"website": "", "description": "<p>The current interface of emails is designed around time and messages, pushing people to focus on what is more recent rather than important. Immersion is a design experiment that centers the email interface on people and the networks that people form.</p>", "people": ["smilkov@media.mit.edu", "djagdish@media.mit.edu", "hidalgo@media.mit.edu"], "title": "Immersion", "modified": "2018-10-19T15:33:06.662Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "immersion-new"}, {"website": "", "description": "<h1><b>An Alternative Autonomous Revolution&nbsp;</b></h1><h2><i>System design for emerging urban contexts and societal aspirations</i></h2><p>The Persuasive Electric Vehicle (PEV) aims to solve urban mobility challenges with a healthy, convenient, sustainable alternative to cars. The PEV is a low-cost, agile, shared-use autonomous bike that can be either an electrically assisted tricycle for passenger commuting or an autonomous carrier for package delivery.</p><p>The PEV uses standard bicycle components and is lightweight (&lt;50kg) yet robust. Its sensors are easy to reconfigure and it has a 250W mid-drive electric motor and 10Ah battery pack that provides 25 miles of travel per charge and a top speed of 20 miles per hour.</p><p>Our vision for the PEV: a rider summons the PEV through a phone app, and the nearest available PEV arrives autonomously to meet the rider. Upon completing the trip, the PEV simply moves on to its next passenger or package pickup.&nbsp; The PEV can be autonomous, operated by the rider, or provide the rider with an electric assist. PEV's operate in bike lanes, avoiding the congestion and adding incentives to make more bikeable cities.</p>", "people": ["lukeji@media.mit.edu", "cq_zhang@media.mit.edu", "mcllin@media.mit.edu", "inigo@media.mit.edu", "kll@media.mit.edu", "ptinn@media.mit.edu", "taiyu@media.mit.edu", "abhia@media.mit.edu"], "title": "Persuasive Electric Vehicle (PEV)", "modified": "2018-07-19T21:12:27.919Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "pev"}, {"website": "", "description": "<p>We build recommender bots that use machine learning and network analytics to create personalized recommendations for users on various social and financial platforms. We show that bots that work not just on the raw user data, but instead build on human intuition, do far better. We are in the process of live testing these bots on various platforms.&nbsp;</p>", "people": ["pkrafft@media.mit.edu", "sandy@media.mit.edu", "emoro@media.mit.edu", "dhaval@media.mit.edu"], "title": "Social Learning Recommender Bots", "modified": "2017-04-05T18:49:11.444Z", "visibility": "PUBLIC", "start_on": "2017-03-10", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "evolution-strategies-applied-to-collective-intelligence"}, {"website": "http://collectivedebate.mit.edu/", "description": "<p>On&nbsp;<a href=\"http://collectivedebate.mit.edu/\">Collective Debate</a>,&nbsp;users take a test of their morality, then debate an artificial agent regarding a controversial claim: that differences in professional outcomes between men and women arise from bias as opposed to biology. Users indicate how much they agree with the claim, then they exchange arguments with the agent (who assumes the opposite position). After the debate, users are asked to re-evaluate their position. The artificial agent is trained to select arguments that nudge the user to become more moderate.</p>", "people": ["annyuan@media.mit.edu"], "title": "Collective Debate", "modified": "2018-02-21T18:31:02.278Z", "visibility": "PUBLIC", "start_on": "2017-11-14", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "collectivedebate"}, {"website": "", "description": "<p>Computation photography applied to imaging the middle ear for infections.</p>", "people": ["ajdas@media.mit.edu"], "title": "Computation Ear Imaging", "modified": "2018-10-22T20:26:44.372Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "computation-ear-imaging"}, {"website": "http://www.jinjoolee.com", "description": "<p>A social robot modifies its behavior to change what you think about it!</p>", "people": ["cynthiab@media.mit.edu", "jinjoo@media.mit.edu"], "title": "Artificial listener with social intelligence", "modified": "2019-04-17T18:38:39.381Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "artificial-listener-with-social-intelligence"}, {"website": "", "description": "<p>We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Q-learning with an $\\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.</p>", "people": [" otkrist@media.mit.edu"], "title": "Architecture selection for deep neural networks", "modified": "2019-04-19T17:47:51.865Z", "visibility": "PUBLIC", "start_on": "2016-05-01", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "architecture-selection-for-deep-neural-networks"}, {"website": "", "description": "<p><b>layer</b> decentralizes recommendation systems and intersects third-party recommendations with your locally stored, personal information to result in privacy-respecting, relevant recommendations.</p><p>We envision growing the repertoire of personal information beyond purchase choices.</p>", "people": ["anderton@media.mit.edu", "lip@media.mit.edu"], "title": "layer", "modified": "2019-04-18T17:13:41.858Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "layer"}, {"website": "", "description": "<p><a href=\"https://www.media.mit.edu/projects/city-science-andorra/overview/\">View the main City Science Andorra project profile.</a></p><p>Research in dynamic tools, mix users (citizens, workers) amenities, services, and land use, with the goal of promoting sustainable development.</p>", "people": ["jiw@media.mit.edu", "alonsolp@media.mit.edu", "agrignar@media.mit.edu", "doorleyr@media.mit.edu", "devisj@media.mit.edu", "kll@media.mit.edu", "noyman@media.mit.edu", "yleng@media.mit.edu", "csmuts@media.mit.edu", "ryanz@media.mit.edu"], "title": "Andorra | Dynamic Urban Planning", "modified": "2019-02-25T15:17:09.063Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "andorra-dynamic-urban-planning"}, {"website": "", "description": "\u200b<p>State-of-the art liquid handling systems are generally pump-driven systems connected with valves and tubes. These systems are manually assembled, expensive, and unreliable. With the growth of the genomic and drug industries, we are moving toward increasingly complex biological processes requiring very small volume liquid manipulation capability. </p><p>Manually assembled mechanical systems do not scale to parallel manipulation of large amounts of small volume liquids. However, the electronics industry has demonstrated how to build robust integrated systems for information manipulation. With this as our motivation, we look toward electronics and integrated circuits to bring miniaturization, complexity, and integration to enable the next generation of biology.</p>", "people": ["udayan@media.mit.edu", "ishii@media.mit.edu"], "title": "Programmable Droplets", "modified": "2018-10-19T15:44:35.016Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "programmable-droplets"}, {"website": "http://deepmoji.mit.edu", "description": "<p><i>Emotional content is an important part of language. There are many use cases now showing that natural language processing is becoming an increasingly important part of consumer products.&nbsp;We are attempting to learn more about human emotions.</i></p><p>In his 2006 book <i>The Emotion Machine</i>, legendary computer scientist Marvin Minsky (co-founder of the field of Artificial Intelligence and one of the founding faculty members of the MIT Media Lab) wrote about the central role of emotions in reasoning\u2014reminding us that AI will only be capable of true commonsense reasoning once it has understood emotions. To Minsky, emotions are not the opposite of rational reason, something to be weeded out before we can think clearly; rather, emotions are just a different way of thinking.</p><p><b><a href=\"http://deepmoji.mit.edu/\">TRY DEEPMOJI</a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href=\"https://deepmoji.mit.edu/contribute/\">HELP TEACH OUR AI ABOUT EMOTIONS</a></b><br></p><p>But this is hardly helpful to a computer scientist trying to construct an emotional machine by programming a concrete set of rules. If you ask two people to explain what makes a particular sentence happy, sad, serious, or sarcastic, you will likely get at least two different opinions. Much of what determines emotional content is context-specific, culturally constructed, and difficult to describe in an explicit set of rules.<br></p>", "people": ["irahwan@media.mit.edu", "nobradov@media.mit.edu", "felbo@media.mit.edu"], "title": "DeepMoji", "modified": "2019-05-29T13:10:23.998Z", "visibility": "PUBLIC", "start_on": "2017-08-02", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "deepmoji"}, {"website": "", "description": "<p>Structurally, zero gravity means that we do not have to contend with architecture's greatest arch-nemesis, gravity. This opens up a new world of possibilities where we can deploy structures that no longer have to counteract/resist gravitational force. We would like to explore new forms of rapid inflatable prototyping. Most importantly, this prototype explores surfaces utilizing materials that would normally fail on Earth, yet flourish in zero gravity.</p><p>This year the MIT Media Lab's City Science group had an opportunity to think of architecture at the scale of the body that was literally out of this world. These are the results.</p>", "people": ["kapeloni@media.mit.edu", "csmuts@media.mit.edu"], "title": "Spatial Flux: Body and architecture in space", "modified": "2019-04-17T19:45:01.556Z", "visibility": "PUBLIC", "start_on": "2017-06-01", "location": "", "groups": ["space-exploration", "city-science"], "published": true, "active": false, "end_on": null, "slug": "spatial-flux"}, {"website": "", "description": "<p>The human ankle provides a significant amount of net positive work during the stance period of walking, especially at moderate to fast walking speeds. Conversely, conventional ankle-foot prostheses are completely passive during stance, and consequently, cannot provide net positive work. Clinical studies indicate that transtibial amputees using conventional prostheses experience many problems during locomotion, including a high gait metabolism, a low gait speed, and gait asymmetry. Researchers believe the main cause for the observed locomotion is due to the inability of conventional prostheses to provide net positive work during stance. The objective of this project is to develop a powered ankle-foot prosthesis that is capable of providing net positive work during the stance period of walking. To this end, we are investigating the mechanical design and control system architectures for the prosthesis. We are also conducting a clinical evaluation of the proposed prosthesis on different amputee participants.</p>", "people": ["mcarney@media.mit.edu", "hherr@media.mit.edu"], "title": "Powered Ankle-Foot Prosthesis", "modified": "2019-04-26T19:09:31.314Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-054", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "powered-ankle-foot-prosthesis"}, {"website": "", "description": "<h1><b>Go Beyond 8K to experience telepresence without VR</b></h1><h2><b>[Concept] </b></h2><p><b>Experience telepresence without VR headset. </b></p><p>This application combines</p><ul><li>&nbsp;<b>360 degree video</b></li><li>&nbsp;<b>8K resolution display (7680pxls x 4320pxls)</b></li><li><b>AR technology</b></li><li><b>Stereophonic sound</b></li></ul><p>...to taste super-realistic sensation.</p><br><h2><b>360 video in 16K resolution</b></h2><p>Six DLR cameras are connected into one, to enable full spherical shooting with 20 K high resolution (down-converted into 16K in this prototype).&nbsp;</p>", "people": ["eno@media.mit.edu"], "title": "Shibuya 360/8K", "modified": "2018-05-09T14:26:47.302Z", "visibility": "LAB-INSIDERS", "start_on": "2018-03-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "shibuya-360-8k"}, {"website": "", "description": "<p>Markets are notorious for bubbles and bursts. Other research has found that crowds of lay-people can replace even leading experts to predict everything from product sales to the next big diplomatic event. In this project, we leverage both threads of research to see how prediction markets can be used to predict business and technological innovations, and use them as a model to fix financial bubbles. For example, a prediction market was rolled out inside of Intel and the experiment was very successful, and led to better predictions than the official Intel forecast 75 percent of the time. Prediction markets also led to as much as a 25 percent reduction in mean squared error over the prediction of official experts at Google, Ford, and Koch industries.</p>", "people": ["sandy@media.mit.edu", "dhaval@media.mit.edu"], "title": "Prediction Markets: Leveraging internal knowledge to beat industry prediction experts", "modified": "2019-04-19T14:51:23.617Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "prediction-markets-leveraging-internal-knowledge-to-beat-industry-prediction-experts"}, {"website": "", "description": "<p>The Design Daydreams augmented drafting table projects the Looking Sideways exploration tool onto a tabletop to allow a more tangible interaction with the information being explored. Combined with a low-tech augmented reality tool that uses any mobile device in a simple holder to project digital animations on top of objects around the user, the tool allows digital and physical concepts to be overlaid on top of each other to provoke new reinterpretations and creative inspiration.&nbsp;</p>", "people": ["pip@media.mit.edu"], "title": "Design Daydreams", "modified": "2018-10-22T21:40:14.068Z", "visibility": "LAB-INSIDERS", "start_on": "2017-10-27", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "design-daydreams"}, {"website": "", "description": "<p><i><a href=\"http://opera.media.mit.edu/projects/deathandthepowers/\">Death and the Powers</a>&nbsp;</i>is a groundbreaking opera that brings a variety of technological, conceptual, and aesthetic innovations to the theatrical world.&nbsp;It is a one-act, full-evening work that tells the story of Simon Powers, a successful and powerful businessman and inventor, reaching the end of his life and facing the question of his legacy. He is now conducting his final experiment, passing from one form of existence to another in an effort to project himself into the future. Simon Powers is himself now a System. His family, friends, and associates must decide what this means, whether or not he is actually alive, how it affects them, and whether to follow.</p><p><i>Death and the Powers</i> was composed by Tod Machover and developed at the MIT Media Lab along with Diane Paulus (director) and Alex McDowell (production designer). The opera uses the techniques of tomorrow to address age-old human concerns of life and legacy. The unique performance environment, including autonomous robots, expressive scenery, new Hyperinstruments, and human actors, blurs the line between animate and inanimate. The opera premiered in Monte Carlo in Fall 2010, with additional performances in Boston and Chicago in 2011 and a new production with a global, interactive simulcast in Dallas in February 2014. <span style=\"font-size: 18px; font-weight: normal;\">The DVD of the Dallas performance of <i>Powers</i> was released in April 2015.</span></p>", "people": ["ejessop@media.mit.edu", "patorpey@media.mit.edu", "sovsey@media.mit.edu", "tod@media.mit.edu", "akito@media.mit.edu", "benb@media.mit.edu"], "title": "Death and the Powers: Redefining Opera", "modified": "2017-04-03T18:57:33.560Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "Swatch Lab", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "death-and-the-powers-redefining-opera"}, {"website": "", "description": "<p>Sensor networks permeate our built and natural environments, but our means for interfacing to the resultant data streams have not evolved much beyond HCI and information visualization. Researchers have long experimented with wearable sensors and actuators on the body as assistive devices. A user's neuroplasticity can, under certain conditions, transcend sensory substitution to enable perceptual-level cognition of \"extrasensory\" stimuli delivered through existing sensory channels. But there remains a huge gap between data and human sensory experience. We are exploring the space between sensor networks and human augmentation, in which distributed sensors become sensory prostheses. In contrast, user interfaces are substantially unincorporated by the body, our relationship to them never fully pre-attentive. Attention and proprioception are key, not only to moderate and direct stimuli, but also to enable users to move through the world naturally, attending to the sensory modalities relevant to their specific contexts.</p>", "people": ["gershon@media.mit.edu", "joep@media.mit.edu"], "title": "Prosthetic Sensor Networks: Factoring attention, proprioception, and sensory coding", "modified": "2019-04-19T14:31:47.208Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "prosthetic-sensor-networks-factoring-attention-proprioception-and-sensory-coding"}, {"website": "", "description": "<p>Giving opaque technology a glass house, Storyboards present the tinkerers or owners of electronic devices with stories of how their devices work. Just as the circuit board is a story of star-crossed lovers\u2014Anode and Cathode\u2014with its cast of characters (resistor, capacitor, transistor), Storyboards have their own characters driving a parallel visual narrative.</p>", "people": ["jbobrow@media.mit.edu", "saquib@media.mit.edu", "shantell@media.mit.edu"], "title": "Storyboards", "modified": "2017-04-03T01:11:34.352Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["social-computing", "playful-systems"], "published": true, "active": false, "end_on": null, "slug": "storyboards"}, {"website": "", "description": "<p>Teshima 8 Million Lab is the first Shinto shrine worshipping a genetically engineered life\u2014a silkworm created in Sputniko!'s new work Red Silk of Fate\u2014Tamaki's Crush. In the Shinto religion, \"Yaoyorozu\" (which literally means \"8 Million\") is a word used to describe the myriad of gods believed to reside in almost anything, such as the wind, the ocean, trees, and animals. Conceived by artist Sputniko!, Teshima 8 Million Lab sets out to create new members of Yaoyorozu, forming a mythology from emerging science and art. Far from the big city and located on a site blessed with an abundance of nature, the facility invites the exploration of alternative perspectives on our future of nature and beliefs, as science continues to move forward. </p>", "people": ["sputniko@media.mit.edu"], "title": "Teshima 8 Million Lab", "modified": "2017-04-05T18:32:01.509Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["design-fiction"], "published": true, "active": false, "end_on": null, "slug": "teshima-8-million-lab"}, {"website": "", "description": "<p>The Scratch extension system enables anyone to extend the Scratch programming language through custom programming blocks written in JavaScript. The extension system is designed to enable innovating on the Scratch programming language itself, in addition to innovating with it through projects. With the extension system, anyone can write custom Scratch blocks that enable others to use Scratch to program hardware devices such as the LEGO WeDo, get data from online web-services such as weather.com, and use advanced web-browser capabilities such as speech recognition.</p>", "people": ["khanning@media.mit.edu", "cwillisf@media.mit.edu", "sdruga@media.mit.edu", "sdg1@media.mit.edu", "ericr@media.mit.edu", "ascii@media.mit.edu"], "title": "Scratch Extensions", "modified": "2017-03-23T13:35:30.856Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratch-extensions"}, {"website": "http://aprendizagemcriativa.org/", "description": "<p><span style=\"font-size: 18px; font-weight: normal;\">Somos uma rede de educadores, artistas, pesquisadores, empreendedores, alunos e outros interessados na implementa\u00e7\u00e3o de ambientes educacionais mais m\u00e3o-na-massa, criativos e interessantes nas escolas, universidades, espa\u00e7os n\u00e3o-formais de aprendizagem e resid\u00eancias de todo o Brasil.</span><br></p><p>A Rede Brasileira de Aprendizagem Criativa surgiu em 2015 a partir de uma parceria entre o Programa\u00ea (uma colabora\u00e7\u00e3o da Funda\u00e7\u00e3o Lemann com a Funda\u00e7\u00e3o Telefonica Vivo) e o Lifelong Kindergarten Group do MIT Media Lab. Atualmente, contamos com centenas de participantes de todo o Brasil.</p>", "people": ["leob@media.mit.edu"], "title": "Rede Brasileira de Aprendizagem Criativa", "modified": "2017-04-19T17:55:54.327Z", "visibility": "PUBLIC", "start_on": "2015-10-12", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "rede-brasileira-de-aprendizagem-criativa"}, {"website": "", "description": "<p>The Digital Construction Environment is the first architectural-scale structure fabricated with the <a href=\"https://www.media.mit.edu/projects/digital-construction-platform-v-2/overview/\">Digital Construction Platform (DCP)</a>.  Using the Mediated Matter group\u2019s Print-In-Place construction technique, an open-domed structure with a diameter of 14.6 m and a height of 3.7 m was manufactured over a print time of 13.5 hours.&nbsp;</p>", "people": ["cail@media.mit.edu", "jleland@media.mit.edu", "stevenk@media.mit.edu", "darweesh@media.mit.edu", "neri@media.mit.edu"], "title": "DCP: Digital Construction Environment", "modified": "2019-02-11T20:03:42.274Z", "visibility": "PUBLIC", "start_on": "2016-07-17", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "3d-printed-hemi-ellipsoidal-dome"}, {"website": "", "description": "<p>We are providing our tools to the community, and also using them within our lab, to analyze how specific brain mechanisms (molecular, cellular, circuit-level) give rise to behaviors and pathological states. These studies may yield fundamental insights into how best to go about treating brain disorders.</p>", "people": ["esb@media.mit.edu"], "title": "Understanding normal and pathological brain computations", "modified": "2019-04-17T18:34:46.177Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": null, "slug": "understanding-normal-and-pathological-brain-computations"}, {"website": "", "description": "<p>Aalto University, Finland, and the MIT Media Lab\u2019s City Science group are co-developing a version of the MIT CityScope platform for urban analysis, efficient resource utilization, and spatial programming for campus development, using Otaniemi as a testbed. Aalto joins a network of City Science collaborators which includes Tongji University (Shanghai), Taipei Tech (Taiwan), HafenCity University (Hamburg), and ActuaTech (Andorra).</p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "kll@media.mit.edu", "noyman@media.mit.edu"], "title": "City Science Lab Aalto", "modified": "2019-05-07T19:59:14.315Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "city-science-lab-aalto"}, {"website": "", "description": "<p>Fluorescence lifetime imaging is a significant bio-imaging tool that finds important applications in life-sciences. Widely known applications include cancer detection and DNA sequencing. To that end, fluorescence microscopy which is at the heart of bio-imaging is an electronically and optically sophisticated device which is prohibitively expensive. Our work is demonstrates the fluorescence microscopy like functionality can be achieved by a simple, consumer sensor such as the Microsoft Kinect which costs about $100. This is done by trading-off the precision in optics and electronics for sophistication in computational methods. Not only this allows for massive cost reduction but leads to several advances in the area. For example, our method is calibration-free in that we do not assume sample's relative placement with respect to the sensor. Furthermore, our work opens new pathways of interaction between bio-imaging, optics and computer vision communities.</p>", "people": ["raskar@media.mit.edu", "ayush@media.mit.edu"], "title": "Blind and reference-free fluorescence lifetime estimation via consumer time-of-flight sensors", "modified": "2019-04-19T17:48:42.345Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "blind-and-reference-free-fluorescence-lifetime-estimation-via-consumer-time-of-flight-sensors"}, {"website": "http://javierhr.com", "description": "<p>Occupational stress can be described as a harmful emotional and physical response that occurs when high demanding job conditions cannot be met by the resources of the worker. This type of stress is usually associated with feelings of frustration, anger, and fear and can lead to dissatisfaction and lack of motivation in the long-term. Furthermore, high levels of stress can impair decision making, decrease productivity, and lead to high amounts of accidents and job absenteeism. Despite the well-studied negative outcomes, workplace stress is still considered a necessary evil by many people as it helps us keep up with the pace of modern society. Leveraging state-of-the-art sensing technologies and AI, this project seeks to advance the measurement, understanding,&nbsp;and management of stress in real-life settings.</p>", "people": ["rmorris@media.mit.edu", "exposito@media.mit.edu", "picard@media.mit.edu", "djmcduff@media.mit.edu", "sfedor@media.mit.edu", "javierhr@media.mit.edu", "amores@media.mit.edu"], "title": "Onsite Stress Measurement", "modified": "2018-11-06T15:18:40.446Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "automatic-stress-recognition-in-real-life-settings"}, {"website": "", "description": "<p>The electro-holographic video display (Holovideo) project has spanned over decades, beginning back in 1989, and continues to be a main research topic within the Object-Based Media research group. As new technologies and manufacturing processes have become available, Holovideo has been continuously improved to create a better, more efficient holographic display. In our team\u2019s most recent iteration, we are now utilizing femtosecond laser fabrication to develop flat-panel, transparent holographic video displays suitable for both wearable see-through lenses and monitor sized displays.</p><p><span style=\"font-size: 18px;\">As with previous versions of Holovideo, the system uses acoustic waves to modulate laser light in order to create a holographic image. </span><span style=\"font-size: 18px;\">In the past, this required specialized, and sometimes bulky, optical components in order to guide the laser light to the viewer in order to give the 3D image effect. Now, using a combination of femtosecond laser fabrication and surface acoustic waves (SAWs), many of the required optics can be \u201cprinted\u201d directly within the lithium niobate substrate. These full-color modulators support hundreds of thousands of pixels per scan line, making them suitable for fixed or wearable holographic displays.</span></p><p>By using femtosecond laser fabrication, we are now able to perform rapid prototyping of optical components, greatly cutting down fabrication time and costs while also allowing full customization of optical components.</p><p>Follow the links below to see previous versions of Holovideo.</p>", "people": ["tjschoep@media.mit.edu", "bdatta@media.mit.edu", "nsavidis@media.mit.edu", "sjolly@media.mit.edu", "vmb@media.mit.edu", "vparth@media.mit.edu"], "title": "Guided-wave light modulator for holographic video", "modified": "2019-04-22T13:02:53.733Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "Garden Conference Room", "groups": ["ultimate-media", "ce-20", "future-storytelling", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "guided-wave-light-modulator-for-holographic-video"}, {"website": "", "description": "<p>SensorChimes aims to create a new canvas for artists leveraging ubiquitous sensing and data collection. Real-time data from environmental sensor networks are realized as musical composition. Physical processes are manifested as musical ideas, with the dual goal of making meaningful music and rendering an ambient display. The Tidmarsh Living Observatory initiative, which aims to document the transformation of a reclaimed cranberry bog, provides an opportunity to explore data-driven musical composition based on a large-scale environmental sensor network. The data collected from Tidmarsh are piped into a mapping framework, which a composer configures to produce music driven by the data.</p>", "people": ["joep@media.mit.edu", "eflynch@media.mit.edu"], "title": "SensorChimes: Musical mapping for sensor networks", "modified": "2019-04-19T14:33:18.956Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "sensorchimes-musical-mapping-for-sensor-networks"}, {"website": "", "description": "<p>The Digital Construction Platform (DCP) is an experimental enabling technology for large-scale digital manufacturing. In contrast to the typical gantry-based approach to digital construction, robotic arm systems offer the promise of greater task flexibility, dynamically expandable workspaces, rapid setup times, and easier implementation with existing construction techniques. Potential applications for this system include fabrication of non-standard architectural forms; incorporation of data gathered on-site in real time into fabrication processes; improvements in construction efficiency, quality, and safety; and exploration of autonomous construction systems for use in disaster relief, hazardous environments, and extraterrestrial exploration.</p>", "people": ["cail@media.mit.edu", "jleland@media.mit.edu", "stevenk@media.mit.edu", "neri@media.mit.edu"], "title": "Digital Construction Platform", "modified": "2017-05-05T16:41:04.046Z", "visibility": "PUBLIC", "start_on": "2015-08-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "digital-construction-platform-v-2"}, {"website": "", "description": "<p>Recent advances in artificial limbs have resulted in the provision of powered ankle and knee function for lower extremity amputees and powered elbow, wrist, and finger joints for upper extremity prostheses. Researchers still struggle, however, with how to provide prosthesis users with full volitional and simultaneous control of the powered joints.  This project seeks to develop means to allow amputees to control their powered prostheses by activating the peripheral nerves present in their residual limb. Such neural control can be more natural than currently used myoelectric control, since the same functions previously served by particular motor fascicles can be directed to the corresponding prosthesis actuators for simultaneous joint control, as in normal limbs. Future plans include the capability to electrically activate the sensory components of residual limb nerves to provide amputees with tactile feedback and an awareness of joint position from their prostheses.</p>", "people": ["esb@media.mit.edu", "crtaylor@media.mit.edu", "hherr@media.mit.edu", "clites@media.mit.edu", "shriyas@media.mit.edu", "rriso@media.mit.edu", "bmaimon@media.mit.edu", "syeon@media.mit.edu", "lfreed@media.mit.edu"], "title": "Neural interface technology for advanced prosthetic limbs", "modified": "2019-04-26T19:09:57.739Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "neural-interface-technology-for-advanced-prosthetic-limbs"}, {"website": "", "description": "<p>The SCALE platform adds <b>interactivity and trackability to everyday objects.&nbsp;</b>This load-sensitive surface is composed of only three loadcels beneath the surface, so that the system achieves <b>unobtrusive sensing</b>.</p><p>To capture and analyze human activities without any body-attached devices, a bunch of enabling technologies has been proposed. However, there have still been limitations in conventional approaches: RFID requires costly modification of the objects, and computer vision causes privacy issues. To address these problems, we propose ubiquitous load-sensing systems, networked platforms for giving interactivity and trackability to everyday objects by applying object-localization technology to every surface in our living environment and connecting all platforms with each other. Each platform is capable of localizing the position of external objects, utilizing pressure patterns acquired over time from multiple sensors as variables for the detection of location and weight. In addition, we treat the mass of an object as a unique identifier so that the system can trace the flow or detect consumption across platforms. As applications, we introduce a prototype that makes everyday objects tangible interfaces, privacy-preserving observation systems for family through medicine pill bottles and tooth brushes, and augmentation of existing interfaces.&nbsp;</p>", "people": ["taka_y@media.mit.edu", "achituv@media.mit.edu", "ishii@media.mit.edu"], "title": "SCALE (2018)", "modified": "2018-10-22T20:46:39.996Z", "visibility": "LAB-INSIDERS", "start_on": "2018-01-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "scale2018"}, {"website": "http://sputniko.com/2015/04/amyglowingsilk", "description": "<p>We collaborated with NIAS (National Institute of Agricultural Science) to genetically engineer silkworms to develop new kinds of silk for future fashion. For an exhibition at Tokyo's Gucci Gallery, we designed a Nishijin-Kimono dress, working with NIAS's glowing silk (created by injecting the genes of a glowing coral and jellyfish into silkworm eggs) .&nbsp;</p>", "people": ["sputniko@media.mit.edu"], "title": "Tranceflora\u2014Amy's Glowing Silk", "modified": "2017-04-05T18:33:46.371Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["design-fiction"], "published": true, "active": false, "end_on": null, "slug": "tranceflora-amys-glowing-silk"}, {"website": "", "description": "<p><b>ATEN\u00c7\u00c3O:&nbsp;</b>Saiu o&nbsp;<b>resultado do Desafio Aprendizagem Criativa Brasil 2019!</b>&nbsp;Clique&nbsp;<a href=\"https://www.media.mit.edu/posts/resultado-do-desafio-aprendizagem-criativa-brasil-2019/\">aqui</a>&nbsp;para conhecer os fellows e os projetos selecionados!</p><p>----</p><p>O Desafio Aprendizagem Criativa Brasil visa fomentar a implementa\u00e7\u00e3o de solu\u00e7\u00f5es inovadoras que ajudem a tornar a educa\u00e7\u00e3o brasileira mais criativa, prazerosa, relevante, colaborativa e inclusiva para crian\u00e7as e jovens de todo o pa\u00eds.</p><p>Organizado pela <a href=\"http://aprendizagemcriativa.org/\">Rede Brasileira de Aprendizagem Criativa</a>, e contando com o apoio da <a href=\"http://fundacaolemann.org.br/\">Funda\u00e7\u00e3o Lemann</a> e do <a href=\"http://media.mit.edu/\">MIT Media Lab</a>, o Desafio tamb\u00e9m tem como objetivo identificar, conectar e apoiar indiv\u00edduos brasileiros \u2013 artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decis\u00e3o \u2013 que possam ter um papel-chave no avan\u00e7o de pr\u00e1ticas de aprendizagem criativa em escolas p\u00fablicas (do Ensino Fundamental ao Ensino M\u00e9dio) e ambientes de aprendizagem n\u00e3o formais de todo o Brasil.</p><p>Os representantes dos projetos selecionados ganhar\u00e3o uma&nbsp;<i>Creative Learning Fellowship&nbsp;</i>para ajudar a implementar seu trabalho.&nbsp;</p><p><b>As inscri\u00e7\u00f5es v\u00e3o at\u00e9 o&nbsp;dia&nbsp; 13 de janeiro de 2019 e devem ser feitas \u00fanica e exclusivamente atrav\u00e9s do formul\u00e1rio abaixo.</b></p><p>Clique&nbsp;<a href=\"https://docs.google.com/document/d/1xmpCN_IDsOiRwd5KqDD3aeAAa46qXCKR5DpLWzwng7U/edit?usp=sharing\">aqui</a>&nbsp;para a&nbsp;<b>chamada de projetos</b>&nbsp;completa.<br></p><p>Clique&nbsp;<a href=\"https://aprendizagemcriativa.fluidreview.com/\">aqui</a>&nbsp;para o&nbsp;<b>formul\u00e1rio de inscri\u00e7\u00e3o</b>.</p><p>Clique&nbsp;<a href=\"https://docs.google.com/document/d/1LpYYcImuoZCeIm7XRcE1bBP62wdDgLQIfPmVN0_7AxM/edit?usp=sharing\">aqui</a>&nbsp;para respostas \u00e0s&nbsp;<b>perguntas mais frequentes</b>.</p><p><b>Aten\u00e7\u00e3o</b>: &nbsp;esta p\u00e1gina ser\u00e1 atualizada periodicamente com mais informa\u00e7\u00f5es sobre o Desafio. Discuss\u00f5es sobre o edital est\u00e3o ocorrendo no&nbsp;<a href=\"https://forum.aprendizagemcriativa.org/t/chamada-de-projetos-desafio-aprendizagem-criativa-brasil-2019/3547/10\">&nbsp;f\u00f3rum da Rede Brasileira de Aprendizagem Criativa.</a>&nbsp;</p>", "people": ["mres@media.mit.edu", "leob@media.mit.edu"], "title": "Desafio Aprendizagem Criativa Brasil 2019", "modified": "2019-02-23T00:21:05.368Z", "visibility": "PUBLIC", "start_on": "2018-12-03", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "desafio-aprendizagem-criativa-brasil-2019"}, {"website": "http://tangible.media.mit.edu/project/aeromorph/", "description": "<p>The project investigates how to make origami structure with inflatables with various materials. We introduce a universal bending mechanism that creates programmable shape-changing behaviors with paper, plastics, and fabrics. We developed a software tool that generates this bending mechanism for a given geometry, simulates its transformation, and exports the compound geometry as digital fabrication files. A custom heat-sealing head that can be mounted on usual three-axis CNC machines to precisely fabricate the designed transforming material is presented. We envision this technology could be used for designing interactive wearables and toys, and for the packaging industry.\n                    \n                </p><p>Visit&nbsp;<a href=\"http://tangible.media.mit.edu/project/aeromorph/\">http://tangible.media.mit.edu/project/aeromorph/</a>.<br></p><p>Honorable Mention Paper Award, UIST 2016</p>", "people": ["heibeck@media.mit.edu", "ishii@media.mit.edu", "nv2247@media.mit.edu", "jifei@media.mit.edu", "chinyich@media.mit.edu"], "title": "aeroMorph", "modified": "2017-04-24T18:56:38.724Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "aeromorph"}, {"website": "https://constantatlas.github.io/", "description": "<p>An interactive atlas of census data for direct consumption by individual citizens.</p>", "people": ["zhangjia@media.mit.edu"], "title": "The Constant Atlas", "modified": "2018-10-23T14:32:44.183Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["center-for-civic-media", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "the-constant-atlas"}, {"website": "", "description": "<p>Lower-extremity amputees face a series of potentially serious post-operative complications. Among these are increased risk of further amputations, excessive stress on the unaffected and residual limbs, and discomfort at the human-prosthesis interface. Currently, conventional, passive prostheses have made strides towards alleviating the risk of experiencing complications, but we believe that the limit of \"dumb\" elastic prostheses has been reached; in order to make further strides we must integrate \"smart\" technology in the form of sensors and actuators into lower-limb prostheses. This project compares the elements of shock absorption and socket pressure between passive and active ankle-foot prostheses. It is an attempt to quantitatively evaluate the patient's comfort.</p>", "people": ["dhill24@media.mit.edu", "hherr@media.mit.edu"], "title": "Effect of a powered ankle on shock absorption and interfacial pressure", "modified": "2019-04-26T19:10:40.472Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "effect-of-a-powered-ankle-on-shock-absorption-and-interfacial-pressure"}, {"website": "", "description": "<p>Red String of Fate is an East Asian mythology in which gods tie an invisible red string between those that are destined to be together. Sputniko! has collaborated with scientists from NIAS to genetically engineer silkworms to spin this mythical \"Red String of Fate\" by inserting genes that produce oxytocin, a social-bonding \"love\" hormone, and the genes of a red-glowing coral into silkworm eggs. Science has long challenged and demystified the world of mythologies: from Galileo's belief that the Earth revolved around the sun, to Darwin's theory of evolution and beyond\u2014but in the near future, could science be recreating our mythologies? The film unravels a story around the protagonist Tamaki, an aspiring genetic engineer, who engineers her own \"Red Silk of Fate\" in the hopes of winning the heart of her crush, Sachihiko. However, strange, mythical powers start to inhabit her creation....</p>", "people": ["sputniko@media.mit.edu"], "title": "Red Silk of Fate\u2014Tamaki's Crush", "modified": "2017-04-05T18:35:25.694Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["design-fiction"], "published": true, "active": false, "end_on": null, "slug": "red-silk-of-fate-tamakis-crush"}, {"website": "http://web.media.mit.edu/~guysatat/", "description": "<h2>Lensless imaging with compressive ultrafast sensing</h2><p>Traditional cameras require a lens and a mega-pixel sensor to capture images. The lens focuses light from the scene onto the sensor. We demonstrate a new imaging method that is lensless and requires only a single pixel for imaging. Compared to previous single pixel cameras our system allows significantly faster and more efficient acquisition. This is achieved by using ultrafast time-resolved measurement with compressive sensing. The time-resolved sensing adds information to the measurement, thus fewer measurements are needed and the acquisition is faster. Lensless and single pixel imaging computationally resolves major constraints in imaging systems design. Notable applications include imaging in challenging parts of the spectrum (like infrared and THz), and in challenging environments where using a lens is problematic.\n                    \n                </p>", "people": ["raskar@media.mit.edu", "guysatat@media.mit.edu"], "title": "Efficient lensless imaging with a femto-pixel", "modified": "2019-04-19T17:50:37.233Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "imaging-without-a-lens-and-only-a-few-pixels"}, {"website": "", "description": "<p>This project focuses on giving transtibial amputees volitional control over their prostheses by combining electromyographic (EMG) activity from the amputees' residual limb muscles with intrinsic controllers on the prosthesis. The aim is to generalize biomimetic behavior of the prosthesis, making it independent of walking terrains and transitions. </p>", "people": ["hherr@media.mit.edu", "olli@media.mit.edu"], "title": "Volitional control of a powered ankle-foot prosthesis", "modified": "2019-04-26T19:11:00.522Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "volitional-control-of-a-powered-ankle-foot-prosthesis"}, {"website": "", "description": "<p></p><h2></h2><h2></h2><p><span style=\"font-size: 18px;\"><b>A transgenic chicken commercial for ovulating women</b></span></p><p>Esgtrogen Farms is a fictional company that raises genetically modified chickens that produce ovulation hormones in their egg whites. The eggs are marketed towards women who are either trying to get pregnant, or work as egg donors for the fertility industry. The slogan reads, \"One egg a day is the fertility way.\" The project highlights a connection between women and chickens as raw commodities for the biotech industry, performing ways in which women are targeted for bio-consumerism. What is the rhetoric and imagery used in birth management products? Moreover, as avian transgenic technologies become further developed, is it possible to imagine a confluence of the poultry industry with the pharmaceutical health industry?</p>", "people": ["maggic@media.mit.edu"], "title": "Egstrogen Farms", "modified": "2017-04-05T18:37:12.885Z", "visibility": "PUBLIC", "start_on": "2015-09-07", "location": "--Choose Location", "groups": ["design-fiction"], "published": true, "active": false, "end_on": null, "slug": "egstrogen-farms"}, {"website": "https://media.mit.edu/people/ryanz", "description": "<h2><br>An Urban Decision-Support System Augmented by Artificial Intelligence<br></h2><p>The decision-making process in urban design and urban planning is outdated. Currently, urban decision-making is mostly a top-down process, with community participation only in its late stages. Furthermore, many design decisions are subjective, rather than based on quantifiable performance and data. Current tools for urban planning do not allow both expert and non-expert stakeholders to explore a range of complex scenarios rapidly with real-time feedback.&nbsp;</p><p>CityMatrix was an effort towards evidence-based, democratic decision-making. Its contributions lie in the application of Machine Learning as a versatile, quick, accurate, and low-cost approach to enable real-time feedback of complex urban simulations and the implementation of the optimization searching algorithms to provide open-ended decision-making suggestions.&nbsp;The goals of CityMatrix were:&nbsp;</p><br><ol><li><i>Designing an intuitive Tangible User Interface (TUI) to improve the accessibility of the decision-making process for non-experts.&nbsp;</i></li><li><i>Creating real-time feedback on multi-objective urban performances to help users evaluate their decisions, thus to enable rapid, collaborative decision-making.&nbsp;</i></li><li><i>Constructing a suggestion-making system that frees stakeholders from excessive, quantitative considerations and allows them to focus on the qualitative aspects of the city, thus helping them define and achieve their goals more efficiently.</i></li></ol><p>CityMatrix was augmented by Artificial Intelligence (AI) techniques including Machine Learning simulation predictions and optimization search algorithms. The hypothesis explored in this work was that the decision quality could be improved by the organic combination of both strengths of human intelligence and machine intelligence.</p><p>The system was pilot-tested and evaluated by comparing the problem-solving results of volunteers, with or without AI suggestions. Both quantitative and qualitative analytic results showed that CityMatrix is a promising tool that helps both professional and non-professional users understand the city better to make more collaborative and better-informed decisions.&nbsp;</p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "kll@media.mit.edu", "ryanz@media.mit.edu"], "title": "CityMatrix", "modified": "2018-10-17T18:10:46.064Z", "visibility": "PUBLIC", "start_on": "2016-02-26", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "citymatrix"}, {"website": "", "description": "<p>Paper circuitry blends conductive craft materials with electronics components to engage learners in circuit building and programming through making arts and crafts.  Learners can take advantage of the expressive richness of paper to create artifacts that are technically functional, aesthetically unique and personally meaningful. Chibitronics circuit stickers are a toolkit designed for paper circuits that transforms flexible circuit boards into interactive stickers for crafting circuits.</p>", "people": ["jieqi@media.mit.edu"], "title": "Paper Circuits", "modified": "2018-10-20T01:08:52.664Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "paper-circuits"}, {"website": "", "description": "", "people": ["anderton@media.mit.edu", "kalli@media.mit.edu", "lip@media.mit.edu", "hbedri@media.mit.edu"], "title": "What's America Listening To?", "modified": "2018-04-21T00:41:22.039Z", "visibility": "PUBLIC", "start_on": "2017-06-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "what-s-america-listening-to"}, {"website": "", "description": "", "people": [], "title": "Near-Infrared Imaging for Detecting Caries and Structural Deformities in Teeth", "modified": "2017-05-01T18:40:10.168Z", "visibility": "PUBLIC", "start_on": "2016-04-04", "location": "", "groups": ["camera-culture"], "published": false, "active": false, "end_on": null, "slug": "near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth-2"}, {"website": "", "description": "<p>Can drones find missing items? Every year, companies lose billions of dollars due to misplaced items and faulty inventory records in their warehouses. Consider that the smallest Walmart warehouse is larger than 17 football fields, making it impossible to keep track of all items in the warehouse. </p><p>To overcome this challenge, we introduce RFly, a drone-based wireless system that can scan and locate items in warehouses. The system leverages cheap, battery-free RFID (Radio Frequency Identifier) stickers, which are attached to every item in the warehouse similar to barcodes. These RFIDs power up and respond with a unique identifier when commanded by a wireless device called a reader. To scan a warehouse, a drone operator dispatches a small, inexpensive, and safe drone which flies throughout a warehouse, cataloging and localizing all the RFIDs in a warehouse. The video below shows how the system operates.</p>", "people": ["nselby@media.mit.edu", "fadel@media.mit.edu", "yunfeima@media.mit.edu"], "title": "RFly: Drones that find missing objects using battery-free RFIDs", "modified": "2018-08-22T16:00:28.691Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "wireless-sensing-for-drones-agile-robots-robotics"}, {"website": "", "description": "<p>Our Personal Research Projects group is dedicated to the closure of the digital and educational divides and gaps as they effect the underserved populations of America. We will broaden the conversation about the right and the value of their inclusion in the innovation and invention space. We will research, develop and create technology-driven innovative solutions to problems in those communities. We will challenge \u201cold school\u201d and rusted out educational models. And, we will explore and design 21st century models as a deterrent to unemployment, poverty, and discrimination. </p><p>The full participation of underserved communities is critical to long term American productivity and competitiveness. We Will Fab the Dream.&nbsp;</p>", "people": ["tcarew@media.mit.edu", "bdunning@media.mit.edu"], "title": "Personal Research Projects", "modified": "2017-04-04T20:59:05.979Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "techquity-research"}, {"website": "", "description": "<p>Augmentation of human locomotion has proved an elusive goal. Natural human walking is extremely efficient, and the complex articulation of the human leg poses significant engineering difficulties. We present a wearable exoskeleton designed to reduce the metabolic cost of jogging. The exoskeleton places a stiff fiberglass spring in parallel with the complete leg during stance phase, then removes it so that the knee may bend during leg swing. The result is a bouncing gait with reduced reliance on the musculature of the knee and ankle.</p>", "people": ["hherr@media.mit.edu"], "title": "Load-bearing exoskeleton for augmentation of human running", "modified": "2019-04-26T19:11:21.770Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-054", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "load-bearing-exoskeleton-for-augmentation-of-human-running"}, {"website": "", "description": "<p>Researchers currently rely on ad hoc datasets to train automated visualization tools and evaluate the effectiveness of visualization designs. These exemplars often lack the characteristics of real-world datasets, and their one-off nature makes it difficult to compare different techniques. In this paper, we present VizNet: a large-scale corpus of over 31 million datasets compiled from open data repositories and online visualization galleries. On average, these datasets comprise 17 records over 3 dimensions and across the corpus, we find 51% of the dimensions record categorical data, 44% quantitative, and only 5% temporal. VizNet provides the necessary common baseline for comparing visualization design techniques, and developing benchmark models and algorithms for automating visual analysis. To demonstrate VizNet's utility as a platform for conducting online crowdsourced experiments at scale, we replicate a prior study assessing the influence of user task and data distribution on visual encoding effectiveness, and extend it by considering an additional task: outlier detection. To contend with running such studies at scale, we demonstrate how a metric of perceptual effectiveness can be learned from experimental results, and show its predictive power across test datasets.</p>", "people": [], "title": "VizNet: Towards A Large-Scale Visualization Learning and Benchmarking Repository", "modified": "2019-05-06T20:24:37.275Z", "visibility": "PUBLIC", "start_on": "2018-06-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "viznet-towards-a-large-scale-visualization-learning-and-benchmarking-repository"}, {"website": "", "description": "<p>This is to show people how it's done</p><p><br></p><p>Learn more here</p>", "people": ["jliberty@media.mit.edu"], "title": "Test project", "modified": "2017-10-03T19:11:00.248Z", "visibility": "LAB", "start_on": "2017-10-03", "location": "", "groups": ["communications"], "published": true, "active": false, "end_on": null, "slug": "test-project-1"}, {"website": "", "description": "<p>As part of its broader work around learning, this project is exploring both individualized and community-based models for promoting empathy by designing training methods and developing complementary &nbsp;technologies. The initiative launched 20 Day Stranger app with Playful Systems . The initiative is also working with the Opera of the Future group on a Vocal Vibrations/ Finding Your Voice interactive exhibition targeted towards awareness, empathy, and empowerment. The exhibition debuted in Paris and Cambridge with plans for Mexico City in 2017.\n                    \n                </p>", "people": ["tod@media.mit.edu", "slavin@media.mit.edu", "tenzin@media.mit.edu"], "title": "Strangers, Voices, and Society", "modified": "2018-12-11T18:23:20.052Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["ethics"], "published": true, "active": false, "end_on": null, "slug": "learning-empathy"}, {"website": "", "description": "<p>Halo Flex is the latest version of a series of wearable lighting devices that illuminates the face. It explores how light manipulates our facial qualities and visual perception. It brings together flexible circuits with wearable sensing to define a new form of dynamically controlled E-makeup.\n                    \n                </p><p>A wire bend circlet functions as the base. A flexible circuit board curves around the frame. Translucent solder mask, copper, and translucent dielectric material assemble a decorative pattern. Ten RGB LEDs are positioned on the bottom layer to achieve a variety of lighting effects. Lighting compositions can be set manually using a Bluetooth-enabled device, such as a smart phone, or automatically using the onboard motion sensor. Halo explores opportunities for wearable lighting.<br></p>", "people": ["joep@media.mit.edu", "nanzhao@media.mit.edu"], "title": "Halo Flex", "modified": "2017-04-05T18:41:05.488Z", "visibility": "LAB-INSIDERS", "start_on": "2015-09-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "halo-flex"}, {"website": "", "description": "<p>\"Without urgent, coordinated action by many stakeholders, the world is headed for a post-antibiotic era, in which common infections and minor injuries which have been treatable for decades can once again kill,\" says Dr. Keiji Fukuda, WHO\u2019s assistant director-general for health security. \"Effective antibiotics have been one of the pillars allowing us to live longer, live healthier, and benefit from modern medicine. Unless we take significant actions to improve efforts to prevent infections and also change how we produce, prescribe, and use antibiotics, the world will lose more and more of these global public health goods and the implications will be devastating.\"</p><p>The WHO's \"Antimicrobial resistance: global report on surveillance 2014 report\" notes that resistance is occurring across many different infectious agents, but the report focuses on antibiotic resistance in seven different bacteria responsible for common, serious diseases such as bloodstream infections (sepsis), diarrhea, pneumonia, urinary tract infections, and gonorrhea. The results are cause for high concern, documenting resistance to antibiotics, especially \"last resort\" antibiotics, in all regions of the world. New research by Dr. Pratik Shah&nbsp;at Harvard Medical School identified a cost-effective way to&nbsp;treat bacterial infections without antibiotics. Dr. Shah described a molecular switch, controlled by bacterial&nbsp;diets, that toggles microbial infectivity in humans. Exploiting bacterial diets to train them to be&nbsp;good residents of our bodies&nbsp;shows that unorthodox ways to combat and treat antibiotic resistance may lead to the next generation of antimicrobials.</p>", "people": ["pratiks@media.mit.edu"], "title": "Antimicrobials to Treat Infectious Diseases without Leading to Resistant Bacteria", "modified": "2018-10-22T17:55:29.309Z", "visibility": "PUBLIC", "start_on": "2014-11-03", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "antimicrobials-that-treat-infectious-diseases-without-leading-to-resistant-bacteria"}, {"website": "http://annafuste.com", "description": "<p>Inertia is a platform for exploring physical interactions between real active agents and virtual elements in an augmented reality environment. The user is able to interact with a tangible active element that reacts to collisions and forces from virtual objects displayed in augmented reality. Physical forces and collisions in real-time can be better understood when applied to a tangible object, raising possibilities for learning and gaming.</p>", "people": ["afuste@media.mit.edu"], "title": "Inertia", "modified": "2018-04-23T18:30:20.455Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "inertia"}, {"website": "", "description": "<p>Using biologically inspired design principles, a biomimetic robotic knee prosthesis is proposed that uses a clutchable series-elastic actuator. In this design, a clutch is placed in parallel to a combined motor and spring. This architecture permits the mechanism to provide biomimetic walking dynamics while requiring minimal electromechanical energy from the prosthesis. The overarching goal for this project is to design a new generation of robotic knee prostheses capable of generating significant energy during level-ground walking, that can be stored in a battery and used to power a robotic ankle prosthesis and other net-positive locomotion modes (e.g., stair ascent).</p>", "people": ["lmooney@media.mit.edu", "mcarney@media.mit.edu", "hherr@media.mit.edu"], "title": "Biomimetic active prosthesis for above-knee amputees", "modified": "2019-04-26T19:12:24.876Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-054", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "biomimetic-active-prosthesis-for-above-knee-amputees"}, {"website": "http://web.media.mit.edu/~guysatat/", "description": "<h2>Seeing through dense, dynamic, and heterogeneous fog conditions. The technique, based on visible light, uses hardware that is similar to LIDAR to recover the target depth and reflectance.</h2><p>The system relies on ultrafast measurements, used to computationally remove inclement weather conditions such as fog, and produce a photo and depth map as if the fog weren\u2019t there (with contrast improved by 6.5x in dense fog conditions).&nbsp;&nbsp;<br></p><h2>Applications</h2><ul><li>Autonomous and augmented driving in challenging weather.</li><li>Airplanes and helicopters take off, landing and low level flight in dense fog conditions.</li><li>Trains traveling at normal speeds during inclement weather conditions.</li></ul>", "people": ["raskar@media.mit.edu", "guysatat@media.mit.edu"], "title": "Seeing Through Realistic Fog", "modified": "2018-08-22T17:18:20.368Z", "visibility": "PUBLIC", "start_on": "2017-03-01", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "seeing-through-fog"}, {"website": "", "description": "<p>Our goals include novel gene logic and data logging systems, as well as DNA scaffolds that can be produced on commercial scales. State of the art in the former is limited by finding analogous and orthogonal proteins for those used in current single-layer gates and two-layered circuits. State of the art in the latter is constrained in size and efficiency by kinetic limits on self-assembly. We have designed and plan to demonstrate cascaded logic on chromosomes and DNA scaffolds that exhibit exponential growth.</p>", "people": ["jacobson@media.mit.edu", "lisanip@media.mit.edu", "njakimo@media.mit.edu"], "title": "Scaling Up DNA Logic and Structures", "modified": "2017-08-29T04:20:18.205Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "scaling-up-dna-logic-and-structures"}, {"website": "", "description": "<p>This initiative is designing and disseminating new tools and pedagogies for introducing \u201cnon-prescriptive\u201d ways of learning about ethical frameworks and values in educational and governmental organizations, including law enforcement agencies, around the world. &nbsp;The program is collaborating with high schools and middle schools to develop new curricula and resources &nbsp;for teachers that integrate critical thinking methodologies with hands-on workshops. &nbsp;The current project has involved designing apps and games and is working with a variety of collaborators including Education Arcade, Learning Games Network, MasterCard Foundation, Hope Lab, WGBH, and Televisa. Programs have already been deployed through organizations in the US, Mexico, Colombia, India, and parts of Africa.\u000b\n                    \n                </p>", "people": ["tenzin@media.mit.edu"], "title": "Ethics and Empathy Learning", "modified": "2017-07-24T18:13:30.834Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["ethics"], "published": true, "active": false, "end_on": null, "slug": "ethics-learning-and-education"}, {"website": "", "description": "<h2>The Mediated Atmosphere project envisions a smart office that is capable of dynamically transforming itself to enhance occupants' work experience.</h2><p>In the knowledge economy, worker satisfaction is paramount to retention and productivity. Recent studies have identified a decline in workplace satisfaction. Our research demonstrates how Mediated Atmosphere address this growing need.&nbsp;We created a workspace prototype equipped with a modular real-time control infrastructure, integrating biosignal sensors, controllable lighting, projection, and sound.</p>", "people": ["richer@media.mit.edu", "joep@media.mit.edu", "azaria@media.mit.edu", "nanzhao@media.mit.edu"], "title": "Mediated Atmosphere", "modified": "2018-07-16T12:52:08.873Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "mediated-atmospheres"}, {"website": "", "description": "", "people": ["jliberty@media.mit.edu"], "title": "Test project II", "modified": "2017-10-03T19:30:16.420Z", "visibility": "LAB", "start_on": "2017-10-03", "location": "", "groups": ["communications"], "published": true, "active": false, "end_on": null, "slug": "test-project-ii"}, {"website": "", "description": "<p>The initiative, working with George Church, professor of genetics at Harvard Medical School, and Kevin Esvelt, head of the Sculpting Evolution Group &nbsp;at the Media Lab, will explore the broader ethical dimensions for developing tools that involve CRISPR and the expansion of gene editing technologies. This work aims to create a dialog between scientists and citizens, and will include spiritual leaders, religious leaders, and community leaders in a wider conversation about the ethical implications and potential repercussions of the introduction and deployment of these emerging technological interventions. This project is exploring novel ways to inform citizens about science and how they can affect policy based on news scientific developments.\u000b\n                    \n                </p>", "people": ["tenzin@media.mit.edu", "esvelt@media.mit.edu"], "title": "Gene Editing and Biomedical Ethics", "modified": "2017-05-30T15:47:22.536Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["ethics"], "published": true, "active": false, "end_on": null, "slug": "gene-editing-and-biomedical-ethics"}, {"website": "", "description": "<p>Aether Muse proposes an extension for live internet streams of musical audio/video performance.  We explore how a two-way data connection between distributed audience listening can enhance the connection between a musician and their audience. By generating synchronized visualization for client watching the performance in the browser, musicians gain a new means to communicate with their fans and grow their audiences. </p><p>This project also showcases \"tcchh,\" a custom digital audio effect for warping and stretching a live audio signal.</p>", "people": ["holbrow@media.mit.edu"], "title": "Aether Muse", "modified": "2017-04-05T18:34:16.388Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "aether-muse"}, {"website": "", "description": "<p>While there are a number of literacy technology solutions developed for individuals, the role of social\u2014or networked\u2014literacy learning is less explored. We believe that literacy is an inherently social activity that is best learned within a supportive community network including peers, teachers, and parents.</p><p>By designing an approach that is child-driven and machine-guided, we hope to empower human learning networks in order to establish an engaging and effective medium for literacy development while enhancing personal, creative, and expressive interactions within communities. We aim to create a network of learners to engage students from different communities in socially collaborative, self-expressive, and playful literacy learning opportunities via mobile devices.</p><p>To learn more about this project, please check out:&nbsp;<a href=\"http://playfulwords.org/\">http://playfulwords.org/</a></p>", "people": ["dkroy@media.mit.edu", "pbansal@media.mit.edu", "anneli@media.mit.edu", "snehapm@media.mit.edu", "minasg@media.mit.edu", "echu@media.mit.edu", "saquib@media.mit.edu", "jnazare@media.mit.edu", "isysoev@media.mit.edu"], "title": "Playful Words", "modified": "2018-04-30T20:28:15.298Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "playful-words"}, {"website": "", "description": "<p>test placeholder text</p>", "people": ["buchthal@media.mit.edu", "esvelt@media.mit.edu"], "title": "Sculpting Evolution test project Joanna/Tuesday", "modified": "2017-05-01T19:55:53.618Z", "visibility": "LAB", "start_on": "2017-03-01", "location": "", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "sculpting-evolution-test-project-joanna-tuesday"}, {"website": "", "description": "", "people": [], "title": "Expansion Microscopy", "modified": "2017-09-11T21:34:49.510Z", "visibility": "PUBLIC", "start_on": "2015-03-05", "location": "", "groups": ["synthetic-neurobiology"], "published": false, "active": false, "end_on": null, "slug": "expansion-microscopy"}, {"website": "", "description": "<h1><b><i>A blockchain protocol for verifiable records</i></b></h1><p>b_verify is the name of an applied research project and experimental protocol developed at the Digital Currency Initiative at the MIT Media Lab. Its purpose is to provide an improved technical foundation for the issuance, verification, and transaction of certain financial instruments and tradable securities, especially in markets pursuing the digitization of paper records. Upon this foundation, applications servicing the b_verify protocol can be customized for different use cases and contexts.</p><p>Forthcoming in 2018 are academic and industry papers along with a \u201cpilot kit\u201d containing open-source reference code (Java), the system architecture for the b_verify protocol, template desktop and mobile applications, and additional considerations for real world experimentation. Once complete, the pilot kit and associated materials will be distributed to governments, multilateral organizations, and industry associations around the world; advisory to pilot implementations may be provided upon request.</p><h2><b><i>Warehouse receipts</i></b></h2><p>The selected use case informing this research is the negotiable warehouse receipt for agricultural commodities. Warehouse receipts are legally defined title documents attesting to a particular quantity, type, and quality of a commodity at a specific storage facility. These instruments can be used to secure inventory as collateral for loans, to facilitate trade, and to settle expiring futures contracts.</p><p>Development agencies and multilaterals have championed the benefits of warehouse receipts for price discovery and access to credit for even the poorest of farmers. One USAID program in Tanzania produced a doubling of the prices farmers were able to command for their harvest immediately upon the installation of a storage and warehouse receipt program. Strengthening the agricultural sector also improves food security and competitiveness at a national level.</p><p>Three problems prevent warehouse receipts from realizing their full potential for farmers and society: forged documents, high transaction costs, and the potential for disparities between the receipt attestation and the physical goods. Applications using the b_verify protocol can help mitigate these problems.</p><p>First, high profile frauds involving forged or duplicated warehouse receipts have cost banks hundreds of millions of dollars; this makes banks wary of lending against them and traders wary of buying them. The b_verify protocol addresses this problem by posting warehouse receipt issuances as cryptographic commitments to the data structure of the Bitcoin blockchain as a secure, public source of record.</p><p>Second, assuming the receipts are authentic, the transaction costs involved in verifying and transporting paper records are extremely high, especially in countries with poor infrastructure. The b_verify protocol addresses this problem using a novel method of coordinating updates to the records using Merkle proofs constructed by a designated server, which need not be trusted.</p><p>Third, again assuming authentic receipts, banks and traders worry about the quality and honesty of warehouse custodianship; perhaps the goods are removed illegally for example. While this problem cannot be completely eliminated by technology, access control measures and Internet of Things (IoT) integrations can combined with the b_verify protocol to reduce these risks. For example, outflows of grain from a silo could require authentication via a query of the blockchain record, while digital devices measuring the outflow can independently commit updates to the record without human interference.</p><p>An added feature the b_verify protocol is the opportunity for the programmatic enforcement of covenants and contracts (also known as \u201csmart contracts\u201d). For example, using an application servicing the b_verify protocol, the pledging of collateral with a warehouse receipt could automate the transfer of the collateral to the lender upon a hard loan default. Covenants such as maximum debt-to-asset ratios or minimum allowed commodity price fluctuations could also be constructed within the b_verify system.</p><p>The verifiable activities of a given business over time, such as inventory turnover and repayment history, can also provide valuable insight into the health of the business, which is the chief consideration in assessing creditworthiness.</p><p>Lastly, the transparency provided by this publicly accessible and verifiable system of record may contribute to safer, more transparent in asset-backed securities and derivatives markets as these develop in emerging economies.</p><p>Thus, we form the following hypothesis: the b_verify protocol can contribute to meaningful reductions in the aforementioned problems, thereby maximizing the potential for warehouse receipts to improve price discovery and access to finance, as well as national food security and competitiveness.</p><h2><b><i>Progress to Date \u2014 From Mexico to Ukraine</i></b></h2><p>This research began in 2016, and originally focused on blockchain-based land title. This grew into a knowledge partnership with the Mexican ministries of Finance and Economy, which in turn motivated a pivot to the warehouse receipt use case, per Mexico\u2019s priorities and a compelling opportunity to promote financial inclusion. 2016 stakeholder interviews and warehouse visits in Mexico informed the development of the first b_verify prototype in 2017. Upon seeing a demo, the Inter-American Development Bank joined the Digital Currency Initiative and contributed funding to further research with the aim of piloting b_verify in Latin America. Meanwhile, new members to the research team from Ukraine motivated an examination of that country\u2019s warehouse receipt system and potential as a pilot destination. 2018 stakeholder interviews and warehouse visits in Ukraine provided valuable insights, which are now informing the development of an updated protocol, pilot kit, and associated papers.</p>", "people": ["mrweber@media.mit.edu"], "title": "b_verify", "modified": "2018-04-08T02:27:49.458Z", "visibility": "PUBLIC", "start_on": "2017-02-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "digitization-of-asset-registries"}, {"website": "", "description": "<p>The Andorra Living Lab project combines different research topics (Tourism, Innovation, Energy &amp; Environment, Mobility, Dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans research, knowledge, methods and tools to carry out such transformation.&nbsp;</p><p>The project aims at bringing urban innovation in the capital of Andorra with an emphasis on a better knowledge of the pattern of the tourists in the country and how it affects to; mobility, entrepreneurship, and urban simulation for decision-making and community engagement.<br></p><p>Goals include helping to develop big data platforms for understanding, utilizing, and leveraging big data; developing concepts that have the potential to establish Andorra as an international center for innovation; and designing interventions that can improve the experience of tourists, encouraging them to visit more often, stay longer, and increase spending.</p>", "people": ["naichun@media.mit.edu", "jiw@media.mit.edu", "alonsolp@media.mit.edu", "agrignar@media.mit.edu", "mcllin@media.mit.edu", "devisj@media.mit.edu", "kll@media.mit.edu", "noyman@media.mit.edu", "yleng@media.mit.edu", "agnis@media.mit.edu", "lrocher@media.mit.edu", "csmuts@media.mit.edu", "noriega@media.mit.edu"], "title": "Andorra Living Lab", "modified": "2017-05-30T17:05:50.125Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": null, "slug": "andorra-living-lab"}, {"website": "", "description": "<p>A better understanding of the biomechanics of human tissue allows for better attachment of load-bearing objects to people. Think of shoes, ski boots, car seats, orthotics, and more. We are focusing on prosthetic sockets, the cup-shaped devices that attach an amputated limb to a lower-limb prosthesis, which currently are made through unscientific, artisanal methods that do not have repeatable quality and comfort from one individual to the next. The FitSocket project aims to identify the correlation between leg tissue properties and the design of a comfortable socket. The FitSocket is a robotic socket measurement device that directly measures tissue properties. With these data, we can rapid-prototype test sockets and socket molds in order to make rigid, spatially variable stiffness, and spatially/temporally variable stiffness sockets.</p>", "people": ["petron@media.mit.edu", "jfduval@media.mit.edu", "neri@media.mit.edu", "hherr@media.mit.edu"], "title": "FitSocket: Measurement for attaching objects to people", "modified": "2019-04-26T19:13:15.776Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["center-for-extreme-bionics", "biomechatronics", "mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "fitsocket-measurement-for-attaching-objects-to-people"}, {"website": "http://ArielNoyman.com", "description": "<h2>Predicting Urban Performance through Behavioral Patterns in Temporal Telecom Data</h2><p>This study explores a novel method to analyze diverse behavioral patterns in large urban populations and to associate them with discrete urban features. This work utilizes machine learning and anonymized telecom data to understand which fragments of the city has greater potential to attract dense and diverse populations over longer periods of time. Finally, this work suggests a road map for building spatial prediction tools in an effort to improve city-design and planning processes.&nbsp;&nbsp;</p><p><b><a href=\"https://cityscope.github.io/CS_Andorra_RNC/\">Click here for an interactive visualization of this study</a>&nbsp;<br><br></b></p><p><b>Advisors:</b>&nbsp;Kent Larson&nbsp;and&nbsp;Esteban Moro<br><b>Thanks to</b> Andorra Telecom, ActuaTech,&nbsp;N\u00faria Maci\u00e0. <br>Data was&nbsp;obtained by Andorra Telecom as part of MIT Media Lab City Science and the State of Andorra collaboration.&nbsp;</p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "doorleyr@media.mit.edu", "kll@media.mit.edu", "noyman@media.mit.edu", "emoro@media.mit.edu"], "title": "Reversed Urbanism", "modified": "2019-02-24T23:21:12.068Z", "visibility": "PUBLIC", "start_on": "2017-07-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "Reversed-Urbanism"}, {"website": "", "description": "", "people": ["pattie@media.mit.edu"], "title": "Theme | Engineering Dreams", "modified": "2019-02-05T14:34:26.214Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "theme-engineering-dreams"}, {"website": "", "description": "<p>The project aims to investigate the relationships between emotion, wellbeing, skin, and skincare cosmetics.&nbsp;In our first study, we measured emotion/wellbeing, heart rate, and respiration using a mobile phone and a wearable sensor&nbsp;during consumer in-use test.</p>", "people": ["akanes@media.mit.edu"], "title": "Emotion/wellbeing x skincare cosmetics", "modified": "2018-05-01T19:59:13.741Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "emotion-wellbeing-x-skincare-cosmetics"}, {"website": "", "description": "<p>There is a wide cultural belief in the power of the Internet and social media as enablers of collective intelligence. They help us spread information rapidly, and learn useful information from each other. But there are fundamental limits to the capabilities of those networks. Understanding these limits is essential to improving social media and allowing society to make the most of it.</p>", "people": ["lorenzoc@media.mit.edu", "irahwan@media.mit.edu"], "title": "Cognitive Limits of Social Networks", "modified": "2017-03-22T21:10:42.720Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "cognitive-limits-of-social-networks"}, {"website": "", "description": "", "people": [], "title": "Trains of Thought: Railroad Access and Knowledge Diffusion in Sweden", "modified": "2018-04-26T13:28:35.448Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": ["collective-learning"], "published": false, "active": false, "end_on": null, "slug": "trains-of-thought"}, {"website": "", "description": "", "people": [], "title": "Lensless and single pixel imaging", "modified": "2017-03-22T21:24:35.019Z", "visibility": "LAB", "start_on": "2015-01-01", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "lensless-and-single-pixel-imaging"}, {"website": "", "description": "<p>PhonoBlocks is an app similar in design and purpose to&nbsp;<a href=\"https://www.media.mit.edu/projects/speech-blocks/overview/\">SpeechBlocks</a>. However, the blocks in this design are not letters, but sounds of the language (phonemes). While the sound of a letter in English changes depending on the word the letter is in, phonemes behave consistently in every word. Therefore, it might be easier for children to master usage of phoneme-based blocks, opening the avenue for early \u201cwriting\u201d. Playing with phonemes is likely to help building phonological awareness (the ability to distinguish and manipulate the sounds within the words) - the key component of early literacy. When competence with phonemes is acquired, phoneme-to-letters correspondences may start to be introduced. PhonoBlocks is part of our exploration attempting to find out what designs facilitate early forms of meaningful writing.</p>", "people": [], "title": "PhonoBlocks", "modified": "2018-05-02T23:01:45.429Z", "visibility": "PUBLIC", "start_on": "2018-04-10", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "phonoblocks"}, {"website": "", "description": "<h2><b>Seeing around corners with a thermal camera. Our technique exploits unique properties of long-wave IR surface reflectance to see around corners.</b></h2><p>Seeing around corners is challenging in the visible spectrum as photons scatter at diffuse surfaces. However, the surface reflectance of common materials in the long-wave IR spectrum has a strong specular component, making it easier for us to see around corners. Furthermore, any heat sources are long-wave IR sources while common objects are not visible light source. We exploit these to recover 2D shape and 3D location of objects around corners with completely passive sensing.<br></p><p><b>Key Idea</b></p><p>Long-wave IR has much stronger specular surface reflectance on common surfaces than visible light. Any heat sources act as light sources in the long-wave IR spectrum, and their emission can be estimated if the temperature is known. We exploit these two properties of long-wave IR to see around corners. We first estimate the bidirectional reflectance distribution function (BRDF) of the wall at a corner. The estimated BRDF gives the mapping between the hidden object's long-wave IR emission and the measurements of a thermal camera. Using this mapping, our technique recovers the 2D shape and the 3D location of the hidden heat sources around corners. This method is completely passive and does not require occlusion geometries that other passive techniques require.&nbsp;</p>", "people": ["raskar@media.mit.edu", "tomotomo@media.mit.edu"], "title": "Seeing around corners with thermal imaging", "modified": "2019-04-18T13:40:06.043Z", "visibility": "LAB-INSIDERS", "start_on": "2018-10-01", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "thermal-nlos-imaging"}, {"website": "", "description": "<p><b>Foundations (2015 - 2016):</b> The Year 1 MIT/Google collaboration program has Pilot Labs in NYC and Oakland. In the Labs, we introduce a cohort of 8th and 9th graders to the rudiments of Coding and Maker Technologies. Our pedagogy emphasizes Making and Doing and is greatly emphasized by Seymour Papert\u2019s theory of Constructivism and Mel King\u2019s work with youth at the South End Technology Center. For the program, we design and author Curricula and Learning Activities for the Coaches/Instructors, the students, and the parents.The Domains include Coding, Design, Fabrication, Games, Digital Music and Interactive Media. The Toolbox includes Scratch, Formit, Tinkercad, Libre Office, YouiDraw, LittleBits, Laser Cutters, 3D printers, Codebender/Arduino, Touchboard, and MaKey MaKey.  In addition, the students receive Leadership Training, Mathematics, and College Preparation.</p>", "people": ["tcarew@media.mit.edu", "bdunning@media.mit.edu", "lholt@media.mit.edu"], "title": "Foundations : Design of Codified Curricula in Coding, Design, Fabrication, Games, Music, Media, etc.", "modified": "2017-04-04T21:48:15.281Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["code-next"], "published": true, "active": false, "end_on": null, "slug": "code-next"}, {"website": "", "description": "<p><br></p>", "people": ["pattie@media.mit.edu"], "title": "Theme | Memory Enhancement", "modified": "2018-12-10T23:33:12.385Z", "visibility": "LAB-INSIDERS", "start_on": "2018-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "theme-memory-enhancement"}, {"website": "", "description": "", "people": [], "title": "ba;lkdfj;aldskfj", "modified": "2017-09-26T18:47:38.465Z", "visibility": "PUBLIC", "start_on": "2017-09-26", "location": "", "groups": ["communications"], "published": false, "active": false, "end_on": null, "slug": "ba-lkdfj-aldskfj"}, {"website": "", "description": "<p>We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: what billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, big data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.</p>", "people": ["raskar@media.mit.edu", "ajdas@media.mit.edu"], "title": "Health-tech innovations with Tata Trusts, Mumbai", "modified": "2019-04-19T17:52:03.224Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["emerging-worlds", "camera-culture"], "published": true, "active": false, "end_on": null, "slug": "health-tech-innovations-with-tata-trusts-mumbai"}, {"website": "", "description": "<p>CityScope Volpe is demonstrating most of the urban planning, analysis, and prediction features developed for the CityScope project. The site, a 14-acre parcel on the northern part of MIT/Kendall Square area of Cambridge, has been acquired and is&nbsp; being developed by MIT. City Science researchers designed and built a CityScope urban performance tool that is aiming to predict the outcomes of multiple planning and development scenarios.&nbsp;</p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "noyman@media.mit.edu", "ryanz@media.mit.edu"], "title": "CityScope Volpe", "modified": "2019-02-25T15:14:11.403Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "cityscope-volpe"}, {"website": "", "description": "<p>Many behaviors spread through social contact. However, different behaviors seem to require different degrees of social reinforcement to spread within a network. We study and visualize the&nbsp;learning paths of hundreds of thousands of programmers on the social coding platform Github. We show the influence of collaborators on technology and knowledge diffusion. This research has interesting applications in understanding key influencers and early adopters in technology teams and software organizations</p>", "people": ["sanjayg@media.mit.edu"], "title": "CodeSpace", "modified": "2018-05-03T20:05:46.108Z", "visibility": "LAB-INSIDERS", "start_on": "2018-04-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "codespace"}, {"website": "", "description": "<p><br></p>", "people": ["pattie@media.mit.edu"], "title": "Theme | Supporting Decision Making", "modified": "2018-12-12T19:06:27.833Z", "visibility": "LAB-INSIDERS", "start_on": "2018-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "supporting-decision-making"}, {"website": "", "description": "<p>We are developing methods of controlling the genetic and cellular composition of microbial communities in the gut. Stably colonized microbes could be engineered to sense disease, resist pathogen invasion, and release appropriate therapeutics in situ.</p><p><br></p>", "people": ["erikad@media.mit.edu", "ave@media.mit.edu", "esvelt@media.mit.edu"], "title": "Engineering Microbial Ecosystems", "modified": "2017-07-11T00:30:42.637Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "engineering-microbial-ecosystems"}, {"website": "", "description": "<h1><p><span style=\"font-size: 18px; font-weight: normal;\">Music software that lets anyone compose music. The first music software program designed to teach students and adults how to compose music simply by drawing lines on the screen.</span><br></p></h1><h2><br></h2>", "people": ["tristan@media.mit.edu", "tod@media.mit.edu"], "title": "Hyperscore", "modified": "2017-04-03T19:38:51.060Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "hyperscore"}, {"website": "", "description": "<h2>Programmable Water-Based Biocomposites for Digital Design and Fabrication across Scales</h2><p>Aguahoja is an exploration of nature\u2019s design space. A collection of natural artifacts were digitally designed and robotically fabricated from the molecular components found in tree branches, insect exoskeletons, and our own bones. Here, we propose a novel water-based design approach and fabrication platform that enable tight integration between material synthesis, digital fabrication, and physical behavior, at scales that approach\u2014and often match\u2014those of natural ecologies.</p>", "people": ["yjtai@media.mit.edu", "dumo@media.mit.edu", "ssunanda@media.mit.edu", "dlizardo@media.mit.edu", "bader_ch@media.mit.edu", "jvanzak@media.mit.edu", "jpcosta@media.mit.edu", "limulus@media.mit.edu", "nah6cz@media.mit.edu", "darweesh@media.mit.edu", "neri@media.mit.edu", "asling@media.mit.edu", "j_duro@media.mit.edu"], "title": "Aguahoja", "modified": "2019-05-07T13:42:35.058Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2019-07-31", "slug": "aguahoja"}, {"website": "", "description": "<p>This project aims to bridge the current gap between controlled-environment agriculture and the computer vision and robotics fields. The intended outcome is to develop an open source toolkit for plant analysis that can be adapted to a wide range of controlled-environment devices (ranging from small personal devices to container-size smart farms), enabling the possibility of accurately measuring and quantifying plant morphological traits.</p>", "people": ["calebh@media.mit.edu", "rbaynes@media.mit.edu"], "title": "Computer Vision and Machine Learning", "modified": "2019-03-19T18:10:32.113Z", "visibility": "PUBLIC", "start_on": "2016-10-03", "location": "", "groups": ["open-agriculture-openag"], "published": true, "active": false, "end_on": null, "slug": "food-computer-project-open-agriculture-initiative"}, {"website": "", "description": "<p>We have developed an autonomous powered exoskeleton capable of providing a significant metabolic benefit to the user when walking on level ground. To date, this device has demonstrated the greatest published metabolic benefit for an exoskeleton, even outperforming some exoskeletons that rely on a tethered power supply. To gain a better sense of the practicality and versatility of this system, an assessment of metabolic and biomechanical performance on uneven ground is warranted. To accomplish this, the performance of the Biomechatronics exoskeleton will be evaluated through measurement of human respiratory rate while walking on an instrumented treadmill. Furthermore, EMG and motion-capture data will be collected to provide insight as to the biomechanical basis of observed metabolic changes. This study aims to further demonstrate the practicality of an autonomous powered exoskeleton for military, recreational, rehabilitative, or other use.</p>", "people": ["mbweber@media.mit.edu", "hherr@media.mit.edu"], "title": "Metabolic and Biomechanical Evaluation of Walking with an Autonomous Exoskeleton on Sloped Terrain", "modified": "2019-04-26T19:06:47.182Z", "visibility": "LAB-INSIDERS", "start_on": "2016-02-01", "location": "--Choose Location", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "metabolic-and-biomechanical-evaluation-of-walking-with-an-autonomous-exoskeleton-on-sloped-terrain"}, {"website": "", "description": "", "people": ["pattie@media.mit.edu"], "title": "Theme | Attention, Mindfulness, and Health", "modified": "2018-12-10T23:57:20.911Z", "visibility": "LAB-INSIDERS", "start_on": "2018-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "theme-attention-mindfulness-and-health"}, {"website": "", "description": "<p><b>Text-to-Motion</b> generates a sequence of contingent robot animations to accompany the sentiment analyzed from an input sentence and its spoken audio. We trained a linear classifier to transfer learn&nbsp;our&nbsp;corpus of animated robot speech from DeepMoji network,&nbsp; a long short-term memory (LSTM) network with an attention model trained on billion tweets.&nbsp;</p>", "people": ["cynthiab@media.mit.edu", "haewon@media.mit.edu"], "title": "Text-to-Motion: Automatic sequencing of animative robot motions", "modified": "2019-04-17T18:48:02.883Z", "visibility": "PUBLIC", "start_on": "2018-01-28", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "text-to-motion-expressive-robot-motion-sequencing"}, {"website": "", "description": "<p>Cultural Lens is a speculative design project that reverses traditional gender responsibilities around clothing and personal appearance, interrogating the status of prevalent historical arguments around modesty and etiquette. Every society, culture, and religion has implicit or explicit expectations for women's clothing and public appearance, which has recently led to debates around issues for human rights, freedom, and self-expression.</p><p>Cultural Lens asks society-at-large to share the burden of enforcing public appearance for women. Instead of forcing women to wear clothing styles that are deemed acceptable by the public, Cultural Lens allows members of the public that might be offended\u2013by, for example, perceived immodesty or improper etiquette\u2013to have the freedom to selectively filter the appearance of the women they see in public to conform  literally to their view of how women should look.&nbsp;</p><p>The system uses a Microsoft HoloLens to implement an Augmented Reality visual field for the user.  The system can identify people and faces in the view, classify gender, and apply visual filters to their appearance according to the user's preference. For example, Cultural Lens can add digital veil to the faces of all women the user observes.&nbsp;<br></p>", "people": ["djfitz@media.mit.edu", "manisham@media.mit.edu", "nv2247@media.mit.edu"], "title": "Cultural Lens", "modified": "2017-03-29T20:15:11.938Z", "visibility": "PUBLIC", "start_on": "2017-01-03", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "cultural-lens"}, {"website": "", "description": "<p>Humanity has harnessed evolution to sculpt domesticated animals, crops, and molecules, but the process remains a black box. Which combinations of evolutionary parameters will enable us to discover the best solutions? We plan to answer this question by performing massively parallel directed evolution experiments. Our system will use phage-assisted continuous evolution (PACE), a method of building synthetic ecosystems in which billions of fast-replicating viruses compete to optimize a molecular function of our choice. We are developing methods of running many experiments in parallel, each with real-time fitness monitoring and customized evolutionary conditions such as mutation rate, selection stringency, and evolutionary goal-switching. We will use these methods to systematically characterize the relationship between evolutionary parameters and outcomes. </p>", "people": ["erikad@media.mit.edu", "esvelt@media.mit.edu"], "title": "Understanding Molecular Evolution", "modified": "2017-03-23T17:11:16.348Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "understanding-molecular-evolution"}, {"website": "", "description": "<p>Imagine if you want to travel to a destination and once you are there hear only car horns, or see only blue, or feel a single spectral color of the city. The project Is about experiencing a city in a different way and to discovery distinct paths to travel. This is by including artificial soundscapes and visuals from other contexts. Part of this is knowing what sounds are around us and what are we looking at when we are walking in the city. The new experiences is an extra layer of sensory stimuli in the city. The study includes experiencing a city with colors and sounds from another location that we haven\u2019t been to. This \"transfer\" process transfers colors and sounds to another location - while still being in the same city. One potentially could transfer emotional content from one city to another one. Overall, the immersive and multi-sensory representation of a map is crucial for allowing participants to fully feel that place. In addition, the goal is to offer a helpful, immersive, subjective - rather than a detached, observational- experience.</p>", "people": ["thomassl@media.mit.edu"], "title": "Panoptic Journey", "modified": "2017-04-04T16:24:50.150Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "panoptic-journey"}, {"website": "", "description": "<p>Despite of gazillions of leadership programs across university campuses and organizations, we increasingly live in an era of leadership deficit. Transformative Leadership helps participants explore how to align professional values with their deep personal values and develop reflective thinking skills for ethics and values-based leadership. The program is tailored for various demographics including leadership in business organizations, governments and law enforcement agencies. This program is equipped with longitudinal assessment protocols to study in-depth transformational abilities of individuals and how they leverage it for organizations.<br><br>Transformative Leadership is currently offered in the US, Mexico, Colombia, India &nbsp;and Canada. &nbsp;At MIT, Transformative Leadership modules are offered through MIT Sloan and Open Leadership at MIT Media Lab.</p>", "people": ["tenzin@media.mit.edu"], "title": "Transformative Leadership", "modified": "2017-07-31T18:25:57.668Z", "visibility": "PUBLIC", "start_on": "2017-02-01", "location": "", "groups": ["ethics"], "published": true, "active": false, "end_on": null, "slug": "transformative-leadership"}, {"website": "", "description": "<p>Time of Flight 3D cameras like the Microsoft Kinect are prevalent in computer vision and computer graphics. In such devices, the power of an integrated laser is amplitude modulated at megahertz (MHz) frequencies and demodulated using a specialized imaging sensor to obtain sub-cm range precision. To use a similar architecture and obtain micron range precision, this paper incorporates beat notes. To bring telecommunications ideas to correlation ToF imaging, we study a form of \"cascaded Time of Flight\" that uses a Hertz-scale intermediate frequency to encode high-frequency pathlength information. We show synthetically and experimentally that a bulk implementation of opto-electronic mixers offers: (a) robustness to environmental vibrations; (b) programmability; and (c) stability in frequency tones. A fiberoptic prototype is constructed, which demonstrates three micron range precision over a range of two meters. A key contribution of this paper is to study and evaluate the proposed architecture for use in machine vision.<br></p>", "people": ["raskar@media.mit.edu", "achoo@media.mit.edu"], "title": "High-frequency LIDAR using beat notes", "modified": "2019-04-19T17:53:06.677Z", "visibility": "PUBLIC", "start_on": "2017-12-21", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "high-frequency-lidar-using-beat-notes"}, {"website": "", "description": "<p>Biodigital is a fictional virtual reality (VR) experience that combines VR film, immersive 3D environments, and VR data visualization. Biodigital tells the story of humanity as seen from the year 2117, after humans merged with machines in the \"biodigital\" space. The story of Biodigital is about a species that transforms each time it develops new communication technologies. It begins with the histories of printing and television, but then explores the rise of neuroimplantable devices, and the new moral dilemmas they bring to the world.&nbsp;</p><p><span style=\"font-size: 18px; font-weight: normal;\">Biodigital is an experiment on mixed media created in a collaboration between Takahito Ito, from NHK, and </span>C\u00e9sar&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">&nbsp;A. Hidalgo, head of the Collective Learning group. The project explores new forms of storytelling in VR.</span></p>", "people": ["hidalgo@media.mit.edu", "itot@media.mit.edu"], "title": "Biodigital", "modified": "2017-04-05T18:38:52.863Z", "visibility": "LAB-INSIDERS", "start_on": "2017-01-09", "location": "", "groups": ["code-next"], "published": true, "active": false, "end_on": null, "slug": "biodigital-space"}, {"website": "http://tangible.media.mit.edu/project/inforce/", "description": "<p>We propose a novel tangible interaction with pin-based shape display that can reproduce haptic perception of shape, material stiffness, and heterogeneous internal structures of volumetric shape. This is enabled by newly developed pin display, inFORCE, that can detect the force that is applied to each pin, and exert arbitrary force to contact body and objects at the same time. Our proposed interaction methods enabled people to \"press through\" computationally rendered shapes to understand the internal structure of 3D volumetric information. Our design space explores a range of interaction capability enabled by the Force Shape Display system including capturing physical material properties.</p>", "people": ["lajv@media.mit.edu", "djfitz@media.mit.edu", "dvlevine@media.mit.edu", "ishii@media.mit.edu", "ken_n@media.mit.edu", "clarkds@media.mit.edu"], "title": "inFORCE", "modified": "2019-03-25T13:24:48.264Z", "visibility": "LAB-INSIDERS", "start_on": "2017-10-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "in-force"}, {"website": "", "description": "", "people": [], "title": "reSpire: Tangible Vital Sign Feedback Interface for Self-regulation of Respiration", "modified": "2018-10-16T23:40:18.495Z", "visibility": "LAB-INSIDERS", "start_on": "2018-06-01", "location": "", "groups": ["tangible-media"], "published": false, "active": false, "end_on": null, "slug": "respire-tangible-vital-sign-feedback"}, {"website": "", "description": "", "people": ["pattie@media.mit.edu"], "title": "Theme | Platforms for Sensing and Interventions", "modified": "2019-03-06T16:39:24.020Z", "visibility": "LAB-INSIDERS", "start_on": "2018-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "theme-platforms-for-sensing-and-interventions"}, {"website": "", "description": "<p>Ditch the truck. Live, collaborative broadcasting through mixed reality.</p>", "people": ["mhjiang@media.mit.edu", "lip@media.mit.edu", "hbedri@media.mit.edu"], "title": "Broadercasting", "modified": "2019-04-18T01:30:20.552Z", "visibility": "PUBLIC", "start_on": "2016-09-06", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "viralcasting"}, {"website": "", "description": "<p>We believe that tough global health problems require an innovation pipeline. We must bring together the people and providers facing health challenges to form what we call an innovation continuum: inventors building new low-cost technologies; developers capable of rapidly iterating on these inventions for use in the real world; clinicians and end users to validate our creations; and entrepreneurs, philanthropists, and development agencies to scale our solutions. We are asking big questions such as: What billion-dollar ideas could impact a billion lives in health, education, transportation through digital interfaces, digital opportunities, and applications for physical systems? Using machine learning, computer vision, big data, sensors, mobile technology, diagnostics, and crowdsourcing, we are conducting research at the Media Lab, and also collaborating with innovators in three centers in India and in other centers worldwide. Innovations like this launched the effort to create the Emerging Worlds initiative.</p>", "people": ["raskar@media.mit.edu", "ajdas@media.mit.edu"], "title": "Hyderabad eye health collaboration with LVP", "modified": "2019-04-19T17:54:21.753Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "hyderabad-eye-health-collaboration-with-lvp"}, {"website": "", "description": "<p>This research track focuses on the use of computational (and experimental) techniques to understand the biomechanical behavior of human tissue as well as the musculoskeletal system.&nbsp; This knowledge feeds into novel methods for computational modeling based design of biomechatronic devices which in turn aim to restore or improve the human body. These devices include prosthetic and orthotic devices, and exoskeletons. <br></p>", "people": ["branger@media.mit.edu", "kmoerman@media.mit.edu", "hherr@media.mit.edu", "danask@media.mit.edu"], "title": "Computational Biomechanics", "modified": "2019-04-26T19:03:38.901Z", "visibility": "PUBLIC", "start_on": "2015-11-01", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "mechanical-interfaces"}, {"website": "", "description": "<p>BubbleSynth translates motion tracking and blob detection data collected from floating soap bubbles into sound, by using the size, position, age, and velocity of the bubbles to adjust the parameters of a harmonic sine wave via Open Sound Control (OSC). Audio samples, which are provided by SuperCollider, may be triggered in real time either by rule sets that link bubble tracking data to specific sounds, or by physical interaction with the bubbles. The system has been designed to provide a framework that allows any motion tracking data to be ported in, while the SuperCollider backend can be changed merely by adding new SynthDefs.<br></p>", "people": ["novysan@media.mit.edu"], "title": "BubbleSynth: Soap to Sound", "modified": "2017-08-15T20:02:28.318Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "bubblesynth-soap-to-sound"}, {"website": "", "description": "<p>Motivated by applications in rehabilitation and robotics, we are developing methodologies to control muscle-actuated systems via electrical stimulation. As a demonstration of such potential, we are developing centimeter-scale robotic systems that utilize muscle for actuation and glucose as a primary source of fuel. This is an interesting control problem because muscles: a) are mechanical state-dependent actuators; b) exhibit strong nonlinearities; and c) have slow time-varying properties due to fatigue-recuperation, growth-atrophy, and damage-healing cycles. We are investigating a variety of adaptive and robust control techniques to enable us to achieve trajectory tracking, as well as mechanical power-output control under sustained oscillatory conditions. To implement and test our algorithms, we developed an experimental capability that allows us to characterize and control muscle in real time, while imposing a wide variety of dynamical boundary conditions.</p>", "people": ["hherr@media.mit.edu"], "title": "Control of muscle-actuated systems via electrical stimulation", "modified": "2019-04-26T19:15:17.614Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-418", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "control-of-muscle-actuated-systems-via-electrical-stimulation"}, {"website": "", "description": "", "people": ["pattie@media.mit.edu"], "title": "Theme |  Immersive and Wearable Learning", "modified": "2018-12-17T15:29:22.813Z", "visibility": "LAB-INSIDERS", "start_on": "2018-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "theme-immersive-and-wearable-learning"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: normal;\">Pillow-Talk is a set of connected objects intended to assist in the capture and recall digitally stored dreams and memories via natural and tangible interfaces.&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">It consists of two devices, one of which is a pillow embedded with a voice recorder that is activated upon squeezing together several conductive patches at the corner of the pillow. This interaction minimizes the steps necessary to record a fresh memory of a dream immediately upon awakening.&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">After the dream is recorded into the pillow, the audio file is transmitted wirelessly to a jar containing shimmering LEDs to display the \"capture\" of a new memory, and electronics in the jar can play back the recordings through a small speaker under its lid when it is opened.</span></p>", "people": ["edwinapn@media.mit.edu", "vmb@media.mit.edu"], "title": "Pillow-Talk", "modified": "2017-04-03T16:47:39.064Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "pillow-talk"}, {"website": "", "description": "<p>Human walking neuromechanical models show how each muscle works during normal, level-ground walking. They are mainly modeled with clutches and linear springs, and are able to capture dominant normal walking behavior. This suggests to us to use a series-elastic clutch at the knee joint for below-knee amputees. We have developed the powered ankle prosthesis, which generates enough force to enable a user to walk \"normally.\" However, amputees still have problems at the knee joint due to the lack of gastrocnemius, which works as an ankle-knee flexor and a plantar flexor. We hypothesize that metabolic cost and EMG patterns of an amputee with our powered ankle and virtual gastrocnemius will dramatically improve.</p>", "people": ["hherr@media.mit.edu"], "title": "Artificial Gastrocnemius", "modified": "2019-04-26T19:15:38.468Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-054", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "artificial-gastrocnemius"}, {"website": "", "description": "", "people": ["snehapm@media.mit.edu"], "title": "Picture Blocks", "modified": "2018-05-06T22:19:25.415Z", "visibility": "LAB-INSIDERS", "start_on": "2017-02-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "picture-blocks"}, {"website": "", "description": "<p>\"I solemnly swear that I am up to no good.\"<br><br>The Marauder's Map from the Harry Potter universe uses UltraWide Band transmitters and receiver base stations to triangulate position and tag identity through time of arrival and angle of arrival.&nbsp;</p><p>The application is written in C++ in openFrameWorks, taking advantage of the ofxTCPClient class to connect to the server and retrieve the TCP/IP stream of tag positions and speed. The TCP/IP stream operates at 10Hz, while the UWB tags operate at up to 200Hz.&nbsp;</p>", "people": ["novysan@media.mit.edu"], "title": "Marauder's Map", "modified": "2017-08-15T19:54:22.621Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "marauders-map"}, {"website": "", "description": "<p>Chemical constituents are the most essential components of the fresh produce items consumed by people around the world. That chemistry can be extremely variable based on a number of factors\u2014but wealthy or poor, privileged or vulnerable, we humans are ultimately subject to the destiny meted out by the chemistry of the food we consume.<br></p><p>Indeed, nutritional content, flavor, color, texture, aroma, and even medicinal benefits are dictated by this chemistry, yet there has never been a thorough and comprehensive analysis of that chemistry in fresh produce items.&nbsp;The National Produce Chemotyping Project sets the stage to assess the chemical variability, or chemotypes, of those items. Ultimately this project will lead to:</p><ol><li>Unprecedented studies seeking to correlate specific chemical attributes of food to health and well-being for all people.</li><li>Improved understanding of how the agricultural pipeline impacts food at the point of consumption, with the specific inclusion of food sources located in underserved communities.</li><li>The piloting of a free, public-facing resource (The National Produce Chemotyping Database) with interpreted data available in a format that is user-friendly for even the most vulnerable populations.</li></ol>", "people": ["calebh@media.mit.edu", "rebekahj@media.mit.edu", "delapa@media.mit.edu"], "title": "National produce chemotyping project", "modified": "2019-04-18T13:44:15.691Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["open-agriculture-openag"], "published": true, "active": false, "end_on": null, "slug": "npcp"}, {"website": "", "description": "<p>How will we feed crews in space for longer missions where the variety of food and eating experiences are limited? The MIT Media Lab Space Exploration Initiative is examining the possibility of 3D printing for reusable packaging and making food in space.&nbsp;</p><p>Waste management is a huge concern for astronauts living on the ISS.&nbsp;What if HDPE/polyethylene printed food packaging&nbsp; could be reused in a 3D printer for plastic parts?&nbsp;<br></p><p>How can we meet the nutritional and emotional needs of astronauts through food? 3D printing may allow us to reproduce the diversity of food in normal life, starting from a basic ingredient. This would allow astronauts to customize their food in outer space. In addition to 3D printed food, VR may help enhance the experience of eating. Recent taste tests at Media Lab with veteran&nbsp;astronauts Paolo Nespoli and Cady Coleman proved that VR was highly effective and emotive as a tool to connect to home and engage with food.</p><p>Projects will be deployed on upcoming zero gravity flights and shared with ISS-mission-veteran astronauts for taste-testing and further development.&nbsp;<br></p><p>For inquiries about the research, please direct questions to&nbsp;<a href=\"https://www.media.mit.edu/people/mcoblent/overview/\">Maggie Coblentz</a>&nbsp;(mcoblent at mit dot edu) and&nbsp;<a href=\"https://www.media.mit.edu/people/aekblaw/overview/\">Ariel Ekblaw</a>&nbsp;(aekblaw at media dot mit dot edu).&nbsp;</p>", "people": [], "title": "3D Printing for Food and Packaging in Space", "modified": "2018-10-22T13:43:58.395Z", "visibility": "LAB-INSIDERS", "start_on": null, "location": "", "groups": ["space-exploration"], "published": false, "active": false, "end_on": null, "slug": "3d-printing-food-in-space"}, {"website": "", "description": "", "people": [], "title": "Time-Critical Social Mobilisation", "modified": "2017-12-06T02:40:28.268Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["scalable-cooperation"], "published": false, "active": false, "end_on": null, "slug": "time-critical-social-mobilisation"}, {"website": "http://www.jinjoolee.com", "description": "<p>Unconventional mixing of research fields introduces a new method to study human behavior using social robots.</p>", "people": ["cynthiab@media.mit.edu", "jinjoo@media.mit.edu"], "title": "Human-Robot Trust", "modified": "2018-10-19T15:20:33.968Z", "visibility": "PUBLIC", "start_on": "2012-11-03", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "human-robot-trust"}, {"website": "", "description": "<p>We are studying the mechanical behavior of leg muscles and tendons during human walking in order to motivate the design of power-efficient robotic legs. The Endo-Herr walking model uses only three actuators (leg muscles) to power locomotion. It uses springs and clutches in place of other essential tendons and muscles to store energy and transfer energy from one joint to another during walking. Since mechanical clutches require much less energy than electric motors, this model can be used to design highly efficient robotic legs and exoskeletons. Current work includes analysis of the model at variable walking speeds and informing design specifications for a collaborative \"SuperFlex\" exosuit project.</p>", "people": ["mfurtney@media.mit.edu", "hherr@media.mit.edu"], "title": "Human walking model predicts joint mechanics, electromyography, and mechanical economy", "modified": "2019-04-26T19:16:23.360Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-054", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "human-walking-model-predicts-joint-mechanics-electromyography-and-mechanical-economy"}, {"website": "", "description": "<p>SpaceHuman is a soft robotics device designed to facilitate the exploration of environments with reduced gravity in a view of democratization and openness towards access to space and its exploration. &nbsp;It is based on the idea that one day, people who have not received a long preparation and training, as happens today with the astronauts, will be able to have access to the space having a type of conformation and physical configuration that is not adapted to this kind of setting.&nbsp;</p><p>The analysis of the unique seahorse's tail structure became the insight of the overall biomimetic design process. In fact, seahorse tail movement, gripping and protection to the seahorse while floating.&nbsp;Moreover, seahorses do not use their tails to swim; instead, they use them to grasp objects in their environment while they camouflage to hide from predators and hunts for prey. Flexibility and resiliency are key features that enable these behaviours.</p><p>SpaceHuman is an additive prosthesis or otherwise definable as a \"supernumerary robot.\" SpaceHuman will facilitate the use of space in zero gravity or reduced gravity restoring the right motion and balance of our body and assigning a new function to a part of our body that until now has not been fully exploited except for the transport of loads, our back. Users will thus be able to build a new poetics of the body and its movements within this radically different space through SpaceHuman, creating new scenarios of its application. Through air chambers specifically designed to be able to change their shape and bend along a reinforcing rib of the material, the people who will use SpaceHuman will be able to cling to useful surfaces inside orbital housing or in Lunar or Martian villages.&nbsp;</p>", "people": ["aekblaw@media.mit.edu", "joep@media.mit.edu", "vsumini@media.mit.edu", "muccillo@media.mit.edu"], "title": "SpaceHuman", "modified": "2019-05-13T21:47:19.873Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "spacehuman"}, {"website": "http://www.jinjoolee.com", "description": "<p>A computational model predicts the degree of trust a person has toward their novel partner, and advances our scientific understanding about interpersonal trust.\n                    \n                </p>", "people": ["cynthiab@media.mit.edu", "jinjoo@media.mit.edu"], "title": "Computational Trust", "modified": "2017-06-05T15:54:07.885Z", "visibility": "PUBLIC", "start_on": "2013-12-04", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "computational-trust"}, {"website": "", "description": "<p>There is a deep fear that human jobs will be replaced by AI. Rather than racing against the machines, our aim is to show that a human-AI combination will perform better than humans and AI working alone. Although no man is better than a machine for some tasks, \"no machine is better than a man with a machine\" (Paul Tudor Jones) . Thus, by building \"bots\" that are compatible with human behavior, and specifically leverage the manner in which humans use social information, we have been able to build bots that extend human intelligence capabilities. In a large-scale financial trading experiment, we have shown that groups of humans and \"socially compatible\" AI bots can successfully incorporate human intuition into their decisions and consequently not only do better than humans alone, but also do better than similar AI bots that use only objective information.\n                    \n                </p>", "people": ["pkrafft@media.mit.edu", "sandy@media.mit.edu", "emoro@media.mit.edu", "dhaval@media.mit.edu"], "title": "Social AI and Extended Intelligence", "modified": "2019-02-12T15:14:05.048Z", "visibility": "PUBLIC", "start_on": "2016-07-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "social-ai-and-extended-intelligence"}, {"website": "", "description": "<p>Wireless networks\u2014consisting of WiFi, LTE, RFIDs, and millimeter-wave devices\u2014have become integral parts of our everyday lives. Our research explores how we can make these networks faster, more robust, and seamlessly mobile. It also explores how we can use these networks for purposes other than communication, such as localization, sensing, and control.\n                    \n                </p>", "people": ["fadel@media.mit.edu"], "title": "Programming Wireless Networks (Computer Networks)", "modified": "2018-08-22T01:30:56.157Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "programming-wireless-networks-computer-networks"}, {"website": "", "description": "<p>Social robots take an active role in the inference process by producing social cues to elicit nonverbal responses from children to better understand their cognitive state.</p>", "people": [], "title": "Interactive Inference of Mental States", "modified": "2017-06-05T15:58:26.432Z", "visibility": "PUBLIC", "start_on": "2016-10-18", "location": "", "groups": ["personal-robots"], "published": false, "active": false, "end_on": null, "slug": "interactive-inference-of-mental-states"}, {"website": "", "description": "<p>How can biological organisms be incorporated into product, fashion, and architectural design to enable the generation of multi-functional, responsive, and highly adaptable objects? This research pursues the intersection of synthetic biology, digital fabrication, and design. Our goal is to incorporate engineered biological organisms into inorganic and organic materials to vary material properties in space and time. We aim to use synthetic biology to engineer organisms with varied output functionalities and digital fabrication tools to pattern these organisms and induce their specific capabilities with spatiotemporal precision.</p>", "people": ["stevenk@media.mit.edu", "ssunanda@media.mit.edu", "moonshot@media.mit.edu", "neri@media.mit.edu"], "title": "Printing Living Materials", "modified": "2017-04-03T20:39:02.693Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "printing-living-materials"}, {"website": "", "description": "<p>Bricoleur allows makers of all ages to explore the creative possibilities of video and audio as programmable media on mobile devices. Using hand-drawn gestures and a Scratch Blocks-based interface, makers can quickly create complex interactive stories, animations, and artworks by capturing and programming images and sounds from the world around them.</p>", "people": ["hisean@media.mit.edu"], "title": "Bricoleur", "modified": "2018-10-21T00:33:16.471Z", "visibility": "PUBLIC", "start_on": "2018-06-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "bricoleur"}, {"website": "", "description": "<p>PubPub reinvents publication to align with the way the web was designed: collaborative, evolving, and open. PubPub uses a graphical format that is deliberately simple and allows illustrations and text that are programs as well as static PDFs. The intention is to create an author-driven,  distributed alternative to academic journals that is tuned to the dynamic nature of many of our modern experiments and discoveries. It is optimized for public discussion and academic journals, and is being used for both.  It is equally useful for a newsroom to develop a story that is intended for both print and online distribution.</p>", "people": ["thariq@media.mit.edu", "hidalgo@media.mit.edu", "lip@media.mit.edu", "kzh@media.mit.edu", "trich@media.mit.edu"], "title": "PubPub", "modified": "2018-05-31T18:16:31.248Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "viral-communications", "collective-learning"], "published": true, "active": false, "end_on": null, "slug": "pubpub"}, {"website": "", "description": "<p>In Mexico City, a cacophony of sounds will come to your window from dusk till dawn. Merolicos\u2014distinctive chants, recorded songs, and particular chimes\u2014announce the commodities of roaming merchants or ambulantes. Locals recognize the whistle of the knife-sharpener, the triangle signaling gas delivery, the pre-recorded drone broadcasting tamales, and the terrify shriek of steam indicating that sweet plantains are nearby.</p><p>These vendors carry their wares in retrofitted tricycles and ad hoc push-carts\u2014weaving through traffic, lining sidewalks, and turning parks into impromptu marketplaces. They blanket the city with a vast, informal, mobile market that crisscrosses Mexico City\u2019s sharply segregated communities. One of the city\u2019s biggest ailments, marginalization and segregation, are exemplified through neighborhoods \u201cfar away so close\u201d\u2014sitting just across the avenue from each other, yet living in drastically different worlds.</p><p>Exquisite Triciclo infiltrates these divided communities, doing so by appropriating the idiosyncratic cargo tricycle and hacking into the ambulantes\u2019 roaming sales network. Rather than selling tamales or drinking water, it turns this tool of commerce into a mobile stage, a public easel. Inspired by the surrealist parlor game of creating \u201cexquisite corpses\u201d the Triciclo endorses co-creation as a way to know thy neighbor.</p><p>Besides its analog output, Exquisite Triciclo is outfitted with GPS, a Go-Pro camera and a \u201cSmart Pen,\u201d registering contrasting aesthetic and idiomatic differences pertaining the residents of each neighborhood.  These devices keep a navigable digital record, as well as a tangible physical one, to be exhibited. The Triciclo\u2019s added technology is powered through its pedaling thanks to a generator installed on the rear wheel.</p><p>Methods of fabrication are meant to serve as an easily replicable model for the region\u2014the tricycles of Mexico City are personal machines, pimped and decorated by their owners to attract attention.  Exquisite Triciclo reflects its spirit in its making: gathering the knowledge and input of a diverse group of designer, fabricators, and community members to create a ludic instrument, from creation through use.</p>", "people": ["edwinapn@media.mit.edu"], "title": "TRICICLO", "modified": "2017-03-24T17:27:41.068Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "triciclo"}, {"website": "", "description": "<p>Most of the underwater world remains far off the map. For many of the most exciting exploration challenges\u2014from Maya cenotes to urban aquifers to archaeological treasures to coral reefs\u2014map-making remains largely pre-industrial and time consuming. The difficulty and expense of mapping these spaces is a major barrier to storytelling for science, conservation, and stewardship. While many tools exist for open-ocean bathymetry (such as multibeam sonars), cost-effective diver-deployable tools for rapidly mapping complex and enclosed spaces are sorely lacking. Our goal is to create diver-deployable tools that are orders of magnitude faster, more precise, and less expensive than current practice\u2013to enable mapping and imaging of these underwater resources at a societal scale.</p><p>To this end we are developing low-cost, high-precision, diver-deployable underwater LIDAR and Depth-Imaging systems\u20143D scanning and navigation systems with which to quickly, safely, and beautifully map caves, aquifers, coral reefs, sunken cities, and other large-scale underwater spaces. To satisfy scientific and storytelling needs, these devices must be easy to use, have fine spatial resolution, map at swimming speed, produce data in industry-standard formats, and be completely open source at both hardware and software levels.</p>", "people": ["katybell@media.mit.edu", "vmb@media.mit.edu"], "title": "Project Prometheus", "modified": "2019-04-22T17:55:43.363Z", "visibility": "PUBLIC", "start_on": "2018-04-02", "location": "", "groups": ["open-ocean"], "published": true, "active": false, "end_on": null, "slug": "project-prometheus"}, {"website": "", "description": "<p>Our group develops technologies that can see through walls and perform motion capture through occlusions. To do so, we rely on wireless signals, like WiFi. These signals traverse walls and reflect off humans behind the wall before returning to a wireless receiver. We design and develop new algorithms and software-hardware systems that can extract these signals and analyze them to capture human motion from behind a wall.\n                    \n                </p>", "people": ["fadel@media.mit.edu"], "title": "Seeing Through Walls", "modified": "2017-08-22T14:30:57.619Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "seeing-through-walls-computer-vision"}, {"website": "", "description": "<p>In 1990 Germany began the reunification of two separate research systems. Yet, the institutional unification of these system does not necessarily imply their actual unification. Here we study the evolution of the network of co-authorships between East and West German scholars between 1974 and 2014 to identify the fields that integrated more successfully, and also, the factors predicting re-unification success. We find that the unification of the German research network was fast during the 1990s, but then stagnated at an intermediate level of integration. Next, we study the integration of the twenty largest academic fields (by number of publications prior to reunification) and find an inverted U-shaped between a field's East or West ``dominance'' (a measure of the concentration of the scholarly output of a field in East or West Germany prior to 1990) and the field's subsequent level of integration. We check for the robustness of these results by running Monte Carlo simulations, and a differences-in-difference analysis. Both methods confirm that fields that were dominated by either West or East Germany prior to the reunification integrated less than those whose output was balanced among East and West. Finally, we explore the origins of this inverted U-shape relationship by comparing the mixing patterns, and show that this inverted U-shaped relationship can be explained as a consequence of a tendency of scholars from the most productive regions to collaborate preferentially with scholars from other top regions. These results shed light on the mechanisms governing the reintegration of networks in the content of scholarly communities that were separated by institutions.</p>", "people": ["flaviopp@media.mit.edu", "hidalgo@media.mit.edu", "bjun@media.mit.edu"], "title": "Meet me in the middle: The reunification of the German research and innovation system", "modified": "2017-10-11T15:39:59.915Z", "visibility": "PUBLIC", "start_on": "2016-04-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "meet-me-in-the-middle-the-reunification-of-the-german-research-and-innovation-system"}, {"website": "http://www.judithamores.com/essence", "description": "<p>The sense of smell is perhaps the most pervasive of all senses, but it is also one of the least understood and least exploited in HCI. We present Essence, the first olfactory computational necklace that can be remotely controlled through a smartphone and can vary the intensity and frequency of the released scent based on biometric or contextual data.</p>", "people": ["amores@media.mit.edu", "pattie@media.mit.edu"], "title": "Essence", "modified": "2019-05-14T13:43:24.753Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "essence"}, {"website": "", "description": "<p>The 2009&nbsp; DARPA Network Challenge -- retrieving the locations of ten balloons placed at undisclosed positions in the US ---brought practical attention to the problems of social mobilization and information acquisition in a networked environment. The MIT Media Laboratory team won the challenge by acting as the root of a query incentive network that unfolded all over the world. However, rather than adopting a&nbsp;<i>fixed-payment</i>&nbsp;strategy, the team designed a novel incentive scheme based on <i>1/2-split contracts</i>. Under such incentive scheme, a node <i>u</i> who does not possess the information can recruit a friend <i>v</i> through a contract stipulating that if the information is found in the subtree rooted at <i>v</i>, then <i>v </i>has to give half of her own reward back to <i>u</i>.</p><p>Due to its empirical and theoretical superior performance, which can&nbsp;be proven mathematically, split contracts&nbsp;scheme has been used around the globe in a&nbsp;number of crowdsourcing competitions concerning&nbsp;information acquisition in a strategic networked environment. Here, we document the most prominent ones.</p>", "people": [], "title": "Time-Critical Social Mobilisation", "modified": "2017-12-06T03:00:35.395Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["scalable-cooperation"], "published": false, "active": false, "end_on": null, "slug": "mobilise"}, {"website": "", "description": "<p>Our published research findings listed below share key examples for reducing&nbsp;dependence on specialized medical devices, biological &amp; chemical processes &amp; creation of new paradigms for low-cost biomarker imaging and clinical diagnoses at the point-of-care. We have also demonstrated series of generative, prediction and classification&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">algorithmic&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">approaches for obtaining medical diagnostic information of organs &amp; tissues from photographs captured by mobile phones and cameras.&nbsp;</span></p><ul><li>a) In collaboration with Brigham and Womens Hospital in Boston MA, we devised and published a novel \u201cComputational staining\u201d system to digitally stain photographs of unstained tissue biopsies with Haematoxylin and Eosin (H&amp;E) dyes to diagnose cancer. The paper also described an automated \"Computational destaining\" algorithm that can remove dyes and stains from photographs of previously stained tissues, allowing reuse of patient samples. Our method uses neural networks to help physicians provide timely information about the anatomy and structure of the organ and saving time and precious biopsy samples. (<a href=\"https://www.media.mit.edu/projects/computational-histological-staining-and-destaining-of-prostate-core-biopsy-rgb-images-with-generative-adversarial-neural-networks-1/overview/\">Project link</a>)</li><li>&nbsp;b)&nbsp; In collaboration with Beth Israel Deaconess Medical Center in Boston MA, we investigated use of dark field imaging of capillary bed under the tongue of consenting patients in emergency rooms for diagnosing sepsis (a blood borne bacterial infection). A neural network capable of distinguishing between images from non-septic and septic patients with more than 90% accuracy was reported for the first time . This approach can rapidly stratify and offer rational use of antibiotics and reduce disease burden in hospital emergency rooms and patients. (<a href=\"https://www.media.mit.edu/projects/machine-learning-algorithms-for-classification-of-microcirculation-images-from-septic-and-non-septic-patients-1/overview/\">Project link</a>)</li><li>c) We successfully predicted signatures associated with fluorescent porphyrin biomarkers (linked with tumors and periodontal diseases) from standard white-light photographs of the mouth, thus reducing the need for fluorescent imaging.We are expanding the repertoire of biomarkers that can be detected in RGB color images acquired at the point-of-care and pairing them with automated machine learning exams. (<a href=\"https://www.media.mit.edu/projects/machine-learning-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images/overview/\">Project link</a>)&nbsp;</li><li>&nbsp;d) We have also communicated research studies reporting automated segmentation of oral diseases from standard photographs by neural networks&nbsp;and correlations with systemic health conditions such as optic nerve abnormalities in patients. These&nbsp;examples communicate our contributions to design novel neural networks and processes that can assist physicians and patients by next-generation computational medicine algorithms at the point-of-care which integrate seamlessly into clinical workflows in hospitals all over the world. (<a href=\"https://www.media.mit.edu/projects/machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images/overview/\">Project link 1</a>, <a href=\"https://www.media.mit.edu/projects/machine-learning-from-biomarker-signatures-and-correlation-to-systemic-health-conditions/overview/\">Project link 2</a>)</li></ul>", "people": ["gyauney@media.mit.edu", "pjavia@media.mit.edu", "kla11@media.mit.edu"], "title": "Research Area | Medical Imaging Technologies Using Unorthodox Artificial Intelligence for Early Disease Diagnoses", "modified": "2019-04-22T20:07:42.253Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "research-area-unorthodox-machine-learning-solutions-for-cancer-oral-and-infectious-diseases"}, {"website": "", "description": "<p>The Tidmarsh Living Observatory Portal is a research project that focuses on the realization of a pavilion that will&nbsp;generate an immersive experience about the Tidmarsh Living Observatory. This&nbsp;site has been restored from a former cranberry farm to natural wetland.&nbsp;Through an extensive Responsive Environments research, this networked&nbsp; and&nbsp;outdoor instrumented site streams live&nbsp; data that will be part of the portal&nbsp;experience.</p>", "people": ["bmayton@media.mit.edu", "joep@media.mit.edu", "vsumini@media.mit.edu", "duhart@media.mit.edu"], "title": "Tidmarsh Living Observatory Portal", "modified": "2019-04-19T18:16:55.498Z", "visibility": "PUBLIC", "start_on": "2019-04-19", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "tidmarsh-living-observatory-portal"}, {"website": "", "description": "<p>Scratch Memories is a web-based visualization tool that empowers children to celebrate and reflect on their creative journey with Scratch.&nbsp;The system dynamically generates personalized visualizations in the form of a video, highlighting a user\u2019s key moments, diverse creations, and collaborative experiences in the online community.&nbsp;</p><p>Existing tools for visualizing children\u2019s progress in computational learning are primarily designed for educators, and often focus exclusively on <i>evaluating</i> predefined concepts in individual projects. The goal of Scratch Memories is to present a new approach towards designing positive reflective experiences that value the full range of children\u2019s contributions as members of a creative community.<br></p><p>The tool engages young people to reflect on their personal growth over time\u2014starting from their first experiments with code to seeing the increasing diversity and complexity of their projects over time; and from their initial interactions in the community to seeing how their projects have inspired others around the world.&nbsp;Such reflective experiences can not only help young creators feel proud about how far they have come, but also to feel inspired by their own trajectories to continue exploring new possibilities.</p>", "people": ["shrutid@media.mit.edu"], "title": "Scratch Memories", "modified": "2018-11-07T15:55:47.871Z", "visibility": "PUBLIC", "start_on": "2016-12-20", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratch-memories"}, {"website": "", "description": "<p>An autostereoscopic (no glasses) 3D display engine is combined with a \"Pepper's Ghost\" setup to create an office chair that appears to contain a remote meeting participant. The system geometry is also suitable for other applications, such as tabletop or automotive heads-up displays.</p>", "people": ["novysan@media.mit.edu"], "title": "3D Telepresence Chair", "modified": "2017-12-06T16:07:52.335Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "3d-telepresence-chair"}, {"website": "", "description": "<p>We are developing new millimeter wave technologies for health sensing and imaging. The technology can potentially detect tumors and track blood flow inside the human body.</p>", "people": ["fadel@media.mit.edu", "yunfeima@media.mit.edu"], "title": "Millimeter Wave Health Sensing", "modified": "2018-08-22T01:32:29.325Z", "visibility": "LAB-INSIDERS", "start_on": "2017-05-01", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "millimeter-wave"}, {"website": "", "description": "<p>Asthma is the most common chronic illness among children. The skills required to diagnose it make it an even greater concern. Our solution is a child-friendly wearable device that allows in-home diagnosis of asthma. The device acquires simultaneous measurements from multiple stethoscopes. The recordings are then sent to a specialist who uses assistive diagnosis algorithms that enable auscultation (listening to lung sounds with a stethoscope). Sound refocusing algorithms enable the specialist to listen to any location in the lungs. The specialist also has access to a sound \"heat map\" that shows the location of sound sources in the lungs.</p>", "people": ["raskar@media.mit.edu", "guysatat@media.mit.edu", "naik@media.mit.edu"], "title": "Identi-Wheez: A device for in-home diagnosis of asthma", "modified": "2019-04-19T18:25:32.160Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "camera-culture"], "published": true, "active": false, "end_on": null, "slug": "identi-wheez-a-device-for-in-home-diagnosis-of-asthma"}, {"website": "", "description": "<p>Unlike traditional randomized controlled trials that generalize relationships in large groups of people, single-case experiments seek to quantify an individual's reaction to an intervention by measuring an independent variable's effect on a dependent variable (i.e., an intervention's effect on an outcome behavior). These single-case experiments are then combined back together using Bayesian Statistics methods in order to learn more general patterns about a population. We are interested in single-case experiments that test the causal relationships between behaviors that have been observed to be correlated with higher wellbeing.</p><p>Thus, instead of using an RCT to find what works for the imaginary \"average\" person, we can learn what works for each individual and then carefully combine data to generalize the results to other real individuals.</p><p>To our knowledge, single-case experiments have not been implemented in a smartphone app format. We believe that a successful app will allow researchers to dramatically scale the number of participants in these studies.</p><p>Code available on <a href=\"https://github.com/mitmedialab/AffectiveComputingQuantifyMeAndroid\">GitHub</a>!&nbsp;</p>", "people": ["amohan@media.mit.edu", "fergusoc@media.mit.edu", "picard@media.mit.edu", "sataylor@media.mit.edu", "akanes@media.mit.edu"], "title": "QuantifyMe", "modified": "2018-05-01T18:27:59.835Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "quantifyme"}, {"website": "", "description": "<h1>Scalable Urban Infrastructure for Human-Machine Cohabitation</h1><h2><i>New infrastructure to help sustain public-sector participation and operation, and maximize public interest and safety.</i></h2><p>Advancements in autonomous technology have led automobile makers and tech companies to focus on reinventing the automobile\u2014increasing computational capability and enhancing sensor systems. But due to strict road-safety regulations, this vehicle-centric, inside-out approach may take years to materialize, and when it does, restricting \u201cautonomy\u201d to selected vehicles will limit autonomy\u2019s impact on street safety and accessibility.</p><p>To address current issues, The City Science group focuses on ways to offload often-heavy computational requirements from the vehicle through affordable interventions to street infrastructure by creating human-machine readable traffic signs and urban markers.</p><p>With the support of a new genre of smart urban infrastructure, we believe this \u201cautonomy-lite\u201d approach will soon allow lightweight autonomous vehicles to be widely deployed and navigate smoothly in most urban environments.&nbsp;</p>", "people": ["mcllin@media.mit.edu", "ptinn@media.mit.edu", "taiyu@media.mit.edu"], "title": "Urban Tattoo", "modified": "2018-03-28T22:18:15.132Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "smart-infra"}, {"website": "", "description": "<p>With networked cameras in everyone's pockets, we are exploring the practical and creative possibilities of public imaging. LensChat allows cameras to communicate with each other using trusted optical communications, allowing users to share photos with a friend by taking pictures of each other, or borrow the perspective and abilities of many cameras.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "LensChat: Sharing photos with strangers", "modified": "2019-04-19T18:29:00.238Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-320", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "lenschat-sharing-photos-with-strangers"}, {"website": "http://cp.media.mit.edu/agnis-stibe", "description": "<p>Persuasive Cities research is aimed at advancing urban spaces to facilitate societal changes. According to social science research, any well-designed environment can become a strong influencer of what people think and do. There is an endlessly dynamic interaction between a person, a particular behavior, and a specific environment. Persuasive Cities research leverages this knowledge to engineer persuasive environments and interventions for altering human behavior on a societal level. This research is focused on socially engaging environments for supporting entrepreneurship and innovation, reshaping routines and behavioral patterns in urban spaces, deploying intelligent outdoor sensing for shifting mobility modes, enhancing environmentally friendly behaviors through social norms, introducing interactive public feedback channels to alter attitudes at scale, engaging residents through socially influencing systems, exploring methods for designing persuasive neighborhoods, and fostering adoption of novel urban systems. More: http://bit.ly/TEDxp</p>", "people": ["kll@media.mit.edu", "rchin@media.mit.edu", "agnis@media.mit.edu"], "title": "Persuasive Cities", "modified": "2017-03-31T20:52:58.972Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "changing-places"], "published": true, "active": false, "end_on": null, "slug": "persuasive-cities"}, {"website": "", "description": "<p>The&nbsp;OpenAg\u2122&nbsp;<b>Tree Computer</b> is a food computer built to explore plant stress, health, longevity, and productivity across a wide range of crops.&nbsp;</p><p>The Tree Computer modulates&nbsp;<b>photosynthetic intensity</b>,&nbsp;<b>spectral range</b>,&nbsp;<b>wind velocity</b>,&nbsp;<b>nutrient levels, </b><b>soil and ambient temperature, and moisture content </b>in order&nbsp;to&nbsp;replicate growing conditions around the globe.&nbsp;</p><p>By reproducing actual and predicted weather patterns in near real time, the Tree Computer&nbsp;helps farmers determine what to plant, where&nbsp;to plant, when to harvest, and how to recover from drought, flood, frost, and disease.&nbsp;</p>", "people": ["calebh@media.mit.edu", "hildreth@media.mit.edu", "snowak@media.mit.edu", "poitrast@media.mit.edu", "zandera@media.mit.edu", "rebekahj@media.mit.edu", "delapa@media.mit.edu", "tsavas@media.mit.edu"], "title": "Tree Computer", "modified": "2019-04-16T14:37:15.732Z", "visibility": "PUBLIC", "start_on": "2017-10-10", "location": "", "groups": ["open-agriculture-openag"], "published": true, "active": false, "end_on": null, "slug": "tree-computer"}, {"website": "", "description": "<p>Can a person look at a portable display, click on a few buttons, and recover his or her refractive condition? Our optometry solution combines inexpensive optical elements and interactive software components to create a new optometry device suitable for developing countries. The technology allows for early, extremely low-cost, mobile, fast, and automated diagnosis of the most common refractive eye disorders: myopia (nearsightedness), hypermetropia (farsightedness), astigmatism, and presbyopia (age-related visual impairment). The patient overlaps lines in up to eight meridians, and the Android app computes the prescription. The average accuracy is comparable to the traditional method\u2014and in some cases, even better. We propose the use of our technology as a self-evaluation tool for use in homes, schools, and at health centers in developing countries, and in places where an optometrist is not available or is too expensive.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "NETRA: Smartphone add-on for eye tests", "modified": "2019-04-19T18:30:08.590Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "netra-smartphone-add-on-for-eye-tests"}, {"website": "", "description": "<p>Time-of-flight (ToF) cameras are commercialized consumer cameras that provide a depth map of a scene, with many applications in computer vision and quality assurance.  Currently, we are exploring novel ways of integrating the camera illumination and detection circuits with computational methods to handle challenging environments, including multiple scattering and fluorescence emission. </p>", "people": ["raskar@media.mit.edu", "achoo@media.mit.edu", "ajdas@media.mit.edu", "ayush@media.mit.edu", "michaf@media.mit.edu"], "title": "New methods in time-of-flight imaging", "modified": "2019-04-19T18:31:33.509Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "new-methods-in-time-of-flight-imaging"}, {"website": "http://nailo.media.mit.edu", "description": "<p>NailO is a wearable input device in the form of a commercialized nail art sticker. It works as a miniaturized trackpad the size and thickness of a fingernail that can connect to your mobile devices; it also enables wearers to customize the device to fit the wearer\u2019s personal style. NailO allows wearers to perform different functions on a phone or PC with different gestures, and the wearer can easily alter its appearance with a nail art design layer, creating a combination of functionality and aesthetics.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">From the fashion-conscious, to techies, and anyone in between, NailO can make a style, art, or a design statement; but in its more neutral, natural-looking example it can be worn and used only for its functionality. As a nail art sticker, NailO is small, discreet, and removable. Interactions through NailO can be private and subtle, for example attracting minimal attention when you are in a meeting but need to reply to an urgent text message. Mimicking the form of a cosmetic extension, NailO blends into and decorates one\u2019s body when attached, yet remains removable at the wearer\u2019s discretion, giving the wearer power and control over the level of intimacy of the device to one\u2019s body.</span></p>", "people": ["artemd@media.mit.edu", "joep@media.mit.edu", "cindykao@media.mit.edu", "geek@media.mit.edu"], "title": "NailO", "modified": "2017-03-27T21:12:03.637Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["living-mobile", "responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "nailo"}, {"website": "", "description": "<p>An alternative viewing experience for large-scale conversations on Twitter. Each conversation tree visualizes the structure of replies to a single tweet. The purpose of this project is to explore a hidden dimension of virality. Not all tweets that elicit a large volume of comments behave the same under the hood.&nbsp; These structural patterns, when combined with sentiment analysis and/or toxicity, can reveal a deeper story.</p>", "people": ["dkroy@media.mit.edu", "bridgitm@media.mit.edu", "bcroy@media.mit.edu"], "title": "Conversation Trees", "modified": "2019-04-18T14:50:10.698Z", "visibility": "PUBLIC", "start_on": "2019-04-17", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "conversation-trees-1"}, {"website": "", "description": "<p>As we know from our \u2018maker\u2019 classes and workshops, different capabilities and approaches can yield remarkable projects. Let\u2019s facilitate a \u2018smart\u2019 building that is at least as much about the people as the tech.</p><p>Because the walls and halls of E14 and E15 are subject to building codes and approvals, we decided to create our own walls that can contain and display, conceal or reveal, and in the true MIT innovative spirit, <i>move</i>. As a result, our ArtCube was born. Once we gather the remaining materials to finish construction, we will open for submissions (beginning of 2019).&nbsp;</p>", "people": ["csbishop@media.mit.edu", "lorrie@media.mit.edu"], "title": "Art Cube: the Ambulatory Art Exhibit", "modified": "2018-12-19T16:16:19.552Z", "visibility": "PUBLIC", "start_on": "2018-12-18", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "artcube"}, {"website": "", "description": "<p>We must proactively tackle the economic, social, and societal implications that accompany the widespread deployment of AI technology. In service to this goal, examining the evolution of AI research itself could provide a valuable input into models of AI's impact (e.g., models of the future of work).&nbsp;</p>", "people": ["cebrian@media.mit.edu", "mrfrank@media.mit.edu", "irahwan@media.mit.edu"], "title": "The Science of AI Research", "modified": "2019-03-13T17:23:51.009Z", "visibility": "PUBLIC", "start_on": "2018-05-01", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "the-science-of-ai-research"}, {"website": "", "description": "<p>DataViva made available data for the entire economy of Brazil, including exports and imports for each municipality and product, and occupation data for every municipality, industry, and occupation.</p><p>You can experience dataviva at legacy.dataviva.info</p>", "people": ["hidalgo@media.mit.edu"], "title": "DataViva", "modified": "2017-04-06T13:07:33.919Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "dataviva-new"}, {"website": "", "description": "<p><a href=\"https://www.media.mit.edu/projects/city-science-andorra/overview/\">View the main City Science Andorra project profile</a></p><p>With no airport or train service, most of the 8 million tourists who visit Andorra each year arrive by car, making traffic management and parking some of the country's most important challenges. We are currently developing different projects spanning from data science to the deployment of autonomous vehicles to help address these issues.<br></p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "mcllin@media.mit.edu", "doorleyr@media.mit.edu", "devisj@media.mit.edu", "kll@media.mit.edu", "ptinn@media.mit.edu", "ryanz@media.mit.edu"], "title": "Andorra | Mobility", "modified": "2017-10-25T05:56:26.309Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "andorra-mobility"}, {"website": "", "description": "<p>We are developing a system of early literacy apps, games, toys, and robots that will triage how children are learning, diagnose literacy deficits, and deploy dosages of content to encourage app play using a mentoring algorithm that recommends an appropriate activity given a child's progress. Currently, over 200 Android-based tablets have been sent to children around the world; these devices are instrumented to provide a very detailed picture of how kids are using these technologies. We are using this big data to discover usage and learning models that will inform future educational development.</p>", "people": ["cynthiab@media.mit.edu", "tinsley@media.mit.edu", "dnunez@media.mit.edu"], "title": "Global Literacy Tablets", "modified": "2017-06-05T16:10:26.858Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "global-literacy-tablets"}, {"website": "", "description": "<p>The aim of this project is to create an open source digital library&nbsp;with open data sets that cross link phenotypic response in plants (<i style=\"font-size: 18px; font-weight: 400;\">taste, nutrition, etc</i>) to environmental variables, biologic variables, genetic variables, and resources required in cultivation (<i style=\"font-size: 18px; font-weight: 400;\">inputs).</i>&nbsp;While plants can be altered genetically to produce different or more desirable traits, plants with the same genetics may naturally vary in color, size, texture growth rate, yield, flavor, and nutrient density depending on the environmental conditions in which they are grown.&nbsp;</p><p>Each specific set of conditions can be thought of as a \"Climate Recipe\" that produces unique phenotypic results. As users experiment with new Climate Recipes, their input data and phenotypic results will be recorded and filed in an open source digital platform so that it can be shared, borrowed, scaled up, and improved upon around the world, instantly.&nbsp;</p>", "people": ["calebh@media.mit.edu", "hildreth@media.mit.edu", "delapa@media.mit.edu", "tsavas@media.mit.edu"], "title": "Open Phenome Project", "modified": "2019-05-14T14:19:49.983Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["open-agriculture-openag"], "published": true, "active": false, "end_on": null, "slug": "open-phenome-project"}, {"website": "", "description": "<p>Our deformable camera exploits new, flexible form factors for imaging in turbid media. In this study we enable a brush-like form factor with a time-of-flight camera. This has enabled us to reconstruct images through a set of 1100 optical fibers that are randomly distributed and permuted in a medium.</p>", "people": ["raskar@media.mit.edu", "barmak@media.mit.edu"], "title": "Optical Brush: Enabling deformable imaging interfaces", "modified": "2019-04-19T18:32:51.175Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "optical-brush-enabling-deformable-imaging-interfaces"}, {"website": "http://rhythm.mit.edu/", "description": "<p>Rhythm is a collection of open-source tools to make it easier for researchers to examine, analyze, and augment human interaction.&nbsp;Rhythm includes hardware to measure face to face interaction, software platforms to quantify social dynamics from online videoconferencing, and analysis and visualization tools to craft interventions that affect social behavior. For more information, visit &nbsp;<a href=\"http://rhythm.mit.edu\">rhythm.mit.edu</a>, or our main <a href=\"https://github.com/HumanDynamics/openbadge\">github repository</a>.</p>", "people": ["amohan@media.mit.edu", "dcalacci@media.mit.edu", "orenled@media.mit.edu"], "title": "Rhythm: Open measurement and feedback tools for human interaction", "modified": "2019-01-21T17:59:43.190Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "rhythm"}, {"website": "http://www.judithamores.com", "description": "\u200b<blockquote><b>The lotus flower is an ancient symbol that has been associated with spiritual awakening or enlightenment.</b></blockquote><p>In yoga and meditation, the lotus flower has been used as a symbolic support for the mind. The heart can be visualized as a lotus flower unfolding at the center of the chest.&nbsp;In Egyptian mythology, Nefertem was the lotus god of healing and perfume. Inspired by these mythologies, symbols, and practices, we created <i>Lotuscent.</i></p>", "people": ["amores@media.mit.edu", "pattie@media.mit.edu"], "title": "Lotuscent: Targeted Memory Reactivation for Wellbeing using Scent and VR biofeedback", "modified": "2019-05-14T18:35:01.765Z", "visibility": "PUBLIC", "start_on": "2018-11-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "lotuscent"}, {"website": "", "description": "<p>We present a near-real-time system for interactively exploring a collectively captured moment without explicit 3D reconstruction. Our system favors immediacy and local coherency to global consistency. It is common to represent photos as vertices of a weighted graph. The weighted angled graphs of photos used in this work can be regarded as the result of discretizing the Riemannian geometry of the high dimensional manifold of all possible photos. Ultimately, our system enables everyday people to take advantage of each others' perspectives in order to create on-the-spot spatiotemporal visual experiences similar to the popular bullet-time sequence. We believe that this type of application will greatly enhance shared human experiences, spanning from events as personal as parents watching their children's football game to highly publicized red-carpet galas.</p>", "people": ["raskar@media.mit.edu", " otkrist@media.mit.edu", "naik@media.mit.edu"], "title": "PhotoCloud: Personal to shared moments with angled graphs of pictures", "modified": "2019-04-19T18:33:30.110Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "photocloud-personal-to-shared-moments-with-angled-graphs-of-pictures"}, {"website": "", "description": "<p>During the last few decades two important intellectual contributions have reshaped our understanding of international trade. On the one hand, work emphasizing trade frictions and extended gravity models has shown that countries trade more with those with whom they share a language, colonial past, or ethnic social relationships. This is interpreted as evidence of trade not being only about differences in factor endowments and transportation costs, but the result of complex social processes where information frictions and social networks play a key role. On the other hand, work emphasizing knowledge diffusion has shown that the probability that a country starts exporting a product increases with the number of related products it already exports. Yet, despite the importance of these two recent findings, little is known about their intersection: does knowledge on how to export to a destination also diffuses among related products? Here, we use bilateral trade data from 2000 to 2015, disaggregated into 1,242 product categories, to create an extended gravity model of bilateral trade that reproduces previous findings (effects of language, distance, colonial past, etc.) and shows that, in addition to these, countries are more likely to increase their exports of a product to a destination when: (i) they export related products to it, (ii) already export that product to some of its neighbors, and (iii) have neighbors who also export the same product to that destination. We interpret these findings as evidence of knowledge diffusion among related products and among geographic neighbors, both in the context of exporters and importers. Then, we explore the magnitude of these effects for new, nascent, and experienced exporters, and also, for groups of products classified according to Lall's technological classification of exports. We find that the effects of product and geographic relatedness are stronger for new exporters, and also, that the effect of product relatedness increases with the technological sophistication of products. These findings support the idea that international trade is shaped by knowledge and information frictions that are partially reduced in the presence of product relatedness.</p>", "people": ["gaojian@media.mit.edu", "aamena@media.mit.edu", "hidalgo@media.mit.edu", "bjun@media.mit.edu"], "title": "Relatedness, Knowledge Diffusion, and the Evolution of Bilateral Trade", "modified": "2017-10-11T16:57:36.018Z", "visibility": "PUBLIC", "start_on": "2017-03-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "relatedness-knowledge-diffusion-and-the-evolution-of-bilateral-trade"}, {"website": "", "description": "", "people": [], "title": "TEst 3", "modified": "2017-10-13T16:16:37.221Z", "visibility": "LAB", "start_on": "2017-10-12", "location": "", "groups": ["emerging-worlds"], "published": false, "active": false, "end_on": null, "slug": "test-3"}, {"website": "", "description": "<p>We demonstrate a new technique that allows a camera to rapidly acquire reflectance properties of objects \"in the wild\" from a single viewpoint, over relatively long distances and without encircling equipment. This project has a wide variety of applications in computer graphics, including image relighting, material identification, and image editing.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "Reflectance acquisition using ultrafast imaging", "modified": "2019-04-19T18:34:19.350Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "reflectance-acquisition-using-ultrafast-imaging"}, {"website": "", "description": "<p>Calliope was designed by building on the lessons learnt from  the NeverEnding Drawing Machine.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">Rather than a static system that lives indoors,  Calliope was inspired by the portability of \"en plein air\" painting and the pochade box that made it possible.  Thinking of \"the world as your palette,\" Calliope is a portable,  paper-based platform for interactive, networked story making which allows physical editing of shared digital media at a distance.&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">With Calliope, we shrank the size and cost by using a system of mirrors and the availability of pocket projectors. We were also interested in exploring the difference between a system that allowed \"from many to many\" collaboration to a more intimate \"one to one\" design.&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">Like the Never-Ending Drawing Machine, Calliope is composed of networked \"creation-stations\" that seamlessly blend analog and digital media which  uses the page-turning book format to synchronize networked co-creation.</span></p><p>When using the Never-Ending Drawing Machine, we noticed people had trouble pressing the \"big red button\" since their hands were mostly busy. Calliope substituted the button for a pedal to be pressed. Unlike the Never-Ending Drawing Machine, Calliope uses human-readable tags, designed as dominoes, which can be drawn directly onto the paper with a marker by the user.</p><p>One of the most valuable outcomes of blending analog and digital media, was the ability to save every version, allowing to then explore the process of creation. The NEDM though did not have a way that the user could access this without having to interface with the computer's file system. For Calliope, we designed a tag which, upon placing it over the desired page, lets you see all the versions that came before the last one.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">Furthermore, Calliope now can record audio! When the \"rooster\" tag is placed, one can record onto that page and play back, extending the palette to the aural realms. </span><span style=\"font-size: 18px; font-weight: normal;\">The intention remains: to offer opportunities for cross-cultural and cross-generational collaboration among peers with expertise in different media.</span></p>", "people": ["edwinapn@media.mit.edu", "vmb@media.mit.edu"], "title": "Calliope", "modified": "2017-04-05T01:17:25.679Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "calliope"}, {"website": "http://www.OrganicPrimitives.com", "description": "<p>A large portion of the chemical and biological processes underlying our everyday experience remains imperceptible to us. Be it the contents of rain, the ocean, or human tears, chemical codes mediate interactions between organic systems from the environment to our bodies and food. <br></p><p>As humans, we understand information mediated by our senses\u2014through textures, symbols, odors, and tastes. In order to design for a wider array of sensory modalities in representing fluid-based information and enable user interaction with these systems, we have developed Organic Primitives. It is a new medium for transforming objects into information displays. Chemical input is converted into human senses through a set of color-, odor-, and form-changing materials. <br></p>", "people": ["kakehi@media.mit.edu", "viirj@media.mit.edu"], "title": "Organic Primitives", "modified": "2017-10-26T00:15:41.352Z", "visibility": "PUBLIC", "start_on": "2014-11-23", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "organic-primitives"}, {"website": "", "description": "<p>DUSK was created as part of the Media Lab's Advancing Wellbeing initiative (supported by the Robert Wood Johnson Foundation) to create private, restful spaces for people in the workplace. DUSK promotes a vision of a new type of \"nap pod,\" where workers are encouraged to use the structure on a daily basis for regular breaks and meditation. The user is provided with the much-needed privacy to take a phone call, focus, or rest inside the pod for short periods during the day. The inside can be silent, or filled with binaural beats audio; pitch black, or illuminated by a sunlamp; whatever works for users to get the rest and relaxation needed to continue to be healthy and productive. DUSK is created with a parametric press-fit design, making it scalable and suitable for fabrication customizable on a per-user basis.</p>", "people": ["bdatta@media.mit.edu", "vmb@media.mit.edu"], "title": "DUSK", "modified": "2017-04-05T01:19:27.607Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "dusk"}, {"website": "", "description": "<p>The biggest problem facing artificial intelligence today is how to teach computers enough about the everyday world so that they can reason about it like we do\u2014so that they can develop \"common sense.\" We think this problem may be solved by harnessing the knowledge of people on the Internet, and we have <a href=\"http://openmind.media.mit.edu\">built a website</a> to make it easy and fun for people to work together to give computers the millions of little pieces of ordinary knowledge that constitute \"common sense.\" Teaching computers how to describe and reason about the world will give us exactly the technology we need to take the Internet to the next level, from a giant repository of web pages to a new state where it can think about all the knowledge it contains; in essence, to make it a living entity.</p>", "people": ["havasi@media.mit.edu", "lieber@media.mit.edu", "eslick@media.mit.edu"], "title": "Open Mind Common Sense", "modified": "2019-02-04T21:50:32.875Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-383", "groups": ["digital-intuition"], "published": true, "active": false, "end_on": null, "slug": "open-mind-common-sense"}, {"website": "", "description": "<p>We have created a 3D motion-tracking system with automatic, real-time vibrotactile feedback and an assembly of photo-sensors, infrared projector pairs, vibration motors, and a wearable suit. This system allows us to enhance and quicken the motor learning process in a variety of fields such as healthcare (physiotherapy), entertainment (dance), and sports (martial arts).</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "Second Skin: Motion capture with actuated feedback for motor learning", "modified": "2019-04-19T18:35:28.951Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "second-skin-motion-capture-with-actuated-feedback-for-motor-learning"}, {"website": "", "description": "<p>Seventy percent of nations have deep-sea environments within their maritime Exclusive Economic Zones (EEZs), yet only 16 percent of them are able to explore those environments. This is especially true for less economically developed countries. The dearth of technological capability and knowledge leads to a lack of exploration, inappropriate or inadequate management decisions, and unaware populations. Our goal is to empower countries to explore their own deep-sea backyards using low-cost technology, while building lasting in-country capacity. </p><p>Our project takes place in two small island developing states\u2014the Republic of Kiribati, and Trinidad and Tobago. It utilizes Deep-Sea Drop Cameras developed by National Geographic\u2019s Exploration Technology team (ExTech) and OpenROV\u2019s Trident Remotely Operated Vehicles. Both technologies collect compelling imagery, but require minimal resources and expertise. In our pilot study during summer 2018, an engineer from ExTech and another team member traveled to each country to train a group of scientists, students, and communicators in the use of these technologies, which are to be left in-country until a scientist, a student, and a communicator from each country travel to the USA for further training in data analysis and creating outreach materials.</p>", "people": ["katybell@media.mit.edu", "simun@media.mit.edu", "ahope@media.mit.edu"], "title": "My Deep Sea, My Backyard", "modified": "2018-10-22T17:59:56.835Z", "visibility": "PUBLIC", "start_on": "2018-04-02", "location": "", "groups": ["open-ocean"], "published": true, "active": false, "end_on": null, "slug": "my-deepsea-mybackyard"}, {"website": "", "description": "", "people": [], "title": "ImmerSound", "modified": "2017-03-31T20:56:15.586Z", "visibility": "PUBLIC", "start_on": "2016-10-08", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "immersound"}, {"website": "", "description": "<p>This work focuses on bringing powerful concepts from wave optics to the creation of new algorithms and applications for computer vision and graphics. Specifically, ray-based, 4D lightfield representation, based on simple 3D geometric principles, has led to a range of new applications that include digital refocusing, depth estimation, synthetic aperture, and glare reduction within a camera or using an array of cameras. The lightfield representation, however, is inadequate to describe interactions with diffractive or phase-sensitive optical elements. Therefore we use Fourier optics principles to represent wavefronts with additional phase information. We introduce a key modification to the ray-based model to support modeling of wave phenomena. The two key ideas are \"negative radiance\" and a \"virtual light projector.\" This involves exploiting higher dimensional representation of light transport.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "Theory unifying ray and wavefront lightfield propagation", "modified": "2019-04-19T18:36:26.091Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-320", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "theory-unifying-ray-and-wavefront-lightfield-propagation"}, {"website": "", "description": "", "people": [], "title": "Modeling Human Ad Hoc Coordination", "modified": "2017-03-27T16:39:47.583Z", "visibility": "PUBLIC", "start_on": "2017-03-27", "location": "", "groups": ["human-dynamics"], "published": false, "active": false, "end_on": null, "slug": "modeling-human-ad-hoc-coordination"}, {"website": "", "description": "<h2><b>EEEeb Spring 2019:&nbsp;&nbsp;Urban Oceans</b></h2><p>March 24,&nbsp;April 7 and 21,&nbsp;May 19,&nbsp;June 2&nbsp;<br><span style=\"font-size: 18px; font-weight: 400;\">To register, please </span><a href=\"https://www.eventbrite.com/e/ecology-evolution-and-engineering-for-empowered-brains-spring-2019-tickets-54681164836\" style=\"font-size: 18px; font-weight: 400;\">visit this link</a><span style=\"font-size: 18px; font-weight: 400;\">.&nbsp;</span></p><p>Sponsored and run by members of the MIT Media Lab and the <a href=\"http://www.empoweredbrain.org/\">Empowered Brain Institute</a>,&nbsp;<i>Ecology, Evolution, and Engineering for Empowered Brains</i> is an eight-week, sensory-friendly series of related educational workshops for neurodiverse individuals (ages 8 - 14) which aims to hone skills in understanding, interpreting, and protecting the natural environment. Through creative, hands-on teaching exercises and field visits, participants become comfortable with basic ecological principles, as well as emerging technologies used to sculpt ecological and evolutionary processes. We discuss contemporary issues related to conservation and highlight engineering strategies with which to address these obstacles. Through project-based learning, students will have the opportunity to develop understanding by experimentation\u2014or play\u2014and workshops will emphasize immersion, rather than memorization.  Wholly, we seek to foster a safe and creative learning space in which students are able to develop the necessary technical literacy to become future leaders in the myriad realms of environmental science.&nbsp;</p><p>For questions, please contact Avery Normandin (ave@media.mit.edu).<br></p>", "people": ["devora@media.mit.edu", "jvanzak@media.mit.edu", "ave@media.mit.edu", "pcuellar@media.mit.edu"], "title": "Ecology, Evolution, and Engineering for Empowered Brains", "modified": "2019-01-10T16:02:03.396Z", "visibility": "PUBLIC", "start_on": "2019-01-10", "location": "", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "empowered-brains"}, {"website": "", "description": "<p>Text here.</p>", "people": [], "title": "Test workshop project", "modified": "2017-04-06T21:00:44.644Z", "visibility": "LAB-INSIDERS", "start_on": "2017-04-06", "location": "", "groups": ["ce-20"], "published": false, "active": false, "end_on": null, "slug": "test-workshop-project"}, {"website": "http://www.sculptingevolution.org/daisydrives", "description": "<p>Who should decide whether, when, and how to alter the environment? These are hard questions, especially when the decisions will impact people in many different communities or nations. Daisy drive systems may help by empowering local communities to make decisions concerning their local environments without imposing them on anyone else.<br></p><p>The problem with current CRISPR-based gene drive systems is that they can spread indefinitely\u2014potentially affecting every population of the target species throughout the world. It's unclear how such \"global\" drives can be safely tested, much less whether nations will ever agree to use them.&nbsp;To return power to the hands of local communities, we devised a new form of drive system called a \"daisy drive\" that can only affect local environments. The trick was to teach DNA to count&nbsp;and limit gene drive spreading to a pre-programmed number of generations.&nbsp;We hope that daisy drives will simplify decision-making and promote responsible use by allowing local communities to decide how to solve their own ecological problems.</p>", "people": ["buchthal@media.mit.edu", "esvelt@media.mit.edu"], "title": "Daisy Drives", "modified": "2019-04-03T16:07:53.609Z", "visibility": "PUBLIC", "start_on": "2016-06-06", "location": "", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "daisydrives"}, {"website": "http://www.jinjoolee.com", "description": "<h2>Machine learning model outperforms human judgement</h2><p>Our computational trust model is capable of predicting\u2014above human accuracy\u2014the degree of trust a person has toward a stranger by observing the nonverbal behaviors&nbsp;expressed in their social interaction. We used machine learning algorithms, specifically hidden Markov models (HMMs), to model the temporal relationship between specific nonverbal behaviors. By interpreting its resulting learned structure, we discovered that the sequence of low and high trusting behaviors a person emits provides further information of their trust orientation toward their partner. These discoveries shaped the feature engineering process that enabled a support vector machine (SVM) model to achieve a prediction performance more accurate than human judgment.</p>", "people": ["cynthiab@media.mit.edu", "picard@media.mit.edu", "jinjoo@media.mit.edu"], "title": "Predicting Trust Between People", "modified": "2018-05-07T17:33:56.290Z", "visibility": "PUBLIC", "start_on": "2013-12-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "predicting-trust-between-people"}, {"website": "", "description": "<p>We present a novel framework to extend the dynamic range of images called Unbounded High Dynamic Range (UHDR) photography with a modulo camera. A modulo camera could theoretically take unbounded radiance levels by keeping only the least significant bits. We show that with limited bit depth, very high radiance levels can be recovered from a single modulus image with our newly proposed unwrapping algorithm for natural images. We can also obtain an HDR image with details equally well preserved for all radiance levels by merging the least number of modulus images. Synthetic experiments and experiments with a real modulo camera show the effectiveness of the proposed approach.</p>", "people": ["raskar@media.mit.edu", "hangzhao@media.mit.edu", "naik@media.mit.edu", "shiboxin@media.mit.edu"], "title": "Unbounded high dynamic range photography using a modulo camera", "modified": "2019-04-19T18:38:01.471Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "unbounded-high-dynamic-range-photography-using-a-modulo-camera"}, {"website": "", "description": "<p>The MIT Media Lab Space Exploration Initiative is pushing the boundaries of space exploration innovation across the Media Lab\u2019s many healthcare-focused and healthcare-adjacent research thrusts. The program focuses on the protection, long-duration preservation, and predictive adaptation of life beyond the earthbound, integrated throughout the Space Exploration Initiative\u2019s portfolio.&nbsp;This research platform builds towards a near future where space-faring humans can both survive and thrive wherever the future takes us\u2014back and forth to the surface of Earth, to low Earth orbit, to the moon, Mars, and beyond.&nbsp;</p><p>In addition to technology for the sake of deep space exploration, we note a dual opportunity for the technologies developed as part of this program to enhance and improve healthcare treatment regimens on Earth, in the long tradition of NASA spinoffs yielding benefit to wider populations. We aim to jumpstart a creative, interdisciplinary approach to space healthcare innovation, in the spirit of the Space Exploration Initiative\u2019s cross-cutting technological development across multiple fields (from synthetic neurobiology to human-robotic interaction and AI).</p><h2>Research Areas<br></h2><br><p><b>Neural Oscillations.&nbsp;</b>The Alzheimer's research aims to use non-invasive sensory-induced gamma entrainment to attenuate space-induced physiological impairments. On an upcoming parabolic flight, we&nbsp;will test the effect of microgravity on gamma entrainment in humans. Mouse models will be used to evaluate the microglial response to sensory-mediated gamma entrainment. Microglia are macrophage-like immune cells in the central nervous system that scavenge extracellular debris, prune synapses, and support neural function.&nbsp;</p><p><b>Tardigrade Cryptobiosis.&nbsp;</b>Explores the feasibility of reverse engineering Tardigrade species cryptobiosis capabilities (i.e., desiccation, radiation and temperature-swing resistance) into other organisms, to \u201cflip the paradigm\u201d of organism survival in space\u2014make the organism inherently space-tolerant, rather than relying strictly on life support systems.</p><p><b><a href=\"https://www.media.mit.edu/projects/space-food/overview/\">Space Food</a>.&nbsp;</b>How can we best meet the nutritional, performance, and emotional needs of astronauts through food? Our space food research area aims to address the unique challenges associated with eating in space\u2014from the microbiome scale to the \u201cenvirome\u201d scale\u2014including fermentation and probiotics,&nbsp; improving waning or shifting appetite, and&nbsp;preservation of freshness and nutrient quality.&nbsp;This research area explicitly addresses earth-based markets as well, as the foods&nbsp; and eating experiences developed for space can be re-used in many Earth contexts.&nbsp;</p><p><b><a href=\"https://www.media.mit.edu/projects/social-robots-in-zero-gravity-scenarios/overview/\">Personal Robots in Space</a>.&nbsp;</b>Can we enable social connectivity between astronauts and people on Earth through an embodied agent?&nbsp;While in zero gravity, the embodied social agent interacts with people on cognitive, creative, and social tasks with varying degrees of proactive behavior. We collect physiological, audio, and video data of the experience as individuals complete a series of tasks with the agent with the goal of designing agents that can enable us to be more socially connected.</p><p><b><a href=\"https://www.media.mit.edu/projects/SensorySynchrony/overview/\">Sensory Synchrony</a>.&nbsp;</b>The primary goal of this research project is to investigate vestibular system stimulation techniques to combat motion sickness and create more intuitive experiences when being in a non-natural gravity environments.&nbsp;A prototype built for multipole vestibular stimulation for simulating acceleration in roll and pitch axis will be tested on the upcoming zero gravity flight for minimizing the effects of alterations between micro and hyper gravity phases.</p><p><b><a href=\"https://www.media.mit.edu/projects/physio-freefall/overview/\">Physio Freefall</a>.&nbsp;</b>This project seeks to examine the effects of altered gravity on an individual\u2019s physiology during parabolic flight. Specifically, we will collect flight participants\u2019 heart rate, heart rate variability, breathing rate, skin temperature, and skin conductance measurements using wearable, wireless sensors in order to determine the response of these biosignals to zero/hyper/microgravity and feelings of nausea.</p><p><b><a href=\"https://www.media.mit.edu/projects/mediated-atmospheres-in-space/overview\">Mediated Atmospheres in Space</a>.&nbsp;</b>Designing the atmosphere and sensorial qualities of physical space can have a remarkable influence on human experience and behavior. This project envisions a workspace or space station that is capable of dynamically transforming to enhance occupants\u2019 work experience and cognitive ability, via both subtle and overt customizations tailored to bio-signal inputs.</p><h2><b>Testing and Product Development</b></h2><p><b>Zero Gravity Flight: </b>We charter an annual parabolic flight (20 zero-g parabolas, 25 researchers, 15 experiments), with a focus on prototypes uniquely designed for the affordances of microgravity.&nbsp;</p><p><b>Research Collaboration with NASA Translational Research Institute for Space Health (TRISH):</b>&nbsp;We are actively developing a suite of health-focused prototypes that will mediate human interaction with interior space habitats to improve cognitive performance and overall wellness.&nbsp;</p><p><b>&nbsp;ISS Interior and External Deployment Tests:</b> Coming in the next 12 months.&nbsp;</p>", "people": ["aekblaw@media.mit.edu"], "title": "Space Health", "modified": "2019-05-30T13:00:55.855Z", "visibility": "PUBLIC", "start_on": "2019-05-01", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "space-health"}, {"website": "https://www.maggiecoblentz.com/", "description": "<p>The human experience of food extends far beyond that of mere nourishment. Debriefs with astronauts tell us that food is a key creature comfort in spaceflight, and it will play an even more significant role for long duration space travel and future life in other orbits. How can we best meet the nutritional, performance, and emotional needs of astronauts through food? Our Space Food and Sensory Experiences research area aims to address the unique challenges associated with eating in space\u2014from the microbiome scale to the \u201cenvirome\u201d scale\u2014including fermentation and probiotics, preservation of freshness and nutrient quality, improving waning or shifting appetite, food stimuli (e.g., spice) to counteract loss of smell, varying the food forms and packaging interaction for increased interest in eating, and communal, cultural experience sharing for mental health and wellbeing. This research area explicitly addresses earth-based markets as well as how the foods, packaging, and eating experiences developed for space can be re-used in many Earth contexts.<br></p><p><b>Research Areas</b></p><p><b>Social Wellbeing.</b> Immersive augmented reality experiences can promote wellbeing and enhance the experience of eating. Recent space food workshops at SEI with veteran astronauts Paolo Nespoli (ESA) and Cady Coleman (NASA ret.) proved that VR was a highly effective and emotive tool to connect to home and engage with food. For example, homesick astronauts could visit family and friends at their favorite restaurant on Earth by way of a virtual, communal dinner gathering. We are designing augmented reality experiences to pair with space food.</p><p><b>Nutritional Science and Edible Forms. </b>We are exploring 3D printing to create new food forms in space, starting from a suite of highly nutritious or taste-stimulating basic ingredients. Innovative packaging materials (based on polyethylene) can be recycled in a new process we are developing, to transform all non-food waste into a 3D printer source of material as well.</p><p><b>Appetite. </b>Many astronauts suffer from loss of appetite in outer space. Appetite is influenced by cross-modal sensory interactions, the physiological elements of oral processing, mood, and food structure and texture. We are designing a) foods and holistic eating experiences to help increase appetite and fulfill nutritional requirements and b) multi-sensory, pre-meal experiences to promote the anticipation of eating and stimulate digestion (e.g., the smell of onions frying, or sounds of cooking preparation). In addition, we are compiling a \u201cZero Gravity Cookbook\u201d to gather space-tested and newly proposed recipes from astronauts and chefs around the world.&nbsp;</p><p><b>Prototypes, Testing, and Product Development</b></p><p>A tasting menu will be deployed on upcoming zero gravity flights and shared with ISS-mission-veteran astronauts for taste testing and further development. We are also developing fermentation food products for longer term food consumption and habitation in space.</p><p>Stay tuned for results from our zero gravity flight, upcoming Blue Origin suborbital launches, and deployments on the ISS. </p><p>For inquiries about the research, please direct questions to <a href=\"https://www.media.mit.edu/people/mcoblent/overview/\">Maggie Coblentz</a> (mcoblent@media.mit.edu) and <a href=\"https://www.media.mit.edu/people/aekblaw/overview/\">Ariel Ekblaw</a>&nbsp;(aekblaw@media.mit.edu).&nbsp;</p>", "people": ["mcoblent@media.mit.edu"], "title": "Space Food and Sensory Experiences", "modified": "2019-06-07T19:51:33.238Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "space-food"}, {"website": "", "description": "<p>The&nbsp;<a style=\"font-size: 18px; font-weight: 400;\" href=\"http://mfc.mit.edu/\">Mobility Futures Collaborative</a>&nbsp;in the MIT Department of Urban Studies and Planning (DUSP) and the&nbsp;<a style=\"font-size: 18px; font-weight: 400;\" href=\"http://cp.media.mit.edu/\">Changing Places group</a>&nbsp;at the MIT Media Lab have developed new interactive tools aimed to better communicate the possible impacts of new transit systems. The Media Lab and DUSP have partnered with the&nbsp;<a style=\"font-size: 18px; font-weight: 400;\" href=\"https://www.barrfoundation.org/\">Barr Foundation</a>&nbsp;to test these tools in a series of community engagement workshops to examine the impacts of Bus Rapid Transit (BRT) systems in greater Boston. These tools include the&nbsp;<a style=\"font-size: 18px; font-weight: 400;\" href=\"http://cp.media.mit.edu/city-simulation/\">CityScope</a>&nbsp;\u2014 an interactive platform that utilizes physical models (built from LEGO bricks) and 3-D projection \u2014 to enable community members to engage in neighborhood and street-level decisions including alternative bus corridor designs and station-level variations (such as pre-pay boarding). The second tool,&nbsp;<a style=\"font-size: 18px; font-weight: 400;\" href=\"http://mfc.mit.edu/innovations-participatory-design-brt-systems\">CoAXs</a>&nbsp;is a new interactive platform for collaborative transit planning that builds on open-source urban analytics tools such as&nbsp;<a style=\"font-size: 18px; font-weight: 400;\" href=\"http://conveyal.com/projects/analyst/\">Conveyal Transport Analyst</a></p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu", "noyman@media.mit.edu", "rchin@media.mit.edu", "ptinn@media.mit.edu"], "title": "CityScope Boston BRT", "modified": "2017-10-16T18:32:03.412Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "CityscopeBostonbrt"}, {"website": "http://www.haewonpark.com/", "description": "<h2>Realtime detection of social cues in children\u2019s voices</h2><p>In everyday conversation, people use what are known as backchannels to signal to someone that they are still listening, paying attention, and engaged. As listeners, we smile, nod, and say \u201cuh-huh\u201d to convey attentiveness, and we do this naturally with little thought. We give this feedback not randomly but at certain moments in the conversation because speakers give off social cues that signal upcoming backchanneling opportunities.</p>", "people": ["cynthiab@media.mit.edu", "jinjoo@media.mit.edu", "gelso@media.mit.edu", "haewon@media.mit.edu"], "title": "Realtime Detection of Social Cues", "modified": "2018-05-07T17:34:51.716Z", "visibility": "PUBLIC", "start_on": "2017-03-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "realtime-detection-of-social-cues"}, {"website": "", "description": "<p>We present Open Badges, an open-source framework and toolkit for measuring and shaping face-to-face social interactions using either custom hardware devices or smart phones, and real-time web-based visualizations. Open Badges is a modular system that allows researchers to monitor and collect interaction data from people engaged in real-life social settings.</p>", "people": ["amohan@media.mit.edu", "sandy@media.mit.edu", "orenled@media.mit.edu"], "title": "Open Badges", "modified": "2019-01-10T19:19:48.517Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["connection-science", "human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "open-badges"}, {"website": "http://www.jinjoolee.com", "description": "<p>Emotion recognition modeled as a goal-directed process!</p>", "people": ["cynthiab@media.mit.edu", "jinjoo@media.mit.edu"], "title": "Intentional Inference of Emotions", "modified": "2017-10-16T14:31:54.022Z", "visibility": "PUBLIC", "start_on": "2016-10-18", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "intentional-inference-of-emotions"}, {"website": "", "description": "<p>We know that it's groups, not individuals, that are capable of the most complex and daunting achievements. Why should AI be any different?&nbsp;</p><p>We show that deep reinforcement learning algorithms that use lessons from how humans learn and communicate with each other can provide large improvements over state of the art reinforcement learning methods.</p><p>Researchers have been studying how groups of problem-solvers organize themselves and communicate for years, under the field of \"collective intelligence.\" It's been shown that there are some surprisingly simple relationships between a group's communication network structure and how well that group is able to perform on different kinds of tasks.&nbsp;</p><p>Using these simple lessons, we designed a deep reinforcement learning algorithm that, instead of using one massive neural network (NN), leverages a community of many smaller NNs. We enable these neural networks to communicate with one another, and to learn from each others' explorations and successes.&nbsp;</p><p>Using this strategy yields significant improvements over the state of the art. By placing these neural nets on a communication network that is similar in structure to how humans communicate,&nbsp;we see a 33% improvement in how fast the networks are able to learn and in how well they are able to perform at a benchmark reinforcement learning task.</p><p><br></p>", "people": ["pkrafft@media.mit.edu", "dubeya@media.mit.edu", "dcalacci@media.mit.edu", "yleng@media.mit.edu", "dhaval@media.mit.edu"], "title": "Deep Reinforcement Learning Inspired by Human Collective Intelligence", "modified": "2018-05-09T14:23:16.868Z", "visibility": "LAB-INSIDERS", "start_on": "2017-07-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "deep-reinforcement-learning-inspired-by-human-collective-intelligence"}, {"website": "", "description": "<p>Definition:</p><p><i>combining form</i></p><p>Prefix: <b>allo-</b></p><ol><li>other; different.<br>\"allopatric\"</li></ol><p><b>im\u00b7ag\u00b7i\u00b7na\u00b7tion</b></p><p>noun: imagination; plural noun: imaginations</p><ol><li>the faculty or action of forming new ideas, or images or concepts of external objects not present to the senses.</li></ol><p><b>allo-i</b><b>(s)&nbsp;</b>is a project exploring alternative imaginations through the use of interactive, immersive experiences.</p><p>How can we leverage new technologies in low-cost environments to create location based experiences similar to theme-parks? Arwa is developing a mixed reality tool kit that can be used by creators to create immersive location based experiences at a low cost. She plays with projection mapping, AR/VR/XR, sensor technologies, theatre design, and narrative/story structures.</p><p>We further investigate the socio-economic and mental health impact of immersive spaces in low income communities across Africa.&nbsp;</p>", "people": ["mboya@media.mit.edu"], "title": "allo-i(s)", "modified": "2019-04-22T18:15:04.065Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "allo-i-s"}, {"website": "", "description": "", "people": [], "title": "Training project the first", "modified": "2018-11-08T15:18:15.398Z", "visibility": "LAB", "start_on": "2018-11-08", "location": "", "groups": ["code-next"], "published": false, "active": false, "end_on": null, "slug": "training-project-the-first"}, {"website": "", "description": "<h2>How can we add the missing \"T\" and \"E\" in preschool STEAM education?</h2>", "people": ["cynthiab@media.mit.edu", "randiw12@media.mit.edu", "haewon@media.mit.edu"], "title": "PopBots: An early childhood AI curriculum", "modified": "2019-05-02T20:11:55.104Z", "visibility": "PUBLIC", "start_on": "2016-10-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "pop-kit"}, {"website": "", "description": "<p>The Huggable is a new type of robotic companion for health care, education, and social communication applications. The Huggable is much more than a fun, interactive robotic companion; it functions as an essential team member of a triadic interaction. Therefore, the Huggable is not meant to replace any particular person in a social network, but rather to enhance it. The Huggable is being designed with a full-body sensitive skin with over 1,500 sensors, quiet back-drivable actuators, video cameras in the eyes, microphones in the ears, an inertial measurement unit, a speaker, and an embedded PC with 802.11g wireless networking. An important design goal for the Huggable is to make the technology invisible to the user. You should not think of the Huggable as a robot but rather as a richly interactive teddy bear.</p>", "people": ["cynthiab@media.mit.edu"], "title": "Huggable: A Robotic Companion for Long-Term Health Care, Education, and Communication", "modified": "2017-06-05T16:14:52.532Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-468", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "huggable-a-robotic-companion-for-long-term-health-care-education-and-communication"}, {"website": "", "description": "", "people": [], "title": "collaborator test", "modified": "2018-05-07T17:39:44.669Z", "visibility": "LAB", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "collaborator-test"}, {"website": "https://findingplaces.hamburg/", "description": "<h1><b>What is FindingPlaces?</b></h1><p>In reaction to the sudden arrival of tens of thousands of refugees in the city of Hamburg (DE) in 2015, the Lord Mayor requested the CityScienceLab (CSL) at HafenCity University to facilitate a public discussion and decision-making process on locations for refugee accommodation in Hamburg neighborhoods. With highly sensitive socio-political implications, this project demanded a well-designed technological and procedural approach. CSL employed an innovative Human-Computer Interaction tool, CityScope, to facilitate public participation and urban decision-making. A workshop process was also designed to help multiple participants and stakeholders interact effectively. Running from May to July 2016, the FindingPlaces (FP) project enabled about 400 participants to identify 160 locations accepted by Hamburg\u2019s citizens, out of which 44 passed legal confirmation by the authorities. Overall, on a qualitative level, the project facilitated surprisingly constructive and collaborative interaction, raising awareness and a sense of ownership among participants.</p>", "people": ["kll@media.mit.edu", "noyman@media.mit.edu"], "title": "FindingPlaces", "modified": "2019-02-12T19:05:58.708Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "finding-places"}, {"website": "", "description": "<p>Swarm robotics traditionally have relied on autonomous organization of swarm robots using localization algorithms and self-actuation. In this project, we introduce and explore a new human-machine paradigm where humans (specially children, in the context of this project) organize and \"actuate\" the swarm units to solve specific educational tasks, and the swarms infer their group's spatial configuration and sense individual interactions with the child to provide feedback on learning/educational outcomes. By giving the child autonomy to manipulate the spatial configuration, we explore a shared cognitive paradigm wherein children and swarms work together to learn.</p>", "people": ["saquib@media.mit.edu"], "title": "Human-organized swarms", "modified": "2019-04-19T18:39:54.427Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "human-organized-swarms"}, {"website": "", "description": "<p>Presenting RFind, a new technology that allows us to locate almost any object with extreme accuracy by transforming low-cost, battery-free wireless stickers into powerful radars. At a high level, our technology operates by measuring the time it takes the signal to travel from the wireless sticker to an access point. By taking into account the speed of propagation of light, we can then map the time to an exact location (with sub-centimeter precision) in 3D space.</p>", "people": ["nselby@media.mit.edu", "fadel@media.mit.edu", "yunfeima@media.mit.edu"], "title": "RFind: Extreme localization for billions of items", "modified": "2018-10-19T14:06:36.714Z", "visibility": "PUBLIC", "start_on": "2016-12-04", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "rfid-localization"}, {"website": "http://electome.org/", "description": "<p><b>The Electome: Where AI meets political journalism</b><br></p><p>The Electome project is a machine-driven mapping and analysis of public sphere content and conversation associated with the 2016 presidential election campaign. Through its unprecedented view of the national election conversation, LSM aims to shift some of our collective focus from who\u2019s winning/losing (traditional \u201chorse race\u201d polls and projections) to the issues the campaign is being fought over (the \u201c<a href=\"http://socialmachines.media.mit.edu/2015/10/29/fueling-the-horse-race-of-ideas-3/\">Horse Race of Ideas</a>\u201d).&nbsp;The Electome is fueled by two primary data streams: the entire Twitter archive and daily output (the so-called 500m Tweet per day \u201cfire hose\u201d) as well as a sample of daily content from 30 digital news sites (5k-6k stories per day). A series of machine learning algorithms <a href=\"http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2014/08/pv_sv_dr_icwsm2016.pdf\">identify those Tweets and stories specifically about the election</a>, then classify them by topic, candidate, organization and a number of other filters. The classified data is then run through various semantic and network analytics that continuously measure and visualize:</p><ul><li>the<a href=\"https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/\"> share of conversation</a> or coverage that <a href=\"https://www.washingtonpost.com/news/the-fix/wp/2016/06/18/how-the-orlando-attack-showed-the-potential-of-an-october-surprise/\">any given issue </a>or candidate commands on Twitter and in the news media, respectively\u2014and how the two platforms are aligned<br></li><li>which issues are <a href=\"https://www.washingtonpost.com/news/the-fix/wp/2016/03/13/the-political-debate-on-twitter-focuses-on-much-different-issues-than-americans-at-large/\">most closely associated with each candidate</a> on Twitter (via co-occurrent candidate/issue references in single Tweets)<br></li><li>how much of the public sphere conversation and coverage is about substantive issues as compared to politics (polls, projections, process) and the candidates\u2019 character and personality<br></li><li>specific sub-topics and representative Tweets within broader conversations about specific issues or candidates<br></li><li>the level of \u201c<a href=\"http://fusion.net/story/304384/election-incivility-on-twitter/\">incivility</a>\u201d (profanity, insults, violence, ethnic/sexual slurs) within the public Twitter conversation about any given issue or candidate<br></li><li>who is<a href=\"http://blogs.wsj.com/washwire/2016/03/01/2016-front-runners-top-list-of-election-influencers-on-social-media/\"> influencing the public sphere election conversation</a> (via a composite Twitter/media influence metrics)<br></li></ul><p>LSM\u2019s deployment of Electome analytics has been <a href=\"http://www.knightfoundation.org/press/releases/new-elections-project-developed-mit-media-lab-will\">supported by the Knight Foundation</a>, with the goal of fueling news coverage that is more responsive to what matters most to the public. To that end, LSM has:&nbsp;<br></p><ul><li>provided customized analysis to several Electome media outlets\u2014including the <a href=\"https://www.washingtonpost.com/news/the-fix/wp/2015/12/15/the-ever-changing-issues-of-the-2016-campaign-visualized-by-twitter/\">Washington Post</a>, <a href=\"http://www.bloomberg.com/politics/articles/2016-05-02/missing-from-2016-race-sense-of-urgency-over-u-s-budget-gaps\">Bloomberg News</a>, <a href=\"http://socialmachines.media.mit.edu/wp-content/uploads/sites/27/2015/10/cnn4.pdf\">CNN Politics</a> and Fusion\u2014as well as publishing its own analysis in Medium<br></li></ul><ul><li>collaborated with the <a href=\"http://www.debates.org/\">Commission on Presidential Debates</a> to offer Electome analysis to the general election debates\u2019 moderators and credentialed journalists<br></li><li>also collaborated with the <a href=\"http://ropercenter.cornell.edu/\">Roper Center for Public Opinion Research</a> at Cornell University for integration of the Center\u2019s polling in Electome analytics/dashboard<br></li><li>and&nbsp;built a self-service dashboard featuring several Electome analytic tools for journalists and analysts to produce their own issue-driven analyses and visualizations.</li></ul><p>Looking beyond the 2016 election, LSM sees Electome technology as enabling new forms \u2014and, importantly, creators\u2014of investigative and explanatory journalism by democratizing access to powerful data mapping, analysis and visualization tools.</p>", "people": ["dkroy@media.mit.edu", "pernghwa@media.mit.edu", "billp@media.mit.edu", "mmv@media.mit.edu", "soroush@media.mit.edu", "russell5@media.mit.edu", "pralav@media.mit.edu", "schaad@media.mit.edu", "aheyward@media.mit.edu", "soph@media.mit.edu"], "title": "The Electome: Measuring responsiveness in the 2016 election", "modified": "2019-04-19T18:40:58.879Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "the-electome-measuring-responsiveness-in-the-2016-election"}, {"website": "", "description": "<p>The Wanderers&nbsp;were unveiled as part of the exhibition: \u2018The  Sixth Element: Exploring the Natural Beauty of 3D Printing' on display  at EuroMold, 25-28 November, Frankfurt, Germany. This work was done in collaboration with <a href=\"http://www.deskriptiv.de/\">Christoph Bader and Dominik Kolb</a>.  The wearables were 3D-printed with Stratasys multi-material 3D printing  technology. Members of the Mediated Matter group led by <a href=\"http://matter.media.mit.edu/people/bio/william-patrick\">Will Patrick</a>&nbsp;and <a href=\"http://matter.media.mit.edu/people/bio/sunanda-sharma\">Sunanda Sharma</a>  are currently working on embedding living matter in the form of  engineered bacteria within the 3D structures in order to augment the  environment.</p>", "people": ["bader_ch@media.mit.edu", "kolb@media.mit.edu", "neri@media.mit.edu"], "title": "Wanderers", "modified": "2018-04-27T17:06:00.697Z", "visibility": "PUBLIC", "start_on": "2014-07-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "wanderers"}, {"website": "", "description": "<p>This project seeks to examine the effects of altered gravity on an individual\u2019s physiology during parabolic flight. Specifically, we will collect flight participants\u2019 heart rate, heart rate variability, breathing rate, skin temperature, and skin conductance measurements using wearable, wireless sensors in order to determine the response of these biosignals to zero/hyper/microgravity and feelings of nausea. </p><p>The results of this research will have both significant scientific and civilian value. To our knowledge, this experiment will be the first to investigate the new Multiple Arousal Theory in the context of motion sickness, as well as altered gravity. This theory was developed in the Affective Computing group at the MIT Media Lab and examines asymmetry in skin conductance signals from right and left wrists as differing metrics of emotional arousal and intensity. The parabolic flight configuration provides an inimitable circumstance to systematically analyze the evolution of these signals over the course of the repeated parabolic flight path. For example, we expect to see globally heightened stress and emotional arousal on the first pass, with maximal skin conductance peaks from both wrists just before the first moment of weightlessness. We expect these peaks to monotonically decrease over time with each pass, but to remain more elevated (relative to an individual\u2019s baseline) for participants experiencing more self-reported nausea during flight. For individuals not experiencing extreme nausea, we expect to see a much higher skin conductance signal from their right wrists compared to their left (for right-handed participants) during the first few passes, with this difference decreasing steadily as the participant habituates to the flight pattern and sensations. </p><p>Note that NASA and other researchers\u2014including the Boston-local scientists at the Ashton Graybiel Spatial Orientation Lab at Brandeis University\u2014have investigated spatial orientation and motion sickness, but they are just beginning to add the use of physiological sensors to their work. Not only does this demonstrate that the proposed experiment is at the forefront of scientific inquiry, but it also facilitates potential collaboration with world-renowned experts in the Boston area!</p><p>In addition to sensor data, we intend to collect pre- and post-flight surveys recording participant reactions to different levels of gravity, including points at which they experienced nausea or discomfort. Pre-flight surveys will include nausea sensitivity metrics, designed to determine how likely a person is to feel nausea (i.e., separating those who feel carsick on a drive through town versus those who approach rollercoasters without hesitation). It will also ask about each participant\u2019s feelings of anxiety, nausea, and excitement in anticipation of flying. Note that while these feelings may be experienced simultaneously, each one has a different effect on one\u2019s physiology. </p><p>After the flight, we will ask participants to rank which sections of the flight (e.g., beginning, middle, end) prompted the greatest sensations of anxiety, nausea, and excitement and to what degree. We will also annotate the flight video recordings to denote periods of high anxiety, nausea, or excitement.</p><p>Then, we will use the survey, annotation, and sensor information to build a model that predicts when an individual might experience distress in altered gravity environments. This aspect of the study will leverage our research group\u2019s unique expertise building machine learning algorithms for physiological data, but the results could have widespread impact. For example, such a system could be deployed to space travelers to help them monitor their physiology and anticipate or prevent feelings of discomfort during flight. As access to space travel becomes more pervasive, it is critical to understand the physiological effects of altered gravity on a population that does not solely include astronauts or specially trained individuals. Our models, along with the use of low-cost, commercially available sensors, would enable \u201cspace hacking\u201d by tourists and other non-technical personnel, allowing them to measure and track their biosignals to achieve optimal wellness during space travel.&nbsp;</p>", "people": ["jaquesn@media.mit.edu", "picard@media.mit.edu", "sfedor@media.mit.edu", "ktj@media.mit.edu", "sataylor@media.mit.edu"], "title": "Physio FreeFall", "modified": "2018-10-22T19:51:21.995Z", "visibility": "PUBLIC", "start_on": "2017-05-11", "location": "", "groups": ["affective-computing", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "physio-freefall"}, {"website": "", "description": "<p>MITeams is an email network visualization tool for teams that allows people to see collaboration among team members and the structure of teams.<br></p>", "people": ["jingxian@media.mit.edu", "dorghian@media.mit.edu", "xiaojiao@media.mit.edu", "hidalgo@media.mit.edu"], "title": "MITeams", "modified": "2017-10-25T01:00:12.114Z", "visibility": "LAB-INSIDERS", "start_on": "2016-10-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "immersion-teams"}, {"website": "https://fiftynifty.org", "description": "<p>This is a grassroots challenge to get friends to participate in democracy by making calls to congresspeople in all 50 states. Live phone calls are the best way to directly express your opinion on an issue to your elected officials. Your mission is to pass message this along to friends who will make calls and also pass the message/link along to others who will do the same. It's a social chain letter and a call to action for a better participatory democracy. &nbsp;<span style=\"font-size: 18px; font-weight: normal;\">We help you make your call and you pass on an invitation for your friends to do the same. Your invite can stress your opinion on a given issue.&nbsp;</span></p><p>The winners are the first ten chains to reach 50 states and accumulate the most challenge points. You get 250 points for making a call, 125 points for a call that your friend makes, 65 points for the call their friend makes, on and on. Everyone on the chain earns points. Points count for your first call to each of your two senators and your representative. You get a bonus for a \"grand slam\"\u2014a network that reaches all 435 representatives and 100 senators.</p><p>There is a leaderboard and a network view so you can track how you are doing. You can also see how much of the country your chain is covering.</p>", "people": ["pewebb@media.mit.edu", "britneyj@media.mit.edu", "lemeb@media.mit.edu", "lip@media.mit.edu", "trich@media.mit.edu", "jasrub@media.mit.edu"], "title": "FiftyNifty", "modified": "2019-06-04T20:46:21.258Z", "visibility": "PUBLIC", "start_on": "2017-02-13", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "fiftynifty"}, {"website": "http://mekatilili.media.mit.edu/", "description": "<p><a href=\"https://www.mekatilili.com\">Mekatilili</a>&nbsp;is a learning initiative that provides a platform for African youth to enhance technical skills through creative learning approaches that strive to empower learners and enable access to broader job opportunities and meaningful work. Founded in 2016, the program has reached over 400 young people whose average age demographic ranges from 14 \u2013 25 years. The program is conducted through hands on, interactive workshops focusing on human-centered design, rapid prototyping, electronics, computer science, and professional development.&nbsp;<br></p><h2>Mekatilili Fellowship Program</h2><p>In 2019, the initiative launched the Mekatilili Fellowship Program (MFP), which is an annual gathering of African innovators that aims to foster open ended, playful, and peer-driven learning to promote the development of appropriate and sustainable local technical solutions.&nbsp;</p>", "people": ["jaleesat@media.mit.edu", "muthui@media.mit.edu"], "title": "Mekatilili Fellowship Program", "modified": "2019-03-29T21:06:55.831Z", "visibility": "PUBLIC", "start_on": "2018-07-03", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "mekatilili"}, {"website": "", "description": "<p>Digital Materials are a way of designing and manufacturing. Rather than building large, monolithic, single-use components, we discretize the material into simple, repeating, functional bits. A discrete set of base elements are combined to form cellular lattices with bulk material properties. This lets us cheat: we can maximize the performance of our material by assembling high-performance sub-elements, and their reversibility maximizes the sustainability and post-life reusability of the product. With all of these discrete units, assembly becomes a chore, and automation becomes crucial. The structured nature of the lattice enables assembler robots to use the geometry of the lattice for locomotion and error-correction. Further, the structured nature of the discretized lattice lends itself to novel design and simulation tools that exploit functional representations of the geometry to open the design space to previously unthinkable regimes of simulation, topological design and manufacture path-planning.</p><p>http://dma.cba.mit.edu</p><p>http://dma.cba.mit.edu/mechanical/index.html</p>", "people": ["neilg@media.mit.edu", "mcarney@media.mit.edu"], "title": "Robotic Assembly of Discrete Cellular Lattices (Digital Materials)", "modified": "2017-03-30T19:52:24.235Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "robotic-assembly-of-discrete-cellular-lattices-digital-materials"}, {"website": "", "description": "<p><b>In \"Let's see a game!\" we&nbsp;expose the different perspectives in&nbsp;TV&nbsp;sports and news in order to build broadcasting systems that unify rather than divide. We use the galvanizing impact of sports and live events as a forum, and then we add production and viewing opportunities to distinguish fact from opinion and to challenge the basis of those opinions.</b></p><p>In 1951, when the Dartmouth football team played against Princeton, there was deep disagreement between the two schools as to what had happened during the game. In \"They Saw a Game: A Case Study,\" the psychologists Albert Hastorf and Hadley Cantril found that when the <i>same </i>motion picture of the game was shown to a sample of undergraduates at each school,&nbsp; each individual <i>perceived a different game</i>, and their versions of the game was just as \"real\" as other versions were to other people.&nbsp;</p><p><b>However,&nbsp; little is known about whether and how broadcasting media are adding fuel to the fire. In order to study the relationship between storytelling/perspectives and opinion formation, we built the following two applications: \"Let's see a game!\" and \"Let's watch news!\"</b></p><p>In the first step, we built an interactive application&nbsp;that exposes different perspectives in sports broadcasting.&nbsp;The application plays two broadcasts of the same game, created for each team's home audience.&nbsp;The user can tune into an audio channel by moving the slider. Additional buttons allow the user to take other actions.</p>", "people": ["mhjiang@media.mit.edu", "lip@media.mit.edu", "hbedri@media.mit.edu"], "title": "Let's see a game!", "modified": "2019-04-18T14:04:57.014Z", "visibility": "PUBLIC", "start_on": "2017-09-11", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "let-s-see-a-game"}, {"website": "http://www.alexrutherford.org/constitutionology/", "description": "<p>Political constitutions describe the fundamental principles by which nation-states are governed, the political and legal state institutions, the powers, procedures, and duties of those institutions, and the rights and responsibilities of individuals. How do these constitutions develop over long periods of time? What is the interplay between colonial history and global, time varying trends in determining the characteristics of a country's constitution? We explore these questions using new techniques of computational social science.</p>", "people": ["cebrian@media.mit.edu", "irahwan@media.mit.edu"], "title": "Evolution of the Social Contract", "modified": "2017-12-11T21:03:28.998Z", "visibility": "PUBLIC", "start_on": "2016-07-01", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "evolution-of-the-social-contract"}, {"website": "", "description": "<p>Cognitarium 2.0&nbsp; is a planetarium for the mind. Enter a world of 40 Hz sounds, multicolored lights and mind-entraining frequencies. Cognitarium 2.0 combines early groundbreaking research in gamma frequencies, multi sensory music and cognition&nbsp;research, to leap into the future of cognitive health and cross-modal experiences. This edition of the&nbsp;Cognitarium is a collaboration&nbsp;combining&nbsp;creative cognition research by Alexandra Rieger, lighting physics&nbsp;and control&nbsp;from Ben Bloomberg, the gamma drones of David Su,&nbsp;and 40&nbsp;Hz sculpted sound as defined by Tod Machover.&nbsp;</p>", "people": ["arieger@media.mit.edu"], "title": "Cognitarium 2.0", "modified": "2018-05-02T16:58:17.658Z", "visibility": "PUBLIC", "start_on": "2017-06-10", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "cognitarium"}, {"website": "", "description": "<p><b style=\"font-size: 18px;\">Spelman</b>: Spelman College is a distinguished Black Women\u2019s college in Atlanta. Nationally, twenty-three percent (23%) of the Black Women in STEM graduate from Spelman College. With their Innovation Center at Spelman, we piloted a two-way Long Distance Learning course in coding, microcontrollers (MaKey MaKey and Arduino), and digital fabrication.&nbsp;Students received credit for the course. For the course, we authored and designed &nbsp;Experiential Learning Tutorials which&nbsp;we first taught to our Spelman teaching counterpart, Dr. Jerry Volcy. He heads Spelman\u2019s Innovation Center. The final products of the course were fabricated interactive lamps, and &nbsp;fabricated digital musical instruments.<br></p><p>The course allowed us to test our Pilot design for a \u201cTelevision Studio in a Suitcase\u201d (TSS) broadcast system. TSS is an affordable, miniaturized and portable interactive broadcast system. It enables us to teach technology online from anywhere. It works. In the next series at Spelman, we will teach Robotics and Mechatronics.</p>", "people": ["tcarew@media.mit.edu", "bdunning@media.mit.edu"], "title": "Spelman: The Design of a two-way \u2018Television Studio in a Suitcase\u2019 Broadcast System to Teach Fabrication and Prototyping Remotely", "modified": "2017-10-11T17:49:30.834Z", "visibility": "PUBLIC", "start_on": "2016-09-15", "location": "", "groups": ["code-next"], "published": true, "active": false, "end_on": null, "slug": "spelman-the-design-of-a-two-way-television-studio-in-a-suitcase-broadcast-system-to-teach-fabrication-and-prototyping-remotely"}, {"website": "", "description": "<p>We propose a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications: 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.</p>", "people": ["jaquesn@media.mit.edu"], "title": "Sequence Tutor", "modified": "2017-07-11T15:22:59.660Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "sequence-tutor"}, {"website": "http://www.mallcong.com", "description": "<p>We present a shape-changing interface\u2014reSpire\u2014that provides tangible feedback based on users\u2019 vital signals through its volume change and pulse generation with the goal of helping them develop better <b>attention-to-breath (ATB)</b>, as well as to&nbsp;calm them and reduce stress. <b>Mindful breathing</b> has been identified as an effective technique&nbsp;<b>to reduce people\u2019s stress level </b>by controlling their heart rate indirectly, known as <b>respiratory sinus arrhythmia (RSA)</b>. To explore this effective way to guide people to calmness, we developed a soft, cushion-like, shape-changing device that renders the chest motion by breathing and small pulses by a heartbeat, which provides tactile feedback with a feeling of comfort and intimacy. To monitor users' heart and respiration rates and guide them to reach the calm state by tangible interactions with reSpire, we also present the sensing and feedback mechanism.</p>", "people": ["yun_choi@media.mit.edu"], "title": "reSpire-M: Tangible vital sign feedback interface for self-regulation of respiration", "modified": "2019-04-11T18:19:16.225Z", "visibility": "LAB-INSIDERS", "start_on": "2018-04-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "respire-tangible-vital-sign-feedback-interface-for-self-regulation-of-respiration"}, {"website": "", "description": "<p>The&nbsp;&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">Humanizing AI in Law (HAL)&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">project aims to build the technical and legal foundations necessary to establish a due-process framework for auditing and improving decisions made by artificial intelligence systems as they evolve over time. This work is directed at the concerning software that has been deployed within the criminal justice system to aid judges in the sentencing of criminal defendants.</span></p>", "people": ["cbarabas@media.mit.edu", "nsaltiel@media.mit.edu", "madars@media.mit.edu", "kdinakar@media.mit.edu", "joi@media.mit.edu", "zittrain@media.mit.edu"], "title": "Humanizing AI in Law (HAL)", "modified": "2018-05-07T14:46:00.241Z", "visibility": "PUBLIC", "start_on": "2017-06-01", "location": "", "groups": ["ethics-and-governance"], "published": true, "active": false, "end_on": null, "slug": "HAL"}, {"website": "", "description": "<p>Through a series of expert workshops and advanced horizon scanning and foresight methods, this foundational project aims at developing an overall mapping of key ethical and governance challenges in the AI space for purposes of enhancing our collaborative network and identifying areas ripe for impact. Outputs include translational tools, such as a visual navigation aid mapping people/institutions, research, and existing efforts related to the Ethics and Governance of AI Initiative \u2028that bridge key technology and policy concepts.This project will be led by  Ryan Budish, Amar Ashar, and Elena Goldstein from the Berkman Klein Center.</p>", "people": [], "title": "AI Challenge Mapping", "modified": "2017-07-11T00:42:01.824Z", "visibility": "PUBLIC", "start_on": "2017-06-01", "location": "", "groups": ["ethics-and-governance"], "published": false, "active": false, "end_on": null, "slug": "ai-challenge-mapping"}, {"website": "", "description": "<p>Nebula is a voice-controlled interactive software app that allow users to conduct a choir of diverse vocal sounds by using only their voice as input. The system is based on the Constellation project by Akito van Troyer that takes sonic material and organizes it visually to let anyone compose creative soundscapes. Nebula uses hundreds of vocal samples that are represented as individual stars and organized by perceptual and spectral audio features. The samples get triggered and activated when the user sings or produces any sound with the voice. The voice is analyzed in real time, and this analysis is then used to to trigger and mix a cascade of sounds with similar features. The voice becomes a kind of conductor's baton that creates a dialogue without words between the individual and the community. And once a participant uses Nebula, their own voice, first used as a controller, is then transformed into a new sample adding an additional star to the experience for all subsequent participants. The result - a final cosmos of voices\u2014provides material that might be used by composer Tod Machover for the final Philadelphia Voices City Symphony.&nbsp;</p>", "people": ["rebklein@media.mit.edu"], "title": "Nebula", "modified": "2018-10-19T19:38:07.641Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "nebula"}, {"website": "", "description": "<p>Analyzing detailed data from SpeechBlocks&nbsp;to understand how kids engage with constructionist literacy learning technologies, with the goal of empowering caregivers (e.g. parents, older siblings, tutors) with these insights.</p>", "people": ["anneli@media.mit.edu", "minasg@media.mit.edu", "echu@media.mit.edu", "jnazare@media.mit.edu", "isysoev@media.mit.edu"], "title": "Play Analytics", "modified": "2018-04-30T20:56:57.445Z", "visibility": "PUBLIC", "start_on": "2016-02-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "play-analytics"}, {"website": "", "description": "<p>Future of clinical development is on the verge of a major transformation due to convergence of large new digital data sources, the computing power to identify clinically-meaningful patterns in the data using efficient artificial intelligence (AI) and machine-learning (ML) algorithms, and regulators embracing this change through new collaborations. This perspective summarizes insights and recommendations for a new digital paradigm for healthcare from academy, biotechnology industry, non-profit foundations, regulators and technology corporations. Analysis and learning from publically available biomedical and clinical trial datasets, real world evidence from sensors and health records by machine learning architectures are discussed. Strategies for modernizing the clinical development process by integration of AI and ML based digital methods and secure computing technologies through recently announced regulatory pathways at the United States Food and Drug Administration are outlined. We conclude by discussing impact of digital algorithmic evidence to improve medical care for patients.</p>", "people": ["pratiks@media.mit.edu", "fkendall@media.mit.edu"], "title": "Artificial Intelligence and Machine Learning in Clinical Development: a Translational Perspective", "modified": "2019-03-11T17:36:08.527Z", "visibility": "PUBLIC", "start_on": "2017-01-23", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "artificial-intelligence-for-drug-discovery-and-clinical-trials"}, {"website": "", "description": "<p>What if we could look at the entire history of humanity at once?&nbsp;</p><p>Pantheon aims to create a data-driven view of history by collecting, visualizing, and analyzing&nbsp;data on the biographies of historical characters. We are particularly interested in what this data can teach us about the production of knowledge across&nbsp;history, the relation between accomplishment and fame, and information wanes from history.</p>", "people": ["hidalgo@media.mit.edu", "crisjf@media.mit.edu", "kzh@media.mit.edu", "amy_yu@media.mit.edu"], "title": "Pantheon", "modified": "2018-12-19T19:14:00.464Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "pantheon-new"}, {"website": "http://moralmachine.mit.edu", "description": "<p>The Moral Machine is a platform for gathering a human perspective on moral decisions made by machine intelligence, such as self-driving cars. We generate moral dilemmas, where a driverless car must choose the lesser of two evils, such as killing two passengers or five pedestrians. As an outside observer, people judge which outcome they think is more acceptable. They can then see how their responses compare with other people. If they are feeling creative, people can also design their own scenarios, for others to view, share, and discuss.</p><p>Visit the <a href=\"http://moralmachine.mit.edu\">Moral Machine</a>.</p>", "people": ["awad@media.mit.edu", "dsouza@media.mit.edu", "irahwan@media.mit.edu"], "title": "Moral Machine", "modified": "2018-09-26T19:22:25.166Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "moral-machine"}, {"website": "", "description": "<p>\u201cPower Props\u201d is a sensor and actuator platform for Live Action Role Playing (LARP), immersive theater, theme park, and other transmedia experiences.<br>The system is an open framework allowing any kind of sensor or actuator to be easily programed and reprogrammed on the fly, allowing novice LARP masters to add magic props and special visual effects to their game.</p>", "people": [], "title": "Power Props", "modified": "2017-10-11T18:42:25.421Z", "visibility": "LAB-INSIDERS", "start_on": "2012-06-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "power-props"}, {"website": "", "description": "<p>Antarctica is a unique and beautiful continent that is key to the global ecosystem and climate. Research based in Antarctica is helping scientists explore cutting-edge questions in oceanography, physics, climate science, ecology, and more. To most members of the public, however, Antarctica appears to be a cold and inhospitable place with little relevance to daily life. This project asks if combining a citizen science campaign and an immersive museum experience can increase a sense of stewardship to care for Antarctica within people around the world.&nbsp;</p><p>For this project, the Space Enabled research group designed a concept for an&nbsp;interactive, room-scale multi-sensory presentation providing education regarding the Antarctic.This presentation is designed to be placed in an informal education venue such as an aquarium or science museum.&nbsp;The immersive presentation helps participants understand this dynamic region through an experience that combines photos, video, sound, smell,&nbsp; and temperature changes.&nbsp;The information draws on multiple types of data from earth science satellites (i.e., ICESAT), airborne science platforms (i.e., IceBridge), and in-situ sensors (i.e., underwater video cameras and photographs). As participants go through the experience, they hear vignettes about specific research areas in Antarctica, including studies on penguin colonies, ice cores, meteorite searches, and ocean food chains.&nbsp;</p><p>In addition to the physical, interactive presentation, the second aspect of the project involves a companion Citizen Science campaign. Specifically, citizen scientists and students located in countries near the Southern Ocean (including Chile, South Africa, New Zealand, and Australia) are invited to participate in a data collection campaign about their part of the Southern Ocean. A proposed mobile application in both English and Spanish could invite participants to submit photographs of the shoreline on the Southern Ocean. Our team is prototyping a small, low-cost kit that allows citizen scientists to take measurements such as temperature, salinity and pH measurements from coastal areas facing the Southern Ocean. Information from these citizen scientists could be incorporated into the physical presentation in the museum. Meanwhile, the visual aspects of the presentation may be provided online as photo or video files that&nbsp; could be downloaded to host on their own platforms.&nbsp;<br></p>", "people": ["lizbethb@media.mit.edu"], "title": "Design for a citizen science and public engagement project celebrating Antarctica and the Southern Ocean", "modified": "2019-04-19T18:42:30.362Z", "visibility": "PUBLIC", "start_on": "2018-03-09", "location": "", "groups": ["space-enabled"], "published": true, "active": false, "end_on": null, "slug": "design-for-a-citizen-science-and-public-engagement-project-celebrating-antarctica-and-the-southern-ocean"}, {"website": "", "description": "<p>Narratarium augments printed and oral stories and creative play by projecting immersive images and sounds. We are using natural language processing to listen to and understand stories being told, and analysis tools to recognize activity among sensor-equipped objects such as toys, then thematically augmenting the environment using video and sound. New work addresses the creation and representation of audiovisual content for immersive story experiences and the association of such content with viewer context. </p>", "people": ["havasi@media.mit.edu", "kmh@media.mit.edu", "jieqi@media.mit.edu", "novysan@media.mit.edu", "vmb@media.mit.edu"], "title": "Narratarium", "modified": "2018-05-10T16:28:11.654Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "future-storytelling", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "narratarium"}, {"website": "", "description": "<p>The Montessori Method is an educational approach that emphasizes independence and respect for a child's natural development process. Montessori materials are a hallmark of the Montessori\nMethod. These self-teaching tools encourage exploration of concepts in the\nareas of mathematics, language, sensorial development, and practical life, and\nallow children to direct their own learning with the light guidance of teachers\nand peers. </p><p>We envision a novel framework of unobtrusive sensor networks to understand and reflect on a child\u2019s learning progress, by instrumenting existing Montessori learning materials using distributed sensing techniques.</p>", "people": ["saquib@media.mit.edu"], "title": "Instrumentation of Montessori Learning Materials", "modified": "2018-05-04T11:39:56.455Z", "visibility": "PUBLIC", "start_on": "2017-09-04", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "instrumentation-of-montessori-learning-materials"}, {"website": "", "description": "<p>Protein-protein interactions (PPIs) are an essential part of many biological pathways in living organisms. With use cases such as regulation of gene expression, enzymatic catalyzation, and muscle contraction, understanding PPIs is a critical step toward a better understanding of life itself. Moreover, aberrant human PPIs may lead to multiple diseases, such as Alzheimer's, Creutzfeldt\u2013Jakob, and cancer. Despite the undisputed importance of PPIs, only a small portion of the human interactome is known. </p><p>The PPI mapping problem is composed of two subproblems: the Interaction Problem\u2014identifying the two or more proteins involved in a particular interaction; and the Position Problem\u2014recognizing the residues within the interacting proteins that are crucial for the interaction (also known as hot spots or interacting residues). Current experimental techniques for PPI mapping, like Yeast 2 Hybrid or Alanine scans, are limited in scale, tedious, and expensive, therefore establishing the need for a fast, efficient, and accurate computational system.</p><p>DeepPPI is a Deep Learning algorithm that uses known PPIs to identify reoccurring patterns in the human interactome. These underlying patterns can be used, in turn, to predict both the existence of a new interaction and the interacting residues within the relevant proteins. Through this project, we hope to answer a fundamental biological question: How does nature, via evolution, create new protein-protein interactions? Additionally, we believe that DeepPPI will serve as a large-scale computational alternative to Alanine Scans and other experimental methods, contributing to the study of diseases and development of new therapeutics.&nbsp;</p>", "people": ["kfirs@media.mit.edu"], "title": "DeepPPI", "modified": "2017-09-05T00:43:44.403Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "deepppi"}, {"website": "http://www.mit.edu/~ajdas", "description": "<p>A smartphone based spectrometer design that is standalone and supported on a wireless platform. The device is low-cost and the power consumption is minimal making it portable to perform a range of studies in the field. Essential components of the device like the light source, spectrometer, filters, microcontroller and wireless circuits have been assembled in a housing that fits into a pocket and the entire device weighs 48 g. The device has a dedicated app on the smartphone to communicate, receive, plot and analyze spectral data. Validations of the device were carried out by demonstrating non-destructive ripeness testing in fruits. Ultra-Violet fluorescence from Chlorophyll present in the skin was measured across various apple varieties during the ripening process and correlated with destructive firmness tests. This demonstration is a step towards possible consumer, bio-sensing and diagnostic applications that can be carried out in a rapid manner.</p>", "people": ["raskar@media.mit.edu", "ajdas@media.mit.edu"], "title": "Smartphone spectrometer for food sensing", "modified": "2017-03-31T23:01:55.075Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "ajdass-untitled-project"}, {"website": "", "description": "<p>Mnemo is an integrated system to support human biographical memory.&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">Mnemo is directed to serve people with impaired memory (e.g. Alzheimer's patients)&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">by providing intuitive ways to benefit from large amounts of personal data.</span></p>", "people": ["mmv@media.mit.edu"], "title": "Mnemo", "modified": "2018-10-20T17:54:46.318Z", "visibility": "PUBLIC", "start_on": "2018-01-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "mnemo"}, {"website": "", "description": "<p><i><a href=\"https://citysymphonies.media.mit.edu/philadelphia.html\">Philadelphia Voices</a></i>&nbsp;is the latest in the series of <a href=\"https://www.media.mit.edu/projects/city-symphonies-massive-musical-collaboration/overview/\">City Symphonies</a> projects that Tod Machover and the Opera of the Future group\thave created since 2012. Previous City Symphonies have centered on&nbsp;Toronto, Edinburgh, Perth, Lucerne, and\tDetroit.</p><p>Each project paints a musical portrait of a city\u2014using \u201ctraditional\u201d musical elements as well as real sounds recorded by residents\u2014to portray the essence of their city's history and future. Everyone living in that city is invited to collaborate to create the symphony,\tresulting in an unprecedented creative collaboration around music, sound, and storytelling.&nbsp;</p><p><i style=\"font-size: 18px; font-weight: 400;\">Philadelphia Voices</i>&nbsp;has been in progress\tsince spring 2017 and will culminate in performances in Philadelphia (Kimmel Center) and New York (Carnegie Hall) in April\t2018. A special mobile app has been developed to allow anyone with a smartphone to collect sounds and video and to upload those files to a communal database for listening and morphing.&nbsp;</p><p>Opera of the Future researchers have created new software that enables anyone to contribute their voice to a specially-designed sonic landscape from Philadelphia. Workshops and special activities have been organized\twith local singers from every age and background, and Tod Machover has chosen several hundred of them to sing in the final performances with The Philadelphia Orchestra under the baton of its music director, Yannick N\u00e9zet-S\u00e9guin.&nbsp;</p><p>Since Philadelphia is considered the birthplace of&nbsp;\tAmerican democracy,&nbsp;<i style=\"font-size: 18px; font-weight: 400;\">Philadelphia Voices</i>&nbsp;will investigate the current state of democracy from a Philly\tperspective. The project will also consider the society in which we want to live, and what we are willing to do to achieve that ideal.</p>", "people": ["tod@media.mit.edu"], "title": "Philadelphia Voices", "modified": "2018-10-19T19:39:16.909Z", "visibility": "PUBLIC", "start_on": "2017-10-13", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "philadelphia-voices"}, {"website": "", "description": "<h2>Giving voice and information to objects and spaces around us</h2><p>Objects in our lives are usually either digital or not; mostly a wall is just a wall. The Sonic Murals project explores what happens when we blur those lines. Implementing touch capacitance and conductive pigments in an innovative way, any surface can become a sensor, a&nbsp;tool for data collection, or a musical instrument, as exhibited in this project. When interacting with touch or proximity sensors on a sonic mural, one can experience spacial exploration and sound creation on a multi-sensory level.</p>", "people": ["arieger@media.mit.edu"], "title": "Sonic Murals", "modified": "2017-10-14T20:32:15.057Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "sonic-murals"}, {"website": "", "description": "<p>Affinity is a high-level machine learning API (Application Programming Interface)  dedicated exclusively to molecular geometry. Affinity is written in TensorFlow, some small proportion of high-performance code is  in low-level C++. &nbsp;Depending on the application it can be configured as multi-CPU, multi-CPU single GPU, or multi-GPU system. Affinity has&nbsp; it's own web page at <a href=\"http://affinity.mit.edu\">affinity.mit.edu</a><br></p>", "people": ["jacobson@media.mit.edu", "kfirs@media.mit.edu", "mkkr@media.mit.edu"], "title": "Affinity: Deep learning API for molecular geometry", "modified": "2017-08-28T16:12:07.806Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "affinity"}, {"website": "", "description": "<p>Water is life.&nbsp;</p><p>Extending the research that we are involved in with Mediated Matter\u2019s water-based fabrication platform, we brought water-based biomaterial solutions into zero gravity observe their behavior. We created several small scaffolds of different geometries and surface textures that used the water-based solutions as their skins, taking advantage of&nbsp;the fact that van der Waal forces are the primary force affecting water in zero g, which allows water to maintain more structure. The solutions covered the body in a noticeable layering that would otherwise collapse at normal gravity.&nbsp;</p><p>The scaffolds were made of a hydrophilic polymer that is graded in its hydrophilicity, which combined with varying geometries, enabled us to vary the amount and type of liquid the scaffold can retain.&nbsp;Aqueous solution was rapidly applied to the scaffolds in zero gravity to see how thick of a skin we could create that is essentially all fluid.&nbsp;</p><p>This project has significance for research in dynamic cladding systems in space that could have biological functionality and carry mobile life-sustaining functions, as well as providing a skin that shields EM radiation.</p>", "people": ["cail@media.mit.edu", "neri@media.mit.edu", "asling@media.mit.edu"], "title": "Water templating and biomaterial skins in zero gravity", "modified": "2018-05-10T18:43:35.423Z", "visibility": "LAB", "start_on": "2017-09-01", "location": "", "groups": ["mediated-matter", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "water-templating-and-biomaterial-skins-in-zero-gravity"}, {"website": "", "description": "<p>Team Edge (2016 - 2017): The Year 2 MIT/Google Collaboration. For it, we design and author Curricula and Learning Activities in the following Domains: Coding, Computational Design, the Design Process, Fabrication, Game Design and Engineering. The Toolbox includes Processing, Arduino (Firmata), Fritzing, Autodesk Circuits, Laser Cutters, and 3D Printers. Similar to Year 1 (Foundations), the students will receive Leadership Training, Mathematics, and College Preparation.</p>", "people": ["tcarew@media.mit.edu", "bdunning@media.mit.edu"], "title": "Team Edge: Design of Codified Curricula in Coding, Design, Fabrication, Games, Engineering, etc.", "modified": "2017-04-04T21:05:00.355Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["code-next"], "published": true, "active": false, "end_on": null, "slug": "team-edge"}, {"website": "", "description": "<p>The OpenAg\u2122 Personal Food Computer is a tabletop-sized, controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber. Climate variables such as carbon dioxide, air temperature, humidity, dissolved oxygen, potential hydrogen, electrical conductivity, and root-zone temperature are among the many conditions that can be controlled and monitored within the growing chamber to yield various phenotypic expressions in the plants.&nbsp;</p><p>Our latest version\u2014the <a href=\"https://wiki.openag.media.mit.edu/pfc_edu_3.0\"><b>PFC v3.0, or \"PFC_EDU\"</b></a>\u2014has been scaled down from previous PFC's in terms of cost, size, and complexity, and designed specifically with educators and children aged 8-14 in mind. It offers a spectrum of control so that users can make their growing experience as manual or as automated as they would like.</p><p><b><a href=\"https://www.media.mit.edu/posts/build-a-food-computer/\">Click here to Build a Food Computer</a>.</b></p><p>Like all OpenAg's Food Computers, the PFC_EDU is open source and can be made from easily accessible components so that #nerdfarmers&nbsp;with a broad spectrum of skills, resources, and interests can build, modify, share, and upgrade over time. Build instructions, design files, and helpful resources for all our OpenAg\u2122 Personal Food Computers are on our <a href=\"https://wiki.openag.media.mit.edu/start\"><b>OpenAg Wiki</b></a>, <a href=\"https://github.com/OpenAgInitiative\"><b>OpenAg Github</b></a>&nbsp;so&nbsp;nerd farmers can band together (using the <a href=\"http://forum.openag.media.mit.edu/\"><b>OpenAg Forum</b></a>) to conduct scientifically rigorous citizen-science experimentation, all over the world.</p>", "people": ["calebh@media.mit.edu", "hildreth@media.mit.edu", "rbaynes@media.mit.edu", "poitrast@media.mit.edu", "rebekahj@media.mit.edu", "tsavas@media.mit.edu"], "title": "Personal Food Computer", "modified": "2019-05-23T17:57:23.969Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["open-agriculture-openag"], "published": true, "active": false, "end_on": null, "slug": "personal-food-computer"}, {"website": "", "description": "<p>This project investigates a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN) using deep reinforcement learning (RL). Our method, which we call Sequence Tutor, allows models to improve sequence quality with RL, while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation for drug discovery. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.</p>", "people": ["jaquesn@media.mit.edu"], "title": "Improving RNN Sequence Generation with RL", "modified": "2017-10-16T19:27:53.053Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "improving-rnn-sequence-generation-with-rl"}, {"website": "", "description": "<p>Sensors, especially biosensors, are indispensable for modern society due to their wide applications in public healthcare, national and homeland security, and forensic industries as well as environmental protection. Detection of biomolecules at ultra-low concentration (sub-picomolar to 100 atto-molar) is necessary for screening many cancers, neurological disorders and early stage infections such as HIV. However, current medical diagnostic tools either have low sensitivity (picomolar detection) or require bulky expensive equipment and extensive procedures that cannot be performed outside well-controlled lab environment. </p><p>Our research established, for the first time, that the material and device technology which has evolved mainly with an aim of sustaining the glorious scaling trend of Information Technology, can also transform a completely diverse field of bio/gas-sensor technology [<i>Nat. Nano.</i> (Research Highlights) 7,275 (2012)] [<i>D. Sarkar et. al.,<i> Appl. Phys. Lett.</i>,</i> 102, 023110 (2013)] [<i>D. Sarkar et. al.,<i> ACS Nano</i>,</i> 8, 3992\u20134003 (2014)] [D. Sarkar et. al.,<i> Nano Lett.,</i> 15, 2852\u201362 (2015)]. We <b>experimentally demonstrated electrical bio- and gas-sensors based on molybdenum disulfide</b> (MoS2), which provides <b>extremely high sensitivity </b>and at the same time offers easy patternability and device fabrication, due to its 2D atomically layered structure. Moreover, we <b>proposed for the first time, biosensors based on low subthreshold swing electronic devices</b> (steep turn-ON characteristics) and theoretically illustrated that they can <b>break the fundamental limits in sensitivity</b> of conventional electrical biosensors and lead toincrease in sensitivity by more than four decades. [<i>D. Sarkar et. al.,<i> Appl. Phys. Lett.</i>,</i> 100, 143108 (2012)] [<i>D. Sarkar et. al.,<i> Appl. Phys. Lett.</i>,</i> 102, 203110 (2013)] [<i>D. Sarkar et. al.,<i> Appl. Phys. Lett.</i>,</i> 102, 023110 (2013)]. </p><p><b>We plan to integrate the ultra-sensitive biosensing capabilities of novel nanoelectronic transducers with read-out circuitry as well as sample processing steps to build</b> <b>a </b>hand-held low-power device which can enable detection of extremely low concentration (single entity) of biomolecules from whole blood. This diagnostic device being ultra-sensitive, will enable <b>early disease detection</b> and being scalable and low-power, will allow point-of-care application and <b>extend diagnosis to remote areas</b>. Such technology can save billions of lives and cause massive reduction in health-care costs.&nbsp; &nbsp;&nbsp;</p>", "people": ["deblina@media.mit.edu"], "title": "Ultra-Sensitive Electrical Biosensors for Point-of-Care Applications", "modified": "2018-09-12T20:26:50.379Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["nano-cybernetic-biotrek"], "published": true, "active": false, "end_on": null, "slug": "ultra-sensitive-electrical-biosensors-for-point-of-care-applications"}, {"website": "https://medrec.media.mit.edu", "description": "<p>Increasingly in the US, people have to take responsibility for their health information.&nbsp; Simultaneously, medical providers must make patient data available. MedRec fully decentralizes access rights via an Ethereum blockchain, thereby giving patients control over record distribution. Our model is the World Wide Web:&nbsp; MedRec is a network. Patients and providers operate nodes that authorize others to retrieve data. It is a basis for a generally useful permissioning system.</p><p>There is no website or central repository of permissions. Instead, patients and medical records originators <i>establish a relationship </i>and based on that, the patient creates <i>smart contracts</i>&nbsp;that other members of the network can use to authorize access to a record database. The parameters of contracts are kept in a <i>blockchain </i>that is maintained by all member providers/originators who at the same time use those contracts to provide access to their database. The patient/user contracts themselves are held by the patients in a <i>wallet </i>that resides on their device[s] as an app. This app is secure and recoverable in case the physical device is lost or damaged.</p><p>For a full overview, technical documentation, and updates, visit the project's <a href=\"https://medrec.media.mit.edu\">website</a>.</p>", "people": ["nchinda2@media.mit.edu", "agnescam@media.mit.edu", "kalli@media.mit.edu"], "title": "MedRec", "modified": "2018-10-17T19:22:59.328Z", "visibility": "PUBLIC", "start_on": "2018-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "medrec"}, {"website": "", "description": "<p>The Gamma SENSE (Sensory Engaging Nebulised Scent Experience) is a pioneering instrumental addition to the gamma instrument series. The device delivers multisensory gamma stimulation through auditory, visual, and olfactory channels.&nbsp;</p><blockquote>This medical/musical instrument is based upon our groundbreaking pilot studies revealing (even non-synesthetic) humans link scent to sonic pitches. Due to the steep fall-off rate once gamma stimulation concludes, this olfactory mechanism probes the possibility of slowing 40 Hz frequency attenuation in patients.&nbsp;</blockquote><p>As research continues, the Gamma SENSE device was developed to aid in the testing and identification of the olfactory perceptive triggers which recruit and sustain identified cognitive frequencies. In collaboration with the Aging Brain Alzheimer\u2019s Initiative at MIT, the Gamma SENSE pilots a novel, testing form-factor with the goal of deployment in large-scale clinical trials. The Gamma SENSE instrument also features one of the first touchpad arrangements designed to support EEG testing via the low-motion/high output format. As motion creates unwanted noise artifacts during electroencephalograms, this instrument requires the subtlest of finger-motions to trigger dynamic musical expression. The Gamma SENSE propels myriad frontiers through expression, function, and design.<br></p>", "people": ["arieger@media.mit.edu"], "title": "Gamma SENSE: Sensory Engaging Nebulised Scent Experience", "modified": "2019-04-18T14:12:24.978Z", "visibility": "PUBLIC", "start_on": "2019-01-02", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "gamma-sense"}, {"website": "", "description": "<p>In collaboration with Prof. Uwe Sleytr and Dr. Andreas Breitwieser (BOKU, Vienna, Austria).</p><p>Bioelectronics is an emerging antidisciplinary field which utilizes biomolecules in electronics, mimics biological architectures, or builds electronic-living organism interfaces. One important aspect of the field is to fabricate sensors for label-free biomolecules detection. Researchers previously designed sensors based on i) metal-oxide-semiconductor (MOSFET), ii) polymers, and iii) inorganic crystalline materials which produce good sensitivity, but lack selectivity. Recent efforts are devoted to directly connecting biological receptors with electronic systems. G protein-coupled receptors (GPCRs) serve as suitable candidates as they are the largest family of membrane receptors that detect information (molecules and lights) and transduce to cell internal signals to regulate body functions.&nbsp;</p><p>There are ~1,000 GPCR proteins in human cells, each one highly specific to a particular signal. QTY-designed, detergent-free GPCR receptors can be modified and attached to recombinant-SbpA proteins, which are capable of reproducing two-dimensional crystalline monolayers on various electronic surfaces, as demonstrated by Prof. Uwe Sleytr in Vienna, Austria. SbpA 2D crystalline guides the orientation of the attached GPCRQTY receptors and exposes their active binding sites. The self-assembly yields functional molecules with high density &gt;1012 molecule/cm2. The bioelectronics platform yields detectable electrical, electrochemical, or optical signals in response to the biological stimuli from the receptor layer. When coupled with different types of receptors, this approach may be a platform for bioelectronics and ultrasensitive-sensing systems.</p>", "people": ["shuguang@media.mit.edu", "ruiqing@media.mit.edu"], "title": "Membrane receptor-based high molecular density bioelectronic platform", "modified": "2019-04-19T18:43:36.215Z", "visibility": "LAB-INSIDERS", "start_on": "2019-04-01", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "membrane-receptor-based-high-molecular-density-bioelectronic-platform"}, {"website": "", "description": "<p>Having good mentors and role models is important for personal growth. Knowledge, advice, and inspiration from people a person admires can help motivate people when making life choices. Empirical research has shown that having a role model and insights from a mentor can positively affect the performance and progression of a person's career. However, such advice is not always available from the right people at the right time. Some of our personal heroes have passed away leaving only their writings and other artifacts. We present Wearable Wisdom, a context-aware, audio-based system in a glasses form factor for mediating wisdom from personal mentors to users. Our novel system offers just-in-time knowledge, advice, and inspiration from these mentors, based on the user's inquiry and current context. It does so by performing automated semantic analysis of the mentors' written text and selecting the most relevant quote to the user's inquiry.&nbsp;</p>", "people": ["tomasero@media.mit.edu", "patpat@media.mit.edu", "pattie@media.mit.edu", "minakhan@media.mit.edu"], "title": "Wearable Wisdom", "modified": "2019-04-28T03:26:25.734Z", "visibility": "PUBLIC", "start_on": "2018-10-18", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "wearable-wisdom"}, {"website": "https://affinity.mit.edu", "description": "<p>Affinity is a high-level machine learning API (Application Programming Interface) dedicated exclusively to molecular geometry. Affinity is written in TensorFlow; a small proportion of high-performance code is in low-level C++.  Depending on the application it can be configured as multi-CPU, multi-CPU single GPU, or multi-GPU system. Affinity has  its own web page at <a href=\"http://affinity.mit.edu\">affinity.mit.edu </a><br></p>", "people": ["jacobson@media.mit.edu", "kfirs@media.mit.edu", "mkkr@media.mit.edu"], "title": "Affinity: Deep Learning API for Molecular Geometry", "modified": "2019-04-17T19:38:38.863Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "affinity-tensorflow"}, {"website": "", "description": "", "people": [], "title": "Ocean Cultures", "modified": "2018-04-26T16:33:44.020Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "ocean-cultures"}, {"website": "", "description": "<p>In collaboration with Prof. Tao Fei\u2019s lab (Shanghai Jiaotong University, China)</p><p>In addition to a large number of membrane proteins that comprise most alpha-helix transmembrane segments, there are also many transmembrane proteins with mostly beta-sheets, called beta-barrel membrane proteins. These beta-barrel proteins are often involved in molecular transports.&nbsp;Some are enzymes that are involved in lipid metabolism. We now use the QTY code to design water-soluble beta-barrel transmembrane proteins. Such water-soluble, beta-barrel proteins will facilitate studies of the molecular mechanisms of high selectivity of molecular transport and how these beta-barrel membrane proteins carry out lipid catalysis and metabolism. New insight gained from these studies may be useful for further designs of new molecular devices. &nbsp;</p>", "people": ["shuguang@media.mit.edu"], "title": "QTY code designs for water-soluble beta-barrel transmembrane proteins", "modified": "2019-04-19T18:45:38.116Z", "visibility": "LAB-INSIDERS", "start_on": "2019-04-01", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "qty-code-designs-for-water-soluble-barrel-transmembrane-proteins"}, {"website": "", "description": "<p>\u201cI feel nearer, more a part of the painting, since this way I can walk round it, work from the four sides and literally be in the painting\u201d \u2014 Jackson Pollock</p><p>Pollock Patterns is a fusion of exploratory data visualizations based on realtime location-aware computing and the aesthetics of Abstract Expressionism. The application was developed in C++ using openFrameworks, OpenCV, and liberal use of OpenGL\u2019s Framebuffer Object system. The program reads data directly from a TCP stream and converts it into X,Y coordinates for UWB tags. Each located instance becomes a paint emitter source as it traverses both the physical space and the simultaneously indexed world of the virtual canvas. A physics-based fluid dynamics system determines the rate of flow, viscosity, and surface tension of each paint emitter, resulting in thin, stringy lines as a brush moves quickly, or pools and spatters as a brush slows or changes direction suddenly. Filter operations on the framebuffer, such as blurs and other convolutions, add to the verisimilitude of liquid paint being drizzled across the canvas. Pixel operations are calculated in the GPU, allowing large numbers of tags and effects to be rendered at a reasonable frame rate.</p>", "people": ["novysan@media.mit.edu"], "title": "Pollock Patterns", "modified": "2017-08-17T18:44:23.519Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "pollock-patterns"}, {"website": "", "description": "<p>The future of human habitation in space, from Low Earth Orbit (LEO) to planetary systems far beyond, lies in self-assembling, adaptive, and reconfigurable structures. Rather than transporting the weight of gantries and risking astronaut Extravehicular Activities (EVAs), we can lower payload weight, reduce assembly complexity and revolutionize space-structure modularity by relying on these new paradigms of construction and structure deployment. This project proposes a multi-year research effort to study, characterize and prototype TESSERAE (Tessellated Electromagnetic Space Structures for the Exploration of Reconfigurable, Adaptive Environments). TESSERAE will function as multi-use, low-cost orbiting modules, thus supplying a critical space infrastructure for the next generation of zero gravity habitats, science labs, staging areas for on-surface exploration and more. Unlike large scale habitats proposed for entire space colonies, the TESSERAE should be thought of as flexible and reconfigurable modules to aid in agile mission operations. Our mission concept focuses on supporting Mars surface operations, with multiple, interlocking TESSERAE acting as an orbiting base, in addition to supporting the coming waves of space tourists in Low Earth Orbit.&nbsp;</p>", "people": ["aekblaw@media.mit.edu"], "title": "TESSERAE: Self-assembling Space Architecture", "modified": "2017-11-20T16:01:50.612Z", "visibility": "LAB", "start_on": "2017-06-01", "location": "", "groups": ["responsive-environments", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "tesserae"}, {"website": "", "description": "<p>Wearable Biocomputer explores the intersection of wearable computation and biological computation. We designed on-body interfaces for culturing genetically engineered bacteria to sense, process, and actuate.&nbsp;</p>", "people": ["patpat@media.mit.edu", "amores@media.mit.edu", "rosello@media.mit.edu", "pattie@media.mit.edu"], "title": "Wearable Biocomputer", "modified": "2019-04-29T15:14:59.273Z", "visibility": "LAB", "start_on": "2019-04-27", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "wearable-biotech"}, {"website": "", "description": "", "people": [], "title": "Affinity", "modified": "2017-08-31T20:43:08.848Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "affinity-API"}, {"website": "", "description": "<p>BayesDB is open-source AI software that lets any programmer answer data analysis questions in seconds or minutes with a level of rigor that otherwise requires hours, days, or weeks of work by someone with good statistical judgment. This project empowers domain experts to solve problems themselves, and allows users to easily check results quality by comparing inferred relationships to common-sense knowledge and comparing model simulations to reality and expert opinion. &nbsp;</p><p>More information at the <a href=\"http://probcomp.csail.mit.edu/bayesdb/\">BayesDB project page.&nbsp;</a></p>", "people": [], "title": "BayesDB", "modified": "2018-05-01T19:32:59.551Z", "visibility": "PUBLIC", "start_on": "2017-07-01", "location": "", "groups": ["ethics-and-governance"], "published": true, "active": false, "end_on": null, "slug": "bayesdb"}, {"website": "http://www.poetofcode.com", "description": "<b><a href=\"http://www.ajlunited.org\">www.ajlunited.org</a></b><br><p>An unseen force is rising\u2014helping to determine who is hired, granted a loan, or even how long someone spends in prison. This force is called the coded gaze.</p><p> </p><p>However, many people are unaware of the growing impact of the coded gaze and the rising need for fairness, accountability, and transparency in coded systems. Without knowing discriminatory practices are at play, citizens are unable to affirm their rights or identify violations.</p><p>The Algorithmic Justice League aims to:</p><ol><li>highlight algorithmic bias through provocative media and interactive exhibitions<br></li><li>provide space for people to voice concerns and experiences with coded discrimination&nbsp;</li><li>develop practices for accountability during the design, development, and deployment phases of coded systems.</li></ol>", "people": ["ethanz@media.mit.edu", "joyab@media.mit.edu"], "title": "Algorithmic Justice League", "modified": "2018-04-30T15:28:35.674Z", "visibility": "PUBLIC", "start_on": "2016-10-14", "location": "", "groups": ["center-for-civic-media", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "algorithmic-justice-league"}, {"website": "", "description": "<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Near-infrared imaging offers a non-ionizing alternative for dental analysis. We describe the construction and validation of a near-infrared imaging device to identify dental caries without the use of radiographs. It uses 850 nm light, allowing for a low-cost sensor and device construction.</p><p><strong>Why is this work important?</strong></p><p>While two-dimensional radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. Much previous work has focused on light at 1310 nm, which strikes a balance between enamel and water attenuation, but such a wavelength often requires expensive sensors to image. NIR light at 850 nm has similar dental imaging properties, but it has not been studied as thoroughly as NIR at 1310 nm. Previous studies have similarly neglected the extent to which indicators of dental health, especially early caries associated with the onset of more severe conditions, can be identified in 850 nm NIR images.</p><p><strong>What are our contributions?</strong></p><p>We describe the construction of a near-infrared imaging device to identify dental caries without the use of radiographs. Light-emitting diodes at 850 nm allow for the use of a low-cost imaging sensor. Its camera-wand design allows for multiple imaging configurations: reflectance, transillumination, and occlusal transillumination. We validate the diagnostic uses for the images produced by our device, determining that they provide insight into the location of caries without ionizing radiation. The camera-wand system was also capable of revealing demineralized areas, deep and superficial cracks, and other clinical features of teeth usually only visualized by X-rays.</p><p><strong>What are the next steps?</strong></p><p>Ongoing work is being done to analyze the extent of features made visible by our device and to model the interaction of light inside teeth in order to provide even more diagnostic power.</p><p><strong>Related projects</strong></p><ol><li><a href=\"https://www.media.mit.edu/projects/near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging/overview/\">Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging</a></li><li><a href=\"https://www.media.mit.edu/projects/replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis/overview/\">Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography</a></li></ol>", "people": ["pratiks@media.mit.edu", "kla11@media.mit.edu"], "title": "Near-Infrared Imaging for Detecting Dental Caries", "modified": "2018-10-21T18:23:04.788Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth"}, {"website": "", "description": "<p>In collaboration with Professor Sara Linse\u2019s laboratory (Lund University, Sweden)</p><p>Although much is known about the 42-residue Abeta peptide (Abeta42) and its detailed molecular structure, there is still no effective remedy for treatment of devastating Alzheimer\u2019s disease that costs the world enormously economically. Thus, additional efforts and innovative ways are needed to find some effective treatment. In collaboration with Professor Sara Linse\u2019s lab in Lund University, we will use the QTY code to design more water-soluble variants of Abeta42. QTY variants have the potential to retain their molecular structures, interact, and incorporate into the natural Abeta42 amyloid fibrils.&nbsp; Such molecular interactions may de-stabilize the natural Abeta42 fibrils that pack very tightly to form plaques in the Alzheimer\u2019s disease brain. &nbsp;With further design, the water-soluble Abeta42 may fuse with specific protein degrading enzymes or other cleaning systems to disrupt and remove pathogenic amyloid fibrilsor plaques. &nbsp;</p>", "people": ["shuguang@media.mit.edu"], "title": "QTY code for highly water-soluble Abeta42 involved in Alzheimer\u2019s disease", "modified": "2019-04-19T18:46:20.877Z", "visibility": "LAB-INSIDERS", "start_on": "2019-06-03", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "qty-code-for-highly-water-soluble-abeta42-involved-in-alzheimer-s-disease"}, {"website": "", "description": "<h2><b>Lab on Body, Synthetic Biology, and Bio-Digital Systems for Health and Human Enhancement</b></h2>", "people": ["dkong@media.mit.edu", "abyjain@media.mit.edu", "patpat@media.mit.edu", "amores@media.mit.edu", "rosello@media.mit.edu", "pattie@media.mit.edu", "avujic@media.mit.edu"], "title": "Theme | Wearable Biotech Enhancement", "modified": "2019-05-10T15:15:39.601Z", "visibility": "PUBLIC", "start_on": "2019-02-03", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "Biological-Enhancement"}, {"website": "", "description": "<p>SYNTHetic Biology \u2014 A Tribute to Greg Bear\u2019s <i>Blood Music</i></p><p>A musical ecosystem &nbsp;driven by and reacting to the movement of living biological organisms. The bio-SYNTH can take input from a realtime USB microscope or utilize captured video (as in this case).</p><p>Bacteria are tracked and elements such as size, age, velocity, and positions relative to the screen and each other are fed as seed values into a granular synthesizer. Waves are altered and samples are triggered according to an editable rule set.</p><p>Samples from the Mercury and Apollo space program, <i>Sputnik</i>, and science fiction films were inspired by Greg Bear\u2019s 1985 Nebula and Hugo award winning novel <i>Blood Music</i>, and suggest the movements of the quickly evolving \"noocytes\" as they begin to explore their god-like host being Vergil Ulam.</p><p>The installation premiered at the Peabody Essex Museum in Salem, MA on September 18, 2014 and part of the After Hours PEM PM Series.</p>", "people": ["novysan@media.mit.edu"], "title": "SYNTHBacteria", "modified": "2017-08-17T18:47:12.594Z", "visibility": "PUBLIC", "start_on": "2014-08-15", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "synthbacteria"}, {"website": "", "description": "<p>SCUBA diving as a sport has enabled people to explore the magnificent ocean diversity of beautiful corals, striking fish, and mysterious wrecks. However, only a small number of people are able to experience these wonders, as diving is expensive, mentally and physically challenging, needs a large time investment, and requires access to large bodies of water. Most existing SCUBA diving simulations in VR are limited to visual and aural displays. We propose a virtual reality system, Amphibian, that provides an immersive SCUBA diving experience through a convenient terrestrial simulator. Users lie on their torso on a motion platform with their outstretched arms and legs placed in a suspended harness. Users receive visual and aural feedback through the Oculus Rift head-mounted display and a pair of headphones. Additionally, we simulate buoyancy, drag, and temperature changes through various sensors.</p>", "people": ["djain@media.mit.edu", "geek@media.mit.edu"], "title": "Amphibian: Terrestrial SCUBA Diving Simulator Using Virtual Reality", "modified": "2018-08-03T18:51:54.353Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "amphibian-terrestrial-scuba-diving-simulator-using-virtual-reality"}, {"website": "", "description": "<p>A conversational, voice-based interface for creating and playing Scratch projects makes Scratch accessible to children regardless of visual ability. Just as Scratch\u2019s visual language lowers the barrier to entry for sighted children, the conversational interface lowers the barrier for children with visual impairments. The screenless interface is inspired by voice assistants and demonstrates the potential for programming through conversation.</p>", "people": ["quacht@media.mit.edu", "ascii@media.mit.edu"], "title": "Agent-based programming interfaces for children", "modified": "2019-04-19T18:38:49.254Z", "visibility": "PUBLIC", "start_on": "2018-02-13", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "making-scratch-accessible"}, {"website": "http://bit.ly/2nMCs3C", "description": "<h2><b>General Overview</b></h2><p>Staining of tissues sections using chemical and biological dyes has been used for over a century for visualizing various tissue types and morphologic changes associated with contemporary cancer diagnosis.&nbsp;The staining procedure however is labor intensive, needs trained technicians, costly, and often results in loss of irreplaceable specimen and delays diagnoses. In collaboration with Brigham and Women's Hospital (Boston, MA), we&nbsp; describe a \u201ccomputational staining\u201d approach to digitally stain photographs of unstained tissue biopsies with Haematoxylin and Eosin (H&amp;E) dyes to diagnose cancer.&nbsp;&nbsp;</p><p>Our method uses neural networks to rapidly stain photographs of non-stained tissues, providing physicians timely information about the anatomy and structure of the tissue.&nbsp; We also report a&nbsp; \"computational destaining\" algorithm that can remove dyes and stains from photographs of previously stained tissues, allowing reuse of patient samples.&nbsp;&nbsp;</p><p>These methods and neural networks assist physicians and patients by novel computational processes&nbsp;at the point-of-care,&nbsp;which can integrate seamlessly into clinical workflows in hospitals all over the world.</p>", "people": ["pratiks@media.mit.edu", "gyauney@media.mit.edu", "arana@media.mit.edu"], "title": "AI Methods for Rapid Automated Staining and Destaining of Tissue Biopsies in Hospitals", "modified": "2018-12-20T15:09:03.246Z", "visibility": "PUBLIC", "start_on": "2018-05-01", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "computational-histological-staining-and-destaining-of-prostate-core-biopsy-rgb-images-with-generative-adversarial-neural-networks-1"}, {"website": "", "description": "<p>Bringing together a diverse group of stakeholders, this pilot examines the intersection between AI and inclusion, and explores the ways in which AI systems can be designed and deployed to support diversity and inclusiveness in society. While promoting learning opportunities through engagement in events, learning calls, and joint research activities, the pilot is making educational resources and other outputs accessible via a variety of platforms. Thematically, an emphasis has been placed on the impact of AI on underserved groups \u2013 whether in terms of age, ethnicity, race, gender and sexual identity, religion, national origin, location, skill and educational level, and/or socioeconomic status \u2013 and on how these communities think about AI systems.\n                    \n                </p>", "people": [], "title": "AI and Inclusion", "modified": "2017-07-13T18:14:15.950Z", "visibility": "PUBLIC", "start_on": "2017-07-01", "location": "", "groups": ["ethics-and-governance"], "published": true, "active": false, "end_on": null, "slug": "ai-and-inclusion"}, {"website": "https://jods.mitpress.mit.edu/", "description": "<p>The worlds of Design and Science are inextricably linked. Science is informed by Design. Design is a science. The interactions between the two are powerful, subtle, often unrecognized.</p><p>The <a href=\"https://jods.mitpress.mit.edu\"><i>Journal of Design and Science</i> (JoDS)</a>, a joint venture of the MIT Media Lab and the MIT Press, forges new connections between science and design, breaking down the barriers between traditional academic disciplines in the process. Targeting readers with open, curious minds, JoDS explores timely, controversial topics in science, design, and society with a particular focus on the nuanced interactions among them.</p><p>Challenging both traditional academic silos and the established publishing practices associated with them, JoDS is hosted on a collaborative, open-access, open-review, rapid publication platform that actively invites community participation and supports rich, multimedia content. JoDS empowers authors and readers to engage in fruitful, ongoing and above all stimulating conversation and debate about their work and ideas.</p><p>All JoDS articles are published under a CC-BY 4.0 License.</p>", "people": [], "title": "The Journal of Design and Science", "modified": "2018-02-05T16:27:05.813Z", "visibility": "PUBLIC", "start_on": "2018-02-04", "location": "", "groups": ["directors-office"], "published": true, "active": false, "end_on": null, "slug": "jods"}, {"website": "", "description": "<p>Social robots are increasingly being developed for long-term interactions with children in domains such as healthcare, education, therapy, and entertainment.&nbsp;In prior research, we have seen that children treat robots as more than mere artifacts, e.g., ascribing them mental states, psychological attributes, and moral standing. Thus, while children\u2019s relationships with robots may not be like the relationships they have with their parents, pets, imaginary friends, or smart devices, they will form relationships of some kind.&nbsp;As such, we need to deeply understand how children\u2019s relationships with robots develop through time, and find ways to characterize and measure these relationships.&nbsp;However, there are few validated assessments for measuring young children\u2019s long-term relationships. Thus, we have adapted or created a variety of assessments for use in this context for children aged 5-6 years.&nbsp;</p><p><a href=\"https://dam-prod.media.mit.edu/x/2018/04/25/KoryWestlund-IDC18.pdf\">Four of these assessments are presented in the associated paper.</a></p><p>This paper shows that children can appropriately respond to these assessments with reasonably high internal reliability, and that these assessments are able to capture child-robot relationship adjustments&nbsp; over a long-term interaction.</p>", "people": ["cynthiab@media.mit.edu", "jakory@media.mit.edu", "randiw12@media.mit.edu", "haewon@media.mit.edu"], "title": "Assessing Children's Relationships with Social Robots", "modified": "2019-04-17T18:39:40.951Z", "visibility": "PUBLIC", "start_on": "2017-04-01", "location": "", "groups": ["ml-learning", "personal-robots"], "published": true, "active": false, "end_on": null, "slug": "relationship-assessments"}, {"website": "http://duoskin.media.mit.edu", "description": "<p>DuoSkin is a fabrication process that enables anyone to create customized functional devices that can be attached directly to the skin. Using gold metal leaf, a material that is cheap, skin-friendly, and robust for everyday wear, we demonstrate three types of on-skin interfaces: sensing touch input, displaying output, and wireless communication. DuoSkin draws from the aesthetics found in metallic jewelry-like temporary tattoos to create on-skin devices which resemble jewelry. DuoSkin devices enable users to control their mobile devices, display information, and store information on their skin while serving as a statement of personal style. We believe that in the future, on-skin electronics will no longer be black-boxed and mystified; instead, they will converge towards the user friendliness, extensibility, and aesthetics of body decorations, forming a DuoSkin integrated to the extent that it has seemingly disappeared.</p><p><span style=\"font-size: 18px; font-weight: normal;\">Credits:<br></span><span style=\"font-size: 18px; font-weight: normal;\">Cindy Hsin-Liu Kao, Asta Roseway*, Christian Holz*, Paul Johns*, Andres Calvo, Chris Schmandt<br></span><span style=\"font-size: 18px; font-weight: 400;\">MIT Media Lab in collaboration with Microsoft Research*</span></p>", "people": ["cindykao@media.mit.edu", "andresc@media.mit.edu", "geek@media.mit.edu"], "title": "DuoSkin", "modified": "2017-10-17T18:38:36.804Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "2ndskin"}, {"website": "", "description": "<p>This research presents water-based robotic fabrication as a design approach and enabling technology for additive manufacturing (AM) of biodegradable hydrogel composites. We focus on expanding the dimensions of the fabrication envelope, developing structural materials for additive deposition, incorporating material-property gradients, and manufacturing architectural-scale biodegradable systems. The technology includes a robotically controlled AM system to produce biodegradable composite objects, combining natural hydrogels with other organic aggregates. It demonstrates the approach by designing, building, and evaluating the mechanics and controls of a multi-chamber extrusion system. Finally, it provides evidence of large-scale composite objects fabricated by our technology that display graded properties and feature sizes ranging from micro- to macro-scale. Fabricated objects may be chemically stabilized or dissolved in water and recycled within minutes. Applications include the fabrication of fully recyclable products or temporary architectural components, such as tent structures with graded mechanical and optical properties.</p>", "people": ["yjtai@media.mit.edu", "dumo@media.mit.edu", "bader_ch@media.mit.edu", "jvanzak@media.mit.edu", "nah6cz@media.mit.edu", "neri@media.mit.edu", "asling@media.mit.edu", "j_duro@media.mit.edu"], "title": "Water-Based Additive Manufacturing", "modified": "2019-06-04T21:35:42.589Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "water-based-additive-manufacturing"}, {"website": "", "description": "<p>The MIT Knowledge Futures Group (KFG), a joint venture of the MIT Media Lab and the MIT Press, is an incubator for early-stage technologies that form part of a new open knowledge ecosystem. The partnership is the first of its kind between a leading publisher and a world-class research lab designing technologies of the future. The KFG seeks to incubate projects that enrich our open knowledge infrastructure, and leading by example, to spark a movement towards greater institutional ownership of that infrastructure. <br></p><p>The KFG currently incubates PubPub, an open authoring and publishing platform initially developed as a Media Lab project, by deploying it with dozens of MIT Press books and journals. PubPub socializes the process of knowledge creation by integrating conversation, annotation, and versioning into short and long-form digital publication. One of the flagship publications on PubPub is the Journal of Design and Science, which forges new connections between science and design and breaks down the barriers between academic disciplines. We envision JoDS as the node in a global online community rooted in the Media Lab\u2019s research and design ethos. </p><p>The KFG also incubates The Underlay, an open, distributed knowledge store architected to capture, connect, and archive publicly available knowledge and its provenance. The Underlay provides mechanisms for distilling the knowledge graph from openly available publications, along with the archival and access technology to make the data and content hosted on PubPub available to other platforms. <br></p>", "people": ["trich@media.mit.edu", "joi@media.mit.edu"], "title": "MIT Knowledge Futures Group", "modified": "2018-08-23T19:38:55.666Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "mit-knowledge-futures-group"}, {"website": "", "description": "<p>Ritual&nbsp;I: The Thing Itself consists of a choreographed robotic body that is in constant flux. It performs a dance of repetitive patterns that&nbsp;become&nbsp;a trance ritual of vibrations and movement. The thing or dancing body stands on a metal sheet that vibrates with every move it makes; this way the body affects its territory with every movement. In return, the vibrations of the metal&nbsp;add&nbsp;to the vibration of the thing itself while it moves, and in this way the body is affected by its territory.&nbsp;</p><p>This is a feedback system, a cyclic loop, a transduction network, a ritual dance between a body and its territory. This ritual explores how agency becomes increasingly distributed among bodies and territories, which opens interactions of hybrid selves, blurring the limits of bodies and its environment, understanding them all as an assemblage of vibrant matter.&nbsp;The architecture&nbsp;comprises complex assemblages\u2014nothing is something by itself, but things are themselves by being in&nbsp;a relationship&nbsp;with others. This is an entangled architecture of bodies. This is a way to explore and diversify the imaginative projections and potentials of a kinetic non-human body and how sound and vibration are key to trigger agency and vibrant presence.</p><blockquote>It is the thing itself that has been allowed to be deployed as multiple, and thus allowed to be grasped through different viewpoints, before being possibly unified in some later stage depending on the abilities of the collective to unify them.<br></blockquote><p>\u2014Bruno Latour,&nbsp;<i>Assembling the Social: An Introduction to Actor-Network-Theory</i>&nbsp;(Oxford: Oxford University Press, 2005), 116.&nbsp;</p>", "people": ["nicolelh@media.mit.edu"], "title": "Ritual I: The Thing Itself", "modified": "2018-10-19T19:41:24.966Z", "visibility": "PUBLIC", "start_on": "2018-05-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "ritual-i-the-thing-itself"}, {"website": "", "description": "<p>The brain uses space to index,&nbsp;organize, and retrieve memories. However, our sense of space depends on our perception of gravity. We plan to test and understand the effect&nbsp;of altering gravity on human memory. Our experiment consists of a virtual reality experience that exposes the user to a sequence of small random mazes. We will compare the results of the experiment under different gravitational conditions.</p>", "people": ["mmv@media.mit.edu"], "title": "VR Maze in Zero Gravity", "modified": "2017-11-21T20:53:28.558Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["fluid-interfaces", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "vr-maze-in-zero-gravity"}, {"website": "", "description": "<p>ghgjhgjghj</p>", "people": [], "title": "test project\nasdasdas", "modified": "2018-03-21T19:23:19.892Z", "visibility": "LAB", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "test-project-9997898779"}, {"website": "", "description": "<p>We demonstrate a method for augmenting existing visual interfaces, including 3D and conventional displays, with haptic feedback capabilities, by utilizing a large number of closely spaced vortex-ring generators mounted along the periphery of the display. We present our first prototype of a multimodal interactive interface platform with 16 independently-controlled air-vortex ring&nbsp; generators with one angular degree of freedom each. Our system has applications as an interactive interface, as a research tool, as an automotive control interface, and as a platform for creative expressions.</p>", "people": ["alims@media.mit.edu", "vmb@media.mit.edu"], "title": "AirTap", "modified": "2018-05-04T11:46:32.043Z", "visibility": "PUBLIC", "start_on": "2017-02-02", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "airtap"}, {"website": "", "description": "<p>What if you could not only see but also feel virtual objects as you interacted with them? This would enable richer and more realistic user experiences. We have designed a low-cost air-vortex generator to provide midair haptic feedback when a user touches virtual objects displayed on holographic, aerial, and other 3D displays. The system consists of a 3D-printed chamber and nozzle, five low-frequency transducers, and a custom-designed driver board. The air-vortex generator can provide localized haptic feedback to a range of over 100cm. With increased driving power and a more optimized nozzle design, this range could be extended to several meters. </p>", "people": ["alims@media.mit.edu", "vmb@media.mit.edu"], "title": "Free-Space Haptic Feedback for 3D Displays", "modified": "2018-05-01T01:37:57.436Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "terrestrial-sensing", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "free-space-haptic-feedback-for-3d-displays"}, {"website": "", "description": "<h2>Combining the art of two worlds\u2013fashion and biology\u2013into one.&nbsp;</h2><p>Conforming Materials is working towards designing fully recyclable clothing. Through the development of a biomaterial that changes from a crystallized powder into a solid and back, at just the right temperatures, we have harnessed a breakthrough technology that could change fashion as we know it. This biomaterial can be manipulated in a multitude of ways due to its hydrophilic and hydrophobic properties. For example, imagine having a dress that can be tailored to encapsulate smell, eliminating the need for perfume. Imagine wanting a new wardrobe and being able to 100% recycle the biomaterial due to it being made solely of biological components. Imagine going for a run and your clothes trap the odor of your sweat and release a pleasant scent in its place. The possibilities of this material to be used in clothing design are endless. Electronics can be integrated into the design process due to the ability of the biomaterial to be conductive and overlaid with conductive ink. This component would enable analysis on biometrics and human data, with a wide variety of applications including sports clothing.</p>", "people": ["manisham@media.mit.edu"], "title": "Conforming Materials", "modified": "2018-04-20T16:47:40.289Z", "visibility": "PUBLIC", "start_on": "2014-12-15", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "conforming-materials"}, {"website": "", "description": "<p>\u201c<i>The Talking Drums</i>\u201d is a sound installation where we created a modular system for sending images through rhythmic patterns. It is an encrypted language to empower a musical community by sending secret messages, avoiding surveillance and listening to each other. It\u2019s inspired by the communication method and instrument used by African communities to send messages across the continent. They did this by drumming, and by a rhythmic language that only could be understood by the community.</p><p>For the installation we collected different objects from MIT\u2019s dump, especially obsolete technologies, such as hard drives and old screens.&nbsp;</p>", "people": ["thomassl@media.mit.edu", "yasushis@media.mit.edu", "nicolelh@media.mit.edu"], "title": "Talking Drums", "modified": "2018-10-19T19:49:21.579Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "talking-drums"}, {"website": "", "description": "<p>Ancient yet modern, enclosing yet invisible, glass was first created in  Mesopotamia and Ancient Egypt 4,500 years ago. Precise recipes for its  production - the chemistry and techniques - often remain closely guarded  secrets. Glass can be molded, formed, blown, plated or sintered; its  formal qualities are closely tied to techniques used for its  formation.&nbsp;From the discovery of core-forming process for bead-making in  ancient Egypt, through the invention of the metal blow pipe during  Roman times, to the modern industrial Pilkington process for making  large-scale flat glass; each new breakthrough in glass technology  occurred as a result of prolonged experimentation and ingenuity, and has  given rise to a new universe of possibilities for uses of the material.  <br></p>", "people": ["m_kayser@media.mit.edu", "inamura@media.mit.edu", "dlizardo@media.mit.edu", "mlstern@media.mit.edu", "neri@media.mit.edu", "j_klein@media.mit.edu"], "title": "Glass I", "modified": "2017-10-13T19:43:29.330Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "g3p"}, {"website": "http://www.jinjoolee.com", "description": "<p>Emotionally supportive robots improve overall team functioning!</p>", "people": ["cynthiab@media.mit.edu", "siggi@media.mit.edu", "jinjoo@media.mit.edu", "ndepalma@media.mit.edu"], "title": "Emotionally Aware Robot Teammates", "modified": "2018-10-19T14:34:12.238Z", "visibility": "PUBLIC", "start_on": "2013-02-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "emotionally-aware-robot-teammates"}, {"website": "", "description": "<h1><b>aDRENA</b></h1><h2><br></h2><h2>A Digital Research and Experimentation Network for Agriculture </h2><p>Addressing complex and interlinked challenges requires a globally collaborative, digitally-enabled agricultural revolution.&nbsp;</p><p>This project focuses on three main elements:</p><ol><li><b>The Platform:</b> building an open access platform of &nbsp;hardware, software, and data libraries</li><li><b>The Community: </b>developing and engaging a community of people around the world who are collaborating and sharing data via a networked approach</li><li><b>OpenAg Proliferation:</b> supporting the launch of the OpenAg platform in 10 diverse locations around the world, creating a global network of research, replication, data collection, sharing, and learning from various applications.</li></ol><p>The three main elements enable the networking, scaling, and deployment of agricultural research while simultaneously educating and building capacity for the next generation of digital farmers. In addition, this approach to data collection and sharing creates a de facto standard to ensure interoperability of agricultural and climatic data sets that are open and accessible. &nbsp;</p>", "people": ["calebh@media.mit.edu", "rbaynes@media.mit.edu", "rebekahj@media.mit.edu", "delapa@media.mit.edu"], "title": "aDRENA", "modified": "2019-04-28T23:47:42.260Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["open-agriculture-openag"], "published": true, "active": false, "end_on": null, "slug": "adrena"}, {"website": "", "description": "<p>ModeSense is a full stack system that enables indoor environments to become aware of what is happening in them, and then enables the environment to locally inform (offline) all nearby phones and other electronic devices about the most appropriate operating mode in the present time and space.&nbsp; We have developed an ultra-low cost, $5 device that can be installed in conference rooms, lecture halls, movie theaters, homes, and cars, which can dynamically determine the contexts in those areas and then locally broadcast the corresponding mode. Any phones in those areas are then aware of the mode in which they ought to be at that time, and can change their behavior accordingly.</p>", "people": ["alims@media.mit.edu", "vmb@media.mit.edu"], "title": "ModeSense", "modified": "2018-05-01T02:35:01.568Z", "visibility": "PUBLIC", "start_on": "2016-10-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "modesense"}, {"website": "", "description": "<p>Replot is a new and soon-to-be-open sourced visualization library for the web, built natively on the ReactJS, the extremely popular open-source web framework that powers most modern websites today.</p><p>Replot is written from the ground up in React, rendering native SVG visualizations and fully leveraging the idea of user interfaces being pure functions of an underlying data \"state.\" This enables developers to construct rich and engaging visualizations with very few lines of code that bind automatically to their datasets, and animate automatically when their data change.&nbsp;</p><p>Replot also introduces a paradigm shift in customizability of your visualizations, enabling you to trigger transformations in your visualizations by simply manipulating state variables.</p>", "people": ["sanjayg@media.mit.edu", "almaha@media.mit.edu", "hidalgo@media.mit.edu"], "title": "Replot", "modified": "2017-10-11T19:58:05.810Z", "visibility": "LAB-INSIDERS", "start_on": "2017-06-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "replot"}, {"website": "", "description": "<p>Activ8 is a system of three short games: See-Saw, a balancing game for Google Glass; Jump Beat, a music beat matching game for Google Glass; and Learning to Fly, a Kinect game where two users keep a virtual bird in the air by flapping their arms in sync. Recent epidemiological evidence points at sitting as being the most common contributor to an inactive lifestyle.  We aim to offer a starting point towards designing and building an understanding about how \"physical casual games\" can contribute to helping address the perils of sitting.</p>", "people": ["sra@media.mit.edu", "geek@media.mit.edu"], "title": "Activ8", "modified": "2018-04-20T16:49:04.978Z", "visibility": "PUBLIC", "start_on": null, "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "activ8"}, {"website": "", "description": "", "people": [], "title": "Test project the zillionth", "modified": "2017-10-27T14:27:58.844Z", "visibility": "PUBLIC", "start_on": "2017-10-04", "location": "", "groups": ["collective-learning"], "published": false, "active": false, "end_on": null, "slug": "test-project-the-zillionth"}, {"website": "", "description": "<p>We investigate the nature of toxic conversations on Twitter, focusing on the important subset of public discourse related to mainstream news. We analyze 71,000 conversations prompted by tweets from five major news outlets, comprising more than 12.3 million tweets and replies posted by 1.13 million users. We study the relationship between tweet toxicity, the reply-tree structure of the tweets, and the network of social connections between the tweet authors at three main levels: the individual level of the tweet authors, the dyadic level (between a tweet author and a reply author), and the overall reply-tree and follow graph structure of the conversation.</p>", "people": ["dkroy@media.mit.edu", "msaveski@media.mit.edu", "bcroy@media.mit.edu"], "title": "Conversation health: Twitter toxicity and network structure", "modified": "2019-04-10T20:19:57.770Z", "visibility": "LAB-INSIDERS", "start_on": "2018-09-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "conversation-health-twitter-toxicity-network-structure"}, {"website": "", "description": "", "people": ["kapeloni@media.mit.edu"], "title": "Escape Pod", "modified": "2017-10-13T16:05:41.451Z", "visibility": "PUBLIC", "start_on": "2017-10-10", "location": "", "groups": ["city-science"], "published": false, "active": false, "end_on": null, "slug": "escape-pod"}, {"website": "", "description": "", "people": ["pewebb@media.mit.edu", "amosg@media.mit.edu", "ken_n@media.mit.edu", "jifei@media.mit.edu"], "title": "Auto-Inflatables", "modified": "2017-11-25T17:16:39.275Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "auto-inflatables"}, {"website": "", "description": "<p>FIBERBOTS is a digital fabrication platform fusing cooperative robotic manufacturing with abilities to generate highly sophisticated material architectures. The platform can enable design and digital fabrication of large-scale structures with high spatial resolution leveraging mobile fabrication nodes, or robotic \"agents\" designed to <i>tune</i> the material make-up of the structure being constructed on the fly as informed by their environment.<br></p><p>Some of nature\u2019s most successful organisms collaborate in a swarm fashion. Nature\u2019s builders leverage hierarchical structures in order to control and optimize multiple material properties. Spiders, for instance, spin protein fibers to weave silk webs with tunable local and global material properties, adjusting their material composition and fiber placement to create strong yet flexible structures optimized to capture prey. Other organisms, such as bees, ants and termites cooperate to rapidly build structures much larger than themselves. </p>", "people": ["m_kayser@media.mit.edu", "cail@media.mit.edu", "bader_ch@media.mit.edu", "sfalcone@media.mit.edu", "jpcosta@media.mit.edu", "darweesh@media.mit.edu", "nassia@media.mit.edu"], "title": "FIBERBOTS: Design of a multi-agent, fiber composite digital fabrication system", "modified": "2019-02-13T16:36:22.730Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "fiberbots"}, {"website": "", "description": "<p>design(human)design is a tool that builds on insights about the design process inspired by research carried out at IDEO Cambridge. The idea is that there are a few design \"variables\" that designers play with, and that they often like to provoke their creativity with \"random but purposeful\" inspirations\u2014which present the designer with a random selection of design variables to act as a \"structured serendipitous\" creative prompt.</p><p>design(human)design comprises a deck of cards that act as a creative game to prompt new design ideas. The deck consists of sets of cards containing examples of each of the design variables, e.g. an object, an app, a book, etc., for the artifact design variable. Designers randomly select a card from each of the variable sets and create their design inspired by the MadLibs-style prompt sentence construct. Blank cards for each of the variables are also included so that designers can add their own examples.</p><p>To make the complete and selective randomization and personalization of the variables even easier, an interactive design(human)design website&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">was created at <a href=\"http://designhumandesign.media.mit.edu\">designhumandesign.media.mit.edu</a>. The list of design variables are contained in a Google Spreadsheet that again can be added to by designers to customise their prompts.</span></p>", "people": ["pip@media.mit.edu"], "title": "design(human)design", "modified": "2017-09-13T15:06:37.907Z", "visibility": "PUBLIC", "start_on": "2016-07-18", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "design-human-design"}, {"website": "", "description": "<p><b>Leveraging sneaker culture to influence civic engagement.&nbsp;</b></p><p>Marginalized groups have influence on digital platforms but are often unheard in other forums. We show how culturally resonant physical artifacts extend community reach and impact. &nbsp;We leverage the sneaker, a growing political symbol and cultural influence, to impact community participation.</p>", "people": ["britneyj@media.mit.edu"], "title": "Kicks x Cliques", "modified": "2019-05-30T18:58:37.122Z", "visibility": "PUBLIC", "start_on": "2018-10-11", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "footwear-in-action"}, {"website": "", "description": "<p>Here we include a suite of projects that migrate information to personal devices and systems instead of centralizes silos. In part this take you personal device seriously\u2014it is more than a window into the cloud\u2014it can store your information for your purposes.</p><p>Blockchains have spawned new ways of looking at security, trust, and consensus. These are now design variables that allow diverse communities to develop networks with permanence&nbsp; and agreement that have no central authority. We explore trust as a variable, building blockchain-based systems that separate transactions and currency from the utility of a shared, uneditable ledger.</p>", "people": ["lip@media.mit.edu"], "title": "Theme | Decentralized Systems", "modified": "2019-04-18T02:42:59.862Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "decentralized-systems"}, {"website": "", "description": "<p>The Guardians project aims to use the same game design principles used in mobile game platforms to create greater engagement with individuals. The Affective Computing group is developing a custom video game with an independent patient reporting outcome tool to increase adherence to completion of&nbsp;patient reported outcomes.</p><p>Forming positive health habits can be difficult. Whether it\u2019s taking medication, sticking to a diet, or going to the gym, it\u2019s tough to commit to a new schedule long enough to form a habit. It is even more difficult when a person is asked to do something regularly that does not directly and immediately benefit them. This is an&nbsp;issue when clinical researchers need study participants to report outcomes regularly over a long period of time. Adherence is lost, resulting in suboptimal clinical outcomes and the loss of important data.</p><p>Mobile video&nbsp;games, on the other hand, generate an increased amount of adherence (<a href=\"https://venturebeat.com/2017/02/01/superdata-mobile-games-hit-40-6-billion-in-2016-matching-world-box-office-numbers/\">as seen by an&nbsp;estimated market revenue of over $40.6 billion in 2016</a>**). Mobile video games have captured the attention of a wide variety of demographics and are often targeted to specific subgroups in order to increase engagement with a number of in-game features. These games use common design techniques and mechanics to produce a loop that draws players to return&nbsp;on a regular schedule and encourages them to watch ads, share on social media, or pay a fee for special rewards within the game.</p><p>By using the same game design principles, we aim to replace typical video game behaviors, like watching ads or sharing on social, with new behaviors that help improve the player\u2019s wellbeing.</p><p>This project is a collaboration between the Affective Computing&nbsp;research group and Media Lab member company Takeda Pharmaceuticals.</p><p>**according to research by SuperData Research and Unity Technologies</p>", "people": ["fergusoc@media.mit.edu", "picard@media.mit.edu", "sataylor@media.mit.edu", "fpeng@media.mit.edu"], "title": "The Guardians", "modified": "2019-05-16T15:17:42.342Z", "visibility": "PUBLIC", "start_on": "2017-04-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "human-adherence"}, {"website": "http://bit.ly/2nMCs3C", "description": "<p><b>General overview:</b></p><p>Sepsis, a life-threatening complication of bacterial infection, leads to millions of worldwide deaths requires significant time and resources to diagnose. This disease is associated with very high mortality rates, making early detection crucial for treatment.&nbsp;</p><p>Researchers have investigated direct clinical evaluation by using dark field imaging of capillary beds under the tongue of septic and healthy subjects for signatures of microcirculatory dysfunction associated with sepsis. Our published results, in collaboration with Beth Israel Deaconess Medical Center, have shown that machine learning and vision can learn higher-order hierarchical diagnostic and prognostic features for rapid and non-invasive diagnosis of sepsis using these dark field microcirculatory images.&nbsp;A neural network capable of distinguishing between images from non-septic and septic patients with more than 90% accuracy is reported for the first time. This approach can help physicians to rapidly stratify patients, facilitate rational use of antibiotics, and reduce disease burden in hospital emergency rooms.</p>", "people": ["pratiks@media.mit.edu", "pjavia@media.mit.edu", "arana@media.mit.edu"], "title": "Helping Emergency Care Physicians Diagnose Sepsis and Bacterial Infections  with Machine Learning and Vision", "modified": "2018-12-20T18:14:27.714Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "machine-learning-algorithms-for-classification-of-microcirculation-images-from-septic-and-non-septic-patients-1"}, {"website": "", "description": "<p>A new method to detect and distinguish between different types of fluorescent materials. The suggested technique has provided a dramatically larger depth range compared to previous methods; thus it enables medical diagnosis of body tissues without removing the tissue from the body, which is the current medical standard. It uses fluorescent probes, which are commonly used in medical diagnosis. One of these parameters is the fluorescence lifetime, that is the average time the fluorescence emission lasts. The new method can distinguish between different fluorescence lifetimes, which allows diagnosis of deep tissues. Locating fluorescence probes in the body using this method can, for example, indicate the location of a tumor in deep tissue, and classify it as malignant or benign according to the fluorescence lifetime, thus eliminating the need for X-ray or biopsy.</p>", "people": ["raskar@media.mit.edu", "guysatat@media.mit.edu", "barmak@media.mit.edu"], "title": "Towards In-Vivo Biopsy", "modified": "2018-03-29T20:34:54.336Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "towards-in-vivo-biopsy"}, {"website": "", "description": "", "people": ["alonsolp@media.mit.edu"], "title": "City Science Anodrra", "modified": "2017-10-10T18:39:43.857Z", "visibility": "PUBLIC", "start_on": "2017-10-09", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "city-science-anodrra2"}, {"website": "http://robotic.media.mit.edu/peple/randi-williams", "description": "", "people": ["cynthiab@media.mit.edu", "randiw12@media.mit.edu"], "title": "Block-Based Programming for Robotics Education", "modified": "2019-03-27T13:40:23.668Z", "visibility": "PUBLIC", "start_on": "2018-01-05", "location": "", "groups": ["personal-robots"], "published": false, "active": false, "end_on": null, "slug": "bot-blocks"}, {"website": "", "description": "<p>asdasdawdasdasda</p>", "people": [], "title": "Test project cristian", "modified": "2017-10-13T18:48:25.703Z", "visibility": "PUBLIC", "start_on": "2017-10-02", "location": "", "groups": ["collective-learning"], "published": false, "active": false, "end_on": null, "slug": "test-project-cristian"}, {"website": "", "description": "<p>Every year, billions of dollars are wasted in lost objects which could be avoided by tagging them with RFIDs.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">Between 2003 and 2011, the US army lost track of $5.8&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">billion of supplies among its warehouses. In 2013&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">alone, Walmart lost $3 billion in sales because of mismatches between&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">its recorded inventory and actual stock. In 2016,&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">the US National Retail Federation reported that shrinkage\u2014i.e.,&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">loss of items in retail stores\u2014averaged 1.38% of retail sales,&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">which translates to $45.2 billion annually.</span></p><p>The fundamental challenge with RFIDs ,however, is that they are only reliable at fairly short distances of a few meters. The distance becomes much lower if the RFID is buried under other objects or if the tag\u2019s antenna is oriented away from that of the reader.<br></p><p>This projects develops a technology that overcomes the range challenge of RFIDs by leveraging drones. By mounting a new compact circuit on the drone, we demonstrate how we can significantly increase the coverage for RFID localization systems. The technology does not &nbsp;require changing the RFID or the readers, and can achieve two orders of magnitude improvement in coverage area of RFIDs\u2014increasing them from few squared meters to hundreds of squared meters.<span style=\"font-size: 16px;\"><br></span></p>", "people": ["nselby@media.mit.edu", "fadel@media.mit.edu", "yunfeima@media.mit.edu"], "title": "Warehouse-scale RFID Localization using Drones", "modified": "2017-11-02T01:34:55.226Z", "visibility": "LAB-INSIDERS", "start_on": "2016-10-10", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "warehouse-RFID"}, {"website": "", "description": "<p>Loneliness is becoming a global epidemic. As many as 33 percent of Americans report being chronically lonely, with similar percentages being reported in countries around the world. Additionally, this percentage has risen in recent years. Many are turning to online forums as a way to connect with others about their feelings of loneliness and to begin to reduce these feelings. However, oftentimes, posts go unresponded to and online conversations do not take place, perhaps because those conversing did not find a connection with each other, potentially leaving the poster feeling even more lonely. This research explores the how health of conversation should be defined in online support conversations and analyzes the characteristics of conversation that contribute to healthier conversation.&nbsp;</p>", "people": ["dkroy@media.mit.edu", "lnfrat@media.mit.edu"], "title": "Conversational Health: Loneliness on Reddit", "modified": "2019-04-17T13:57:33.414Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "conversational-health-loneliness-on-reddit"}, {"website": "", "description": "", "people": [], "title": "Open Phenome Project", "modified": "2017-11-27T19:51:25.732Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "open-phenome-project-rob"}, {"website": "", "description": "", "people": [], "title": "GenAI: An Early Childhood Artificial Intelligence Curriculum", "modified": "2019-03-27T06:21:38.656Z", "visibility": "PUBLIC", "start_on": "2019-02-01", "location": "", "groups": ["personal-robots"], "published": false, "active": false, "end_on": null, "slug": "gen-ai-k-5"}, {"website": "", "description": "<p>Emerging pervasive games use sensors, graphics and networking technologies to provide immersive game experiences integrated with the real world. Existing pervasive games commonly rely on a device screen for providing game-related information, while overlooking opportunities to include new types of contextual interactions like jumping, a punching gesture, or even voice to be used as game inputs. <a href=\"http://dl.acm.org/citation.cfm?id=2802687&amp;CFID=661831763&amp;CFTOKEN=43505533\">Spellbound is a physical mobile team-based game</a> that aims to nurture a spirit of togetherness. Spellbound takes advantage of the above mentioned opportunities and integrates real-world actions like jumping and spinning with a virtual game world. It also replaces touch-based input with voice interaction and provides glanceable and haptic feedback using custom hardware in the true spirit of social play characteristic of traditional children\u2019s games. Spellbound is a form of digital outdoor gaming that anchors enjoyment on physical action, social interaction, and tangible feedback.<br></p>", "people": ["sra@media.mit.edu", "geek@media.mit.edu"], "title": "Spellbound", "modified": "2018-04-20T16:50:11.649Z", "visibility": "PUBLIC", "start_on": "2012-11-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "spellbound"}, {"website": "", "description": "<h2>Expressive Performance for Body-Mind Wellbeing</h2><p>The <i>Vocal Vibrations</i> music is now available for exclusive download from <a href=\"http://www.bowers-wilkins.com/Society_of_Sound/Society_of_Sound/TodMachover\">Bowers &amp; Wilkins</a>.&nbsp;Vocal Vibrations was exhibited at <a href=\"http://www.lelaboratoirecambridge.com/\">Le Laboratoire</a> Cambridge in March 2015.&nbsp;The original installation at <a href=\"http://www.lelaboratoire.org/en/archives-18.php\">Le Laboratoire Paris</a> ran from March to September 2014.</p>", "people": ["ejessop@media.mit.edu", "rebklein@media.mit.edu", "tod@media.mit.edu", "holbrow@media.mit.edu"], "title": "Vocal Vibrations", "modified": "2018-10-19T19:52:05.771Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "vocal-vibrations-expressive-performance-for-body-mind-wellbeing"}, {"website": "", "description": "<p>The <a href=\"https://www.media.mit.edu/groups/space-exploration/overview/\">Media Lab Space Exploration Initiative</a> is driven by an effort to democratize the future of space exploration. As such, we are committed to the development of meaningful community engagement endeavors related to achieving an open and hackable \"New Space Age.\"<br></p><p>&nbsp;Beginning in Spring 2018, in an aim to fulfill NASA\u2019s vision of <i><a href=\"https://www.nasa.gov/content/cubesat-launch-initiative-50-cubesats-from-50-states-in-5-years\">50 CubeSats from 50 States in 5 Years</a>,&nbsp;</i>we launched our&nbsp;<b>Climate CubeSat Co-building Outreach Program (C</b><b>3</b><b>)</b>, in which we mentor 20 Boston-area high school students through designing, building, testing, and launching a cubesat that makes climate science and earth-observation measurements.&nbsp;</p><p>The program is themed around Climate Science and Remote Sensing; the payloads are designed and developed as part of a communal, participatory satellite constellation that takes numerous atmospheric (greenhouse gas) measurements. Student groups can either build CubeSats to support the citizen-science constellation or low-cost ground stations to support the active measurements as CubeSats fly over. A third means of involvement centers on the open data set to be released as measurements are taken, offering an inclusive and easily scalable means of participating in the program: become part of the data analysis community effort as this global data set is gathered and reviewed.&nbsp;</p><p>We view this three-part citizen science program (build a CubeSat; build a ground station; analyze the open data) as a critical opportunity to engage a broader and younger population in climate science discussions and awareness of climate change.</p><p>Our local program (the beta test for expanding nationally and globally) currently supports 20 high school students, focusing on under-represented minorities and young women. The students are supported throughout the program with background knowledge curriculum sessions, an open journaling and learning exchange platform, technical mentorship for design and building tasks, part kits, testing and launch support, and training for post-flight data analysis.&nbsp;We are actively building out this program to scale nationally via high schools, public libraries, and maker spaces across the nation (and soon internationally!).</p><p>&nbsp;The expanded program will include:</p><ul><li>thoughtful and efficiently distributed curriculum development,&nbsp;</li><li>extensive open-source build guidelines (e.g., instructables) and part lists,</li><li>&nbsp;a unified platform for data storage and analysis,&nbsp;</li><li>an <a href=\"https://c3.pubpub.org/\">online community via PubPub</a> for sharing knowledge and curriculum, and</li><li>access to free or reduced-cost launch opportunities.&nbsp;</li></ul><br><p><b>As young students are empowered to build their own cubesats and collect and analyze their own data, they will both be participating in something larger than themselves and learning about the health and future of our planet.</b></p><p>C3 collaborators:&nbsp;</p><p>MIT Media Lab Learning Initiative, &amp; PLIX Library Network<br>MIT Lincoln Laboratory<br>MIT Beaverworks<br>MIT AeroAstro STAR Lab<br>Clay Center Amateur Radio Club<br>J.D. O'Bryant School of Mathematics &amp; Science</p>", "people": ["devora@media.mit.edu", "aekblaw@media.mit.edu", "ave@media.mit.edu", "kamcco@media.mit.edu"], "title": "Climate CubeSat Co-building Outreach Program (C3)", "modified": "2019-01-09T15:10:31.255Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "cubesat"}, {"website": "https://medialabmoduland.wordpress.com/", "description": "<h2>MODULAND is a playground kit for learning&nbsp;electronic music.</h2><p>MODULAND is an interactive project created by the&nbsp;<a href=\"https://www.media.mit.edu/events/mlberlin-signalandnoise/\">MIT Media Lab Berlin \u2013 Signal &amp; Noise prototyping workshop</a>,&nbsp;within the track \u201cPlayful Machines that Make Music.\"&nbsp;</p><p>With MODULAND, playgrounds become modular synthesizers to raise curiosity, exploration, and connection to electronic music making.</p><p>By creating playful machines that use LEGO bricks, sensors, and microcontrollers, it creates an embodied and interactive music lesson in an urban space.</p>", "people": ["akito@media.mit.edu", "x_x@media.mit.edu"], "title": "MODULAND", "modified": "2018-08-29T14:44:22.539Z", "visibility": "PUBLIC", "start_on": "2018-08-14", "location": "", "groups": ["ml-learning"], "published": true, "active": false, "end_on": null, "slug": "moduland"}, {"website": "", "description": "<p>Networked Playscapes re-imagines outdoor play by merging the flexibility of the digital world with the tangible, sensorial properties of physical play to create hybrid interactions for the urban environment.&nbsp;</p><p>Dig Deep takes the classic sandbox found in children's playgrounds and merges it with the common fantasy of \"digging your way to the other side of the world\" to create a networked interaction in tune with child cosmogony. </p>", "people": ["edwinapn@media.mit.edu", "vmb@media.mit.edu"], "title": "Networked Playscapes: Dig Deep", "modified": "2017-10-13T18:29:16.756Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "networked-playscapes-dig-deep"}, {"website": "", "description": "<p>Exploring your city is a great way to make friends, discover new places, find new interests, and invent yourself. Spotz is an Android app where everyone collectively tags the places they visit and the places in turn tag them back. This means going to a place tagged \"fun\" increases your \"fun\" quotient. If people tag a spot \"geeky\" then going there pushes up your \"geeky\" score. This tag + location data is used to generate a tag-based persona for you that is constantly evolves based on where you go. You can also follow tags you like to find new \"fun\" places or find people with similar tag clouds as your own and experience new places together. In addition, you can create tags for individual items at places that others can track, for example, to find who has the #bestchocolatecake in town!</p>", "people": ["sra@media.mit.edu", "geek@media.mit.edu"], "title": "Spotz", "modified": "2018-04-20T16:50:33.154Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "spotz"}, {"website": "", "description": "<p>How can we design relationships between the most primitive and the most  sophisticated life forms? Can we design wearables embedded with  synthetic microorganisms that can enhance and augment biological  functionality? Can we design wearables that generate consumable energy  when exposed to the sun?</p>", "people": ["stevenk@media.mit.edu", "ssunanda@media.mit.edu", "moonshot@media.mit.edu", "bader_ch@media.mit.edu", "kolb@media.mit.edu", "neri@media.mit.edu"], "title": "Living Mushtari", "modified": "2017-10-13T23:28:11.103Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "living-mushtari"}, {"website": "", "description": "<p>The Lifelong Kindergarten group develops technologies and activities that engage people in creative learning. We\u2019re exploring a few directions with our work in Africa:</p><p><b>1. Supporting creative learning across contexts</b></p><p>Instruction-focused learning and an emphasis on narrow outcomes continue to dominate formal and informal environments. As school systems seek to adapt to changes in technology, we\u2019re exploring ways to support school systems, educators, and informal learning organizations in reconstructing outcomes and pedagogy that better prepare young people for lifelong, creative learning.</p><p><b>2. Creative Learning Communities</b></p><p>We\u2019re exploring two kinds of communities in particular: (a) a maker-focused community that connects makers with employers while helping makers document and translate their skills to professional contexts and (b) supporting the development of creative learning communities\u2014connecting individuals and organizations working to support creative learning in formal and informal learning environments.&nbsp;</p><p>Building on the work of <a href=\"https://www.media.mit.edu/projects/creative-learning-in-brazil/overview/\">Aprendizagem Criativa no Brasil</a> (Creative Learning in Brazil\u2014a decentralized network of educators, designers, systems leaders, foundations, and companies all involved in or hoping to support creative learning) and other initiatives, we hope to first gather stories of educators across a range of contexts, connect them with one another (including at the 2019 Africa Scratch conference), support co-development of resources, and hopefully engage in movement building.&nbsp;</p><p><b>3. Creative <i>vocational</i> learning</b></p><p>Vocational, 21st-century skills trainings are in high demand across the world\u2014and especially in areas with high unemployment. Trainings tend to be dominated by traditional instruction and often limited to a particular set of skills. Moreover, while some of these trainings might help a student get a specific job or progress on a particular skill, they don\u2019t prepare students for lifelong, creative learning\u2014and the world that students will be entering.<br></p><p>In this environment, we\u2019re exploring how to bring creative learning principles to the design and facilitation of \u201cvocational trainings\u201d\u2014with the aspiration to create a <b>model</b>&nbsp;for more open-ended, playful, passionate, and peer-driven vocational learning that provides a springboard for lifelong learning.&nbsp;&nbsp;</p><p>We're piloting this work with the <a href=\"http://mekatilili.media.mit.edu\">Mekatilili Initiative</a> in Nairobi, Kenya.&nbsp;</p>", "people": ["yusufa@media.mit.edu", "muthui@media.mit.edu"], "title": "Creative Learning Africa", "modified": "2019-02-14T16:32:44.034Z", "visibility": "PUBLIC", "start_on": "2018-10-03", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "creative-learning-africa"}, {"website": "", "description": "<p>As mobile device screens continue to get smaller (smartwatches, head-mounted devices like Google Glass), touch-based interactions with them become harder. With OnTheGo, our goal is to complement touch- and voice-based input on these devices by adding interactions through in-air gestures around the devices. Gestural interactions are not only intuitive for certain situations where touch may be cumbersome like running, skiing, or cooking, but are also convenient for things like quick application and task management, certain types of navigation and interaction, and simple inputs to applications.</p>", "people": ["sra@media.mit.edu", "geek@media.mit.edu"], "title": "OnTheGo", "modified": "2018-04-20T16:50:52.900Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "onthego"}, {"website": "", "description": "<p><a href=\"http://koreajoongangdaily.joins.com/news/article/article.aspx?aid=3060788&amp;cloc=joongangdaily%7Chome%7Conline\"><i>Symphony for the Koreas</i></a> will be the latest installment of the celebrated <a href=\"https://www.youtube.com/watch?v=Cmmk6hDj7do\">City Symphony</a> series. Over the next few years, Tod Machover and his team will collaborate with citizens from both South and North Korea to create a symphony that reflects what both sides have in common, where conflicts remain, and what might be effective, realistic, and peaceful ways to resolve conflicts through music. Through collaboration with the <a href=\"http://lindenbaumschool.com/index.php/Main\">Lindenbaum</a> orchestra and festival, a South Korean organization dedicated to bringing peace to the Korean Peninsula through music, Machover and his team plan to invite musicians from both Koreas to participate in ongoing creative activities\u2014as well as to live performances\u2014of <i>Symphony for the Koreas</i>. The Lindenbaum organization has been granted unprecedented permission by the South Korean government to communicate and collaborate with the North Korean Government. It has also secured an MOU with the North Korean Ministry of Culture to hold a joint concert between the two Koreas. The final performance is expected to take place at or near the Korean Demilitarized Zone, and will then be toured worldwide.</p><br><p>Since 2012, Tod Machover and his Opera of the Future Group have created <a href=\"https://citysymphonies.media.mit.edu/\">City Symphonies</a> for <a href=\"https://www.nytimes.com/2018/04/09/arts/music/cheese-steak-philadelphia-orchestra-carnegie-hall.html\">Philadelphia</a>, <a href=\"https://www.nytimes.com/2015/11/15/arts/music/detroit-symphony-tackles-an-adventurous-premiere.html\">Detroit</a>, Lucerne, Perth, Edinburgh, Miami and Toronto. In addition to <i>Symphony for the Koreas</i>, Machover is currently working on new City Symphonies for Chennai (India) and for Boston (in collaboration with <a href=\"https://hubweek.org/\">HUBweek</a>).</p><br>", "people": ["tod@media.mit.edu"], "title": "Symphony for the Koreas", "modified": "2019-04-18T14:21:02.865Z", "visibility": "PUBLIC", "start_on": "2019-04-17", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "symphony-for-the-koreas"}, {"website": "", "description": "", "people": [], "title": "Diastrophisms", "modified": "2018-10-17T03:09:44.482Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["opera-of-the-future"], "published": false, "active": false, "end_on": null, "slug": "diastrophisms-installation"}, {"website": "", "description": "<p>One two three test</p>", "people": [], "title": "Test project 4", "modified": "2017-11-19T17:43:33.006Z", "visibility": "LAB", "start_on": "2017-09-01", "location": "", "groups": ["human-dynamics"], "published": false, "active": false, "end_on": null, "slug": "test-project-4"}, {"website": "", "description": "", "people": [], "title": "Biota Beats", "modified": "2017-12-08T22:29:52.435Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["community-bio"], "published": false, "active": false, "end_on": null, "slug": "biota-beats"}, {"website": "", "description": "<p>Our social networks influence our sense of what's possible: we can't aspire to be cancer researchers, activists, or artificial intelligence engineers if we've never been exposed to these as possibilities.  Unfortunately, many children grow up in environments replete with exposure gaps, impeding awareness and ultimately limiting their conceptions of which opportunities are available to them.</p><p>Pathways is a web application that seeks to scaffold career exploration and introspection among young people in order to help them explore a) what kinds of topics they might pursue in the future, b) in which capacities they might pursue these topics, and c) examples of education and career pathways others have traversed to get where they are today.  The tool uses several data science and machine learning techniques to process self-reported education and career data from thousands of individuals in the Greater Boston area.</p><p>Ultimately, we hope tools like Pathways can help enhance exposure and spark new social network ties that help foster greater upward mobility and an improved quality of life.</p>", "people": ["dkroy@media.mit.edu", "ngillani@media.mit.edu", "pbeshai@media.mit.edu", "belen@media.mit.edu", "pralav@media.mit.edu"], "title": "Pathways", "modified": "2019-04-10T23:26:21.291Z", "visibility": "PUBLIC", "start_on": "2018-05-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "pathways"}, {"website": "", "description": "<p>Media filter bubbles sacrifice shared reality amongst US citizens. We aim to burst these echo chambers by presenting short, automatically summarized news clips to users through a mobile app. The user watches these short clips in sequence and has the option to press a lightbulb that signifies whether the news segment have been enlightening. These clips are generated from SuperGlue metadata and is based on&nbsp;<a href=\"https://www.media.mit.edu/projects/news-2/overview/\">News*2</a>. It uses an \u201cAnti-recommender system\u201d that actively expands the user\u2019s horizon\u2014contrary to traditional recommender systems that aim to thicken the walls of echo chambers.</p><p>Follow this <a href=\"https://viral.pubpub.org/pub/enlightened/\">link</a> for more information.</p>", "people": ["mhjiang@media.mit.edu", "lip@media.mit.edu"], "title": "Enlightened: Broaden Your Views", "modified": "2019-04-18T01:36:42.924Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "enlightened"}, {"website": "", "description": "<p>The Synthetic Apiary proposes a new kind of environment, bridging urban and organismic scales by exploring one of the most important organisms for both the human species and our planet: bees. We explore the cohabitation of humans and other species through the creation of a controlled atmosphere and associated behavioral paradigms. The project facilitates Mediated Matter's ongoing research into biologically augmented digital fabrication with eusocial insect communities in architectural, and possibly urban, scales. Many animal communities in nature present collective behaviors known as \"swarming,\" prioritizing group survival over individuals, and constantly working to achieve a common goal. Often, swarms of organisms are skilled builders; for example, ants can create extremely complex networks by tunneling, and wasps can generate intricate paper nests with materials sourced from local areas.</p>", "people": ["m_kayser@media.mit.edu", "ssunanda@media.mit.edu", "neri@media.mit.edu", "j_duro@media.mit.edu"], "title": "Synthetic Apiary", "modified": "2017-10-13T23:33:55.831Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "synthetic-apiary"}, {"website": "", "description": "<p>inSight is a brain decoding system. It uses&nbsp; generative models (BigGAN and MusicVAE) to stimulate the brain with synthetic, but natural-looking videos and melodies. The brain activity of the user is then recorded using an EEG headset. The recorded data is then processed to train an encoder to interpret the brain activity in terms of the latent space of the generative model, effectively allowing the system to generate video and music from the user's brain activity. inSight can be used for applications such as neurofeedback therapies, creativity, human-human communication and search.&nbsp;</p>", "people": ["mmv@media.mit.edu"], "title": "inSight: Deep Neurofeedback", "modified": "2019-04-08T18:18:55.780Z", "visibility": "PUBLIC", "start_on": "2018-12-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "insight-deep-neurofeedback"}, {"website": "http://civilservant.io", "description": "<p>The CivilServant project supports online communities to run their own experiments on the effects of moderation practices on antisocial behavior, harassment, discrimination, and community well-being online.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">All results are published to an open repository of collective knowledge on practices that contribute to fair, flourishing social life online.</span></p><p>The first experiment, in a 13.2 million subscriber community, showed that <a href=\"http://civilservant.io/moderation_experiment_r_science_rule_posting.html\">posting rules at the top of conversations prevents problems and increases engagement</a>.</p>", "people": ["ethanz@media.mit.edu", "jnmatias@media.mit.edu"], "title": "CivilServant: User-led randomized trials online", "modified": "2019-04-19T18:55:02.391Z", "visibility": "PUBLIC", "start_on": "2016-07-01", "location": "", "groups": ["civic-media", "ethics-and-governance"], "published": true, "active": false, "end_on": null, "slug": "civilservant"}, {"website": "", "description": "<p><b>The Observatory of Economic Complexity (OEC)</b> is the world's leading data visualization tool for international trade data. The OEC makes more than 50 years of international trade data available through dozens of millions of interactive visualizations.</p><p>Visit the OEC at: <a href=\"http://atlas.media.mit.edu\">http://atlas.media.mit.edu</a></p>", "people": ["hidalgo@media.mit.edu"], "title": "The Observatory of Economic Complexity (OEC)", "modified": "2018-05-03T20:06:37.513Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "oec-new"}, {"website": "", "description": "<p>Looking Sideways is an online inspiration browsing tool that seeks to provoke unexpected inspiration and guide pathways to new ideas by providing users with a selection of semi-randomly chosen, loosely related, diverse sources from art, design, history, and literature for every search query.&nbsp;</p>", "people": ["pip@media.mit.edu"], "title": "Looking Sideways", "modified": "2018-10-21T20:23:24.814Z", "visibility": "LAB-INSIDERS", "start_on": "2018-04-16", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "looking-sideways"}, {"website": "http://www.lvn.org", "description": "<p><b>What is Cortico and the Local Voices Network?</b></p><p><a href=\"https://www.cortico.ai/\">Cortico</a>, a non-profit 501(c)(3) in cooperation with <a href=\"https://www.media.mit.edu/groups/social-machines/overview/\">MIT\u2019s Laboratory for Social Machines</a>, seeks to foster constructive public conversation in communities and in the media to improve our understanding of one another. To this end, we\u2019re developing a <b>public conversation network</b>&nbsp;called the&nbsp;<a href=\"https://lvn.org/\">Local Voices Network</a>&nbsp;(LVN), designed to bring under-heard community voices, perspectives and stories to the center of a healthier public dialogue.</p><p>LVN combines in-person and digital listening to host, analyze and connect community conversations at scale.&nbsp;Launching in Wisconsin, New York, and Alabama in 2019 with ambitions to scale nationally, the Local Voices Network&nbsp;is designed around three core efforts:</p><ul><li>Facilitating in-person community dialogue that enables participants to listen, learn, and be heard</li><li>Connecting facilitators and conversations digitally across boundaries</li><li>Opening a new listening channel for journalists, leaders, and the community at large<br></li></ul><p><b>Why is this work important?</b></p><p>Our media environment prioritizes national perspectives and our politics reinforce divisive tribalism. Local citizenries, however, share a lived community experience. And those local voices, be they from red counties or blue cities, go unheard in the current media environment, drowned out by hyperpartisan noise and toxic dialogue. Social media, designed to connect us, has also divided us into insular \u201ctribes\u201d hostile toward outside views and ripe for the spread of false news, hateful discourse, and extremism. We need to create a new civic space for local voices to be heard in civil, empathic public conversation that heals divisions from the inside of communities out.</p>", "people": ["dkroy@media.mit.edu", "eyi@media.mit.edu", "dougb5@media.mit.edu", "pbeshai@media.mit.edu", "chrwang@media.mit.edu", "wschen@media.mit.edu", "russell5@media.mit.edu", "ajking@media.mit.edu", "wesc@media.mit.edu"], "title": "Cortico: The Local Voices Network", "modified": "2019-04-16T16:12:25.376Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "local-voices-network"}, {"website": "", "description": "<p>SuperGlue is a core news research initiative that is a \"digestion system\" and metadata generator for mass media. An evolving set of analysis modules annotate 14 DirecTV live news broadcast channels as well as web pages and tweets. The video is archived and synchronized with the analysis. Currently, the system provides named-entity extraction, audio expression markers, face detectors, scene/edit point locators, excitement trackers, and thumbnail summarization. We use this to organize material for presentation, analysis, and summarization. SuperGlue supports other news-related experiments.</p><p>SuperGlue is a framework for media digestion and metadata generation. The digestion work flow also has applications for media more broadly including conversational ecommerce.</p>", "people": ["weller@media.mit.edu", "anderton@media.mit.edu", "mhjiang@media.mit.edu", "lip@media.mit.edu"], "title": "SuperGlue", "modified": "2019-04-08T16:48:12.894Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["ultimate-media", "viral-communications"], "published": true, "active": false, "end_on": null, "slug": "superglue"}, {"website": "", "description": "<p>Adding augmented reality to the living-room TV, we are exploring the technical and creative implications of using a mobile phone or tablet (and possibly also dedicated devices like toys) as a controllable \"second screen\" for enhancing television viewing. Thus, a viewer could use the phone to look beyond the edges of the television to see the audience for a studio-based program, to pan around a sporting event, to take snapshots for a scavenger hunt, or to simulate binoculars to zoom in on a part of the scene. Recent developments include the creation of a mobile device app for Apple products and user studies involving several genres of broadcast television programming.</p>", "people": ["talfaro@media.mit.edu", "vmb@media.mit.edu"], "title": "SurroundVision", "modified": "2018-05-14T15:16:07.844Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "Garden Conference Room", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "surroundvision"}, {"website": "", "description": "<p>The goal of this project is providing machines with the ability of understanding what a person is experiencing from her frame of reference, taking into account the scene context: where is this person, what is this person doing, how does this person look, etc.&nbsp;</p><p>You can find more information about this project on this <a href=\"http://sunai.uoc.edu/emotic/\">website</a>.</p>", "people": ["picard@media.mit.edu", "agata@media.mit.edu"], "title": "Emotion Recognition in Scene Context", "modified": "2018-10-20T16:59:10.142Z", "visibility": "PUBLIC", "start_on": "2017-01-02", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "emotion-recognition-in-context"}, {"website": "", "description": "<p>Decades ago development scholars argued that the productive structure of a country (i. e. the mix of industries operating in the country) constrains its ability to generate and distribute income. They were correct! It was recently shown that the mix of products that a country exports is predictive of its future pattern of diversification and economic growth. But what is the link between a country's productive structure and its ability to distribute income?&nbsp;Here, we combine methods from econometrics, network science, and economic complexity, together with data on income inequality and world trade, to show that countries exporting complex products have lower levels of income inequality than countries exporting simpler products. Using multivariate regression analysis, we show that economic complexity is a significant and negative predictor of income inequality and that this relationship is robust to controlling for aggregate measures of income, institutions, export concentration, and human capital. Moreover, we introduce a measure that associates a product to a level of income inequality equal to the average GINI of the countries exporting that product (weighted by the share the product represents in that country\u2019s export basket). The Product-GINI index, or PGI,&nbsp;can provide important insights on the constraints to inequality imposed by a country's productive structure. Finally, we integrate our results to the Observatory of Economic Complexity, an online resource that allows its users to visualize the structural transformation of over 150 countries&nbsp;and their associated changes in income inequality during 1963\u20132008.</p>", "people": ["arista@media.mit.edu", "hidalgo@media.mit.edu", "crisjf@media.mit.edu", "hartmado@media.mit.edu"], "title": "Inequality and the impact of industrial structures", "modified": "2017-10-10T16:03:06.641Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "economic-complexity-and-income-inequality"}, {"website": "", "description": "<p>With the exponential increase of personal data in the forms of images, videos, emails, and social media posts, the time is ripe for building personal AIs that utilize these data to enhance the productivity and creativity of the users. Training AI algorithms require labeled and processed data.</p><p><span style=\"font-size: 18px; font-weight: 400;\">However,&nbsp;annotating data is time-consuming and often regarded as the bottleneck of supervised learning. Most tools used for data labeling are tailored for the needs of data-scientists and researchers and are far from being useful for general users.&nbsp; The users of these systems need to know the ontology of possible labels beforehand and use complex interfaces and workflows to maintain the consistency and quality of the resulting dataset. \"</span><span style=\"font-size: 18px;\">Q\"</span><span style=\"font-size: 18px; font-weight: 400;\"> aims to reformulate data annotation as an engaging conversation by asking appropriate questions and automatically highlighting possible regions of interest. To come up with relevant questions, Q learns from the Wikidata public knowledge graph by computing the probable properties and relationships of entities. It also utilizes the previously annotated pieces of data to speed up the process.</span></p>", "people": ["mmv@media.mit.edu"], "title": "Q: An intelligent conversational interface for personal data labeling", "modified": "2019-04-08T18:21:50.443Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "q-ai-that-makes-the-graph-of-your-memories"}, {"website": "", "description": "<p>Civic technology should empower us as citizens. But despite its breadth as a field, civic technology often takes its lead from Silicon Valley companies that espouse design goals potentially hazardous to participatory democracy. As an example, Facebook has been used to help organize democratic social movements around the world, but it has also allowed undemocratic actors to inflame partisanship and hate at the same time. I explore: How might we design civic technologies for citizen empowerment and evaluate their impact on this goal?&nbsp;</p><p>With their increasingly important role as mediators of democracy, it is insufficient for civic technology designers to evaluate their designs in terms of ease of use and increased engagement with their platform. Research from political and developmental psychology shows the importance to lifelong civic engagement of learning experiences that cultivate a citizen's perception they can make change (political efficacy) and their belief in having responsibilities to the public good (civic identity). To achieve these positive feedback loops, we need a richer framework for civic technology design.&nbsp;</p><p>This project proposes two solutions: 1) empowerment-based design principles for civic technology, and 2) a prototype toolkit for evaluating the impact of civic technology on political efficacy. Because empowerment is contextual, the proposals here focus on tools and platforms built to support \"monitorial citizenship,\" an increasingly popular form of civic engagement aimed at holding institutions accountable.</p>", "people": ["erhardt@media.mit.edu"], "title": "Empowerment-based design and evaluation of civic technology", "modified": "2019-04-19T18:57:20.471Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["center-for-civic-media", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "empowering-civic-tech"}, {"website": "", "description": "<p>We are creating consumer-grade appliances and authoring methodologies that will allow hallucinatory phenomena to be programmed and utilized for information display and narrative storytelling.</p>", "people": ["novysan@media.mit.edu", "vmb@media.mit.edu"], "title": "Programmable Synthetic Hallucinations", "modified": "2019-04-16T16:26:42.218Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "future-storytelling", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "programmable-synthetic-hallucinations"}, {"website": "", "description": "<p>SNaSI (Social Navigation through Subtle Interactions) is a wearable system to help blind people in face-to-face interactions. Assistive systems for the blind have been an important area of research in the wearable community for many decades. Most of those systems focus on spatial navigation issues. In the last few years, we start noticing a move toward technologies to assist the blind with social navigation. Most of those systems treat social navigation the same way as spatial navigation, focusing mainly on utilitarian aspects of human interaction (what is needed to obtain information, what information is exchanged, etc.).&nbsp;</p><p>In this project we argue that when moving into this space we need to think about social accessibility and respect of human connection first because, most of the time, face-to-face interaction aims primarily to create and reinforce human connection rather than exchanging information. With this in mind, we defend the importance of designing with subtlety in regards of framing, reasoning and design challenges. To this end, we present SNaSI\u2014Social Navigation through Subtle Interactions\u2014a wearable garment designed to help blind and visually impaired people in face-to-face interaction with sighted peers and we describe how our design criteria were guided by subtlety and social acceptability. Designed in collaboration with Microsoft Research UK, HXD group, Morrison Cecily, Huburn Josh, Grayson Martin.</p>", "people": ["rebklein@media.mit.edu"], "title": "SNaSI: Wearable device to help the blind with social navigation", "modified": "2017-10-13T20:15:18.696Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "snasi"}, {"website": "", "description": "<p>Many drugs, such as monoclonal antibodies, are administrated using parenteral delivery devices via subcutaneous injections. Unfortunately, needle phobia, anxiety before and during needle insertion and pain during injections are key aspects that lead to poor therapeutic compliance and prevent wider applicability and acceptance of this technology across patient groups. Therefore, in order to improve patient experience, traditional pain scores using a visual analog scale (VAS) or other similar techniques have been used to compare and investigate different subcutaneous injection methodologies. However, they are subjective and it is difficult to power a clinical study to show significant differences in pain.</p><p>In this study, we propose to use electrodermal activity (EDA), heart rate variability (HRV), and facial expression analysis as potential endpoints to determine quantitative pain scores during the injection process, together with other secondary endpoints such as wellness aspects of patients (e.g. sleep quality). Therefore, the objective of this study is to evaluate these endpoints in subcutaneous injections for different injection methodologies (consisting of different dose volumes, flow rates, needle gauges and injectate viscosity) in a clinical setting in humans. The data will be used to understand pain upon injection and see if there is any correlation between traditional pain scores (e.g. visual analog scale) and our proposed endpoints.</p>", "people": ["dlmocdm@media.mit.edu", "picard@media.mit.edu"], "title": "Injection Study", "modified": "2018-09-04T19:52:33.082Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "injection-study"}, {"website": "", "description": "<p>Drawing inspiration from the notion of cognitive incongruence associated with Stroop's famous experiment, from musical principles, and from the observation that music consumption on an individual basis is becoming increasingly ubiquitous, we present the SoundSignaling system\u2014a software platform designed to make real-time, stylistically relevant modifications to a personal corpus of music as a means of conveying information or notifications. From the substantial body of HCI research demonstrating the negative attentional implications of a daily inundation of notifications, we highlight two challenges associated with standard audio notifications\u2014a \"switch cost\" that impedes productivity, and a lack of awareness of a user's cognitive load\u2014that have the potential to be addressed by such a system without active activity estimation. Through this work, we suggest a re-evaluation of the age-old paradigm of binary notifications in favor of a system designed to operate upon the relatively unexplored medium of a user's musical preferences.</p>", "people": ["ishwarya@media.mit.edu"], "title": "SoundSignaling", "modified": "2018-10-19T20:43:41.491Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "soundsignaling"}, {"website": "", "description": "<p>How do people who lead communities on online platforms join together in mass collective action to influence platform operators? Going Dark analyzes a protest against the social news platform reddit by moderators of 2,278 communities in July of 2015. These moderators collectively disabled their communities, preventing millions of readers from accessing major parts of reddit and convincing the company to negotiate over their demands. This study reveals social factors\u2014including the work of moderators, relations among moderators, relations with platform operators, factors within communities, and the isolation of a community\u2014that can lead to participation in mass collective action against a platform.</p>", "people": ["jnmatias@media.mit.edu"], "title": "Going Dark: Collective action in the Reddit Blackout", "modified": "2019-04-19T18:58:39.891Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["center-for-civic-media", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "going-dark-collective-action-in-the-reddit-blackout"}, {"website": "", "description": "<p>We present VisualSoundtrack, a system designed  as a tool for soundtrack composers to experiment with original musical  content in differing musical \u201cstyles.\" The system allows a user to  rapidly prototype musical ideas with respect to the target media (such  as a film or podcast) by having him/ her input original musical motifs,  capitalizing on a corpus of existing soundtrack samples to source  various styles, and allowing the user to identify the most appropriate  style sources for the target media by visually architecting a path  through a highly abstracted feature space.</p>", "people": ["ishwarya@media.mit.edu", "joep@media.mit.edu"], "title": "VisualSoundtrack", "modified": "2018-10-19T20:45:43.275Z", "visibility": "PUBLIC", "start_on": "2016-12-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "visualsoundtrack"}, {"website": "http://www.chrisoulakapelonis.com", "description": "<p><br>In architecture, the building skin is the primary interface for mediating the environment of the external with the internal. But today, this mediation is mechanical, deterministic, and static\u2014often seeing the human as a generalizable and problematic input. With advances in material science however, there is great potential to disrupt these traditional manufactured environments of architecture and turn them into responsive mediated environments. What this thesis aims to explore is this idea of the receptive skin\u2014a sensate and dynamic multi-material interface for environmental mediation. This suggests that by departing from the view that buildings are static artifacts, we may instead begin to see buildings as organic, living entities.</p><p>Through the development of a working prototype, this project explores how such an interface may manifest itself, through dynamic material composites, instead of mechanical and electronic means. The final prototype is a \u201cproof of concept,\u201d a built example of this novel design methodology, which unites material performance with sensate technologies, as a way to enable new interactions between building and environment.&nbsp;</p>", "people": ["kapeloni@media.mit.edu", "kll@media.mit.edu"], "title": "Receptive Skins: The Breathing Wall", "modified": "2018-06-26T21:12:49.214Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "receptive-skins"}, {"website": "", "description": "<p>Rottlace is a family of masks designed for Icelandic singer-songwriter Bj\u00f6rk. Inspired by Bj\u00f6rk\u2019s most recent album\u2014Vulnicura\u2014the Mediated Matter Group explored themes associated with self-healing and expressing \"the face without a skin.\" The series originates with a mask that emulates Bj\u00f6rk\u2019s facial structure and concludes with a mask that reveals a new identity, independent of its origin. What originates as a form of portraiture culminates in reincarnation.</p>", "people": ["bader_ch@media.mit.edu", "kolb@media.mit.edu", "neri@media.mit.edu"], "title": "Rottlace", "modified": "2018-05-07T19:47:18.431Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "rottlace"}, {"website": "", "description": "<p>In the age of ubiquitous connectivity and the Internet of Things, our security and privacy have taken on new dimensions. For example, how can we ensure that our locations are not being tracked from our cellphones? And, how can we prevent an unauthorized user from hacking into our smart home systems? Our research aims at developing primitives that can address these challenges. To do so, we explore intrinsically new security mechanisms that operate across all computing stacks to secure not only the bits but also the integrity of the sensed signals, and to protect the privacy of the sensed environments.\n                    \n                </p>", "people": ["fadel@media.mit.edu"], "title": "Cyber-Physical Security and Privacy (System & Network Security)", "modified": "2018-09-05T01:10:16.706Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "cyber-physical-security-and-privacy-system-network-security"}, {"website": "", "description": "<p>Social robots are seen as a potential device to promote and enable older adults to age-in-place. The work here is a component of our long-term, co-design process with older adults happening in 2019.&nbsp; Through co-creating artworks of&nbsp; interactions with a robot with older adults, abstract technology concepts such as security and privacy, accountability, and autonomy are translated into icons that older adults can leverage to express their thoughts around various interactions. The interactions focused upon for this study include medical adherence, exercise and physical therapy, body signal monitoring, cognitive health monitoring, emotional wellness, social connectedness, and financial literacy.&nbsp;</p><p>In this component, participants create a self-representation and a representation of the robot using physical models. These models are then scanned into a digital space where older adults can scale the representations and add icons to represent their thoughts and desires around the interactions in the artwork.</p>", "people": ["cynthiab@media.mit.edu", "akostrow@media.mit.edu", "haewon@media.mit.edu"], "title": "Design Inquiry: Translating abstract technology concepts to artwork", "modified": "2019-04-18T14:28:23.440Z", "visibility": "LAB-INSIDERS", "start_on": "2019-04-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "design-inquiry"}, {"website": "", "description": "<p>How something is presented can be as important as the message itself. We use various artificial intelligence techniques to model the \"subcarriers of information\" present in a TV newscast, to automatically detect and understand visual and auditory cues beyond the spoken word including the layout of the set, the affect of the participants, the nature of the motion, and other cues. This would enable a broad-range, comprehensive analysis of <b>how news presentation is trying to shape the public debate</b>. Insights in this area are of vital importance in the age of political polarization and lead directly to applications that can break our bubbles.</p><p>See:&nbsp; viral.media.mit.edu/pub/tbd</p>", "people": ["anderton@media.mit.edu", "lip@media.mit.edu"], "title": "Unspoken News", "modified": "2019-04-18T16:50:14.749Z", "visibility": "PUBLIC", "start_on": "2018-02-05", "location": "", "groups": ["viral-communications"], "published": false, "active": false, "end_on": null, "slug": "newssense"}, {"website": "", "description": "<p>The&nbsp;Telemetron&nbsp;is a musical instrument specially designed to be performed in microgravity environments. It is created to explore the poetics of movement in outer space and the relational aspects of an antigravitational performance between human and non-human bodies. Through this line of work, we explore how the creation of culture might evolve as we leave Earth. The Telemetron project proposes a space in space for everybody - a space to share, to create, to listen.</p><p>During the summer of 2018,&nbsp; the Telemetron was presented on different occasions:</p><p>The Telemetron was exhibited&nbsp;at <a href=\"https://ars.electronica.art/news/en/\">Ars Electronica</a>&nbsp;as part of the exhibition <a href=\"https://ars.electronica.art/error/en/a-glitch-in-the-stars/\">\"A Glitch in the Stars\"</a> curated by the MIT Media Lab Space Exploration Initiative. Nicole L'Huillier designed the exhibition along with Sands Fish and Xin Liu.</p><p>The Telemetron was featured at <a href=\"https://sonarplusd.com/\">S\u00f3nar+D</a>, a festival that explores how creativity is changing our present and imagining new futures. Nicole was invited to speak in the <a href=\"https://sonarplusd.com/en/programs/barcelona-2018/areas/talks/making-music-in-space\">\"Making Music in Space\"</a> panel. She also gave a workshop called <a href=\"https://sonarplusd.com/en/programs/barcelona-2018/areas/workshops/antigravitational-luthiers\">\"Antigravitational Luthiers\"</a>. Also, the Telemetron was part of <a href=\"https://sonarplusd.com/en/programs/barcelona-2018/areas/the-zero-gravity-band/the-zero-gravity-band\">The Zero Gravity Band</a> Exhibition.</p><p>We published the paper <a href=\"http://www.nime.org/proceedings/2018/nime2018_paper0066.pdf\">\u201cTelemetron: a musical instrument for performance in zero gravity\u201d</a> at <a href=\"http://nime2018.icat.vt.edu/\">NIME</a>, and Sands Fish presented it at the international conference on New Interfaces for Musical Expression, describing the technical design of our first Telemetron.&nbsp;</p><p>Nicole also gave a talk about the Telemetron at the<a href=\"https://www.eventrid.cl/eventos/atenea/en-orbita-2018\"> En Orbita Festival</a> in NYC.</p><p>The Telemetron was created by Nicole L'Huillier and Sands Fish. With the assistance of Thomas Sanchez Lengeling, Sarah Hua, and Matt Carney. It was created on the context of the Space Exploration Initiative first zero gravity research flight. We are currently working on more space instruments, stay tuned.</p>", "people": ["sands@media.mit.edu", "nicolelh@media.mit.edu"], "title": "The Telemetron Adventures", "modified": "2019-02-14T19:38:59.554Z", "visibility": "PUBLIC", "start_on": "2018-06-13", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "the-telemetron-adventures"}, {"website": "", "description": "<p>Speech Companion is an exploration in the domain of real-time extraction of musicality from speech. Speech is one of the richest and most ubiquitous modalities of communication used by human beings. Its richness lies in the combination of linguistic and nonlinguistic information. Musicality is one of the most crucial nonlinguistic components of speech and covers tempo and rhythms of the speaker as well as the pitch variation and unique texture of the vocal sounds. Abstracting musicality from a speech in real time presents several challenges from latency to subjective pitch identification or recognizing voiced/unvoiced sounds. In this paper, we describe a new system for real-time extraction of the music present in everyday speech based on time and pitch quantization. Our system offers several modes from a simple synchronized melody line to a more complex accompaniment much like a singer accompanying herself at the guitar.&nbsp;</p><p>&nbsp;With such a system, we offer a proof of concept and a working prototype to explore the real-life situations where the music of speech impacts speakers or listeners such as in the contexts of infant-directed speech, language acquisition, human-animal communication, speech pathology, aphasia reeducation, or even music learning and musical composition.<br></p>", "people": ["rebklein@media.mit.edu"], "title": "Speech Companion", "modified": "2018-04-27T15:50:50.384Z", "visibility": "PUBLIC", "start_on": "2018-01-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "speech-co"}, {"website": "", "description": "<p>The Silk Pavilion explores the relationship between digital and  biological fabrication on product and architectural scales.The primary  structure was created of 26 polygonal panels made of silk threads laid  down by a CNC (Computer-Numerically Controlled) machine. </p>", "people": ["m_kayser@media.mit.edu", "cdgu@media.mit.edu", "jlaucks@media.mit.edu", "neri@media.mit.edu", "j_duro@media.mit.edu"], "title": "Silk Pavilion", "modified": "2019-02-06T21:02:24.163Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "silk-pavilion"}, {"website": "", "description": "<p>Receiving a shot or discussing health problems can be stressful, but does not always have to be. We measure participants' skin conductance as they use medical devices or visit hospitals and note times when stress occurs. We then prototype possible solutions and record how the emotional experience changes. We hope work like this will help bring the medical community closer to their customers.</p>", "people": ["picard@media.mit.edu", "hedman@media.mit.edu"], "title": "Mapping the Stress of Medical Visits", "modified": "2018-05-15T23:26:23.593Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "mapping-the-stress-of-medical-visits"}, {"website": "", "description": "<p>Media Cloud is a platform for studying media ecosystems. By tracking millions of stories published online, the system allows researchers to track the spread of memes, media framings, and the tone of coverage of different stories.&nbsp;</p><p>We aggregate data from over 50,000 news sources from around the world and in over 20 languages including Spanish, French, Hindi, Chinese, and Japanese. Our tools help analyze, deliver, and visualize &nbsp;information about media conversations on three primary levels: attention and coverage peaks of issues, network analysis, and clustered language use.&nbsp;</p><p>The platform is open source and open data, designed to be a substrate for a wide range of communications research efforts. Media Cloud is a collaboration between Civic Media and the Berkman Klein Center for Internet and Society at Harvard Law School.</p><p>To learn more or register for a free account, check out&nbsp;<a href=\"http://www.mediacloud.org/\">www.mediacloud.org</a>.</p>", "people": ["sands@media.mit.edu", "ngyenes@media.mit.edu", "anushkas@media.mit.edu", "ethanz@media.mit.edu", "rahulb@media.mit.edu", "ahope@media.mit.edu"], "title": "Media Cloud", "modified": "2018-04-30T15:24:53.807Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["center-for-civic-media", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "media-cloud"}, {"website": "http://www.eduardocastello.com", "description": "<p>Modern cities have to respond to the growing demands of more efficient and sustainable urban development, as well as an increased quality of life. In this context, the cities of the future will need the ability to gain insight about current urban conditions and react dynamically to them. According to this view, \"smart cities\" can be seen as cybernetic urban environments in which different agents (e.g., citizens) and actuators (e.g., robots) exploit the city-wide infrastructure as a medium to operate synergistically.<b><i> Urban Swarms</i></b> explores the feasibility of swarm robotics systems in urban environments. By using bio-inspired methods, a swarm of robots is able to handle important urban systems and infrastructures, improving their efficiency and autonomy. A diverse set of simulation experiments were designed and conducted using real-world GIS data. Results show that the proposed combination is able to outperform current approaches. <i><b>Urban Swarms</b></i> not only aims to show the efficiency of our proposed solution, but also to give insights about how to design and customize these systems.&nbsp;<a href=\"https://www.media.mit.edu/projects/cityscope-volpe/overview/\" style=\"font-size: 18px; font-weight: 400;\">CityScope</a><span style=\"font-size: 18px; font-weight: 400;\">&nbsp;Volpe ABM model has been customized to integrate Swarm behavior using the </span><a href=\"https://gama-platform.github.io/\" style=\"font-size: 18px; font-weight: 400;\">Gama Platform</a><span style=\"font-size: 18px; font-weight: 400;\"> as an </span><a href=\"https://github.com/mitmedialab/UrbanSwarms\" style=\"font-size: 18px; font-weight: 400;\">open source project</a><span style=\"font-size: 18px; font-weight: 400;\">.&nbsp;</span></p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "ecstll@media.mit.edu", "kll@media.mit.edu", "sandy@media.mit.edu"], "title": "Urban Swarms", "modified": "2019-03-12T15:37:36.573Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "urban-swarms"}, {"website": "", "description": "<p>One of the methods to recognize human activity is to place sensors in the environment. We have developed wireless sensors that measure acceleration (movement) as well as open-close events, so that the interaction of an individual with the environment can be measured. When someone carries out an activity, that person interacts with the environment--opening and closing drawers, cabinets, containers, or sitting on a couch. The timing and sequence of those activations can be used to recognize what activity a person is carrying out. This approach has some advantages, such as fewer privacy concerns (compared with the use of cameras and audio); in addition, it is less intrusive (for people suffering from dementia or mental illnesses).</p>", "people": ["emunguia@media.mit.edu", "kll@media.mit.edu", "intille@media.mit.edu"], "title": "MITes: Wireless State-Change Sensors", "modified": "2018-03-26T20:00:18.098Z", "visibility": "GROUP", "start_on": "2003-09-01", "location": "1CC-4th Floor", "groups": ["changing-places", "city-science"], "published": true, "active": false, "end_on": null, "slug": "mites-wireless-state-change-sensors"}, {"website": "", "description": "<p>We present <a href=\"http://norman-ai.mit.edu\">Norman</a>, world's first psychopath AI. Norman was inspired by the fact that the data used to teach a machine learning algorithm can significantly influence its behavior. So when people say that AI algorithms can be biased and unfair, the culprit is often not the algorithm itself, but the biased data that was fed to it. The same method can see very different things in an image, even \"sick\" things, if trained on the wrong (or, the right!) data set. Norman suffered from extended exposure to the darkest corners of Reddit, and represents a case study on the dangers of artificial intelligence gone wrong when biased data is used in machine learning algorithms.&nbsp;</p><p>Norman is an AI that is trained to perform image captioning; a popular deep learning method of generating a&nbsp;textual description of an image. We trained Norman on image captions from an infamous subreddit (its name is redacted due to its graphic content) that is dedicated to documenting and observing the disturbing reality of death. Then, we compared Norman's responses with a standard image-captioning neural network (trained on&nbsp;<a href=\"http://mscoco.org/\">MSCOCO</a>&nbsp;dataset) on <a href=\"https://en.wikipedia.org/wiki/Rorschach_test\">Rorschach inkblots</a>\u2013a test that is used to detect underlying thought disorders.</p><p>Visit <a href=\"http://norman-ai.mit.edu\">norman-ai.mit.edu</a> to explore what Norman sees!<br></p>", "people": ["cebrian@media.mit.edu", "pinary@media.mit.edu", "irahwan@media.mit.edu"], "title": "Norman", "modified": "2019-02-14T19:39:41.066Z", "visibility": "PUBLIC", "start_on": "2018-04-01", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "norman"}, {"website": "", "description": "\u200b<p><br>ArtBoat is a tool for communities to make collaborative light paintings in public spaces and reimagine the future of their cities.&nbsp;&nbsp;Visit our project website at&nbsp;<a href=\"http://artboatcommunity.com/\">artboatcommunity.com</a>.</p><p>ArtBoat consists of a remote control boat with light strips that change color in response to a custom light design board. During ArtBoat events, community members can drive remote controlled boats, control the color of lights on the boats, or take long exposure photographs to make collaborative light graffiti that explores how art can help communities claim public spaces.&nbsp;</p><p>Our first ArtBoat community event was on July 29, 2018 at <a href=\"https://magazinebeach.org/\">Magazine Beach Park</a> in Cambridge, MA. Thanks to our Magazine Beach host Cathie Zusy, our photographers&nbsp;<a href=\"http://www.jorgevaldezphotography.net/\">Jorge Valdez</a>&nbsp;(IG: jorgelvma), Ron Hoffmann (IG: archiscapes.us),&nbsp;<a href=\"http://www.facebook.com/exploreThePlanetEarth\">Neil Gaikwad</a>&nbsp;(IG: Neil Gaikwad Photography), Garance Malivel, and Jimmy Day, our&nbsp;boat facilitators, and our participants.&nbsp; Our second ArtBoat event was on&nbsp;September 22, 2018&nbsp; at Herter Park Amphitheater in Boston, MA, as part of the 15th anniversary&nbsp;<a href=\"https://www.revels.org/shows-events/riversing/\">Revels RiverSing</a>!&nbsp;</p><p>If you'd like to join our mailing list to find out about future ArtBoat events, email artboat@lauraperovich.com or fill out this <a href=\"https://goo.gl/forms/TWoXkU7jCR4j8dpx1\">form</a>.&nbsp;</p><p>ArtBoat is a sister project of <a href=\"https://www.media.mit.edu/projects/thermal-fishing-bob-in-place-environmental-data-visualization/overview/\">SeeBoat</a>. It is an open source project licensed under GPLv2 and Creative Commons.&nbsp;<br></p>", "people": ["perovich@media.mit.edu"], "title": "ArtBoat", "modified": "2019-02-25T15:19:42.293Z", "visibility": "PUBLIC", "start_on": "2018-04-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "artboat"}, {"website": "", "description": "<p>This WebGL app created in the <b>MIT Center for Bits and Atoms</b> simulates how any origami crease pattern will fold. It may look a little different from what you typically think of as \"origami\" - rather than folding paper in a set of sequential steps, this simulation attempts to fold every crease simultaneously. It does this by iteratively solving for small displacements in the geometry of an initially flat sheet due to forces exerted by creases. It calculates the geometry of folded or partially folded origami using a dynamic, GPU-accelerated solver; the solver extends work from the papers&nbsp;<a href=\"http://www2.eng.cam.ac.uk/~sdg/preprint/5OSME.pdf\">Origami Folding: A Structural Engineering Approach</a>&nbsp;and&nbsp;<a href=\"http://www.tsg.ne.jp/TT/cg/TachiFreeformOrigami2010.pdf\">Freeform Variations of Origami</a>. It also supports an immersive, interactive VR mode using&nbsp;<a href=\"https://webvr.info/\">WebVR</a>. More information about the solver and the app is being compiled into a paper and will be posted here soon.&nbsp;<br></p>", "people": ["ghassaei@media.mit.edu"], "title": "Origami Simulator", "modified": "2017-11-29T21:37:37.554Z", "visibility": "LAB", "start_on": "2017-03-01", "location": "", "groups": ["center-for-bits-and-atoms-1"], "published": true, "active": false, "end_on": null, "slug": "origami-simulator"}, {"website": "", "description": "<p>Virtual reality can help realize mediated social experiences where we interact as richly with those around the world as we do with those in the same room. The design of social virtual experiences presents a challenge for remotely located users with differently sized, room-scale setups like those afforded by recent commodity virtual reality devices like the HTC Vive. This work explores how we can allow remote users to learn to dance together in VR by mapping their individual physical spaces to a shared virtual space.&nbsp; Video and paper available here:&nbsp;<a href=\"http://web.media.mit.edu/~sra/dancing.html\">http://web.media.mit.edu/~sra/dancing.html</a></p>", "people": ["sra@media.mit.edu"], "title": "Learning to dance in VR", "modified": "2018-12-17T15:39:47.482Z", "visibility": "PUBLIC", "start_on": "2017-02-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "learning-to-dance-in-vr"}, {"website": "", "description": "<p>(In Portuguese here)</p><p>The Lemann-MIT Creative Learning Challenge aims to support the design and implementation of innovative technologies, products, services and initiatives that help make Brazilian education more meaningful, hands-on, collaborative and playful.</p><p>The Challenge also aims to identify, support, and connect Brazilian individuals -- artists, researchers, educators, technology developers, entrepreneurs and decision makers -- who might play a key role in fostering creative learning practices, especially coding and making, in Brazilian public K12 schools and non-formal educational environments. </p><p>Representatives from the selected projects will receive a Creative Learning Fellowship to support the implementation of their work. Among other things, the Fellows will:</p><ul><li>Receive technical, educational and entrepreneurial support for the implementation of their projects during the Fellowship period;<br></li><li>Get more familiarized with Creative Learning and the work of the MIT Media Lab's Lifelong Kindergarten Group (LLK);<br></li><li>Help LLK researchers (and the broader Media Lab community) learn more about challenges and opportunities inherent to Brazilian education;<br></li><li>Join an active community of other Creative Learning fellows and supporters.<br></li></ul><p>By joining a network of practitioners, decision makers and researchers who share the same ideals, we hope the Fellows will learn from each others\u2019 experiences and, together, help catalyze a large and sustainable transformation of Brazilian education and contribute to the development of a more participatory and creative society for all.</p>", "people": [], "title": "Lemann-MIT Creative Learning Challenge", "modified": "2018-01-22T14:42:54.607Z", "visibility": "PUBLIC", "start_on": "2017-01-12", "location": "", "groups": ["lifelong-kindergarten"], "published": false, "active": false, "end_on": null, "slug": "creative-learning-challenge"}, {"website": "", "description": "<p><i>Man of My Words</i> is a wearable self-feedback voice changer for women to challenge internalized sexism. The experience is designed in two parts:</p><p><b>Auditory</b>&nbsp;The electronic part of the device consists of a Bluetooth microphone and earbuds. When the female user speaks into the microphone, her voice is altered into a male voice and returned through the earbuds. By giving the&nbsp;perception of speaking in a male voice, this device is intended to&nbsp; break the association that the users have between themselves, their female, \"weak\" voice, and lack of authority.</p><p><b>Visual&nbsp;</b>The top hat and a fake mustache, apart from their functional purpose of holding the microphone, were designed to create a more immersive, satirical experience. This device aims to more easily approach a serious social issue through comfortable humor.</p>", "people": ["hanelee@media.mit.edu"], "title": "Man of My Words", "modified": "2018-09-22T17:32:15.088Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "man-of-my-words"}, {"website": "", "description": "\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b<p>The Orbit Weaver Suit for&nbsp;zero gravity was&nbsp;designed by Media Lab Director's Fellow Andrea Lauer in collaboration with Xin Liu.&nbsp;</p><p>The design was inspired by a drawing Jordan Piantedosi made for Xin Liu, in which she&nbsp;is in a suit and casting&nbsp;strings out into space.&nbsp;The drawing is a reference to Orbit Weaver, a performance piece Xin created to test in a zero-gravity environment in November 2017.&nbsp;</p>", "people": ["xxxxxxin@media.mit.edu", "alauer@media.mit.edu"], "title": "Orbit Weaver Suit", "modified": "2017-11-30T15:54:37.271Z", "visibility": "PUBLIC", "start_on": "2017-10-17", "location": "", "groups": ["directors-fellows", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "orbit-weaver-suit"}, {"website": "", "description": "<p>This podcast features interviews with experts on technology, society, and ethics. Check out our interviews <a href=\"https://podcasts.apple.com/us/podcast/grey-mirror-mit-media-labs-digital-currency-initiative/id1254196635?mt=2\">here</a>!&nbsp;</p>", "people": ["rhysl@media.mit.edu"], "title": "Grey Mirror: A podcast on technology, society, and ethics", "modified": "2019-04-16T17:47:05.046Z", "visibility": "PUBLIC", "start_on": "2018-12-20", "location": "", "groups": ["digital-currency-initiative-dci"], "published": true, "active": false, "end_on": null, "slug": "grey-mirror"}, {"website": "", "description": "<p>The Storytelling project uses machine-based analytics to identify the qualities of engaging and marketable media. By developing models with the ability to \u201cread\u201d emotional arcs and semantic narrative video content, our researchers aim to map video story structure across many story types and formats.</p><p>To complement this content-based analysis, our researchers are also developing methods to analyze how emotional and semantic narratives affect viewer engagement with these stories. By tracking \u201creferrals\u201d of video URLs on social media networks, our researchers hope to identify how stories of different types and genres diffuse across networks, who influences this spread, and how video story distribution might be optimized. Given this project\u2019s two-pronged strategy, our hope is to develop a robust story learning machine that uniquely maps the relationship between story structure and engagement across networks.</p>", "people": ["dkroy@media.mit.edu", "echu@media.mit.edu", "soroush@media.mit.edu", "russell5@media.mit.edu", "pralav@media.mit.edu"], "title": "The Story Learning Machine", "modified": "2018-06-12T19:49:38.670Z", "visibility": "PUBLIC", "start_on": "2017-02-23", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "story-learning-machine"}, {"website": "", "description": "<p>We are transforming a classic music jewelry box into a digital memory box and Skype portal that enable those not familiar with technology to stay in touch with their family and friends. The box has three different modes. To switch mode the user only has to turn the small crank in the back, like they would do with a regular music box. The crank is linked to a rotary encoder. The back of the box is covered with a two-way mirror covering a small LCD screen; when the screen is turned off, it looks like a regular mirror but when the screen is on, it looks like a display. In the first mode, the box plays the favorite music of the user with the screen off. In the second mode, the display shows photographs of family and friends. By turning the crank or by clicking on the characters in the photographs, the box goes into mode 3, which is a Skype portal enabling the user to instantly call a family member face-to-face. This device is mainly imagined for elderly parents with dementia or memory loss.&nbsp;</p>", "people": ["rebklein@media.mit.edu"], "title": "Memory Music Box", "modified": "2018-04-27T15:51:14.777Z", "visibility": "PUBLIC", "start_on": "2017-10-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "memory-music-box"}, {"website": "", "description": "<p><a href=\"https://dive.media.mit.edu\"><b>DIVE</b></a> is a web-based data exploration system that lets non-technical users create stories from their data without writing code.&nbsp;DIVE combines semantic data ingestion, recommendation-based visualization and analysis, and dynamic story sharing into a unified workflow.&nbsp;</p><p><b>Links</b><br></p><ul><li>Public version:&nbsp;<a href=\"https://dive.media.mit.edu/\">dive.media.mit.edu</a></li><li>Demo video:&nbsp;<a href=\"https://dive.media.mit.edu/video\">dive.media.mit.edu/video</a></li><li>Front-end repository:&nbsp;<a href=\"https://github.com/MacroConnections/dive-frontend\">github.com/MacroConnections/dive-frontend</a></li><li>Back-end repository:&nbsp;<a href=\"http://github.com/MacroConnections/dive-backend\">github.com/MacroConnections/dive-backend</a></li></ul>", "people": ["hidalgo@media.mit.edu", "kzh@media.mit.edu"], "title": "DIVE", "modified": "2018-06-28T14:01:39.643Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "dive"}, {"website": "", "description": "<p>&nbsp;News reporting today suffers from sensationalism. News agencies are constantly fighting for attention and clicks, leading to headlines and photos that exaggerate a single perspective.&nbsp;</p><p>What if you could get a full perspective on certain news topics by exploring the news in AR? The spatial nature of AR allows a user to gain a more complete perspective on a story.&nbsp; Having a constant holographic widget on your desk also allows you to follow developing stories, such as a foreign conflict. In addition, the interactive nature of AR means that users can explore the news in a delightful way.</p>", "people": ["lip@media.mit.edu", "hbedri@media.mit.edu"], "title": "Holonews", "modified": "2018-10-22T16:52:05.905Z", "visibility": "PUBLIC", "start_on": "2017-09-28", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "holonews"}, {"website": "", "description": "<p>We developed a concept of transformative appetite, where edible 2D films made of common food materials (protein, cellulose or starch) can transform into 3D food during cooking. This transformation process is triggered by water adsorption, and it is strongly compatible with the \u2018flat packaging\u2019 concept for substantially reducing shipping costs and storage space. To develop these transformable foods, we performed material-based design, established a hybrid fabrication strategy, and conducted performance simulation. Users can customize food shape transformations through a pre-defined simulation platform, and then fabricate these designed patterns using additive manufacturing. Three application techniques are provided: &nbsp;2D-to-3D folding, hydration-induced wrapping, and temperature-induced self-fragmentation, to present the shape, texture, and interaction with food materials. Based on this concept, several dishes were created in the kitchen, to demonstrate the futuristic dining experience through materials-based interaction design.</p>", "people": ["wwen@media.mit.edu", "liningy@media.mit.edu", "dvlevine@media.mit.edu", "ishii@media.mit.edu"], "title": "Transformative Appetite", "modified": "2019-02-14T19:43:51.271Z", "visibility": "PUBLIC", "start_on": "2017-05-07", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "transformative-appetite"}, {"website": "", "description": "<p>Gravity anchors all existence on Earth.<br></p><p>It pulls a chaotic world to one single point in every moment of life. Even though gravity is everywhere and unending, for most of the universe, vast empty space dominates, leaving us free from gravity\u2019s tether.&nbsp;Is the weightless state a moment of true autonomy, or does the ungrounded body simply lose control?</p><p>Orbit Weaver is a series of imaginative interactions with moving bodies in zero gravity. The project aims to understand, create, and share the beautiful and sentimental moments of being weightless and lost in outer space.&nbsp;</p>", "people": ["xxxxxxin@media.mit.edu"], "title": "Orbit Weaver", "modified": "2019-03-13T14:18:01.925Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "orbit-weaver"}, {"website": "", "description": "<p>Despite recent efforts, current fact-checking organizations cannot keep up with the amount of information that is produced and spread throughout the internet [1]. One of the biggest challenges is to minimize the time it takes to verify the claims of a story. We are building a decentralized, crowd-sourced news verification system that aims to leverage the \"wisdom of crowds\" to generate timely labels that can be used to put a badge on news articles. The labels are stored on <a href=\"https://ipfs.io/\">IPFS</a> using <a href=\"https://underlay.mit.edu/\">The Underlay</a>\u2014a protocol developed by the Knowledge Futures Group at MIT. Governance mechanisms and incentive structures are implemented to hold all parties accountable and to prevent unbalanced concentration of power.</p><p>Follow this&nbsp;<a href=\"https://viral.pubpub.org/pub/defacto/\">link</a>&nbsp;for more information.</p><p>[1] <a href=\"https://www.washingtonpost.com/news/fact-checker/wp/2018/06/25/rapidly-expanding-fact-checking-movement-faces-growing-pains/?utm_term=.069157def6eb\">https://www.washingtonpost.com/news/fact-checker/wp/2018/06/25/rapidly-expanding-fact-checking-movement-faces-growing-pains/?utm_term=.069157def6eb</a>.</p>", "people": ["mhjiang@media.mit.edu", "lip@media.mit.edu"], "title": "Defacto: Decentralized crowdsourced news verification system", "modified": "2019-04-18T14:34:43.198Z", "visibility": "PUBLIC", "start_on": "2019-02-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "defacto"}, {"website": "", "description": "", "people": [], "title": "Viralcasting-hisham", "modified": "2018-04-20T20:45:32.150Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["viral-communications"], "published": false, "active": false, "end_on": null, "slug": "viralcasting-hisham"}, {"website": "", "description": "<p>Expressing the hidden things inside, working with one of the most fragile of media.</p><p>Hand carved illuminated Hen, Duck, Blue Duck and Goose egg shells. After emptying and cleaning the shells, I use a hand rotary tool to carve out the surface.</p>", "people": ["rebklein@media.mit.edu"], "title": "Egg Shell Carving", "modified": "2018-06-29T19:09:48.567Z", "visibility": "PUBLIC", "start_on": "2016-07-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "egg-shell-carving"}, {"website": "https://rubezchong.com/", "description": "<p>LinkedOut aims to define and build solutions to facilitate societal reentry for formerly incarcerated individuals.</p><p>In collaboration with the Office of Returning Citizens (ORC), an office under the City of Boston that facilitates reentry, LinkedOut is working to design a reentry infrastructure to shut the revolving door of incarceration and reincarceration.</p><p>Our work seeks to break down the structural and societal barriers to successful reentry. At the structural level, we are building technologies to develop richer data biographies of the lives of returning citizens, and in doing so provide policymakers with the robust data required for effective reentry programs. Importantly, our goal is to embed values in technology design to humanize the reentry process. On the societal level, we have begun to expose holes in the moral fabric of society as we draw attention to the dehumanization and stigmatization of returning citizens. These invisible societal norms disempower returning citizens from rebuilding their lives. Thus, our work demands a reimagination of our current justice system\u2014one that rehabilitates rather than retributes, that embraces rather than excludes\u2014to design for a successful reentry journey.</p><h1><b>Ongoing Project</b></h1><h2><b>Pathfinder: How it works</b></h2><p>Pathfinder is a personalized case management platform that helps case managers design and keep track of returning citizens\u2019 transition back into society. It makes real-time recommendations in the categories of healthcare, housing, employment, education, financial health, and community engagement based on the unique needs of each returning citizen. Beyond tracking the reentry journey of returning citizens, Pathfinder helps service providers identify and track community service gaps in meeting the needs of their clients.</p><p>In summary, Pathfinder has three key aims:</p><ol><li>Develop individualized case management plans for returning citizens</li><li>Streamline work process for service providers to improve efficacy of services</li><li>Capture community-level determinants of reentry for more informed reentry policies and programs</li></ol><p><b>Collaborators</b></p><p>Pathfinder is a collaboration between, with, and for returning citizens. We are partnering with <a href=\"https://www.codersbeyondbars.org/\">Coders Beyond Bars</a>, a non-profit that teaches returning citizens to code; through this partnership, returning citizens will co-design and co-develop Pathfinder with the MIT Media Lab.</p>", "people": ["ethanz@media.mit.edu", "rubezc@media.mit.edu", "mavipasi@media.mit.edu"], "title": "LinkedOut: Codesigning societal reentry with returning citizens", "modified": "2019-04-16T16:34:52.993Z", "visibility": "PUBLIC", "start_on": "2018-09-17", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "linkedout"}, {"website": "http://cities.media.mit.edu/", "description": "<h2>We propose that fundamentally new strategies must be found for creating the places where people live and work, and the mobility systems that connect these places, in order to meet the profound challenges of the future.</h2><p><span style=\"font-size: 18px; font-weight: normal;\">Building on current work at the Media Lab, City Science researchers will initially focus on the following project themes. Additional project themes will be added in response to the priorities of corporate members, MIT researchers, and the City Science advisory board. These six initial themes represent a cross section of the interdisciplinary research that will be undertaken to address the major challenges associated with global urbanization.</span><br></p><p>The world is experiencing a period of extreme urbanization. In China alone, 300 million rural inhabitants will move to urban areas over the next 15 years. This will require building an infrastructure equivalent to the one housing the entire population of the United States in a matter of a few decades.</p><p>In the future, cities will account for nearly 90% of global population growth, 80% of wealth creation, and 60% of total energy consumption. Developing better strategies for the creation of new cities, is therefore, a global imperative.</p><p>Our need to improve our understanding of cities, however, is pressed not only by the social relevance of urban environments, but also by the availability of new strategies for city-scale interventions that are enabled by emerging technologies. Leveraging advances in data analysis, sensor technologies, and urban experiments, City Science will provide new insights into creating a data-driven approach to urban design and planning. To build the cities that the world needs, we need a scientific understanding of cities that considers our built environments and the people who inhabit them. Our future cities will desperately need such understanding.</p>", "people": ["kll@media.mit.edu"], "title": "Cities Network (UNPUBLISHED)", "modified": "2019-04-17T17:26:57.486Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["city-science"], "published": false, "active": false, "end_on": null, "slug": "cities-network-unpublished"}, {"website": "", "description": "<p><b>Motivation</b></p><p>The need for inexpensive, reliable 3D, 360-degree display technologies grows as augmented reality applications continue to increase in popularity. &nbsp;There is room for innovation in the field, as many volumetric displays have moving components, are prohibitively expensive, aren\u2019t 360-degree, or some combination of those factors. &nbsp;By creating a 360-degree autostereoscopic display that is cost-effective and reliable, we could expand the audience who would benefit from collaborative, interactive experiences. Additionally, while augmented reality headsets are increasingly popular, a volumetric display readily encourages accessible, collaborative interaction without separating users by wearable technology. &nbsp;This technology could impact a variety of industries and demographics as the connected media landscape continues to expand.</p><p><b>Approach -&nbsp;</b><b>Hardware</b></p><p>This system is comprised on three main components. &nbsp;First, a 4K monitor displays properly distorted, lenticularized content using a custom shader. &nbsp;That light-field information is then partitioned by a refracting medium, either a radial lenticular array or a holographic optical element. &nbsp;After the rays of light from the pixels bend through the refracting medium, the rays then bounce off the conical mirror, producing the proper image for each viewing angle.</p><p><b>Hardware - Radial Lenticular</b></p><p>The radial lenticular design is dictated by several parameters which all trade-off with one another. &nbsp;For the design of this lenticular, I plan to focus on maximizing the resolution and size of the image rendered in the cone, while also maintaining a reasonable number of viewing angles. &nbsp;Variables that can be changed include the number of cameras (which corresponds to the number of discreet viewing zones), the number of views per lenticular slice, and the number of times a particular sequence of views should repeat. &nbsp;Those variables dictate the minimum viewing radius size, the angular size of each lenticular slice, the angle of influence for each viewing zone and the size of the image rendered in the cone.  It is important to consider what a user can reasonably see reflected in the cone, before the rays no longer bounce back to the user.</p><br><p>Resolution is one over the number of views per each lenticular wedge. &nbsp;As the user moves radially around the display, the rendered views that are opposite the user (i.e. on the other side of the radial mirror) cannot be seen. &nbsp;Because of this assertion that the user doesn\u2019t need to see all of the potential views from a static position, radial lenticulars operate differently than linear lenticulars. &nbsp;With a linear lenticular, every view that will be displayed needs to exist under every lenticular lens for the effect to work.  For example, if there are 8 potential images lenticularized, there would be a slice of each of those 8 views underneath each lenticular lens. &nbsp;However, with this setup, it is not</p><p>necessary to produce every view under every lenticular lens because each lenticular lens only needs to display the views that the user could potentially see from that lens. &nbsp;&nbsp;Because of that, there is a rolling priority system for which views to display under each lenticular lens.  If the user is standing directly in front of where a viewing zone is located for a particular image, the lenticular lenses closest to the user should have that view centrally located under the lenticular. &nbsp;The lenticulars that are adjacent to that area should still have information that can reach the user, however the views have to shift to accommodate the off-axis position.  In that way, there is a rolling priority system, where the generated view that is closer to the user will appear centrally under the lenticular lens and incrementally move off-center and eventually disappear as the user moves radially around the display.</p><p><b>Hardware - Interactivity</b></p><p>For hardware integration for interactivity, I plan to use Intel Realsense cameras, microphones and arduino powered LEDs that all communicate with Unity. &nbsp;There is a plugin to use Intel Realsense in the unity environment, along with libraries that allow for more detailed signal processing and pose analysis.  I will use serial messaging in unity to send message to the arduino that will control the LEDs that will wrap around the housing of the device. &nbsp;The LEDs will be used to indicate important signals to users.  Those signals include that the system can see the user and whether the user appears to be engaging as well as can signal when the program is performing a request that requires time.  Additionally, I will use unity to receive and process microphone inputs.</p><p><b>Software -&nbsp;</b><b>Custom Shader</b></p><p>Unity has built in support for creating complex shaders that can be applied real-time. &nbsp;In Unity, I will write a shader that appropriately partitions the views based on the number of cameras in the scene, the number of views per each lenticular lens, and the number of times each sequence of views should repeat. &nbsp;Before the views can be lenticularized the media from the camera must be rotated around the display to match where it will physically appear on the reflected cone.  If you don\u2019t do this step, all of the views will be rendered on top of each other, resulting in an incorrect result. &nbsp;By rotating the views to their appropriate location, the imagery you should see when standing on the left of the display will appear on the left and same for all other</p><p>directions. &nbsp;After the views have been rotated to the appropriate location, each of the camera\u2019s views are sliced and reordered within the shader to produce the lenticularized result.</p><br><p><b>Software - Interactivity</b></p><br><p>Intel Realsense and similar depth sensing devices like Kinect and Leap Motion provide SDKs that allow developers to readily stream depth and color data into Unity. &nbsp;The Intel Realsense data can be further processed and interpreted using software like Nuitrack to do skeleton tracking which exposes body pose and joint positions.  This can be used to allow the system to know when a user is close enough to the display or standing a particular place. &nbsp;This is good for gesture tracking as well.  The color data from the Intel Realsense camera can be processed in OpenCV which has a Unity SDK.  With OpenCV, the color data can be analyzed for object detection, face detection and recognition and face pose analysis.  This allows the system to recognize objects, people and face pose (which could be used to interpret affective state). &nbsp;Face and gaze detection could be used in lieu of trigger words like \u201cOK Google\u201d and \u201cAlexa\u201d as presumably the users have intent to interact with the system if they\u2019re looking at the character/sensors.</p><p>The sound recordings are used to detect volume, pitch and speech. &nbsp;The speech is analyzed using a cloud-based service, which then streams the input and a response back to unity to influence how the character animates and responds. &nbsp;The speech analysis could be used to interpret special assigned words to trigger activities, games or special animations or content in the display.  The response generated by the cloud-service can be used to animate the characters mouth if the character audibly responds to users. &nbsp;The coupling of depth/RGB data and audio allow for more nuanced understanding of a user\u2019s intent and affective state.  In combination, this could be used to drive sympathetic animations from the character.  Because the RGB data allows for face recognition, the character can potentially store information about users to be retrieved whenever that user interacts with the system.</p><br><p><b>Software - Procedural Character</b></p><br><p>I chose an animated dog character as the embodied representative for several key affordances. &nbsp;The face and body of this character have already been rigged and those parameters are accessible in Unity. &nbsp;The design of the character is ideal for this system and for expressing emotion because the head size is large relative to the body and the facial features of the character are contrasted which will read better with this device. &nbsp;Additionally, because this character is familiar and has recognizable physical embodiments of emotion, the user will more readily understand key animation poses.  For example, if the character is happy, it will wag its tail and stick out its tongue, but if it\u2019s nervous or upset, it will put its head and ears down. &nbsp;Based on the signals received and processed from the Intel Realsense camera and microphone, the dog character will animate.  Those inputs will impact the character\u2019s emotional pose, LookAt( ) behavior and special animation sequences.</p><br><p><b>Evaluation</b></p><p>The ultimate goal of this project is to produce a low-cost, reliable, 3D, 360-degree autostereoscopic display. &nbsp;I will visually inspect the hardware/software implementation to determine if the desired 360-degree display is produced and measure the specifications of this display to other closely related devices. &nbsp;I will evaluate our software/hardware implementation by creating test patterns to discretize each individual view zone.  Additionally, we will perform user studies to verify that the interaction system is intuitive and engaging.</p><br><p><b>User Study</b></p><p>The questions I\u2019d like to answer about this project lie at the intersection of the affordances of the display itself and the media users can interact with. &nbsp;Because I\u2019m designing a procedural character for this display, I want to explore believability and engagement through the user study as well as whether placing a character in-situ in a space impacts user experience. &nbsp;The nearest neighbor devices for the study will be an AR headset and a 2D monitor.  In this individual-use user study, participants will engage in activities that evaluate the character\u2019s ability to engage.  The participants will complete a close ended warm-up task both meant to acclimate and provide a concrete objective. &nbsp;After the warm-up task is completed (e.g. playing catch), the participants will be asked to engage in a more exploratory capacity with the character on each of the displays.  The activity will involve engaging the character\u2019s LookAt( ) behavior, object detection, face-pose detection, speech recognition, and visual accessories (like the LED light-strip). &nbsp;The order in which the participants use each display will be randomized to avoid conflating intuitive use with prior experience with the activity they\u2019re asked to compete.</p><br><p>If this device were to become commercially viable, it would have to be accessible for a wide variety of ages and expertise level. &nbsp;Because of that, I will look for a diverse group of participants with varying degrees of prior experience with technologies like video games, AR/VR, and voice assistants. &nbsp;As I design and implement features for my project, I will carefully prioritize scope that builds towards the user-study experience.</p><br><p><b>Future Work</b></p><p>If the radial lenticular works optimally, it would be amazing to explore a portable version of this novel volumetric display. &nbsp;Because the optical elements of this device have no moving parts and are lightweight, this display would be a great candidate for portability. &nbsp;It\u2019d be fascinating to expand on the AI character\u2019s capabilities by adding context aware infrastructure that changes how the character responds depending on localized metadata curation. &nbsp;</p><br><p>Ethically, this technology has the potential to make volumetric displays incredibly accessible at scale, because both the mylar cone and radial lenticular would cost less than one dollar to fabricate when productized. &nbsp;This would severely reduce cost on a market where displays typically cost thousands of dollars per unit.<br></p>", "people": ["emilysa@media.mit.edu"], "title": "Funnel Vision", "modified": "2019-01-21T23:00:29.197Z", "visibility": "PUBLIC", "start_on": "2018-02-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "funnel-vision"}, {"website": "", "description": "<p>&nbsp;Making 3D aniation easier by utilizing mobile AR, enabling manipulation of spatial objects using a spatial interface.</p>", "people": ["hbedri@media.mit.edu"], "title": "AR Animation Tool", "modified": "2018-04-20T23:24:12.477Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "ar-animation-tool"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: 400;\">Mobile phones are one of the fastest growing technologies in the developing world with global penetration rates reaching 90%. Mobile phone data, also called CDR, are generated every time phones are used and recorded by carriers at scale. CDR have generated groundbreaking insights in public health, official statistics, and logistics. However, the fact that most phones in developing countries are prepaid means that the data lacks key information about the user, including gender and other demographic variables. This precludes numerous uses of this data in social science and development economic research. It furthermore severely prevents the development of humanitarian applications such as the use of mobile phone data to target aid towards the most vulnerable groups during crisis.&nbsp;</span></p><p><span style=\"font-size: 18px; font-weight: 400;\">We developed a framework to extract more than 1,400 features from standard mobile phone data and used them to predict useful individual characteristics and group estimates. We here present a systematic cross-country study of the applicability of machine learning for dataset augmentation at low cost. We validate our framework by showing how it can be used to reliably predict gender and other information for more than half a million people in two countries. We show how standard machine learning algorithms trained on only 10,000 users are sufficient to predict individual\u2019s gender with an accuracy ranging from 74.3 to 88.4% in a developed country and from 74.5 to 79.7% in a developing country using only metadata. This is significantly higher than previous approaches and, once calibrated, gives highly accurate estimates of gender balance in groups. Performance suffers only marginally if we reduce the training size to 5,000, but significantly decreases in a smaller training set. We finally show that our indicators capture a large range of behavioral traits using factor analysis and that the framework can be used to predict other indicators of vulnerability such as age or socio-economic status. Mobile phone data has a great potential for good and our framework allows this data to be augmented with vulnerability and other information at a fraction of the cost.</span></p>", "people": ["eaman@media.mit.edu", "sandy@media.mit.edu", "yva@media.mit.edu"], "title": "Improving official statistics in emerging markets using machine learning and mobile phone data", "modified": "2018-10-19T20:56:21.936Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "improving-official-statistics-in-emerging-markets-using-machine-learning-and-mobile-phone-data"}, {"website": "", "description": "<p>We propose using modular building blocks and robotic mechanisms to create scalable and reconfigurable satellites and other relevant structures in orbit. Large, monolithic satellites take many years to build, at relatively high material and financial costs. But the falling costs of space launches present new opportunities for space-based assembly of satellite components. Thus, inspired by modular structural designs, we aim to create reconfigurable structures in space that will&nbsp;enable mission profile changes and life-cycle reuse, efficiently.</p><p>The Astrofactory will be the first on-orbit manufacturing and assembling satellite. The satellite houses a factory, which uses the building blocks of scalable structures, modular platforms, and robotic arms to create small satellites in low Earth orbit (LEO). The first mission is a&nbsp;rapid proto-flight demonstration of the manufacturing and assembling of a 1U cube satellite in a relevant space environment. The goal is to&nbsp;deploy a 1U&nbsp;(10 cm per side)&nbsp;cube satellite with &nbsp;\"plug-n-play\" components that is created on-orbit. Components such as a wireless sensor module, GPS, batteries, and solar panels will be made on Earth and transported, while several boards (memory, control and telecommunication boards, solar panels, etc.) will be built into the structures and assembled on-orbit. The complete system will fly in LEO and beyond for educational, scientific, and commercial purposes.</p>", "people": ["ezinne@media.mit.edu"], "title": "Astrofactory: The on-orbit manufacture and assembly of small satellites", "modified": "2019-01-14T19:55:39.534Z", "visibility": "LAB-INSIDERS", "start_on": "2018-10-01", "location": "", "groups": ["space-enabled"], "published": true, "active": false, "end_on": null, "slug": "astrofactory"}, {"website": "", "description": "<p>The widespread synthesis of common organic building blocks in space could have biased life beyond Earth towards chemical similarities to life as we know it. Meteoritic exchange might also have produced shared ancestry, most plausible for Earth and Mars. We are building an instrument to target nucleic acids (DNA), called the <a href=\"http://setg.mit.edu/\">Search for Extra-Terrestrial Genomes</a> (SETG). Our approach integrates automated extraction and sequencing of DNA using the first commercially available nanopore device, the Oxford Nanopore Technologies MinION.</p><p>A NASA team tested an earlier version of this technology during parabolic flight and on the international space station. Here we tested the latest iteration, with updated chemistry, flow cells, and software, in combination with high resolution acceleration and vibration measurement. Our goals are to 1) Quantify the impact of g-level and vibration on sequencing, and 2) perform sequencing during simulated Mars gravity for the first time.</p>", "people": [], "title": "Search for Extra-Terrestrial Genomes (SETG)", "modified": "2018-01-24T15:44:21.657Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "search-for-extra-terrestrial-genomes-setg"}, {"website": "", "description": "<p>Audio on a mobile device can be used to form low-resolution 3D images. The resolution of audio ranging and imaging is heavily limited by bandwidth. Here is an attempt to increase the resolution through continuous basis pursuit, a grid-less sparse algorithm. Comparison of backprojection, basis-pursuit denoising, and continuous basis pursuit shown.</p><p><br></p><p>Open access PDF:&nbsp;<a href=\"http://www.cv-foundation.org//openaccess/content_iccv_2015_workshops/w18/papers/Bedri_Exploring_the_Resolution_ICCV_2015_paper.pdf\">http://www.cv-foundation.org//openaccess/content_iccv_2015_workshops/w18/papers/Bedri_Exploring_the_Resolution_ICCV_2015_paper.pdf</a></p>", "people": ["raskar@media.mit.edu", "michaf@media.mit.edu", "hbedri@media.mit.edu"], "title": "Grid-free super-resolution for audio imaging", "modified": "2018-04-20T23:52:23.241Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "grid-free-super-resolution-for-audio-imaging"}, {"website": "", "description": "<p>Recent advances in medications for neurodegenerative disorders are expanding opportunities for improving the debilitating symptoms suffered by patients. Existing pharmacologic treatments, however, often rely on systemic drug administration, which result in broad drug distribution and consequent increased risk for toxicity. Given that many key neural circuitries have sub-cubic millimeter volumes and cell-specific characteristics, small-volume drug administration into affected brain areas with minimal diffusion and leakage is essential. We report the development of an implantable, remotely controllable, miniaturized neural drug delivery system permitting dynamic adjustment of therapy with pinpoint spatial accuracy. We demonstrate that this device can chemically modulate local neuronal activity in small-animal (rodent) and large-animal (nonhuman primate) models, while simultaneously allowing the recording of neural activity to enable feedback control.&nbsp;</p>", "people": ["canand@media.mit.edu"], "title": "Miniaturized Neural System for Chronic, Local Intracerebral Drug Delivery (MiNDS)", "modified": "2018-09-18T17:32:14.261Z", "visibility": "PUBLIC", "start_on": "2018-01-24", "location": "", "groups": ["conformable-decoders"], "published": true, "active": false, "end_on": null, "slug": "miniaturized-neural-system-for-chronic-local-intracerebral-drug-delivery"}, {"website": "", "description": "", "people": [], "title": "reSpire: Tangible Vital Sign Feedback Interface for Self-regulation of Respiration", "modified": "2018-10-17T12:40:03.552Z", "visibility": "LAB-INSIDERS", "start_on": "2018-06-01", "location": "", "groups": ["tangible-media"], "published": false, "active": false, "end_on": null, "slug": "respire-tangible"}, {"website": "", "description": "<p>Trying to catch up to a rapidly advancing technology, government agencies, industry, and academia are looking for guidance and best practices around AI and ethics. Many applications of AI can be characterized as some form of algorithmic decision making which raises the question of when we, as a society, should attempt to delegate decision making making processes to computer programs.</p><p>There is a difference between using a computer predicted risk score to decide which buildings to inspect and to decide a criminal sentence. As we create a theoretical taxonomy of decisions and their contexts, asking how their differences and similarities should be reflected in a society's norms, laws and policies around automation, we have a pressing practical application in mind. Governments and private actors are using AI to make decisions now.<br></p><p>Working with governments internationally, we are developing best practices around government use and regulation of AI and AVs.</p><p>We have designed surveys, conducted interviews, written case studies, directed legal research, created workshops, developed technical and policy tools, written guidance and facilitated regional and international conversations and collaboration between government leaders and other partners.<br></p><p>Our goal is to help policy makers and practitioners around the world make good decisions around  AI and AV strategy, policy, ethics, governance and risk.</p>", "people": [], "title": "Algorithmic Decision Making and Governance in the Age of AI", "modified": "2018-09-22T17:52:50.084Z", "visibility": "PUBLIC", "start_on": "2018-01-15", "location": "", "groups": ["ethics-and-governance"], "published": true, "active": false, "end_on": null, "slug": "algorithmic-decision-making-and-governance-in-the-age-of-ai"}, {"website": "", "description": "<p>As children tinker with materials in the world, they are constantly putting things together and taking them apart. They are learning through play\u2014trying out new ideas, exploring alternate paths, making adjustments, imagining new possibilities, expressing themselves creatively. In the process, they learn about the creative process and develop as creative thinkers.</p><p>As digital technologies enter the lives of children, there is risk that they will crowd out tinkering, with children spending more time watching screens than tinkering with materials. Yet, in our work, we have seen how digital technologies can also be used to open up new opportunities for tinkering.</p><p>Working in collaboration with the Tinkering Studio at the Exploratorium, Fondazione Reggio Children and the LEGO Foundation, we are developing a new generation of tools, activities, and spaces to support playful investigation and experimentation, integrating digital and physical materials.&nbsp;</p><p>The new activities will enable children to engage in new types of inquiry into light, sound, motion, and storytelling. In the initial set of activities, called \"light play,\" children can program colored lights and moving objects to make dynamic patterns of shadows.<br></p>", "people": ["tarmelop@media.mit.edu", "mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "Computational Tinkering", "modified": "2018-03-27T10:44:38.668Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "computational-tinkering"}, {"website": "", "description": "<p>The Digital Currency Initiative is helping \"build a field\" of cryptocurrency and blockchain technology by starting a new peer-reviewed journal and conference. Check out our first step (and experimental newsletter), <a href=\"https://mitcryptocurrencyresearch.substack.com/\">here</a>.</p>", "people": ["rhysl@media.mit.edu"], "title": "Cryptocurrency Research Review", "modified": "2019-04-16T17:50:24.675Z", "visibility": "PUBLIC", "start_on": "2018-12-07", "location": "", "groups": ["digital-currency-initiative-dci"], "published": true, "active": false, "end_on": null, "slug": "cryptocurrency-research-review"}, {"website": "", "description": "<p>Our visual realities are flooded with complex and robust natural scenes. New findings in curvature estimation illustrate where our expectation of reality can break down under very simple mechanisms.</p>", "people": ["elawson@media.mit.edu", "vmb@media.mit.edu"], "title": "Synthetic Shape Motion", "modified": "2018-10-23T19:51:45.892Z", "visibility": "LAB-INSIDERS", "start_on": "2018-05-05", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "synthetic-shape-motion"}, {"website": "", "description": "<p>3D interaction is a magical way for users to interact with a system without touching it. This is usually accomplished using a camera, but that can intrude heavily on privacy. Using RF, we can create interaction mechanisms that do not take pictures of us and help to preserve our privacy.</p>", "people": ["raskar@media.mit.edu", " otkrist@media.mit.edu", "michaf@media.mit.edu", "hbedri@media.mit.edu"], "title": "Through Wall/Table interaction using RF", "modified": "2018-04-21T00:07:04.872Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "through-wall-table-interaction-using-rf"}, {"website": "", "description": "<p>I'm showcasing a lightsaber with a beam that can be switched on and off. All the electronics and optics are contained within the handle, so when the lightsaber is off, all you see is the handle. When the lightsaber is on, a beam of light emerges that ends at the tip of the lightsaber. The lightsaber works in a fog-filled environment, where the beam can reflect off of the tiny fog-particles. The beam energy is safe, AKA it won\u2019t be cutting through anything anytime soon , but it looks really cool :)</p>", "people": ["hbedri@media.mit.edu"], "title": "LIGHTSABER! LIGHTFIELDS + DIFFUSORS", "modified": "2018-04-21T00:35:58.195Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "lightsaber-lightfields-diffusors"}, {"website": "", "description": "<p>For decades, the International Space Station (ISS) has operated as a bastion of international cooperation and a unique testbed for microgravity research. Beyond enabling insights into human physiology in space, the ISS has served as a microgravity platform for numerous science experiments. In recent years, private industry has also been affiliating with NASA and international partners to offer transportation, logistics management, and payload demands. As the costs of flying projects to the ISS decrease, the barriers limiting non-traditional partners, like emerging space nations and startups, from accessing the ISS as a platform also decrease.&nbsp; </p><p>However, the ISS in its current form cannot be sustained forever. As NASA looks towards commercialization of the low Earth orbit (LEO) space and the development of a cislunar station, concrete plans for shifting the public-private relationship of the ISS are unclear. With the consistent need to continue microgravity research\u2014from governments and private industry\u2014<b>understanding the socio-technical and policy issues that affect the marketplace for future microgravity platforms is essential to maintaining an accessible and sustainable space economy.&nbsp;</b></p><p>How will the US and other governments design public-private partnerships to pursue economic and social goals in the LEO microgravity ecosystem? What governance structures will influence who is eligible to operate platforms for activities including tourism, research, manufacturing, and outreach? How will international collaboration occur in the future LEO microgravity ecosystem?&nbsp;</p><p>This project contributes to progress on these questions by offering technology policy insight using methods from Systems Engineering. Through case study research and numerous expert interviews, this project examines the stakeholders, needs, objectives, system functions and forms for the ISS and microgravity research platforms now and in the future. Particular attention is paid towards explaining the market dynamics that affect the administrative and economic barriers to entry for emerging space nations and non-traditional spaceflight participants.</p>", "people": ["drwood@media.mit.edu", "cjoseph1@media.mit.edu"], "title": "Accessibility of the microgravity research ecosystem", "modified": "2019-03-13T17:40:41.842Z", "visibility": "PUBLIC", "start_on": "2018-04-01", "location": "", "groups": ["space-enabled"], "published": true, "active": false, "end_on": null, "slug": "accessibility-of-the-microgravity-research-ecosystem"}, {"website": "", "description": "<p>Improvements in ingestible electronics with the capacity to sense physiological and pathophysiological states have transformed the standard of care for patients. Yet, despite advances in device development, significant risks associated with solid, non-flexible gastrointestinal transiting systems remain. Here, we report the design and use of an ingestible, flexible piezoelectric device that senses mechanical deformation within the gastric cavity. We demonstrate the capabilities of the sensor in both in vitro and ex vivo simulated gastric models, quantify its key behaviours in the gastrointestinal tract using computational modelling and validate its functionality in awake and ambulating swine. Our proof-of-concept device may lead to the development of ingestible piezoelectric devices that might safely sense mechanical variations and harvest mechanical energy inside the gastrointestinal tract for the diagnosis and treatment of motility disorders, as well as for monitoring ingestion in bariatric applications.</p>", "people": ["zijunw@media.mit.edu", "canand@media.mit.edu"], "title": "Flexible piezoelectric devices for gastrointestinal motility sensing", "modified": "2018-06-29T21:58:46.618Z", "visibility": "PUBLIC", "start_on": "2017-10-10", "location": "", "groups": ["conformable-decoders"], "published": true, "active": false, "end_on": null, "slug": "lead-zirconate-titanate-gastrointestinal-sensor-pzt-gi-s"}, {"website": "", "description": "", "people": [], "title": "TESSERAE: Suborbital Microgravity Test", "modified": "2019-04-29T16:18:52.389Z", "visibility": "LAB", "start_on": "2019-05-02", "location": "", "groups": ["responsive-environments", "space-exploration"], "published": false, "active": false, "end_on": null, "slug": "tesserae-suborbital"}, {"website": "", "description": "<p>Growth in leisure travel has become increasingly significant economically, socially, and environmentally. However, flexible but uncoordinated travel behaviors exacerbate traffic congestion. Mobile phone records not only reveal human mobility patterns, but also enable us to manage travel demand for system efficiency. We propose a location recommendation system that infers personal preferences while accounting for constraints imposed by road capacity in order to manage travel demand. We first infer unobserved preferences using a machine learning technique from phone records. We then formulate an optimization method to improve system efficiency. Coupling mobile phone data with traffic counts and road network infrastructures collected in Andorra, this study shows that uncoordinated travel behaviors lead to longer average travel delay, implying opportunities in managing travel demand by collective decisions. The interplay between congestion relief and overall satisfied location preferences observed in extensive simulations indicate that moderate sacrifices of individual utility lead to significant travel time savings. Specifically, the results show that under full compliance rate, travel delay fell by 52 percent at a cost of 31 percent less satisfaction. Under 60 percent compliance rate, 41 percent travel delay is saved with a 17 percent reduction in satisfaction.This research highlights the effectiveness of the synergy among collective behaviors in increasing system efficiency. </p>", "people": ["sandy@media.mit.edu", "yleng@media.mit.edu"], "title": "Managing Travel Demand: Location recommendation for system efficiency", "modified": "2018-10-19T21:02:37.566Z", "visibility": "PUBLIC", "start_on": "2015-12-01", "location": "--Choose Location", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "location-recommendations-based-on-large-scale-call-detail-records"}, {"website": "http://deepangel.media.mit.edu/", "description": "<p><b>Deep Angel&nbsp;</b>is an artificial intelligence that erases objects from photographs. The algorithm is hosted on <a href=\"http://deepangel.media.mit.edu\">http://deepangel.media.mit.edu</a>, which enables anyone&nbsp;to interact with the AI and explore what it can disappear.</p><p>Part philosophy, part technology, and part art, Deep Angel is designed to spark a series of conversations on technology in our daily lives and AI and media manipulation.&nbsp;&nbsp;</p><p>Deep Angel draws from&nbsp; Walter Benjamin's description of Paul Klee's Angelus Novus, the angel of history who has clairvoyance into the dark side of what appears to be progress. The angel sees the unravelling of all that matters in the world and would like to alert the world about his vision, but he's caught in the storm of progress and can't communicate any messages. The images that Deep Angel generates are intended to deliver the message that Angelus Novus would have sent if he could.&nbsp;</p><p>The algorithm applies computer vision techniques to automatically (1) detect and outline objects in images, (2) remove the outlined object from the image, and (3) imagine what the image would look like if that outlined object were removed from the image. Any image uploaded and transformed by Deep Angel can be published on the Deep Angel website by clicking the \"Publish to Deep Angel\" button.&nbsp;</p><p>The AI's performance varies across photographs. Sometimes, it's impossible to tell what has been disappeared. Other times, the images appear similar to the images from Adrian Piper's <i>Everything</i> series. The more people interact with the algorithm, the more attuned people will be to the potential and limitations of modern AI to manipulate the media. It's now possible to automate the vanishing commissar in Soviet photography, but the AI is not yet perfect. Below are two examples of the Deep Angel AI effect: (1) a gif generated by Deep Angel showing a father and daughter disappearing in the wilderness and (2) two images showing the before and after of Deep Angel peering into a photo of a professional surfer.&nbsp;</p>", "people": ["cebrian@media.mit.edu", "dubeya@media.mit.edu", "niccolop@media.mit.edu", "zive@media.mit.edu", "groh@media.mit.edu", "irahwan@media.mit.edu", "nobradov@media.mit.edu"], "title": "Deep Angel: The AI behind the aesthetics of absence", "modified": "2019-02-14T19:46:25.924Z", "visibility": "PUBLIC", "start_on": "2018-08-06", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "deep-angel-ai"}, {"website": "http://datacultureproject.org", "description": "<p><b>Struggling to build your organization's ability to work with data? Use our hands-on learning program to kickstart your data culture.</b></p><p><a href=\"http://datacultureproject.org\">datacultureproject.org</a></p><p>Data is everywhere right now. But many organizations like yours are struggling to figure out how to build capacity to work with data. You don't need a data scientist; you need a <strong>data culture</strong>.</p><p>Use our self-service learning program to facilitate fun, creative introductions for the non-technical folks in your organization. These are not boring spreadsheet trainings. The free tools and activities on the linked website are hands-on and designed to meet people where they are across your organization, and build their capacity to work with data. Try to kickstart your data culture by running one activity per month as a brown bag lunch. Our videos and facilitation guides will lead you through running them yourself.</p>", "people": ["rahulb@media.mit.edu"], "title": "Data Culture Project", "modified": "2018-05-06T22:26:19.647Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "data-culture-project"}, {"website": "", "description": "<p>KinderCity is a pilot project that attempts to understand the intangibles of city perception. It attempts to understand how people, across ages, children and adults, perceive places and spaces that are playful, creative and inspiring.&nbsp;</p>", "people": ["layanasu@media.mit.edu"], "title": "KinderCity", "modified": "2018-05-03T15:49:31.795Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "kindercity"}, {"website": "", "description": "<p>There are over one million registered charities in the United States alone, and many more worldwide. How do you choose among them?&nbsp;<br></p><p>MyGoodness is a simple game that helps you understand how you give. In the game, you will make 10 giving decisions. Each decision is between two choices, and you tell us which you prefer.</p><p>At the end of the game, we give you a summary of your \u2018goodness\u2019 and how it&nbsp;compares to others. You can share that feedback with whomever you would&nbsp;like.</p>", "people": ["awad@media.mit.edu", "irahwan@media.mit.edu"], "title": "MyGoodness", "modified": "2017-12-15T23:10:00.218Z", "visibility": "PUBLIC", "start_on": "2017-12-11", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "mygoodness"}, {"website": "", "description": "", "people": [], "title": "Test project", "modified": "2018-09-24T15:06:09.018Z", "visibility": "LAB", "start_on": "2018-09-24", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "test-project-3"}, {"website": "", "description": "", "people": [], "title": "Sensory Synchrony", "modified": "2019-04-16T19:58:02.336Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["fluid-interfaces"], "published": false, "active": false, "end_on": null, "slug": "sensorysynchrony"}, {"website": "", "description": "<p>How something is presented can be as important as the message itself.</p><p>In the age of political polarization and election meddling, it is of vital importance to understand which factors contribute to the formation of public opinions. Television is one of the main sources of information for a large portion of the general population.</p><p>In order to understand discrepancies in news perception, what they are caused by and which implications they might have on shaping public political debate, we first need to understand how television news are constructed and define presentational aspects of the news. Given that, we can build tools that analyze news consumption by the public.</p><p>We aim to use various artificial intelligence techniques to model the \"subcarriers of information\" present in a TV newscast, to automatically detect and understand visual and auditory cues beyond the spoken word including the layout of the set, the affect of the participants, the nature of the motion, and other cues. Our goal is to develop an algorithmic understanding of journalistic choices in the way news content is presented. We also attempt develop an understanding of higher-level characteristics of television news such as television set atmosphere or political bias. This altogether would enable a broad-range, comprehensive algorithmic analysis of <b>how news presentation is trying to shape the public political debate</b>.</p><p><a href=\"https://viral.media.mit.edu/pub/unspoken\">Project updates</a> via PubPub</p>", "people": ["mhjiang@media.mit.edu", "eickhoff@media.mit.edu", "lip@media.mit.edu"], "title": "Unspoken News", "modified": "2019-04-18T16:51:12.445Z", "visibility": "PUBLIC", "start_on": "2018-11-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "unspoken-news"}, {"website": "", "description": "<p>Pain is a subjective experience commonly measured through patients' self reports. Unfortunately,&nbsp;self-report measures only work when the subject is sufficiently alert and cooperative, and hence they lack utility in multiple situations (such as during drowsiness) and patient populations (such as patients with dementia or paralysis).&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">To circumvent the limitations of pain self-reports, in this project we are developing automatic methods for pain estimation based on physiological signals&nbsp; and/or facial expressions.</span></p>", "people": ["dlmocdm@media.mit.edu", "orudovic@media.mit.edu", "picard@media.mit.edu"], "title": "Machine Learning for Pain Measurement", "modified": "2018-10-19T21:32:53.150Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "pain-measurement"}, {"website": "", "description": "<p>Move Your Glass is an app built using the JavaScript-based Wearscript library to communicate with the native sensors and camera on the device.&nbsp;The app is an&nbsp;<span style=\"font-size: 18px;\">activity and behavior tracker that also tries to increase wellness by nudging the wearer to engage in positive behaviors.&nbsp;</span><span style=\"font-size: 18px;\">In particular, data from the tri-axial accelerometer and camera are collected and analyzed offline using a designated server. These results are then fed into a k-nearest neighbors machine learning routine and compared to training data to provide continuous differentiation between sitting/standing, walking, and running while the user is on the go. The ultimate goal is to log this information (and additional activity parameters) and convey activity summaries and, if necessary, prompts for increasing activity, back to users.</span></p>", "people": ["nfarve@media.mit.edu", "pattie@media.mit.edu"], "title": "Move Your Glass", "modified": "2018-10-18T01:17:21.506Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "move-your-glass"}, {"website": "", "description": "<p>MoCho (short for \"Mobility Choices\") is a&nbsp;<a href=\"http://cityscope.media.mit.edu/\">CityScope</a>&nbsp;module focused on mobility choices and societal impacts. This tool helps predict the choices of mobility modes made at the individual level throughout the entire Boston Metro area.</p><p><i style=\"font-size: 18px; font-weight: 400;\"><a href=\"https://cityscope.media.mit.edu/CS_choiceModels/index.html\">Check out a live demo of MoCho predictions here</a>.&nbsp;</i></p>", "people": ["yasushis@media.mit.edu", "doorleyr@media.mit.edu", "noyman@media.mit.edu"], "title": "MoCho: Mobility choices and societal impacts", "modified": "2019-04-19T19:11:18.982Z", "visibility": "PUBLIC", "start_on": "2018-06-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "mobcho"}, {"website": "https://vimeo.com/155460417", "description": "<p><span style=\"font-size: 18px; font-weight: normal;\">Makeup has long been used as a body decoration process for self-expression and for the transformation of one's appearance. While the material composition and processes for creating makeup products have evolved, they still remain static and non-interactive. But our social contexts demand different representations of ourselves; thus, we propose ChromoSkin, a dynamic color-changing makeup system that gives the wearer ability to alter seamlessly their appearance. We prototyped an interactive eye shadow tattoo composed of thermochromic pigments activated by electronics or ambient temperature conditions. We present the design and fabrication of these interactive cosmetics, and the challenges in creating skin interfaces that are seamless, dynamic, and fashionable.</span></p>", "people": ["manisham@media.mit.edu", "joep@media.mit.edu", "cindykao@media.mit.edu", "katiav@media.mit.edu", "geek@media.mit.edu"], "title": "ChromoSkin", "modified": "2018-02-08T20:50:38.533Z", "visibility": "PUBLIC", "start_on": "2015-12-01", "location": "--Choose Location", "groups": ["living-mobile", "responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "chromoskin"}, {"website": "", "description": "<p>In September 2014, 150 parents, engineers, designers, and healthcare practitioners gathered at the MIT Media Lab for the \"Make the Breast Pump Not Suck!\" Hackathon. As one of the midwives at our first hackathon said, \"Maternal health lags behind other sectors for innovation.\" This project brought together people from diverse fields, sectors, and backgrounds to take a crack at making life better for moms, babies, and new families. </p>", "people": ["dignazio@media.mit.edu", "ethanz@media.mit.edu", "tjlevy@media.mit.edu", "achituv@media.mit.edu", "cwwang@media.mit.edu", "ahope@media.mit.edu"], "title": "Make the Breast Pump Not Suck Hackathon", "modified": "2018-04-11T16:55:16.177Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "make-the-breast-pump-not-suck-hackathon"}, {"website": "", "description": "<p>In nature, hair has numerous functions such as providing warmth, adhesion, locomotion, sensing, and a sense of touch, as well as its well-known aesthetic qualities. This work presents a computational method of 3D printing hair structures. It allows us to design and generate hair geometry at 50 micrometer resolution and assign various functionalities to the hair. The ability to fabricate customized hair structures enables us to create superfine surface texture, mechanical adhesion properties, new passive actuators, and touch sensors on a 3D-printed artifact. We also present several applications to show how the 3D-printed hair can be used for designing everyday interactive objects.</p>", "people": ["gershon@media.mit.edu", "ishii@media.mit.edu", "jifei@media.mit.edu"], "title": "Cilllia: 3D-printed micro pillar structures for surface texture, actuation, and sensing", "modified": "2019-05-23T19:18:17.084Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "cilllia-3d-printed-micro-pillar-structures-for-surface-texture-actuation-and-sensing"}, {"website": "http://web.media.mit.edu/~barmak/Time-folded.html", "description": "<h2>Rethinking photography optics in the time dimension</h2><p><i>What if we could design optics in time instead of space?</i></p>", "people": ["raskar@media.mit.edu", "guysatat@media.mit.edu", "barmak@media.mit.edu"], "title": "Time-folded optics", "modified": "2018-09-24T18:15:42.266Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "time-folded-optics"}, {"website": "", "description": "<p>Industries are more likely to enter and less likely to exit regions that are densely populated by related industries. Unfortunately, the measures used to estimate the relatedness of industries often combine information about multiple forms of relatedness. Here, we use data on the entire formal sector economy of a large country to construct five different measures of relatedness and compare their ability to predict diversification events. We interpret differences in the ability of these metrics to predict entry events as evidence of the relative importance of each relatedness channel for specific industries. These findings advance our understanding of the forms of relatedness that are more likely to predict regional diversification events for specific industries.</p>", "people": ["mkaltenb@media.mit.edu", "hidalgo@media.mit.edu", "crisjf@media.mit.edu", "hartmado@media.mit.edu"], "title": "Untangling Relatedness: What forms of relatedness predict diversification?", "modified": "2018-05-03T20:55:25.540Z", "visibility": "LAB-INSIDERS", "start_on": "2017-02-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "which-industries-follow-relatedness"}, {"website": "", "description": "<p>Electronics are ubiquitous in today\u2019s world, with applications ranging from smart cities to health care, defense, economy, government, education, research, entertainment and the list goes on. While the computing demands of these applications are ever increasing, the capabilities of electronics have hit fundamental limitations and have plateaued. In the past, the growth in computing capabilities of electronics has been sustained by scaling both the dimensions as well as power-supply voltage of the Field-Effect-Transistors (FETs), that form the building blocks of the integrated circuits. However, <b>as the scaling roadmap enters the sub-5 nm regime, today\u2019s transistor reaches its physical limits</b> in dimensional scalability and can no longer offer effective electrostatics, leading to exponential increase in static leakage power. In addition, the power supply voltage, which is the most effective way for lowering the dynamic power, can no longer be scaled effectively as in earlier technology generations. The <b>voltage in-scalability has fundamental roots in the thermal distribution of carriers</b>, <b>which limits the steepness of turn-on characteristics or subthreshold swing</b> of conventional FETs. These dimensional and voltage scalability issues, lead to exponential increase in power-density and has ushered in the dead end of the glorious growth episode of \"Information Society.\" Moreover, <b>the fundamental nature of the problem suggests the inability of evolutionary solutions</b> to address this growing energy crisis and demands radically new innovations on multiple fronts.</p><p>Our research involves a holistic approach towards solving this energy crisis, starting from exploration of beyond-Silicon nanomaterial technology, to in-depth understanding of the physics of fundamentally different device-working mechanisms and finally, experimental demonstration of novel, highly energy-efficient and scalable electronic devices. </p><p>We invented the <b>world\u2019s thinnest channel (6 atoms thick) quantum mechanical transistor</b> involving band-to-band-tunneling, which <b>overcomes the fundamental thermal limitations </b>in subthreshold swing&nbsp;and leads to record energy reduction by more than 75% [<a href=\"https://www.nature.com/articles/nature15387\"><b>D. Sarkar et.al., Nature, 526(7571),&nbsp;91-95, (2015)</b></a>] [Nature (News and Views)&nbsp;526, 51\u201352(2015)]. This atomically thin and layered semiconducting-channel tunnel FET (ATLAS-TFET), is based on the idea, that we conceived, of a unique tunneling heterojunction combining the best attributes of 3D (matured doping technology) and 2D (excellent electrostatics and ultra-low tunneling barrier) materials to achieve extremely efficient and controllable electron wave propagation through the energy barrier. This device is the <b>first and</b> <b>only tunneling-transistor till date, in any architecture and any material platform, to achieve ITRS prescription </b>of sub-thermal subthreshold swing over four decades of current at an ultra-low power-supply voltage of 0.1V(thus, allowing voltage scalability). Moreover, the atomically-thin 2D channel provides near-ideal device electrostatics, which <b>allows dimensional scalability to beyond Silicon scaling era</b><b> (sub 5nm)</b>. Thus, this device can crack the long-standing issue of simultaneous dimensional and power-supply voltage scalability.</p>", "people": [], "title": "Ultra-Scalable and Energy-Efficient Nanoelectronics", "modified": "2018-09-21T13:47:00.959Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["nano-cybernetic-biotrek"], "published": true, "active": false, "end_on": null, "slug": "nano-machine-life-synergism"}, {"website": "", "description": "<p>The first thing we humans do after migration is satisfy our basic needs, including growing food. If we were to have a base on the moon or Proxima Centauri, would we grow green plants? If we retain our molars and taste buds, what would we consume besides thermo-stabilized and freeze-dried food?&nbsp;Why not introduce self-sufficiency on the moon and use the same space-enabled designs to feed people on Earth?</p><p><span style=\"font-size: 18px;\">This is the first open source lunar plant habitat that will test seed germination and initial plant growth when subjected to the effects of lunar gravity and ionized radiation. The plant habitat is sized as a 1U cube satellite (10 cm per side) with a modular platform design.&nbsp;The base will contain a seed module holding 50 Arabidopsis seeds and provide approximately 0.5 liters of normal air at standard pressure. The plant habitat has been designed to fly in a near circular lunar orbit at an altitude of 100 km after a series of maneuvers. The habitat will remain in this polar orbit for the duration of the nominal mission. This is to ensure global coverage for the gravity experiment with a period of 14 days. Subsequently, the altitude of the plant habitat will be reduced to an average of 50 km to calibrate the gravity field and the extended mission would include students reconfiguring the plant habitat based for optimal plant growth.&nbsp;</span></p>", "people": ["ezinne@media.mit.edu"], "title": "Space Horticulture: Nutritious Food for Space Travelers", "modified": "2019-01-14T19:55:57.531Z", "visibility": "LAB-INSIDERS", "start_on": "2018-10-01", "location": "", "groups": ["space-enabled"], "published": true, "active": false, "end_on": null, "slug": "space-horticulture"}, {"website": "", "description": "", "people": [], "title": "Test Project checking topics", "modified": "2018-09-24T18:14:34.944Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "test-project-checking-topics"}, {"website": "", "description": "<p>The process by which children learn native languages is markedly different from the process of learning a second, or non-native, language. Children are typically immersed in their native languages. They receive input from the adults and other children surrounding them, based on immediate need and interaction, during every waking hour. &nbsp;</p><p>Second language learners are exposed to input from the new language in very different ways, most commonly in a classroom setting.&nbsp;The second language learner relies heavily on memory skills with sparse interaction, in contrast to the first language learner that can rely on environmental reinforcement and social interaction to learn words.&nbsp;</p><p>Social robots have the potential to drastically improve on this paradigm, making the second-language learning experience more like the experience of learning a native language by engaging the child in a rich, interactive exposure to the target language, especially aspects not typically covered by traditional technological solutions, such as prosody, fundamental phonetics, common linguistic structures, etc.</p><p>Our project explores how to design child-robot &nbsp;interactions that encourage child-driven language learning, that adapt and personalize each child\u2019s learning experience. We incorporate game design and machine learning into the child-robot interaction design. The child and robot play through a suite of educational games together. Using real-time sensor data and gameplay features, the robot constructs a model of each child's learning and emotional trajectory, then uses these models to inform its own decision making during the game. Thus, the robot's behaviors become personalized to individual children based on their learning style, personality and knowledge/emotional states during gameplay.&nbsp;</p><p></p><p></p><p></p><p></p><p></p><p></p>", "people": ["cynthiab@media.mit.edu", "hchen25@media.mit.edu", "samuelsp@media.mit.edu", "haewon@media.mit.edu"], "title": "Personalized  Interaction for Language Learning", "modified": "2018-05-03T20:56:50.720Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "personalized-interaction-for-language-learning"}, {"website": "https://dci.mit.edu/lightning-network/", "description": "<p>Lit, the lightweight Lightning Network software developed at the MIT Media Lab, works with multiple Bitcoin-like blockchains. The DCI team has created this demo to experience how Lightning can be used at point-of-sale terminals as well as allow users to do device-to-device transactions using multiple coins. We show how an electronic mobile wallet works with three currencies (Bitcoin, Litecoin, and USD).</p>", "people": ["adragos@media.mit.edu", "narula@media.mit.edu", "tdryja@media.mit.edu", "joe@media.mit.edu"], "title": "Bitcoin vending machine", "modified": "2019-02-14T19:49:02.951Z", "visibility": "PUBLIC", "start_on": "2018-07-02", "location": "", "groups": ["digital-currency-initiative-dci"], "published": true, "active": false, "end_on": null, "slug": "bitcoin-vending-machine"}, {"website": "", "description": "<p>This is a joint proposal from graduate students <a href=\"https://www.media.mit.edu/people/vparth/overview/\">Vik Parthiban</a> (Media Lab) and <a href=\"https://www.csail.mit.edu/person/andrew-spielberg\">Andrew Spielberg</a> (<a href=\"https://www.csail.mit.edu/research/distributed-robotics-laboratory\">MIT CSAIL, Distributed Robotics Laboratory</a>).&nbsp;</p><p>With the release of the <a href=\"https://www.magicleap.com/creator\">Magic Leap Creator</a>&nbsp;and <a href=\"https://developer.leapmotion.com/northstar/\">Leap Motion NorthStar</a> platform for new mixed-reality and augmented-reality applications, we propose a new computer-aided design (CAD) tool for simulating and deploying robots. Our goal is to better understand locomotion in 3D space and diagnose character movement under multiple constraints.&nbsp;</p><p>We will develop the \"Magic Leap Design and Control Toolbox,\" a suite of new interactive algorithms and implementations to directly build structures within the environment using captured information about the world. A new system for gesturing the desired path or degrees of freedom of an object will automatically translate the gestures into robot and character control inputs. The system will be based on algorithms that synthesize natural gestural control based on the desired motion.</p>", "people": ["vparth@media.mit.edu"], "title": "Designing robots in mixed reality", "modified": "2019-04-18T14:43:04.547Z", "visibility": "PUBLIC", "start_on": "2018-05-04", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "mixed-reality-robots"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: 400;\">The Language Simplification project is developing automatic methods to simplify complex texts to be more easily read and understood by a broader audience, such as children and non-native English speakers. Using neural networks, complex words and phrases can be substituted, the sentence can be split and rephrased, and the overall text can be summarized and compressed.&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">These capabilities can be wrapped into a reading assistance tool for end users, or as a pre-processing step for other NLP tasks.</span></p>", "people": ["ngillani@media.mit.edu", "exposito@media.mit.edu", "echu@media.mit.edu"], "title": "Language Simplification", "modified": "2019-04-17T20:51:49.717Z", "visibility": "PUBLIC", "start_on": "2019-02-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "language-simplification"}, {"website": "", "description": "<p>The interactive Nutrition Justice art exhibit uses&nbsp;space-based assets, data science, and&nbsp;projection mapping to showcase local hunger in ways that inspire measurable local action&nbsp;in food insecure communities.</p><p>Hunger in the US has many faces: children, seniors, veterans, and people from rural and marginalized communities are not immune. And as humanity is urbanizing rapidly, so too are food deserts expanding and densifying. Several urban agriculture solutions bring new life to food deserts and serve as a driving force for community development. Yet we need all hands on deck&nbsp;to create inclusive food oases. </p><p>In the spirit of collective action, this exhibit visualizes local hunger to encourage the re-introduction of self-reliance in food deserts and marginalized communities.</p>", "people": ["ezinne@media.mit.edu", "lizbethb@media.mit.edu"], "title": "Nutrition Justice", "modified": "2019-01-14T19:56:36.741Z", "visibility": "LAB-INSIDERS", "start_on": "2018-10-01", "location": "", "groups": ["space-enabled"], "published": true, "active": false, "end_on": null, "slug": "nutrition-justice"}, {"website": "", "description": "<p>&nbsp;Seemingly an old TV box which turns out to offer an abundance of control over content selection: each aspect of news can be selected for using old-style controls. The main goal is to make people aware of their choices and biases and to make them curious to tune the controls and to watch something different. Hitting the TV resets the controls to random values and lets user discover completely new unexpected content.&nbsp;</p><p>The main underlying assumption behind the idea of YouTune is that while it is very difficult to influence people\u2019s opinions and change their beliefs explicitly without a willingness on their part to change these beliefs, we could attempt to influence people implicitly by providing a tool that, first, would make users aware of the choices they take: to watch what they want or are used to, the user has to explicitly set controls. Unlike traditional broadcast and online media platforms, YouTune does not allow users to select content based on channel or program, so in order to find content that the person is usually interested in, with YouTune they would need to manually select characteristics of the content they want to watch. And second, we believe that YouTune will make people curious to change the control values they are used to and therefore to discover unexpected content that could potentially influence their opinions in an implicit way.&nbsp;</p><p>With YouTune the user can choose between different characteristics and tune into some value of the chosen characteristic to see short news stories. The idea is to make statistics experienceable without showing graphs: a person using YouTune would subconsciously aggregate statistical patterns and understand which tricks and stylistic tools channels tend to use to cover certain topics.</p><p><a href=\"https://viral.media.mit.edu/pub/youtune\">Project updates</a> via PubPub</p>", "people": ["eickhoff@media.mit.edu", "lip@media.mit.edu"], "title": "YouTune", "modified": "2019-04-18T16:51:54.465Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "youtune"}, {"website": "", "description": "<p>Today\u2019s health sensors (which monitor breathing, heartbeats, steps, etc.) require their users to wear them on their bodies. In contrast, our technologies can monitor human health without requiring the user to wear any device on his/her body. To do so, we capture and analyze wireless signals reflected off the human body; we then use these reflected signals to extract breathing and heartbeats without any physical contact with the human body. We are currently exploring techniques to remotely sense additional health metrics like blood pressure, oxygen saturation, and glucose levels. Monitoring these health metrics can render ICU (intensive care unit) vital sign monitors completely noninvasive and enable continuous monitoring of diabetes patients.</p>", "people": ["fadel@media.mit.edu"], "title": "Health sensing using wireless signals", "modified": "2019-04-17T18:06:52.094Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "health-sensing-human-computer-interaction"}, {"website": "http://www.artemdementyev.com", "description": "<p>We introduce SkinBot: a lightweight robot that moves over the skin's surface with a two-legged suction-based locomotion mechanism and captures a wide range of body parameters with an exchangeable multipurpose sensing module. We believe that robots that live on our skin, such as SkinBot, will enable a more systematic study of the human body and offer great opportunities to advance our knowledge in many areas such as telemedicine, human-computer interfaces, body care, and fashion.</p>", "people": ["artemd@media.mit.edu", "joep@media.mit.edu", "javierhr@media.mit.edu", "sfollmer@media.mit.edu"], "title": "SkinBot: A wearable, skin-climbing robot", "modified": "2019-04-19T14:35:05.713Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["affective-computing", "responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "skinbot-a-wearable-skin-climbing-robot"}, {"website": "", "description": "<p>The development of biotechnologies since the era of recombinant DNA in the 1970s has occurred largely via the interaction of academic, industrial, and governmental institutions.&nbsp;Largely absent from this ecosystem are the informed inputs of grassroots communities at any point in the technology developmental cycle.</p><p>The parallel rise of high-throughput, next generation DNA sequencing and advanced DNA synthesis technologies (reading and writing DNA), along with invention of precise genome editing technologies (e.g., CRISPR) has humanity at the brink of a new era, one where living technologies rule. Given the vital importance of living technologies, not only to human health, manufacturing, the economy, and environment, but to our social fabric and culture, we ask:</p><ul><li>How should living technologies be developed?<br></li><li>How can we ensure there is broad, diverse participation in biotechnology?<br></li><li>How can marginalized, under-represented and indigenous communities be agents of change in this era?<br></li><li>What types of institutions and design practices can be employed to ensure just outcomes?<br></li><li>How can humanity work harmoniously, in concert with nature, to co-evolve and flourish?</li></ul><p><br></p><p>We will host a <a href=\"https://www.media.mit.edu/events/participatory-design-conference/\">workshop</a>&nbsp;on this topic during the biennial Participatory Design Conference in Genk and Hasselt, Belgium.&nbsp;</p>", "people": ["dkong@media.mit.edu", "nbakker@media.mit.edu"], "title": "Participatory Biotechnology", "modified": "2018-05-02T14:59:36.627Z", "visibility": "PUBLIC", "start_on": "2018-05-27", "location": "", "groups": ["community-bio"], "published": true, "active": false, "end_on": null, "slug": "participatory-biotechnology"}, {"website": "", "description": "<p>The view on Alzheimer\u2019s Disease (AD) diagnosis has shifted towards a more dynamic process in which clinical and pathological markers evolve gradually before diagnostic criteria are met. Given the&nbsp;wide variability in data available per subject, inherent per-person differences, and the slowly changing nature of the disease, accurate prediction of AD progression is a significant, difficult challenge. The goal of this project is to devise novel Personalized Machine Learning Models that can accurately capture future changes in the key biomarkers and cognitive scores related to AD and other neurological conditions. As the basis for our framework, we use the&nbsp;Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI)&nbsp;dataset\u2013the largest publicly available dataset&nbsp; for AD research.&nbsp;&nbsp;These data are highly heterogeneous and multi-modal, and include imaging (MRI, PET), cognitive scores, CSF biomarkers, genetics, and demographics (e.g. age, gender, race). The developed models are the break-through in machine learning for health-care as they allow&nbsp; personalized forecasting of the diseases' progression - in contrast to the traditional \"one-size-fits-all\" approaches. This capability is of&nbsp; great importance to both clinicians and those at risk of AD since it is critical to early identification of at-risk subjects, construction of informative clinical trials, and timely detection of AD.</p>", "people": ["orudovic@media.mit.edu", "picard@media.mit.edu"], "title": "Personalized Machine Learning for Future Health", "modified": "2019-02-14T19:49:51.362Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "personalized-machine-learning-for-future-health"}, {"website": "https://www.linkedin.com/in/johndelaparra/", "description": "<p>The OpenAg\u2122 Food Server is a shipping container-sized, <b>controlled environment agriculture technology</b> that can be built to utilize hydroponic or aeroponic technology. It can serve as both a<b> research platform for simulating precise environments at scale</b> (see <a href=\"https://www.media.mit.edu/projects/openag-flavor-ecology/overview/\">Flavor, environment, and the phenome</a>) , and a <b>production unit</b> for any specified crop of interest. It is intended to produce <b>larger quantities of food</b> than a <a href=\"https://www.media.mit.edu/projects/personal-food-computer/overview/\">Personal Food Computer</a> and appeals to <b>interdisciplinary researchers</b> as well as&nbsp;<b>small-scale cafeterias, restaurants, and boutique operators.</b></p>", "people": ["jrye@media.mit.edu", "calebh@media.mit.edu", "poitrast@media.mit.edu", "rebekahj@media.mit.edu", "delapa@media.mit.edu", "tsavas@media.mit.edu"], "title": "Food Server", "modified": "2019-03-20T12:41:06.829Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["open-agriculture-openag"], "published": true, "active": false, "end_on": null, "slug": "food-server"}, {"website": "", "description": "<p>As we generally experience on earth, there is no space without sound and there is no sound without space. Building on the understanding of music and architecture as creators of spatial experience, this project presents a novel way of unfolding music\u2019s spatial qualities in the physical world.&nbsp;<i>Spaces That Perform Themselves&nbsp;</i>exposes an innovative response to the current relationship between sound and space: where we build static spaces to contain dynamic sounds. What if we change the static parameter of the spaces and start building dynamic spaces to contain dynamic sounds?&nbsp;</p><p>A multi-sensory kinetic architectural system is built in order to augment our sonic perception through a cross-modal spatial choreography that combines sound, movement, light, color, and vibration. By breaking down boundaries between music and architecture, possibilities of a new typology that morphs responsively with a musical piece can be explored. As a result,&nbsp;spatial&nbsp;and musical composition can exist as one synchronous entity. These spatial choreographies build up the scenario to study the possible relationships between a human body and a robotic architectural body, throughout a dance of perception and matter.&nbsp;</p><p>This project seeks to contribute a novel perspective on leveraging technology, art, science, and design to provide a setting to enrich and augment the way we relate to the built environment. The objective is to enhance our perception and challenge models of thinking by presenting a post-humanistic phenomenological encounter of the world.</p><p><i>Update</i>: A trigger system has been implemented to enable users to play the cube in real time and explore interactive ways to compose spaces with a palette of movements, sounds, light, color, and vibration.</p>", "people": ["nicolelh@media.mit.edu"], "title": "Spaces that Perform Themselves", "modified": "2019-01-30T16:50:51.633Z", "visibility": "PUBLIC", "start_on": "2017-04-02", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "spaces-perform-themselves"}, {"website": "https://snapshot.media.mit.edu/", "description": "<p>The SNAPSHOT study seeks to measure Sleep, Networks, Affect, Performance, Stress, and Health using Objective Techniques. It is an NIH-funded collaborative research project between the Affective Computing and Collective Learning groups, and Harvard Medical School's Brigham &amp; Women's hospital. Since fall 2013, we've run this study to collect one month of data every semester from 50 undergraduate students who are socially connected. We have collected data from about 250 participants, totaling over 7,500 days of data. We measure physiological, behavioral, environmental, and social data using mobile phones, wearable sensors, surveys, and lab studies. We investigate how daily behaviors and social connectivity influence sleep behaviors and health, and outcomes such as mood, stress, and academic performance. Using this multimodal data, we are developing models to predict onsets of sadness and stress. This study will provide insights into behavioral choices for wellbeing and performance.</p>", "people": ["jaquesn@media.mit.edu", "dlmocdm@media.mit.edu", "ehinosa@media.mit.edu", "orudovic@media.mit.edu", "asma_gh@media.mit.edu", "picard@media.mit.edu", "sataylor@media.mit.edu", "fpeng@media.mit.edu", "cvx@media.mit.edu", "akanes@media.mit.edu", "terumi@media.mit.edu"], "title": "SNAPSHOT Study", "modified": "2019-05-24T08:52:52.725Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "snapshot-study"}, {"website": "http://matthematic.com", "description": "<p>The design of next-generation bionic ankles and knees aims to improve bionic actuators on all metrics: range of motion, power density, bandwidth, mass, while adopting a futuristic aesthetic. We are pushing the limits of materials and magnetics, combined with new control topologies to enforce a new paradigm in both autonomous and volitional controlled powered prostheses. <br></p>", "people": ["thhsieh@media.mit.edu", "tonyshu@media.mit.edu", "jfduval@media.mit.edu", "mcarney@media.mit.edu", "syeon@media.mit.edu"], "title": "High power bionic joints for dynamic gait actions", "modified": "2019-04-26T19:02:31.522Z", "visibility": "PUBLIC", "start_on": "2017-03-15", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "moment-coupled-cantilever-beam-series-elastic-actuator"}, {"website": "", "description": "", "people": [], "title": "Online Support for Caregivers of Patients with Alzheimer\u2019s Disease", "modified": "2018-06-16T18:39:48.115Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "online-support-for-caregivers-for-alzheimer-disease"}, {"website": "", "description": "<p>Can a modified snap bracelet be used to land infrastructure on an asteroid?&nbsp;</p><p>It is notoriously difficult to stick a landing on a low gravity body, particularly if locomotion across the body is desired.&nbsp;We have been studying the use of arrays of bistable pinching elements for grappling onto the unpredictable contours of asteroids and other distant low gravity bodies. Each pinching element is mechanically actuated via an impact force, much like a snap bracelet. By coupling together arrays of such elements, we seek to demonstrate that the chain&nbsp; can conform with added precision to the topological structure of the body, as well as grapple more effectively.&nbsp;</p><p>This mechanism can ultimately be used to land large-scale structures like nets and tethers across the body, which then serve as infrastructure for crawling distributed sensors or sensory membranes, among other possibilities.&nbsp;</p><p>Additionally, we completed a study on a candidate low cost spectral imager payload for determining iron content in rock samples.&nbsp;</p><p>A&nbsp;<a href=\"https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=3738&amp;context=smallsat\">concept paper</a>&nbsp;on the broader work was published by the 31st Annual AIAA/USU Conference on Small Satellites.&nbsp; A prototype was deployed on a microgravity flight and results summarized at <a href=\"https://arc.aiaa.org/doi/pdf/10.2514/6.2019-0871\">AIAA Scitech 2019</a>.&nbsp;</p><p>There may also be compelling uses for the technology on Earth for adhering sensors to&nbsp; terrain that is erratic and difficult to access, like the roof of a cave, or structures at the bottom of the sea floor.&nbsp;</p><p>Two prototypes\u2014one equipped with sensors\u2014were tested on a microgravity flight by throwing them at a rocky target object. Data from the flight will be used to characterize the behavior of&nbsp; chains of one vs three bistable elements in order to inform future design decisions.&nbsp;</p>", "people": ["cherston@media.mit.edu", "paul_str@media.mit.edu"], "title": "Grappler:  Arrays of bistable elements for landing distributed sensor networks on low gravity bodies", "modified": "2019-06-05T16:39:54.329Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["responsive-environments", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "Grappler"}, {"website": "", "description": "<p>This project is a collaboration between the MIT Media Lab and the San Diego Zoo to design and build interactive sonic enrichment systems for animals in managed care. Our approach is based on the potential of animal-animal and human-animal relationship as an environmental enrichment for the welfare of zoo-housed animals specifically in terms of animal vocal communication.&nbsp; Enrichment is a way for caregivers to provide animals with the opportunity to express natural behaviors and reduce stereotypic behaviors.&nbsp;</p>", "people": ["rebklein@media.mit.edu", "janetb@media.mit.edu", "gabem@media.mit.edu"], "title": "Sonic enrichment at the zoo", "modified": "2019-04-17T20:03:16.443Z", "visibility": "PUBLIC", "start_on": "2018-04-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "sonic-enrichment-at-the-zoo"}, {"website": "", "description": "", "people": ["slotnick@media.mit.edu"], "title": "test project for people page", "modified": "2018-06-27T16:47:05.352Z", "visibility": "PUBLIC", "start_on": "2018-05-21", "location": "", "groups": ["living-mobile"], "published": false, "active": false, "end_on": null, "slug": "test-project-for-people-page"}, {"website": "", "description": "<p>Voice-user interfaces (VUIs), such as Amazon Echo and&nbsp;Google Home, are increasingly becoming present in domestic&nbsp;environments. Users attribute agency and personality traits to these AI agents. Due to the social attributes of these technologies,&nbsp; users try to understand the agents' characteristics based on social norms. These factors affect user experience quality and overall engagement, which, when considering first experiences, can impact continuous usage and engagement with VUI technology.</p><p>Our work examines users\u2019 first impressions and interactions&nbsp;with&nbsp; VUI agents, such as Google Home, Amazon Echo, and Jibo, with varying brands and modalities. Using personality and experience questionnaires, we seek to understand how VUI modalities, form, and personality affect engagement with VUIs.</p>", "people": ["cynthiab@media.mit.edu", "akostrow@media.mit.edu", "haewon@media.mit.edu"], "title": "Engagement with Voice-User Interface Agents", "modified": "2018-10-09T20:17:18.440Z", "visibility": "PUBLIC", "start_on": "2018-08-13", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "shaping-engagement"}, {"website": "", "description": "<h2>EngageME: Personalized machine learning and humanoid robots for measuring affect and engagement of children with autism</h2><p>EngageME is a project aimed at building a new technology to enable automatic monitoring of affect and engagement of children with ASC (Autism Spectrum Conditions) in communication-centered activities.</p><p>This <a href=\"https://www.media.mit.edu/publications/personalized-machine-learning-for-robot-perception-of-affect-and-engagement-in-autism-therapy/\">work</a> has been published in&nbsp;<a href=\"http://robotics.sciencemag.org/content/3/19/eaao6760\">Science Robotics</a>, June 2018.</p>", "people": ["orudovic@media.mit.edu", "picard@media.mit.edu"], "title": "Personalized Machine Learning for Autism Therapy", "modified": "2019-02-14T19:50:51.284Z", "visibility": "PUBLIC", "start_on": "2016-10-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "engageme"}, {"website": "http://ArielNoyman.com", "description": "<p>This project depicts the design, deployment and operation of a Tangible Regulation Platform, a physical-technological apparatus made for the distilment of regulations. The platform is set to exemplify the effects of regulations on a designated territory, allowing planners, designers, stakeholders and community members a common ground for discussion and decision making. An accessible and self-explanatory tool, this platform illustrates the relationship between urban form and regulations, offering a seamless and transparent process of regulation-based urban design. Lastly, projecting on the foreseen future of law and urbanism, this project proposes an alternative data and performance-based approach for the making of new regulations. Beyond excelling the processes of design under regulations, this platform and other new tools are offered to help facilitate a discussion on the way future regulations will be devised, improving both the design processes and their final outcome.</p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu", "noyman@media.mit.edu"], "title": "CityScope PlayGround: MIT East Campus", "modified": "2019-04-09T14:40:07.692Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "cityscope_playground"}, {"website": "", "description": "<p>Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform&nbsp;for urban analysis, efficient resource utilization, and spatial programming. The MIT CityScope is a tangible, augmented reality platform used to visualize complex urban relationships, simulate the impact of multiple urban interventions, and support decision-making in a dynamic, iterative, evidence-based process.&nbsp;</p>", "people": ["mdchurch@media.mit.edu", "alonsolp@media.mit.edu", "agrignar@media.mit.edu", "yasushis@media.mit.edu", "mcllin@media.mit.edu", "doorleyr@media.mit.edu", "kll@media.mit.edu", "noyman@media.mit.edu", "ptinn@media.mit.edu", "ryanz@media.mit.edu"], "title": "City Science Lab Shanghai", "modified": "2019-05-16T20:43:38.677Z", "visibility": "PUBLIC", "start_on": "2017-02-14", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "city-science-lab-shanghai"}, {"website": "", "description": "<p>Using a femtosecond laser and a camera with a time resolution of about one trillion frames per second, we recover objects hidden out of sight. We measure speed-of-light timing information of light scattered by the hidden objects via diffuse surfaces in the scene. The object data are mixed up and are difficult to decode using traditional cameras. We combine this \"time-resolved\" information with novel reconstruction algorithms to untangle image information and demonstrate the ability to look around corners.</p>", "people": ["raskar@media.mit.edu", " otkrist@media.mit.edu", "achoo@media.mit.edu", "ayush@media.mit.edu", "michaf@media.mit.edu", "naik@media.mit.edu"], "title": "Looking Around Corners", "modified": "2019-06-05T17:53:27.697Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-320", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "looking-around-corners"}, {"website": "", "description": "<p>One of the current foci within the HCI community is to understand and augment human capabilities using physiological and biological data. Microbes living on, inside, and around the human play significant roles in life, from improving health to causing infectious diseases. As the knowledge of human-microbe interaction continues to unfold, we propose a framework for microbial HCI based on a growing body of work aiming to observe, integrate, and modify microorganisms in interactive systems. Our motivation for the framework is to advancing the next generation of biological HCI and exploring novel human-microbe interfaces across contexts, scales, and species.</p>", "people": ["dkong@media.mit.edu", "patpat@media.mit.edu", "pattie@media.mit.edu", "avujic@media.mit.edu"], "title": "Microbial Augmentation Interfaces", "modified": "2019-04-29T16:39:12.651Z", "visibility": "LAB-INSIDERS", "start_on": "2018-11-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "human-microbe-interaction"}, {"website": "", "description": "<p>Did you know that submarines today still cannot wirelessly communicate with airplanes? For decades, communicating between underwater and the air has remained an unsolved problem. Underwater, submarines use acoustic signals (or SONAR) to communicate; in the air, airplanes use radio signals like cellular or WiFi. But neither of these signals can work across both water and air.</p><p>We present TARF (Translational Acoustic-RF&nbsp;communication), the&nbsp;first technology that enables communication between underwater and the air. A TARF transmitter sends standard sound (or SONAR signals).&nbsp; Sound travels as pressure waves; when these waves hit the surface, they cause it to vibrate. To pick up these vibrations, a TARF receiver in the air uses a very sensitive radar. The radar transmits a signal which reflects off the water surface and comes back. As the water surface vibrates, it causes small changes to the received radar signal, enabling a TARF receiver to sense the tiny vibrations caused by the underwater acoustic transmitter.</p><p>The video below explains how TARF works and some of its applications.</p>", "people": ["tonolini@media.mit.edu", "fadel@media.mit.edu", "junsuj@media.mit.edu"], "title": "Wireless communication from underwater to the air", "modified": "2019-04-17T18:09:35.809Z", "visibility": "PUBLIC", "start_on": "2018-08-21", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "translational-acoustic-rf-tarf-communication"}, {"website": "http://joi.ito.com/", "description": "<p>Beard dreamcatcher knausgaard godard, four loko wayfarers cloud bread. Pour-over echo park vinyl franzen listicle tumeric chicharrones. Heirloom microdosing intelligentsia deep v chartreuse, kinfolk photo booth. Everyday carry gentrify DIY, freegan snackwave taxidermy listicle trust fund brooklyn direct trade.</p><p>Mumblecore sriracha humblebrag +1, brooklyn pabst kickstarter. Quinoa la croix edison bulb chia, lyft selvage farm-to-table biodiesel shaman 3 wolf moon narwhal woke sriracha. Meh kinfolk mustache vaporware kogi air plant hexagon. Tbh bicycle rights vaporware deep v humblebrag..</p>", "people": ["chialynn@media.mit.edu", "slotnick@media.mit.edu", "jliberty@media.mit.edu"], "title": "RAMPage", "modified": "2018-05-22T18:59:03.439Z", "visibility": "LAB", "start_on": "2018-05-01", "location": "", "groups": ["communications"], "published": true, "active": false, "end_on": null, "slug": "rampage"}, {"website": "", "description": "<p>Gammalan is an interactive musical experience that uses music information retrieval techniques in conjunction with game design principles to engage audiences in creative behavior that combines the power of familiar songs with neural entrainment on multiple temporal scales.</p><p>A preliminary system analyzes and processes existing recordings, manipulating properties such as rhythm and harmony, while introducing synthesized frequencies in a musically informed manner. The recordings are then presented in an exploratory <dfn class=\"dictionary-of-numbers\">3D game-like </dfn>environment that encourages active and playful engagement with the recorded music.</p>", "people": ["davidsu@media.mit.edu"], "title": "Gammalan", "modified": "2018-08-18T22:07:44.931Z", "visibility": "PUBLIC", "start_on": "2018-02-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "gammalan"}, {"website": "", "description": "<p><b>Object Classification through Scattering Media&nbsp;with Deep Learning</b></p><p>A method for classifying objects hidden behind a scattering layer with a neural network. Training on synthetic data with variations in calibration parameters allows the network to learn a model that doesn't require calibration during lab experiments.<br></p><p>Traditional techniques to see through scattering media rely on a physical model that needs to be precisely calibrated. Computationally overcoming the scattering relies heavily on accurately calibrated physical models. Thus, such systems are extremely sensitive to a precise and lengthy calibration process. </p><p>In this work we overcome this bottleneck by utilizing neural networks and their ability to learn models that are invariant to data transformation. In our case, the transformations are variations in the imaging system calibration parameters. To that end, we create a synthetic dataset that contains variations in all calibration parameters (we use a Monte Carlo forward model to render the measurements). The system is then tested on actual lab experiments without specific calibration or tuning.</p>", "people": ["raskar@media.mit.edu", " otkrist@media.mit.edu", "guysatat@media.mit.edu", "barmak@media.mit.edu"], "title": "Calibration Invariant Imaging", "modified": "2018-10-20T00:40:19.319Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "calibration-invariant-i"}, {"website": "", "description": "<h1><strong>A Case for Breastfeeding Innovation</strong></h1><h2><strong>Breastfeeding saves lives.</strong></h2><p>If women globally were able to meet the WHO's public health goal to exclusively breastfeed for the first six months, we would prevent 823,000 infant deaths. For every 597 women who optimally breastfeed, one maternal or child death is prevented.</p><h2><strong>Breastfeeding promotes long-term wellness for mother and baby.</strong></h2><p>Breastfeeding protects against child infections and malocclusion, increases intelligence, and reduces the risk of obesity and diabetes for children\u00b9. Breastfeeding decreases mothers' risk of breast cancer and optimal breastfeeding would lead to 20,000 fewer cases every year\u00b9. It may also protect against ovarian cancer and diabetes\u00b9. Women who are supported to successfully establish breastfeeding in early months have a lower risk for postpartum depression.</p><p><strong>Breastfeeding saves healthcare costs.</strong></p><p>If women in the US were able to meet the WHO's public health goal to exclusively breastfeed for the first six months, we would save $17.2 billion dollars in annual costs treating preventable events, including infant and maternal deaths, SIDS, ear infections and necrotizing enterocolitis in babies, and heart attacks, diabetes and breast cancer in mothers.</p><h2><strong>Work environments and policies in the US are hostile to breastfeeding.</strong></h2><p>The US is one of only three nations worldwide without paid parental leave. The other countries in this club are Papua New Guinea and Lesotho\u2074. Women's return to work outside the home is the leading factor for early weaning\u2075. Most US work environments do not provide material or policy-based support for breastfeeding women, including parental leave, flexible schedules, on-site daycare, breaks and spaces for nursing and pumping.</p><h2>The Hackathon</h2><p><a href=\"https://www.makethebreastpumpnotsuck.com/team\">Our team</a>&nbsp;is thrilled to produce a weekend with the leading innovators in breastfeeding and postpartum health, along with many mamas, papas, babies, students, and newcomers. This time around we have a focus on equity and inclusive innovation in breastfeeding. We want to catalyze the development of tech, products, spaces, clothing, programs and services that have an eye on affordability and access as well as cultural diversity.</p><p>REFERENCES:</p><ol><li>Bartick, M. C., Schwarz, E. B., Green, B. D., Jegier, B. J., Reinhold, A. G., Colaizy, T. T., Stuebe, A. M. (2017). Suboptimal breastfeeding in the United States: Maternal and pediatric health outcomes and costs. Maternal &amp; Child Nutrition, 13(1), e12366.&nbsp;<a href=\"http://doi.org/10.1111/mcn.12366\">http://doi.org/10.1111/mcn.12366</a></li><li>Watkins, S., Meltzer-Brody, S., Zolnoun, D., &amp; Stuebe, A. (2011). Early Breastfeeding Experiences and Postpartum Depression. Obstetrics &amp; Gynecology, 118(2, Part 1), 214\u2013221.&nbsp;<a href=\"http://doi.org/10.1097/AOG.0b013e3182260a2d\">http://doi.org/10.1097/AOG.0b013e3182260a2d</a></li><li>Breastfeeding in the 21st century: epidemiology, mechanisms, and lifelong effect. Victora, Cesar G et al. The Lancet, Volume 387, Issue 10017, 475 - 490.</li><li><a href=\"https://en.wikipedia.org/wiki/Parental_leave\">https://en.wikipedia.org/wiki/Parental_leave</a></li><li>Why invest, and what it will take to improve breastfeeding practices? Rollins, Nigel C et al. The Lancet , Volume 387, Issue 10017, 491 - 504.</li></ol>", "people": ["dignazio@media.mit.edu", "ahope@media.mit.edu"], "title": "Make the Breast Pump Not Suck Hackathon 2018", "modified": "2019-04-19T19:02:17.039Z", "visibility": "PUBLIC", "start_on": "2017-04-02", "location": "", "groups": ["center-for-civic-media", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "make-the-breast-pump-not-suck-hackathon-2018"}, {"website": "http://enavigation.media.mit.edu", "description": "<p>Before automobiles were invented and widely adopted, animals like horses were the most common mode of transportation. While this change brought significant improvements in terms of reliability and efficiency, it also removed a core component: the emotional relationship that existed between the person and the animal.</p><p>While largely ignored, the emotional states of drivers are quite important, as they influence not only driving behavior but also the safety of all road users. For instance, driving can be quite an emotionally stressful experience and, while certain amounts of stress help the driver to remain alert and attentive, too much or too little can negatively impact driving performance and safety. Furthermore, stress in large doses has been linked to a large array of adverse health conditions such as depression and various forms of cardiovascular disease.</p><p>The&nbsp;Emotion Navigation&nbsp;special interest group is led by Dr. Javier Hernandez with the goal of stimulating research efforts at the intersection of&nbsp;Automotive and&nbsp;Affective Computing.</p>", "people": ["fergusoc@media.mit.edu", "picard@media.mit.edu", "javierhr@media.mit.edu"], "title": "Emotion Navigation", "modified": "2019-04-19T19:34:00.344Z", "visibility": "PUBLIC", "start_on": "2018-04-02", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "emotional-navigation-system"}, {"website": "", "description": "<p>Storytelling is a fundamental way in which human beings understand the world. Imagine watching a movie telling the story of your life, how would you respond to it and how would it change your perception of your own memories? Personalized animated movies are generated from Unity, customized to each user's mood and behavior date collected through self-reports. Our study shows that personalized animations can elicit strong emotional responses from participants and lengthier writing of self-reflection compared to a non-personalized control. Moving forward, we're looking at using personalized animation to encourage cognitive reappraisal and positive thinking.<br></p>", "people": ["picard@media.mit.edu", "fpeng@media.mit.edu"], "title": "Personalized Animated Movies", "modified": "2019-06-05T18:33:04.324Z", "visibility": "PUBLIC", "start_on": "2016-10-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "personlized-animation"}, {"website": "", "description": "<p>The University of Guadalajara, referred to as UdeG, is a university network composed of 15 campuses within the state of Jalisco and one online system. The University offers undergraduate and graduate studies to around 130,000 students. UdeG strives to understand urban performance metrics using evidence-based decision making tools, facilitated through a collaboration with the MIT Media Lab City Science group.</p>", "people": ["mdchurch@media.mit.edu", "alonsolp@media.mit.edu", "agrignar@media.mit.edu", "kll@media.mit.edu", "maitanei@media.mit.edu"], "title": "City Science Collaboration Guadalajara", "modified": "2019-05-10T19:44:20.937Z", "visibility": "PUBLIC", "start_on": "2018-11-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "city-science-guadalajara"}, {"website": "", "description": "<p>The CRISPR-Cas9 system has proven to be a versatile tool for genome editing, with&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">numerous implications in medicine, agriculture, bioenergy, food security, and beyond. The&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">range of targetable DNA sequences is limited, however, by the need for a short sequence of DNA beside the target site, called the PAM. In total, there are only a handful of CRISPR enzymes with a short enough PAM sequence to be able to target a large portion of the total DNA in a genome. In this study, we identify a natural Cas9 enzyme from the bacterial genome of </span><i style=\"font-size: 18px; font-weight: 400;\">Streptococcus canis</i><span style=\"font-size: 18px; font-weight: 400;\"> that has a PAM sequence with only a single G as its PAM sequence (5\u2019-NNG-3\u2019), allowing flexible targeting of up to 50% of all DNA sequences in living organisms. This new molecular tool potentially grants unprecedented access to correct disease-related mutations, enhance agricultural methods, and expand research efforts.&nbsp;</span></p>", "people": ["jacobson@media.mit.edu", "pranam@media.mit.edu", "njakimo@media.mit.edu"], "title": "Opening wider genomic access with a flexible CRISPR enzyme", "modified": "2018-10-25T16:09:08.650Z", "visibility": "PUBLIC", "start_on": "2017-01-04", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "opening-wider-genomic-access-with-a-flexible-crispr-enzyme"}, {"website": "", "description": "<p>We present PsychicVR, a proof-of-concept system that integrates a brain-computer interface device and virtual reality headset to improve mindfulness while enjoying a playful immersive experience.&nbsp;The fantasy that any of us could have superhero powers has always inspired us, and by using virtual reality and real-time brain activity sensing we are moving one step closer to making this dream real. We non-invasively monitor and record electrical activity of the brain and incorporate this data into the VR experience using an Oculus Rift and the MUSE headband. By sensing brain waves using a series of EEG sensors, the level of activity is fed back to the user via 3D content in the virtual environment. When users are focused, they are able to make changes in the 3D environment and control their powers. Our system increases mindfulness and helps achieve higher levels of concentration while entertaining the user.</p>", "people": ["amores@media.mit.edu", "novysan@media.mit.edu", "pattie@media.mit.edu", "xavib@media.mit.edu"], "title": "PsychicVR", "modified": "2019-04-18T17:06:09.570Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "psychicvr"}, {"website": "https://www.media.mit.edu/groups/health-0-0/overview/", "description": "<p><b>Technical summary</b></p><p>Unstructured learning problems without well-defined rewards are unsuitable for current reinforcement learning (RL) approaches. Action-derived rewards can allow RL agents to fully explore state and action trade-offs in scenarios that require specific outcomes yet are unstructured by external reward. Clinical trial dosing choice is an example of such a problem. We report the successful formulation of clinical trial dosing choice as an RL problem using action-based rewards and learning of dosing regimens to reduce mean tumor diameters (MTD) in patients undergoing simulated temozolomide (TMZ) and procarbazine, 1-(2-chloroethyl)-3-cyclohexyl-l-nitrosourea, and vincristine (PCV) chemo- and radiotherapy clinical trials. The use of action-derived rewards as partial proxies for outcomes is described for the first time. Novel dosing regimens learned by an RL agent in the presence of action-derived rewards achieve significant reduction in MTD for cohorts and individual patients in simulated TMZ and PCV clinical trials while reducing treatment cycle administrations and dosage concentrations compared to human-expert dosing regimens. Our approach can be easily adapted for other learning tasks where outcome-based learning is not practical.</p><p><b>Glioblastoma (brain tumors) background:&nbsp;</b>Glioblastoma is an aggressive type of cancer that can occur in the brain\nor spinal cord. A hard-hitting treatment typically involves combining surgery\nwith radiation therapy and chemotherapy, which is necessary to combat the\naggressive nature of a glioblastoma. Chemo-and Radiotherapy Treatments (CRT) may\nslow the progression of cancer and reduce signs and symptoms, but are unable to\ncure the disease. Survival rates are low (about 14-18 months) and only about\n10% of patients live five years or longer. CRTs are often given as a\ncombination of drugs. Procarbazine, 1-(2-chloroethyl)-3-cyclohexyl-l-nitrosourea,\nand Vincristine (PCV) is triple drug chemotherapy for glioblastomas and can be\ntoxic for the patients. Temozolomide (TMZ), is less toxic, has shown greater\nefficacy when compared to radiotherapy alone, in the treatment of glioblastoma.\nThere is significant and urgent need for novel CRT dosing regimens in human subjects\nto optimize for maximum therapeutic benefit for patients while reducing\ntoxicity.</p><p><b>Reinforcement learning background:&nbsp;</b>Reinforcement learning (RL) is an area of machine learning and AI inspired by behaviorist psychology. RL agents can self-learn how to solve complex tasks in a relatively unstructured environment so as to maximize some notion of cumulative&nbsp;rewards and reduce penalties set by human programmers. Reward functions in RL domains are typically derived from a measure that is external to the chosen representation of the states (data) and actions (steps) used for the self-training algorithm. Using RL to solve tasks without readily accessible external scalar outcomes is a relatively unexplored field, as many currently studied domains have well-defined outcomes and associated rewards as part of their definitions.<br></p><p><b>Clinical trials background:&nbsp;</b>Clinical trials to evaluate new drugs, therapies, and vaccines are among the most complex experiments performed in medicine. Nearly half of phase 2 and phase 3 trials fail. For oncology trials, the failure rate rises to two-thirds. A common theme is the difficulty of predicting clinical results in a wide patient base given limited knowledge of key parameters which need to be considered to test candidate molecules, eliminate adverse events, and identify the drugs half maximal inhibitory concentration. Optimal CRT dosing for patients enrolled in glioblastoma clinical trials thus provides one example of an open-ended problem characterized by complex interactions between different drug properties, dosage and timing of administrations (actions), and effects on tumors (state) and where survival (outcomes) may not be available.</p>", "people": ["pratiks@media.mit.edu", "gyauney@media.mit.edu"], "title": "Self-Learning AI Model Learns from Patient Data to Design Novel Clinical Trials", "modified": "2018-10-22T14:52:11.990Z", "visibility": "PUBLIC", "start_on": "2018-08-10", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "self-learning-ai-model-learns-from-patient-data-to-design-novel-clinical-trials"}, {"website": "", "description": "<p><i>Scratch BlockArt </i>is an experimental visualization tool designed to let children discover their own computational patterns on Scratch. Existing methods often utilize data about the types of programming blocks used in children\u2019s projects to generate a quantitative assessment of a project\u2019s computational complexity based on limited criteria. BlockArt presents an alternative approach for revealing this data to young creators themselves.&nbsp;Rather than&nbsp;datafying children\u2019s creations, BlockArt is designed to transform the data about their code into creative objects that can spark children's curiosity and enable them to reflect on their own styles and choices.&nbsp;</p><p>For a given username, the tool dynamically generates colorful visualizations representing the number and diversity of programming blocks used in each of their shared projects over time. Children can also click to see the project behind the visualization. The idea is not to evaluate whether they use more or less 'complex' blocks, but to reveal how the types of blocks children use are based on their motivations and interests behind creating a specific project. It also shows how looking at the diversity of code in all their projects is a better representation of their learning trajectory than providing a quantitative assessment on individual projects without additional context.&nbsp;</p>", "people": ["shrutid@media.mit.edu"], "title": "Scratch BlockArt", "modified": "2018-11-13T20:03:22.125Z", "visibility": "PUBLIC", "start_on": "2018-01-26", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratch-blockart"}, {"website": "", "description": "<p>When we form memories, not everything that we perceive is noticed; not  everything that we notice is remembered. Humans are excellent at  filtering and retaining only the most important parts of their  experience\u2014what if our audio compression had the same ability?&nbsp;</p><p>Our goal is to understand what makes sound memorable. With this  work, we hope to gain insight into the cognitive processes that drive  auditory perception and predict the memorability of sounds in the world  around us more accurately than ever before.  Ultimately, these models  will give us the ability to generate and manipulate the sounds that  surround us to be more or less memorable.&nbsp;</p><p>We envision this research introducing new paradigms into the  space of audio compression, attention-driven user interactions, and  auditory AR, amongst others.</p>", "people": ["dramsay@media.mit.edu", "ishwarya@media.mit.edu"], "title": "Cognitive Audio", "modified": "2018-05-06T23:28:44.204Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "cognitive-audio"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: 400;\"></span><a href=\"https://www.media.mit.edu/projects/city-science-andorra/overview/\"><span style=\"font-size: 18px; font-weight: 400;\">View the main City Science Andorra project profile.</span></a><br></p><p><span style=\"font-size: 18px; font-weight: 400;\">With more than eight million visitors a year, tourism represents almost 30% of the economy of Andorra. By gathering and analyzing data from social media, call detail records, and wifi, we can understand the country's dynamics of tourism and commerce as well as design interventions that can improve the experience for tourists, encouraging them to visit Andorra more frequently, stay longer, and increase spending.&nbsp;</span><br></p><h2><b>Current Projects</b></h2><ul><li>Event Analysis<br></li><li>Social Network<br></li><li>Location Recommendation system<br></li></ul><p> </p><h2><b>EVENT ANALYSIS</b></h2><p>Based on the analysis of call detail records and social media, the goal of this project is to understand the tourist behaviors in Andorra.&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">After mining those anonymized data, we have been able to learn different patterns and behaviors of the tourism in Andorra thanks to an agent-based model developed in order to represent the flow of people. This simulation is also coupled with an interactive table called CityMatrix.</span></p>", "people": ["jiw@media.mit.edu", "alonsolp@media.mit.edu", "agrignar@media.mit.edu", "doorleyr@media.mit.edu", "devisj@media.mit.edu", "kll@media.mit.edu", "noyman@media.mit.edu", "nlutz@media.mit.edu"], "title": "Andorra | Tourism", "modified": "2019-02-25T15:33:28.936Z", "visibility": "PUBLIC", "start_on": "2015-08-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "andorra-tourism"}, {"website": "", "description": "<p>VisionBlocks is an on-demand, in-browser, customizable, computer-vision application-building platform for the masses. Even without any prior programming experience, users can create and share computer vision applications. End-users drag and drop computer vision processing blocks to create their apps. The input feed could be either from a user's webcam or a video from the Internet. VisionBlocks is a community effort where researchers obtain fast feedback, developers monetize their vision applications, and consumers can use state-of-the-art computer vision techniques. We envision a Vision-as-a-Service (VaaS) over-the-web model, with easy-to-use interfaces for application creation for everyone.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "VisionBlocks", "modified": "2018-10-20T01:02:47.699Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "visionblocks"}, {"website": "", "description": "<p>To better understand and improve the quality of our lives, there has been a need for measuring non-economic capital such as social capital and natural capital in addition to economic capital. Quantifying non-economic capital, however, is not easy and has not been widespread. In this project, we propose a system where individuals can start measuring their social capital, turning them into a real-world asset that enables the improvement their economic wellbeing, while preserving individual privacy and security.</p>", "people": ["nishikat@media.mit.edu", "sandy@media.mit.edu"], "title": "Social Capital Accounting", "modified": "2018-11-15T19:26:04.996Z", "visibility": "PUBLIC", "start_on": "2018-06-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "social-capital-accounting"}, {"website": "", "description": "<p>Interneternity is a dynamic musical composition built into the World Wide Web.&nbsp;The Internet changed the way that we listen to music, but music itself remains essentially unchanged. Can we create music that uses the Internet for more than a distribution medium? Can we make music that could not exist without the Internet?</p><p><span style=\"font-size: 18px; font-weight: 400;\">Interneternity is an experimental integration of music composition and the World Wide Web.&nbsp;Instead of releasing this music as a static file, it was released in the form of a website. When the site launched, users could play the composition continuously from beginning to end. But each new connection to the web server changes the musical structure in some small way\u2026and eventually the original composition is unrecognizable.</span><br></p><p>Certain interactions with the site may extend its lifetime, while certain interactions nudge the music towards inevitable disintegration.&nbsp;Will you get to hear the piece before it is broken forever?<br></p>", "people": ["holbrow@media.mit.edu"], "title": "Interneternity", "modified": "2018-05-06T23:29:39.711Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "interneternity"}, {"website": "https://explore.pubpub.org/", "description": "<p>In collaboration with&nbsp;<a href=\"https://www.media.mit.edu/groups/viral-communications/overview/\">Viral Communications</a>&nbsp;and the&nbsp;<a href=\"https://www.media.mit.edu/groups/space-exploration/overview/\">Space Exploration initiative</a>, Open Ocean is using the&nbsp;<a href=\"https://www.media.mit.edu/projects/pubpub/overview/\">PubPub</a>&nbsp;platform&nbsp; to launch the Journal of Open Exploration. We want to make the process and results of exploration collaborative, open, playful, and\u2013most importantly\u2013shared with a wider audience than traditional academic journals.</p>", "people": ["katybell@media.mit.edu", "aekblaw@media.mit.edu", "trich@media.mit.edu"], "title": "Journal of Open Exploration", "modified": "2018-09-25T20:43:42.274Z", "visibility": "LAB", "start_on": "2018-02-26", "location": "", "groups": ["viral-communications", "space-exploration", "open-ocean"], "published": true, "active": false, "end_on": null, "slug": "journal-of-open-exploration"}, {"website": "", "description": "<p>Self-Advertising&nbsp;reclaims and repurposes attention from&nbsp;advertising&nbsp;media online. As we use the Internet, we often feel advertising following us around, distracting our focus, and leading us down misaligned paths, or we block it out entirely. Here we explore repurposing the advertising space online to be aligned with our goals and desires.</p>", "people": ["anderton@media.mit.edu", "lip@media.mit.edu"], "title": "Self-Advertising", "modified": "2019-06-04T19:35:02.989Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "self-advertising"}, {"website": "", "description": "<p>A basketball net incorporates segments of conductive fiber whose resistance changes with degree of stretch. By measuring this resistance over time, hardware associated with this net can calculate force and speed of a basketball traveling through the net. Applications include training, toys that indicate the force and speed on a display, dunk competitions, and augmented-reality effects on television broadcasts. This net is far less expensive and more robust than other approaches to measuring data about the ball (e.g., photosensors or ultrasonic sensors) and the only physical change required for the hoop or backboard is electrical connections to the net. Another application of the material is a flat net that can measure velocity of a ball hit or pitched into it (as in baseball or tennis); it can measure position as well (e.g., for determining whether a practice baseball pitch would have been a strike).</p>", "people": ["novysan@media.mit.edu", "vmb@media.mit.edu"], "title": "Slam Force Net", "modified": "2018-05-04T11:46:51.323Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "slam-force-net"}, {"website": "", "description": "<p>We want to create immersive, personalized, and scalable digital experiences. In order to do that, we need to fundamentally rethink the way intelligent agents are built. Currently, agent technology is designed to quickly get a user with a single problem to their solution; success is how quickly you accomplish this task. We are rethinking agents to be about story quality, experience, and interaction design. We plan on separating the bits of the agent: the dialogue generation, world modeling, conversational planning, and emotion. In order to do that, we must first design NLP that work within the confines of a world that is often very different than our own\u2014with limited data to do it!</p>", "people": [], "title": "Conversational Characters:  AI agents for entertainment", "modified": "2019-04-17T14:06:16.963Z", "visibility": "PUBLIC", "start_on": "2018-12-17", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "conversational-characters"}, {"website": "", "description": "<h2><b>Humans can accurately sense the position, speed, and torque of their limbs, even with their eyes shut. This sense, known as proprioception, allows humans to precisely control their body movements. </b></h2><p>Today\u2019s conventional prosthetic limbs do not provide feedback to the nervous system. Because of this, people with amputated limbs cannot feel the position, speed, and torque of their prosthetic joints without looking at them, making it difficult to control their movement. In order to create a more complete prosthetic control experience, researchers at the Center for Extreme Bionics at the MIT Media Lab invented the&nbsp;<b>agonist-antagonist myoneural interface (AMI)</b>. The AMI is a method to restore proprioception to persons with amputation.</p>", "people": ["hherr@media.mit.edu", "clites@media.mit.edu", "mcarty@media.mit.edu", "shriyas@media.mit.edu", "lfreed@media.mit.edu"], "title": "Agonist-antagonist Myoneural Interface (AMI)", "modified": "2018-08-17T16:20:19.891Z", "visibility": "PUBLIC", "start_on": "2014-06-01", "location": "", "groups": ["center-for-extreme-bionics", "biomechatronics"], "published": true, "active": false, "end_on": null, "slug": "agonist-antagonist-myoneural-interface-ami"}, {"website": "", "description": "<p>We have added inexpensive, low-power, wireless sensors to product packages to detect user interactions with products. Thus, a bottle can register when and how often its contents are dispensed (and generate side effects, like causing a music player to play music when the bottle is picked up, or generating an automatic refill order when near-emptiness is detected). A box can understand usage patterns of its contents. Consumers can vote for their favorites among several alternatives simply by handling them more often. </p>", "people": ["novysan@media.mit.edu", "vmb@media.mit.edu"], "title": "Bottles and Boxes: Packaging with sensors", "modified": "2019-04-17T18:11:22.180Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "bottlesboxes-packaging-with-sensors"}, {"website": "", "description": "<h1>Utreexo: a dynamic accumulator for bitcoin state&nbsp;</h1><p>A description of research by Thaddeus Dryja</p><p>One of the earliest-seen and most persistent problems with Bitcoin has been scalability. Bitcoin takes the idea of \"be your own bank\" quite literally, with every computer on the bitcoin network storing every account of every user who owns money in the system. In Bitcoin, this is stored as a collection of \"Unspent transaction outputs,\" or \"utxos\", which are somewhat unintuitive, but provide privacy and efficiency benefits over the alternative \"account\" based model used in traditional finance.</p><p>It's important to distinguish between the transaction history and the current state of the system. The transaction history in Bitcoin is currently 200GB, and contains every transaction since Bitcoin was launched in early 2009. The size of this history can of course only increase with time. The current system state, however, is much smaller, at under 4GB, and deals with only who owns what right now. This state size has generally increased over time, but has in fact decreased a bit this year.</p><p>The history, despite its much larger size, is not in fact the scalability concern, as it is not used in any time-critical fashion; one can discard the history after processing with no loss of security. The increasing state size, however, is a concern\u2014one which utreexo solves.</p><p>Utreexo is a novel hash based dynamic accumulator, which allows the millions of unspent outputs to be represented in under a kilobyte\u2014small enough to be written on a sheet of paper. There is no trusted setup or loss of security; instead, the burden of keeping track of funds is shifted to the owner of those funds.  </p><p>Current transactions specify inputs and outputs, and verifying an input requires you to know the whole state of the system. With Utreexo, the holder of funds maintains a proof that the funds exist, and provides that proof at spending time to the other nodes. These proofs are compact (under 1KB) but do represent the main downside in the utreexo model; they present an additional data transmission overhead, which allows much smaller state.</p><p>Utreexo pushes the costs of maintaining the network to the right place: an exchange creating millions of transactions may need to maintain millions of proofs, while a personal account with only a few unspent outputs will only need to maintain a few kilobytes of proof data. Utreexo also provides a long-term scalability solution, as the accumulator size grows very slowly with increasing size of the underlying set (the accumulator size is logarithmic with the set size).</p>", "people": ["tdryja@media.mit.edu"], "title": "Utreexo", "modified": "2019-04-09T18:22:46.627Z", "visibility": "PUBLIC", "start_on": "2018-11-28", "location": "", "groups": ["digital-currency-initiative-dci"], "published": true, "active": false, "end_on": null, "slug": "utreexo"}, {"website": "http://learn.media.mit.edu/lcl", "description": "<p>Learning Creative Learning is an online course and community of educators, designers, technologists, and tinkerers exploring creative learning. &nbsp;Participants create hands-on projects based on their interests, explore new technologies, and share ideas with peers from more than 100 countries.&nbsp; The course is organized by the Lifelong Kindergarten group at the MIT Media Lab.</p><p>The course is free, and open to everyone.&nbsp; Participants can use the materials at their own pace at any time, or participate with a cohort each fall.&nbsp;</p>", "people": ["tarmelop@media.mit.edu", "shrutid@media.mit.edu", "srishti@media.mit.edu", "jaleesat@media.mit.edu", "morant@media.mit.edu", "yusufa@media.mit.edu", "ps1@media.mit.edu", "eschill@media.mit.edu", "hisean@media.mit.edu", "gabaree@media.mit.edu", "mres@media.mit.edu", "muthui@media.mit.edu", "ascii@media.mit.edu", "kamcco@media.mit.edu", "nrusk@media.mit.edu", "yumikom@media.mit.edu"], "title": "Learning Creative Learning", "modified": "2019-04-01T15:49:20.436Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["ml-learning", "lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "learning-creative-learning"}, {"website": "", "description": "<p>We report a novel method that processes biomarker images collected at the point of care and uses machine learning algorithms to provide a first level of screening against oral diseases. A machine learning classifier is trained to learn pixel-by-pixel mappings from RGB oral images and output areas with disease. This method can be adapted to&nbsp; process biomarker images from other organs as well.</p><p><strong>Why is this work important?</strong></p><p>Visual inspection and probing techniques have been traditionally used for diagnosis of oral diseases in patients. These traditional methods are subjective and not scalable. We describe the use of RGB color images acquired by low-cost camera devices coupled with machine learning to detect areas with poor oral health.</p><p><strong>What has been done before?</strong></p><p>Currently the gold standard for oral diagnosis is visual inspections by a dentist followed by X-rays. These methods are expensive and invasive.&nbsp;</p><p><strong>What are our contributions?</strong></p><p>We implement a novel technique to combine medical expert knowledge with biomarker signatures.&nbsp; We&nbsp; use RGB color images taken directly at the point-of-care, using low-cost hand-held devices, to provide a first level machine-learning powered screening for patients.</p><p><strong>What are the next steps?</strong></p><p>We are expanding the repertoire of biomarkers that can be detected in RGB color images acquired at the point-of-care and pairing them with automated machine learning exams.</p><p><strong>Related projects</strong></p><ol><li><a href=\"https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/\">Technology-Enabled Mobile Phone Screenings Augment Routine Primary Care</a></li></ol>", "people": [" otkrist@media.mit.edu", "pratiks@media.mit.edu", "gyauney@media.mit.edu", "arana@media.mit.edu"], "title": "Machine Learning and Automated Segmentation of Oral Diseases Using Biomarker Images", "modified": "2018-08-09T17:24:03.930Z", "visibility": "PUBLIC", "start_on": "2017-04-24", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "machine-learning-and-automated-segmentation-of-oral-diseases-using-biomarker-images"}, {"website": "", "description": "<p>In this SIG, lasting 18 months, MIT proposes to design and develop algorithms for novel low-cost &amp; non-invasive mental condition estimation, in order to proceed towards the larger goal of unobtrusive, but medically acceptable preventive healthcare. MIT will continuously monitor TCS support services employees for 30 days using wearable sensors to examine the physiological, psychological and behavioral correlates of wellbeing. In addition, we will present surveys on their mobile phone in the morning and evening about employees\u2019 mood, sleep, interaction with others and wellbeing. The mobile phone will also be used to monitor the activity and the location of the employees when they are not at work. In particular, we propose to explore the following research questions:</p><ul>\n <li>Are sleep irregularity\n     and short sleep associated with stress, performance and other factors?</li>\n <li>Is stress associated with\n     factors such as hierarchical job ranking or team dynamics?</li>\n <li>Are there apparent\n     differences in stress across hierarchical levels?</li>\n <li>What are the main\n     predictive parameters of stress (e.g. sleep, ranking, sensor data)?</li>\n <li>How is wellbeing related\n     to stress at work?</li>\n <li>Are work habits (work\n     outside working hours) related to stress/wellbeing?</li>\n <li>Are there any\n     communication patterns (e.g., e-mail) and work settings (e.g., open/close\n     office, number of meetings) correlated with stress/wellbeing?</li>\n <li>How are daily mobility\n     patterns related to stress/wellbeing?</li>\n <li>Are working hour\n     schedules related to employees\u2019 performance, stress and mental health?</li></ul>", "people": [], "title": "Psychophysiological correlates of wellbeing in office workers", "modified": "2019-02-14T19:55:12.974Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "psychophysiological-correlates-of-wellbeing-in-office-workers"}, {"website": "", "description": "<p>Industrial diversification is a path-dependent process that leverages knowledge, skills, and new technologies. Because such resources are difficult to move, geography plays a crucial role in determining the future economic activities of countries, regions, and cities. Yet most of the evidence on the geographic diffusion of economic activities is restricted to the last 60 years and relies on correlations.&nbsp;</p><p>This paper analyzes the geographic diffusion of economic activities for Swedish towns between 1850 and 1950, using the evolution of the railroad network as a way to address endogeneity. We use the straight line between Sweden's 10 largest towns as an instrument for train adoption. Our instrumental variable estimates show that regions are more likely to diversify into sectors that are present in their train neighbors, suggesting that the impact of connectivity goes beyond access to markets: connectivity also promotes diffusion of economic activities, even at early stages of development.&nbsp;<br></p>", "people": ["hidalgo@media.mit.edu", "crisjf@media.mit.edu"], "title": "Railroad Access and Diffusion of Industries: Evidence from Sweden during the Second Industrial Revolution", "modified": "2018-05-07T00:25:12.390Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "trains-of-thought-railroad-access-and-knowledge-diffusion-in-sweden"}, {"website": "https://globalcxi.org/", "description": "<p>The&nbsp;<a href=\"http://standards.ieee.org/\">IEEE Standards Association (IEEE-SA)</a>&nbsp;and the&nbsp;<a href=\"https://www.media.mit.edu/\">MIT Media Lab</a>&nbsp;are joining forces to&nbsp;launch a global Council on Extended Intelligence (CXI) composed of individuals who agree&nbsp;on the following:</p><p>One of the most powerful narratives of modern times is the story of scientific and technological progress. While our future will undoubtedly be shaped by the use of existing and emerging technologies \u2013 in particular, of autonomous and intelligent systems (A/IS) \u2013 there is no guarantee that progress defined by \u201cthe next\u201d is beneficial. Growth for humanity\u2019s future should not be defined by reductionist ideas of speed or size alone but as the holistic evolution of our species in positive alignment with the environmental and other systems comprising the modern algorithmic world.</p><p>We believe all systems must be responsibly created to best utilize science and technology for tangible social and ethical progress. Individuals, businesses and communities involved in the development and deployment of autonomous and intelligent technologies should mitigate predictable risks at the inception and design phase and not as an afterthought. This will help ensure these systems are created in such a way that their outcomes are beneficial to society, culture and the environment.</p><p>Autonomous and intelligent technologies also need to be created via participatory design, where systems thinking can help us avoid repeating past failures stemming from attempts to control and govern the complex-adaptive systems we are part of. Responsible living with or in the systems we are part of requires an awareness of the constrictive paradigms we operate in today. Our future practices will be shaped by our individual and collective imaginations and by the stories we tell about who we are and what we desire, for ourselves and the societies in which we live.</p><p>These stories must move beyond the \u201cus versus them\u201d media mentality pitting humans against machines. Autonomous and intelligent technologies have the potential to enhance our personal and social skills; they are much more fully integrated and less discrete than the term \u201cartificial intelligence\u201d implies. And while this process may enlarge our cognitive intelligence or make certain individuals or groups more powerful, it does not necessarily make our systems more stable or socially beneficial.</p>", "people": ["cbarabas@media.mit.edu", "nsaltiel@media.mit.edu", "lessig@media.mit.edu", "picard@media.mit.edu", "andreuhl@media.mit.edu", "kadec@media.mit.edu", "tenzin@media.mit.edu", "drwood@media.mit.edu", "minow@media.mit.edu", "joyab@media.mit.edu", "hidalgo@media.mit.edu", "neri@media.mit.edu", "irahwan@media.mit.edu", "kdinakar@media.mit.edu", "joi@media.mit.edu"], "title": "Council on Extended Intelligence", "modified": "2018-07-29T23:53:45.243Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["directors-office"], "published": true, "active": false, "end_on": null, "slug": "council-on-extended-intelligence"}, {"website": "", "description": "<p>Scratch Day (day.scratch.mit.edu) is a network of face-to-face local gatherings, on the same day in all parts of the world, where people can meet, share, and learn more about Scratch, a programming environment that enables people to create their own interactive stories, games, animations, and simulations. We believe that these types of face-to-face interactions remain essential for ensuring the accessibility and sustainability of initiatives such as Scratch. In-person interactions enable richer forms of communication among individuals, more rapid iteration of ideas, and a deeper sense of belonging and participation in a community. The first Scratch Day took place in 2009. In 2015, there were 350 events in 60 countries. </p>", "people": ["kaschm@media.mit.edu", "abisola1@media.mit.edu", "sleggss@media.mit.edu", "mres@media.mit.edu", "champika@media.mit.edu"], "title": "Scratch Day", "modified": "2018-10-20T01:12:28.959Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratch-day"}, {"website": "", "description": "<p>Launched in 2007, the Scratch Online Community enables children, primarily between the ages of 8 and 16, to share interactive media such as games, stories, and animations created with the Scratch programming environment. As of September 2016, Scratch members had shared more than 16.3 million projects, and had exchanged over 87.4 million project comments.</p>", "people": ["christan@media.mit.edu", "otts@media.mit.edu", "eschill@media.mit.edu", "mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "Scratch Online Community", "modified": "2018-10-20T01:16:05.838Z", "visibility": "PUBLIC", "start_on": "2013-01-30", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratch-online-community"}, {"website": "http://xxxxxxxxxinliu.com", "description": "<p>Living Distance is a mission and a fantasy realized, in which a wisdom tooth is sent to outer space and back down to Earth again. Carried by a crystalline robotic device called EBIFA, the tooth tells the inconsequential but unique story of a person in this universe.&nbsp;EBIFA's form and function follow an unusually personal approach to our technological space futures, one centered on visceral, active, empathic, and poetic engagement.<br></p>", "people": ["xxxxxxin@media.mit.edu", "gershon@media.mit.edu"], "title": "Living Distance", "modified": "2019-06-03T13:32:35.379Z", "visibility": "PUBLIC", "start_on": "2018-09-03", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "living-distance"}, {"website": "", "description": "<p><br>Diastrophisms is a sound installation with a modular system that sends images through rhythmic patterns. It is built on a set of debris from the Alto R\u00edo building that was destroyed by the 27F earthquake in 2010 in Chile. With&nbsp;&nbsp;Diastrophisms we were looking for a poetical, critical and political crossing between technology and matter, in order to raise questions about the relationship between human beings and nature, and to consider the construction of memory in a community by questioning the notion of monument, as well as to imagine new forms of communication in times of crisis.</p><p>Work by:&nbsp;Nicole L\u2019Huillier, Thomas Sanchez Lengeling, and Yasushi Sakai</p><p>Exhibited at Siggraph Art Gallery 2018,&nbsp;curated by Andres Burbano. A&nbsp;paper about this work was published&nbsp;in Leonardo Journal for the special edition of Siggraph 2018 Art Papers and Art Gallery Exhibition. The paper was written by Nicole L\u2019Huillier and Valentina Montero.</p><p>Diastrophisms&nbsp;was also exhibited as<a href=\"http://www.bienaldeartesmediales.cl/13/obra/talking-rock/\"> \"Diastrofismos\"</a> at the <a href=\"http://www.bienaldeartesmediales.cl/13/\">Media Arts Bienal,</a> Santiago de Chile, 2017, curated by Valentina Montero.</p>", "people": ["thomassl@media.mit.edu", "yasushis@media.mit.edu", "nicolelh@media.mit.edu"], "title": "Diastrophisms", "modified": "2019-02-14T19:56:31.323Z", "visibility": "PUBLIC", "start_on": "2017-10-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "diastrophisms"}, {"website": "", "description": "<p>Friends don\u2019t let friends vote alone. 2018 featured a suite of grassroots mobilization ideas for voting; 2019 generalizes that theme for civic organizations at any scale or number.</p><p>We construct bottom-up ways to get people civically engaged. The idea is independent, culturally and community-based ways to reinforce participation. We replace centrally radiated messaging and mobilization with local, iterative, and potentially orthogonal cues that diffuse through a population. The core project is CivicLink, a, private, local org-in-a-box, owned by a moderator who mediates member participation and networking with other similar orgs.&nbsp; Related work tests and validates the design.</p>", "people": ["lip@media.mit.edu"], "title": "Theme | Viral Civic Action", "modified": "2019-04-19T16:05:13.425Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "viral-political-action"}, {"website": "", "description": "<p>SPRING is a custom-built hardware and software platform for children with neuro-differences. The system automates data acquisition, optimizes learning progressions, and encourages social, cognitive, and motor development in a positive, personalized, child-led play environment. The quantitative data and developmental trajectories captured by this platform enable systematic, mutli-modal, long-term studies of different therapeutic and educational approaches to autism and other developmental disorders, as well as a better understanding of motivation, engagement, and learning for the general population.</p>", "people": ["picard@media.mit.edu", "ktj@media.mit.edu"], "title": "SPRING: A Smart Platform for Research, Intervention, and Neurodevelopmental Growth", "modified": "2019-04-19T17:26:39.652Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["ml-learning", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "spring"}, {"website": "http://emotivemodeler.media.mit.edu/", "description": "<p>Whether or not we're experts in the design language of objects, we have an unconscious understanding of the emotional character of their forms. EmotiveModeler integrates knowledge about our emotive perception of shapes into a CAD tool that uses descriptive adjectives as an input to aid both expert and novice designers in creating objects that can communicate emotive character.</p>", "people": ["pip@media.mit.edu", "vmb@media.mit.edu"], "title": "EmotiveModeler: An emotive form design CAD tool", "modified": "2019-04-17T18:13:40.516Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "emotivemodeler-an-emotive-form-design-cad-tool"}, {"website": "", "description": "<p>Organizations are deploying gratitude-tracking systems to encourage appreciation, promote pro-sociality, and monitor employee wellbeing. We present the case study of one such system, called Gratia, adopted by a Fortune 500 company for over four years. We analyzed 422,209 messages of thanks and examined temporal patterns of appreciation, reciprocity, and repeated interactions. We also compared the formal organizational chart to the informal network expressed through the system. We found that gratitude is strongly reciprocated, that time between thanks is relatively long, and that it is predominantly given to peers outside one's immediate team.</p>", "people": ["ethanz@media.mit.edu", "jnmatias@media.mit.edu"], "title": "Peer Appreciation in the Workplace", "modified": "2018-10-20T01:44:04.200Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "peer-appreciation-in-the-workplace"}, {"website": "https://reynoldscuellar.com", "description": "<p>In the Civic Media group, we are exploring new theoretical frameworks for design, and novel design education methodologies that serve a future celebrating the pluriversal rather than the universal.</p><p>The \u201cTechnology Design for Coffee Production in Colombia\u201d is an educational intervention exploring this concept. The course convened 16 participants from seven countries to explore technological, social, and business solutions for small-scale coffee production along with coffee growers in rural Colombia.&nbsp;The course was a unique, multidisciplinary, and multicultural design experience in which people came together to co-design technologies that connected with rural coffee growers invention practices in the Sumapaz region in Colombia.</p><p>The course immersed participants into different agricultural practices, primarily coffee growing, as well as in the ontologies traditional to these practices. In an effort to expose participants to non-mainstream design methods and mechanisms of invention, our research team focused on surfacing local knowledge through research materials and hands-on activities.&nbsp;</p>", "people": ["pcuellar@media.mit.edu"], "title": "Technology Design for Coffee Production", "modified": "2019-06-05T18:40:55.600Z", "visibility": "PUBLIC", "start_on": "2019-01-07", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2020-01-31", "slug": "technology-design-for-coffee-production"}, {"website": "", "description": "<p>We are developing a new and exciting tool for expression in paint, combining technology and art to bring together the physical and the virtual through the use of robotics, artificial intelligence, signal processing, and wearable technology. Our technology promotes expression in paint not only by making it a lot more accessible, but also by making it flexible, adaptive, and fun, for everyone across the entire spectrum of abilities. With the development of the technology, new forms of art also emerge, such as hyper, hybrid, and collaborative painting. All of these can be extended to remote operation (or co-operation) thanks to the modular system design. For example, a parent and a child can be painting together even when far apart; a disabled person can experience an embodied painting experience; and medical professionals can reach larger populations with physical therapy, occupational therapy, and art therapy, including motor/neuromuscular impaired persons.</p>", "people": ["achituv@media.mit.edu", "pattie@media.mit.edu"], "title": "Express", "modified": "2018-05-07T19:44:41.673Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "express"}, {"website": "http://conceptnet.io/", "description": "<p>ConceptNet, or Open Mind Common Sense, is a long-standing project designed to help computers understand the meanings of words that people use. Starting in 1999, we were the first crowd-sourced project used to train an Artificial Intelligence and one of the first uses of crowdsourcing.&nbsp;</p><p>ConceptNet originated from the crowdsourcing project Open Mind Common Sense, which was launched in 1999 at the MIT Media Lab. It has since grown to include knowledge from other crowdsourced resources, expert-created resources, and games with a purpose.&nbsp;</p><p>It is currently an open data project providing freely-available knowledge graphs and NLP models in 73 languages.&nbsp;</p><p>Over the years this project has had many collaborators and have lived in the Society of Mind, Software Agents, and Digital Intuition groups as well.&nbsp;</p>", "people": [], "title": "Common sense for artificial intelligence", "modified": "2019-04-17T14:06:46.693Z", "visibility": "PUBLIC", "start_on": "1999-09-15", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "conceptnet-new"}, {"website": "", "description": "<p><a href=\"https://www.media.mit.edu/projects/city-science-andorra/overview/\">View the main City Science Andorra project profile.</a></p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "devisj@media.mit.edu", "kll@media.mit.edu", "csmuts@media.mit.edu"], "title": "Andorra | Energy + Environment", "modified": "2018-10-22T21:46:23.783Z", "visibility": "PUBLIC", "start_on": "2016-09-02", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "andorra-energy-environment"}, {"website": "http://listentree.media.mit.edu/", "description": "<p>ListenTree is an audio-haptic display embedded in the natural environment. Visitors to our installation notice a faint sound emerging from a tree. By resting their heads against the tree, they are able to hear sound through bone conduction. To create this effect, an audio exciter transducer is weatherproofed and attached to the tree's roots, transforming it into a living speaker, channeling audio through its branches, and providing vibrotactile feedback. In one deployment, we used ListenTree to display live sound from an outdoor ecological monitoring sensor network, bringing a faraway wetland into the urban landscape. Our intervention is motivated by a need for forms of display that fade into the background, inviting attention rather than requiring it. We consume most digital information through devices that alienate us from our surroundings; ListenTree points to a future where digital information might become enmeshed in material.</p>", "people": ["gershon@media.mit.edu", "edwinapn@media.mit.edu", "joep@media.mit.edu", "vmb@media.mit.edu"], "title": "ListenTree: Audio-haptic display in the natural environment", "modified": "2019-04-17T18:14:43.903Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["object-based-media", "responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "listentree-audio-haptic-display-in-the-natural-environment"}, {"website": "", "description": "<p>PerForm explores the intuitive meanings associated with the shape of objects, and how a shape-changing tool can allow for different forms of tangible interaction.&nbsp;Is it possible to map how ideas feel, and use the connections between senses to create more intuitive interfaces?&nbsp;PerForm addresses that question by allowing users to transform a physical tool&nbsp;to fit their intentions. This way, a user can play different musical instruments or take different actions in games, simply by varying the shape of the tool. Since the meanings associated with the shapes would be dependent on context, we are giving special focus to studying possible mappings of between the perception of sound and shape.</p><p><b>SOUND-SHAPE CORRESPONDENCES</b></p><p><b>\u201c<i>Music is not limited to the world of sound. There exists a music of the visual world.</i>\u201d</b></p><p><b>\u0003\u2014Oskar Fischinger, 1951.</b></p><p>When the German-American animator and filmmaker Oskar Fischinger created musically inspired animations and works of art, he touched on the intuitive associations our minds make between all the different sensory stimuli received from the environment.&nbsp;There is strong evidence that our brains forge relationships between shapes and seemingly corresponding sounds.</p><p>PerForm explores how the associations between visual and auditory perception can be used in interaction design. We developed a physical interface that users can transform by bending to create geometric shapes or symbols.&nbsp;By investigating possible correlations, natural or forged, between perceptual components of shape and its correlates in sound, we enable the&nbsp;tool to become a new instrument, with different sound timbre depending on the geometry of the object.</p><p><b>A SHAPE-SHIFTING GAMING CONTROLLER</b></p><p>One of the applications of this shape-shifting device would be to enable different modes of interaction through changes in shape. Instead of having to buy multiple controller devices for each genre of gaming or kind of interaction, or simply using a single, fixed-form controller that limits the embodied experience, a device capable of transformation would enable users to have a more imaginative and creative gaming experience, even enabling new kinds of games in which the user can invent tools by varying shapes.</p>", "people": ["irmandy@media.mit.edu", "carolx@media.mit.edu", "jaleesat@media.mit.edu"], "title": "PerForm", "modified": "2019-04-18T14:46:14.807Z", "visibility": "PUBLIC", "start_on": "2018-05-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2019-09-01", "slug": "perform-1"}, {"website": "", "description": "<p>Novel technologies for additive manufacturing are enabling design and production at nature\u2019s scale. We can seamlessly vary the physical properties of materials at the resolution of a sperm cell, a muscle cell, or a nerve cell. Stiffness, color, hygroscopy, transparency, conductivity, even scent, can be individually tuned for each three-dimensional pixel within a physical object. The generation of products is therefore no longer limited to assemblages of discrete parts with homogeneous properties. Rather like organs, objects can be computationally \"grown\" and 3D printed to form materially heterogeneous and multi-functional products.</p>", "people": ["ssunanda@media.mit.edu", "bader_ch@media.mit.edu", "rssmith@media.mit.edu", "limulus@media.mit.edu", "kolb@media.mit.edu", "neri@media.mit.edu"], "title": "Vespers II", "modified": "2018-05-07T19:48:07.209Z", "visibility": "PUBLIC", "start_on": "2016-12-12", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "vespers"}, {"website": "", "description": "\u200b<p>We have developed a wireless system that leverages the inexpensive RFID tags already on hundreds of billions of products to sense potential food contamination. Our system, called RFIQ (Radio Frequency IQ), aims at democratizing food quality and safety, bringing it to the hands of consumers.&nbsp;</p>", "people": ["fadel@media.mit.edu", "unsoo@media.mit.edu", "jleng@media.mit.edu", "yunfeima@media.mit.edu"], "title": "RFIQ: Food quality and safety detection using wireless stickers", "modified": "2019-02-13T16:42:33.755Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "learning-food-quality-and-safety-using-wireless-stickers"}, {"website": "", "description": "<p>The MIT Media Lab Space Exploration Initiative is developing \u201cThe Interplanetary Cookbook\u201d\u2014a collection of thought-provoking recipes, tools for eating, and sensory experiences for life in space.</p><h1><b>Open call for entries!</b></h1><p><b>Deadline to apply: July 15, 2019</b></p><p><b><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSc7hi7n_4Z5XlYqVTKT8LmKTtBGf525cG1qeKjSlYt_pa4xCg/viewform\">SUBMIT ENTRY HERE&nbsp;</a></b></p><p>Food is a key creature comfort in spaceflight, and it will play an even more significant role on long duration space travel and future life in space habitats. Currently, space food is freeze-dried and prepackaged in ways consistent with the demands of present day space travel. The advancement of deepspace exploration and the development of an interplanetary space tourism industry will make new cultural events and experiences never encountered before in human history possible. How can we design new and unforeseen foodways and gastronomic experiences that extend beyond basic sustenance?</p><p>Thoughtfully designed foods and culinary experiences could allow humans to feel more connected to their loved ones and histories on Earth, as well as promote the beginning of a food culture that fosters deeper relationships with new worlds. The function of food is not simply to provide nourishment\u2014it evokes the imagination, engages our senses, and contains our cultural heritage. \u201cThe Interplanetary Cookbook\u201d seeks to explore the unique food culture that will undoubtedly emerge as humans venture into new orbits through reconsidering, and even redesigning, the future of food beyond Earth-based practices.&nbsp;</p><p>We welcome all forms of submissions from project briefs to recipes and designs.&nbsp;</p><p>Topics may include, but are not limited to, the following:<br></p><ul><li>Recipe based on environment (e.g., zero gravity, geography) </li><li>Recipe inspired by space (e.g., Sci-Fi, aesthetics)</li><li>Mealtime tradition (e.g., cultural, daily routine, special occasion) &nbsp;</li><li>Cuisine style (e.g., regional, fusion, haute cuisine and homestyle/comfort food) </li><li>Space tourism (e.g., restaurant concept) </li><li>Tool for eating (e.g., utensil, dishware) </li><li>Food packaging design </li><li>Sensory experience design (e.g., aroma, flavor, texture) </li></ul><p>Space food products and eating experiences should celebrate the unique affordances of zero gravity while addressing the challenges associated with eating in space\u2014decreased sense of smell, waning and shifting appetites, food flyaways in zero gravity, degrading nutritional value, and a need for communal and cultural experience sharing for mental health and wellbeing.</p><p>For inquiries about the project please email Maggie Coblentz (<a href=\"mailto:mcoblent@media.mit.edu\">mcoblent@media.mit.edu</a>).&nbsp;<br></p>", "people": ["mcoblent@media.mit.edu"], "title": "The Interplanetary Cookbook", "modified": "2019-06-06T14:31:21.257Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "interplanetary-cookbook"}, {"website": "", "description": "", "people": [], "title": "Distributed learning and collaborative learning", "modified": "2018-10-18T03:12:28.622Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "distributed-learning-and-collaborative-learning"}, {"website": "", "description": "<p>Paraffin wax (common candlewax) shows promise as a high-performing hybrid rocket propellant for chemical propulsion systems.&nbsp; Its inherent safety and simplicity advantages and low cost (less than $4/kg) make it well-suited for widespread adoption for launch and in-space applications.&nbsp; Its benign nature compared to the toxicity and carcinogenicity which characterize currently-used propellants, such as hydrazine and nitrogen tetroxide, make paraffin an especially strong candidate for new entrants to the propulsion community.</p><p>The Space Enabled Research Group is focused on the use of paraffin wax for small satellite missions.&nbsp; Specifically, we are investigating the centrifugal casting of paraffin into annular geometries on Earth as well as in microgravity.&nbsp; The research group envisions the repurposing of paraffin thermal insulation at end of life for deorbit maneuvers.&nbsp; However, such a mission would require centrifugal casting of paraffin in orbit\u2014a task which has never been done before.&nbsp; The microgravity environment is expected to reduce rotation rate demands for uniform casting, and the overall experimental investigation aims to quantify the differences between 1-g and microgravity centrifugal casting.</p>", "people": ["stober@media.mit.edu"], "title": "Candlewax Rockets: A green alternative for in-space propulsion", "modified": "2019-04-19T20:14:35.126Z", "visibility": "PUBLIC", "start_on": "2018-04-01", "location": "", "groups": ["space-enabled"], "published": true, "active": false, "end_on": null, "slug": "candlewax-rockets-a-green-propellant-alternative"}, {"website": "", "description": "", "people": ["aekblaw@media.mit.edu", "andreuhl@media.mit.edu", "prathima@media.mit.edu", "nicolelh@media.mit.edu"], "title": "Myths of the Cosmos: Indigenous Cosmologies", "modified": "2019-05-30T15:05:50.758Z", "visibility": "LAB", "start_on": "2018-01-01", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "myths-of-the-cosmos-indigenous-cosmologies"}, {"website": "", "description": "<p>Lorem ipsum dolor sit amet, quo ne laudem scripserit, has quis instructior no. Has minimum similique instructior ne. Ludus definitiones mel cu. Mundi consectetuer mei ex. Ex ius modus definiebas, an molestie scripserit nam.</p><p>Ferri debet inimicus ei vel, ea nibh vocibus similique mea, ius ea inani adipisci. Mandamus philosophia vel in, omittam facilisi disputando eum ex, ancillae menandri cum an. Legimus indoctum ex has, et docendi ponderum perpetua mel. Iracundia aliquando ex eum. Assum novum tacimates ad vix. Vis at appareat qualisque inciderint, ei sed veri feugait disputando, ut nam assum appareat aliquando.\u203a</p>", "people": [], "title": "TestJL", "modified": "2018-08-13T13:17:00.075Z", "visibility": "LAB", "start_on": null, "location": "", "groups": ["communications"], "published": false, "active": false, "end_on": null, "slug": "testjl"}, {"website": "", "description": "<h1>Overview:&nbsp;</h1><p>How will we build the coming generations of Space Architecture--the modules, space ships, and space stations that will ensconce our space-faring species? Can we move beyond the 20th century paradigm of cylindrical tubes in orbit, to&nbsp;<b>geodesic dome habitats,&nbsp; to microgravity concert halls, to space cathedrals</b>?The next generation of space architecture should delight, inspire, and protect humanity for our future in the near, and far, reaches of space.&nbsp;</p><p>The future of human habitation in space lies in self-assembling, adaptive, and reconfigurable structures. Rather than transporting fixed, rigid habitation modules and risking astronaut Extravehicular Activities (EVAs) during construction, we can lower payload weight, reduce assembly complexity, and revolutionize space-structure modularity by relying on reconfigurable, self-assembly.&nbsp;</p><p>This project proposes a multi-year research effort to study, characterize, prototype and test \"TESSERAE\":&nbsp; Tessellated Electromagnetic Space Structures for the Exploration of Reconfigurable, Adaptive Environments.&nbsp; Each TESSERAE structure is made from a set of tiles. These tiles are tuned to self-assemble into a particular geometry--in our initial prototypes, we have focused on the buckminsterfullerene (20 hexagonal tiles, 12 pentagonal tiles).&nbsp; Each tile at minimum includes&nbsp; a rigid outer shell, responsive sensing for bonding diagnosis, electro-permanent magnets for dynamically controllable bonding actuation, and an on-board power harvesting and power management system. Habitat-scale TESSERAE tiles will also including clamping and sealing for pressurization.&nbsp; Tiles are released in microgravity testing environments to quasi-stochastically self assemble.&nbsp;</p><p>The \u201cTESSERAE\u201d name and multi-tile structure hearken to the small, colored tiles used in Roman mosaics, where many standard pieces, or \u201ctesserae,\u201d interlock to form a larger creation. We make this reference to ancient history, when designing an artifact of our space exploration future, to tie architectural elements together across scales and across millennia.</p><p>&nbsp;TESSERAE will function as multi-use, low-cost orbiting modules that supply a critical space infrastructure for the next generation of zero gravity habitats, science labs, staging areas for on-surface exploration, and more. Unlike large-scale habitats proposed for entire space colonies, the TESSERAE should be thought of as flexible and reconfigurable modules to aid in agile mission operations.&nbsp;Our mission concept focuses on supporting LEO, Lunar and Mars operations, with dual-use orbit &amp; surface capability:&nbsp;</p><ul><li>Tiles are packed flat and condensed for launch</li><li>Tiles are released after orbit insertion to quasi-stochastically self-assemble into the target geometry, while floating in microgravity</li><li>Once assembled, the structure can be reconfigured on demand (e.g., where a berthing port tile was needed yesterday, a cupola tile can be replaced tomorrow)</li><li>Tiles can be disassembled entirely, packed flat again in an EDL (Entry, Descent and Landing) vehicle, and then deployed and \"snap-assembled\" with astronaut assists on the lunar or martian surface</li></ul><p>Multiple, interlocking TESSERAE can serve as a larger volume orbiting base (e.g.&nbsp; \"MOSAIC\": Mars Orbiting Self-Assembling Interlocking Chambers), in addition to supporting the coming waves of space tourists and space hotels in Low Earth Orbit.&nbsp;</p>", "people": ["aekblaw@media.mit.edu"], "title": "TESSERAE: Self-Assembling Space Architecture", "modified": "2019-05-01T20:55:49.503Z", "visibility": "PUBLIC", "start_on": "2017-06-01", "location": "", "groups": ["responsive-environments", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "tesserae-self-assembling-space-architecture"}, {"website": "", "description": "<p>Paper diagnostics can be used for affordable and scalable biomarker detection. We propose new methods for stabilizing cellulose-based immunoassays, leading to a low-cost paper diagnostic assay for the detection of oral disease biomarkers in human saliva.<br></p><p><strong>Why is this work important?</strong></p><p>Low-cost detection of biomarkers associated with the majority of diseases is not feasible.</p><p><strong>What has been done before?</strong></p><p>Paper-based assays have been used to detect biomarkers in human serum or blood. The paper-based detection of biomarkers in saliva has not been extensively studied.</p><p><strong>What are our contributions?</strong></p><p>We propose methods for stabilizing paper-based immunoassays for robust and low-cost detection of salivary biomarkers. This method is low-cost and can facilitate rapid detection of oral biomarkers. We specifically describe the detection of biomarkers associated with oral diseases, MMP-8 and -9.</p><p><strong>What are the next steps?</strong></p><p>We are expanding this approach to assay biomarkers in serum and blood.</p>", "people": ["pratiks@media.mit.edu"], "title": "Detecting Biomarkers with Printable Paper Diagnostics", "modified": "2018-05-04T20:38:05.791Z", "visibility": "PUBLIC", "start_on": "2016-05-02", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "detecting-biomarkers-with-printable-paper-diagnostics"}, {"website": "https://www.shadaalsalamah.com/", "description": "<h1><b>Open Algorithms (OPAL)</b></h1>", "people": ["alotaibi@media.mit.edu", "ecstll@media.mit.edu", "jobalbar@media.mit.edu", "sandy@media.mit.edu", "shada@media.mit.edu"], "title": "OPAL 4 Health", "modified": "2018-10-19T21:07:55.443Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "opal-health"}, {"website": "http://www.biancadatta.com", "description": "<p>Advances in science and engineering are bringing us closer and closer to systems that respond to human stimuli in real time. Scientists often look to biology for examples of efficient, spatially tailored multifunctional systems, drawing inspiration from photonic structures like multilayer stacks similar to those in the&nbsp;<i>Morpho butterfly</i>. &nbsp;In this project, we develop an understanding of the landscape of responsive, bio-inspired, and active materials, drawing from principles of photonics and bio-inspired material systems. We are exploring various material processing techniques to produce and replicate structurally colored surfaces, while developing simulation and modeling tools (such as inverse design processes) to generate new structures and colors. Such complex biological systems require advanced fabrication techniques. Our designs are realizable through fabrication using direct laser writing techniques such as two photon polymerization. We aim to compare our model system and simulations to fabricated structures using optical microscopy, scanning electron microscopy, and angular spectrometry. This process provides a toolkit with which to examine and build other bio-inspired, tunable, and responsive photonic systems and expand the range of achievable structural colors.</p><p>Unlike with natural structures, producing biomimetic surfaces allows researchers to test beyond tunability that occurs naturally and explore new theory and models to design structures with optimized functions.&nbsp;The benefits of such biomimetic nanostructures are plentiful: they provide brilliant, iridescent color with mechanical stability and light steering capabilities.&nbsp;&nbsp;By producing biomimetic nanostructures, designers and engineers can capitalize on unique properties of optical structural color, and examine these structures based on human perception and response.</p>", "people": ["bdatta@media.mit.edu", "sjolly@media.mit.edu"], "title": "MORPHO: Bio-inspired photonic materials, producing structurally colored surfaces", "modified": "2019-04-18T17:28:00.132Z", "visibility": "LAB-INSIDERS", "start_on": "2018-06-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "bio-inspired-photonic-materials-producing-structurally-colored-surfaces"}, {"website": "", "description": "", "people": [], "title": "Conversation Trees", "modified": "2018-10-17T16:33:31.224Z", "visibility": "PUBLIC", "start_on": "2018-04-01", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "conversation-trees"}, {"website": "", "description": "<p>Most current virtual reality interactions are mediated by hand-held input devices or hand gestures, and usually display only a partial representation of the user in the synthetic environment. We believe that representing the user as a full avatar controlled by the natural movements of the person in the real world can lead to a greater sense of presence in VR. <a href=\"http://dl.acm.org/citation.cfm?id=2817802&amp;CFID=661831763&amp;CFTOKEN=43505533\">MetaSpace I</a> is a virtual reality system that allows co-located users to explore a VR world together by walking around in physical space. Each user\u2019s body is represented by an avatar that is dynamically controlled by their body movements. Users can see their own avatar and the other person\u2019s avatar allowing them to perceive and act intuitively in the virtual environment.</p>", "people": ["sra@media.mit.edu", "geek@media.mit.edu"], "title": "MetaSpace I", "modified": "2018-05-07T20:12:11.319Z", "visibility": "PUBLIC", "start_on": "2014-11-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": null, "slug": "meta-physical-space-vr"}, {"website": "", "description": "<p>A large proportion of the American population currently suffers from sleep disorders. Among them are patients with obstructive sleep apnea (OSA), who repeatedly stop breathing while asleep. Current screening methods and devices are impractical for widespread screening. We introduce a new model for low-cost OSA screening consisting of an at-home, wearable sleep mask that can easily track the wearer's sleep patterns. The data collected overnight by this sensory mask provides a determination of a patient's OSA risk.<br></p><p><strong>Why is this work important?</strong></p><p>There are 7-18 million Americans suffering from sleep disorders. Among them are patients with OSA, who stop breathing either completely or partially while asleep. This is a serious condition with few reliable low-cost devices available for primary diagnosis without expert supervision.</p><p><strong>What has been done before?</strong></p><p>The gold standard for OSA diagnosis is overnight polysomnography (PSG). Apart from that there are many home diagnostics devices available. However, many at-home devices offer poor diagnostic quality and some of them also require expert intervention, from installation of the device to analysis of the data.</p><p><strong>What are our contributions?</strong><br></p><p>We report the construction and validation of a design for low-cost OSA screening built around a simplified screening device embedded in an at-home wearable sleep mask. This simplified screening system allows for OSA diagnosis without imposing the costs or time commitment of a full PSG.</p><p><strong>What are the next steps?</strong></p><p>In the next iterations of the device, we aim to improve the mechanical design and ease of use, as well as automate data analysis and screening so that the device can be evaluated in larger studies.</p>", "people": ["guysatat@media.mit.edu", "pratiks@media.mit.edu", "rohan@media.mit.edu"], "title": "At-Home Sleep Apnea Screening", "modified": "2018-05-06T23:20:12.777Z", "visibility": "PUBLIC", "start_on": "2016-05-09", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "at-home-sleep-apnea-screening"}, {"website": "", "description": "<p>What if our mobile devices could sense and then adapt to the spatial, temporal, and social context of their local environments? Imagine if your smartphone was smart enough to know that it should not be ringing loudly when you are in an important meeting, or that it should not be in silent mode when you are trying to find where you have misplaced it at home. We have created an inexpensive secure system that delivers this goal by embedding contextual information into the environment rather than the phone.  In that way, all mobile devices at a given location can detect the broadcasted contextual information using Wi-Fi and change their behavior accordingly,  without requiring any handshake or internet connection. By leveraging the latest and most inexpensive Wi-Fi modules on the market, and by building our own embedded firmware, server-side software, and mobile app, we are able to deploy this system in a secure and massively scalable way. <br></p>", "people": ["alims@media.mit.edu", "vmb@media.mit.edu"], "title": "QuieSense:  Distributed context-awareness system for Wi-Fi enabled mobile devices", "modified": "2019-04-17T18:20:16.620Z", "visibility": "PUBLIC", "start_on": "2016-10-21", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "quiesense"}, {"website": "http://www.quantizer.media.mit.edu", "description": "<p>Inspired by previous work in the field of data sonification, we built a data-driven composition platform that enables users to map collision event information from experiments in high-energy physics to audio properties, and thus make music from real-time data. The tool is used for outreach purposes, allowing physicists and composers to interact with collision data through novel interfaces. Three real-time compositions were streamed from May 2016\u2013July 2016. Two additional compositions are streamed in fall 2018. This project can inspire the development of strategic mappings that facilitate the auditory perception of hidden regularities in high-dimensional datasets, and one day evolve into a useful analysis tool for physicists as well, possibly for the purpose of monitoring slow control data in experiment control rooms. The project is accessible at <a href=\"http://quantizer.media.mit.edu\">Quantizer.media.mit.edu.</a></p>", "people": ["cherston@media.mit.edu", "joep@media.mit.edu"], "title": "Quantizer: Sonification Platform for High-Energy Physics Data", "modified": "2018-10-09T20:36:43.077Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": null, "slug": "quantizer-sonification-platform-for-high-energy-physics-data"}, {"website": "", "description": "<p>Introducing the&nbsp;newest edition of the gamma musical/medical instruments - the&nbsp;Gamma MOON (<b>M</b>usical&nbsp;<b>O</b>mnisensory&nbsp;<b>O</b>rbital&nbsp;<b>N</b>euroinstrument).&nbsp;This instrument features a capacitive interface which delivers multisensory gamma stimulation through audio, visual, haptic and tactile feedback. In collaboration with the Aging Brain Alzheimer\u2019s Initiative at MIT, the Gamma MOON pilots a novel treatment form-factor with the goal of device deployment in large-scale clinical trials. Research reveals that gamma instrument interaction can strengthen cognitive function and sensory perception while increasing focus even in neurocognitively healthy individuals.&nbsp;Contact arrangement allows both patients and performers to create high-level musical abstractions as well as follow traditional notational melodies. Gamma MOON's heightened sensorial engagement recruits increased cognitive entrainment, multimodal expression and creative freedom.&nbsp;</p>", "people": ["arieger@media.mit.edu"], "title": "Gamma MOON: Musical Omnisensory Orbital Neuroinstrument", "modified": "2018-10-20T15:22:00.051Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "the-gamma-moon-musical-omnisensory-orbital-neuroinstrument"}, {"website": "", "description": "<p>AlterEgo&nbsp;is a&nbsp; non-invasive, wearable, peripheral neural interface that allows humans to converse in natural language with machines, artificial intelligence assistants, services, and other people without any voice\u2014without opening their mouth, and without externally observable movements\u2014simply by articulating words internally.&nbsp; The feedback to the user is given through audio, via bone conduction,&nbsp; without disrupting the user's usual auditory perception, and making the interface closed-loop. This enables an human-computer interaction that is subjectively experienced as completely internal to the human user\u2014like speaking to one's self.&nbsp;&nbsp;AlterEgo seeks to combine humans and computers\u2014such that computing, the Internet, and AI would weave into human personality as an internal \u201csecond self\u201d and augment human cognition and abilities.</p><p>The wearable system captures peripheral neural signals when internal speech articulators are volitionally and neurologically activated, during a user's internal articulation of words. This enables a user to transmit and receive streams of information to and from a computing device or any other person without any observable action, in discretion, without unplugging the user from her environment, without invading the user's&nbsp; privacy.&nbsp;</p>", "people": ["arnavk@media.mit.edu", "ewadkins@media.mit.edu", "pattie@media.mit.edu"], "title": "AlterEgo", "modified": "2019-05-17T00:56:53.288Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "alterego"}, {"website": "", "description": "<p>Social learning has shown that people are more likely to learn from those who are seen as prestigious, talented, or who share demographic attributes with learners. In order to demonstrate that, many experiments and data-based studies have been conducted in many different systems; however, classroom environments have been understudied, because of different complications in both designing experiments and collecting data.</p><p>Combining both new technologies that are able to capture children's attention, e.g. video games, as well as experimental game theory, which provides us a formal framework to capture children's revealed preferences\u2014a school classroom can provide an ideal environment for controlled social dilemma experiments, whose results can be contrasted against real-life indicators of school-life.</p><p>The connection between cooperation inside a classroom and social relationships is central in our framework. Here, we navigate the social network structure by running a non-anonymous dyadic cooperative (video) game (Fig. 1), in 50 different public primary school classrooms, between grades 3-5, allowing us to map cooperation networks for each classroom.</p><p>From the video game decisions, we build a weighted cooperation network for each classroom. The resulting network structure is able to capture different properties of the classroom, such as academic performance and social co-existence (Fig. 2). First,&nbsp;we find that positions in the social network have a significant power to identify, in an early stage, children who are susceptible to becoming&nbsp; the victims of bullying, and children who have a high probability to&nbsp;be bullies&nbsp;(Fig. 2A). Second,&nbsp;we find a positive and statistically significant relationship between network centrality\u2014measured as the sum of the outcome on the video game\u2014and student\u2019s academic performance (measured as GPA, even controlling for others socio-behavioral characteristics that are correlated with GPA (Fig. 2 B)).</p><p>These results don't just help us to understand the elementary school environment, but also open new avenues for the role of networks in the education system, with a huge potential impact in education public policy. These results are useful inputs for decision makers and physiologists to prevent bullying and improve learning.</p>", "people": ["ccandiav@media.mit.edu", "hidalgo@media.mit.edu"], "title": "When bullying meets (video) game theory: A novel framework to understand elementary school environments", "modified": "2018-10-23T15:23:30.759Z", "visibility": "PUBLIC", "start_on": "2017-10-17", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "when-bullying-meets-video-game-theory"}, {"website": "", "description": "<p>We'd like to introduce you to a very special neuroscience project that we are currently conducting in the setting of a traditional fine arts museum.</p><h1><b>Join the conversation on the&nbsp;<a href=\"https://www.responsivescience.org/brainstorm\">Responsive Science Brainstorm project site</a>. </b></h1><p>Responsive Science uses the&nbsp;<a href=\"https://www.pubpub.org/\">PubPub platform</a>, which allows for direct interaction. PubPub was developed at MIT Media Lab.</p>", "people": ["lunshof@media.mit.edu"], "title": "Brainstorm: Anima Mundi", "modified": "2018-10-23T15:25:53.714Z", "visibility": "PUBLIC", "start_on": "2018-06-09", "location": "", "groups": ["sculpting-evolution"], "published": true, "active": false, "end_on": null, "slug": "brainstorm-anima-mundi"}, {"website": "", "description": "<p>Exploring the connection between our two minds: the one in our head and the one in our body.&nbsp;</p><p>The mind-gut connection has flourished as a research area in the past few decades, elucidating the&nbsp; key role of the enteric nervous system (ENS or \"gut-brain\") in stress, affect, and memory. However, this connection has not been explored for wearable technology\u2014applications and research for cognitive phenomena remain biased towards the cerebrum. In this project, we are non-invasively acquiring gastric myoelectric activity from the abdomen to evaluate the potential for a new area of wearable technology that can inform users on affective, stress, or memory states based on signals produced by the ENS.</p>", "people": ["pattie@media.mit.edu", "avujic@media.mit.edu"], "title": "Serosa", "modified": "2018-05-09T13:05:47.805Z", "visibility": "LAB-INSIDERS", "start_on": "2017-10-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "serosa"}, {"website": "", "description": "<p>Money is at the heart of the financial system\u2014its most basic element. Fundamental reform of the system starts with addressing how money works today and how it could work in the future. The emergence of digital currency has led several central banks to consider how this new technology affects their ability to discharge their mandates. One of the most significant questions is whether digital versions of fiat currencies can be issued and what the role of the central bank should be in a financial system being changed by new technology.</p><p>We are working to address some of the fundamental questions which need to be addressed to bring about this reformation of the financial system.</p>", "people": ["robleh@media.mit.edu"], "title": "Digital Fiat Currency", "modified": "2019-04-18T19:24:02.529Z", "visibility": "PUBLIC", "start_on": "2017-03-28", "location": "", "groups": ["digital-currency-initiative-dci"], "published": true, "active": false, "end_on": null, "slug": "central-banks-and-digital-currency"}, {"website": "", "description": "<p>In order to understand how exogenous shocks, like death, impact memorability by remembering, we use a data-set of biographies from Wikipedia for all individuals who have more than 15 different language editions. Here, we focus on different external shocks that are able to trigger remembering, such as Death, Nobel Prize, Academy Awards (Oscars), Ballon d'Or, Golden Globes, and Grammy's. All of these events show an exogenous-critical non-trivial herd behavior, as described by <a href=\"http://www.pnas.org/content/105/41/15649\">Crane and Sornette 2008</a>.</p>", "people": ["ccandiav@media.mit.edu", "hidalgo@media.mit.edu", "crisjf@media.mit.edu"], "title": "The laws of forgetting II: How death and exogenous events shape our collective memory", "modified": "2018-10-20T16:47:24.281Z", "visibility": "PUBLIC", "start_on": "2017-10-16", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "the-laws-of-forgetting-ii"}, {"website": "", "description": "<p>Biodiversity on planet Earth is under momentous threat, with extinction rates estimated between 100 and 1,000 times their pre-human level. The Mediated Matter group has been in search of materials and chemical substances that can sustain and enhance biodiversity across living systems, and that have so far endured the perils of climate change. Melanin is one such substance illustrating biodiversity at the genetic, species, and ecosystem levels.</p>", "people": ["ssunanda@media.mit.edu", "bader_ch@media.mit.edu", "rssmith@media.mit.edu", "jpcosta@media.mit.edu", "josephk@media.mit.edu", "fkraemer@media.mit.edu", "neri@media.mit.edu"], "title": "Totems", "modified": "2019-05-07T13:44:31.523Z", "visibility": "PUBLIC", "start_on": "2019-02-27", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "totems"}, {"website": "http://www.sangww.net/2018/04/guitar-machine-i.html", "description": "<p>Symbiotic guitar playing between human and machine fingers. The system can be used as a learning tool or a real-time augmentation to the human guitar player, offering previously impossible combinations of notes.&nbsp;</p>", "people": ["sangwon@media.mit.edu"], "title": "Guitar Machine", "modified": "2018-05-07T01:50:19.235Z", "visibility": "PUBLIC", "start_on": "2017-04-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "guitar-machine"}, {"website": "", "description": "<p>ShAir is a platform for instantly and easily creating local content-shareable spaces without requiring an Internet connection or location information. ShAir-enabled devices can opportunistically communicate with other mobile devices and optional pervasive storage devices such as Wi-Fi SD cards whenever they enter radio range of one another. Digital content can hop through devices in the background without user intervention. Applications that can be built on top of the platform include ad-hoc photo/video/music sharing and distribution, opportunistic social networking and games, digital business card exchange during meetings and conferences, and local news article-sharing on trains and buses.</p>", "people": ["bandy@media.mit.edu", "holtzman@media.mit.edu", "vmb@media.mit.edu", "arata@media.mit.edu"], "title": "ShAir: A platform for mobile content sharing", "modified": "2019-04-17T18:21:12.490Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "shair-a-platform-for-mobile-content-sharing"}, {"website": "http://aprendizagemcriativa.org/", "description": "<p>The Lemann Creative Learning Program is a collaboration between the MIT Media Lab and the Lemann Foundation to foster creative learning in Brazilian public education.&nbsp;</p><p>Established in February 2015, the program designs new technologies, support materials, and innovative initiatives to engage Brazilian public schools, afterschool centers, and families in learning practices that are more hands-on, creative, and centered on students' interests and ideas.&nbsp;</p><p>---</p><p>O Programa Lemann de Aprendizagem Criativa \u00e9 uma colabora\u00e7\u00e3o entre o MIT Media Lab e a Funda\u00e7\u00e3o Lemann visando incentivar a aprendizagem criativa na educa\u00e7\u00e3o p\u00fablica do Brasil.</p><p>Criado em fevereiro de 2015, o programa cria novas tecnologias, materiais de apoio e iniciativas que ajudem escolas p\u00fablicas, organiza\u00e7\u00f5es de educa\u00e7\u00e3o n\u00e3o formal, e fam\u00edlias a implementar pr\u00e1ticas de aprendizagem que sejam mais m\u00e3o na massa, criativas e centradas nos interesses dos alunos.</p>", "people": ["mres@media.mit.edu", "leob@media.mit.edu"], "title": "Creative Learning in Brazil / Aprendizagem Criativa no Brasil", "modified": "2018-12-11T21:36:57.377Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "creative-learning-in-brazil"}, {"website": "", "description": "<p>Climate change is going to alter the environments that we depend on in myriad ways. We're using data to identify and quantify these potential human impacts.&nbsp;</p>", "people": ["cebrian@media.mit.edu", "emoro@media.mit.edu", "pinary@media.mit.edu", "irahwan@media.mit.edu", "nobradov@media.mit.edu", "felbo@media.mit.edu"], "title": "Identifying the human impacts of climate change", "modified": "2019-04-19T17:44:02.613Z", "visibility": "PUBLIC", "start_on": "2016-07-01", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "identifying-the-human-impacts-of-climate-change"}, {"website": "http://www.mallcong.com", "description": "<p><b>\"What happens if a static object starts to move and react to your gestures like a living creature?\"</b></p><p><span style=\"font-size: 18px; font-weight: 400;\">The pillow we know and use every night is not passive anymore: a</span>s an interaction between active and active materials, we have developed a soft pillow that can breathe with you.&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">reSpire is a soft actuated pillow that breathes in synchronization with the user's respiration.&nbsp; With it, users can touch and feel their breath.</span></p><p>By projecting the breath pattern of the user onto the inflation motion of the neck pillow, reSpire not only enables the user to realize how they have been breathing, like an augmented prosthetic lung, but can also let a loved one feel your presence even when you are apart.</p>", "people": ["yun_choi@media.mit.edu", "ishii@media.mit.edu"], "title": "reSpire: A soft actuated pillow synchronized with breath patterns for tactile telecommunication", "modified": "2018-05-07T16:45:39.369Z", "visibility": "LAB-INSIDERS", "start_on": "2018-02-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "respire"}, {"website": "", "description": "<p>In-Vivo Networking (IVN)&nbsp;is the new technology that can wirelessly power and communicate with tiny devices implanted deep within the human body. Such devices could be used to deliver drugs, monitor conditions inside the body, or treat disease by stimulating the brain with electricity or light.&nbsp;&nbsp;</p><p>The implants are powered by radio frequency waves, which are safe for humans. In tests in animals, we showed that the waves can power devices located 10 centimeters deep in tissue, from a distance of one meter.</p>", "people": ["fadel@media.mit.edu", "zhluo@media.mit.edu", "yunfeima@media.mit.edu"], "title": "In-Vivo Networking: Powering and communicating with tiny battery-free devices inside the body", "modified": "2019-02-14T19:47:21.377Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "ivn-in-vivo-networking"}, {"website": "", "description": "<h2>How can we get more value from the same buildings?&nbsp;</h2><p>Cities contain many different resources and spaces and typically, these resources operate as products with&nbsp; a single function and a single owner and/or renter. However, the owner's demand&nbsp;for space often varies daily or seasonally, meaning that many buildings tend to be underutilized and are often vacant or partially vacant for large portions of each day.&nbsp;</p><p>Meanwhile, the \"sharing economy\" has been one of the most significant economic shifts in the last 10 years, with companies like Uber and Airbnb experiencing explosive growth. Along these lines, Aalto University\u2014a member of the MIT Media Lab City&nbsp;Science network\u2014has developed the concept&nbsp;of City-as-a-Service, where building space and other resources are shared among institutions, businesses, and citizens in a community. Aalto has already begun experimenting with School-as-a-Service, as a prototype of City-as-a-Service on their campus in Espoo.</p>", "people": ["alonsolp@media.mit.edu", "doorleyr@media.mit.edu", "kll@media.mit.edu"], "title": "Aalto Campus-as-a-Service Simulations", "modified": "2019-05-07T20:00:40.076Z", "visibility": "PUBLIC", "start_on": "2017-07-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "aalto-saas"}, {"website": "", "description": "<h1>Split Learning: Distributed deep learning without sharing raw data</h1><p><strong>Abstract:</strong>&nbsp;Can a server utilize deep learning models for training or inference without accessing raw data from clients?&nbsp;Split learning naturally allows for various configurations of cooperating entities to train (and infer from) machine learning models without sharing any raw data or detailed information about the model.&nbsp;</p><p><strong>Key idea:</strong>&nbsp;In the simplest of configurations of split learning, each client (for example, radiology center) trains a partial deep network up to a specific layer known as the cut layer. The outputs at the cut layer are sent to another entity (server/another client) which completes the rest of the training without looking at raw data from any client that holds the raw data. This completes a round of forward propagation without sharing raw data. The gradients are now back propagated again from its last layer until the cut layer in a similar fashion. The gradients at the cut layer (and only these gradients) are sent back to radiology client centers. The rest of back propagation is now completed at the radiology client centers. This process is continued until the distributed split learning network is trained without looking at each others raw data.</p><p><b>Split Learning Papers:</b></p><p><b>1.) Split learning for health: Distributed deep learning without sharing raw patient data, Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, Ramesh Raskar,&nbsp;<a href=\"https://arxiv.org/pdf/1812.00564.pdf\">(PDF)</a>&nbsp;(2018)</b></p><p><b>2.) \u201cDistributed learning of deep neural network over multiple agents\u201d, Otkrist Gupta and Ramesh Raskar, In: Journal of Network and Computer Applications 116,&nbsp;<a href=\"https://www.sciencedirect.com/science/article/pii/S1084804518301590\">(PDF)</a>&nbsp;(2018)</b></p><p><b>3.) Survey paper: No Peek: A Survey of private distributed deep learning, Praneeth Vepakomma, Tristan Swedish, Ramesh Raskar, Otkrist Gupta, Abhimanyu Dubey,&nbsp;<a href=\"https://arxiv.org/pdf/1812.03288.pdf\">(PDF)</a>&nbsp;(2018)</b></p><h2>Split learning\u2019s computational and communication efficiency on clients:</h2><p>Client-side communication costs are significantly reduced as the data to be transmitted is restricted to initial layers of the split learning network (splitNN) prior to the split. The client-side computation costs of learning the weights of the network are also significantly reduced for the same reason. In terms of model performance, the accuracies of Split NN remained competitive to other distributed deep learning methods like federated learning and large batch synchronous SGD with a drastically smaller client side computational burden when training on a larger number of clients as shown below in terms of teraflops of computation and gigabytes of communication when split learning is used to train Resnet and VGG architectures over 100 and 500 clients with CIFAR 10 and CIFAR 100 datasets.</p><p><b>Versatile plug-and-play configurations of split learning</b></p><p>Versatile configurations of split learning configurations cater to various practical settings of&nbsp;i) multiple entities holding different modalities of patient data, ii) centralized and local health entities collaborating on multiple tasks, iii) learning without sharing labels, iv) multi-task split learning, v) multi-hop split learning&nbsp;and other hybrid possibilities to name a few as shown below and further detailed in our paper here&nbsp;<a href=\"https://arxiv.org/pdf/1812.00564.pdf\">(PDF)</a>&nbsp;</p><h2>News stories about this work</h2><p><strong><a href=\"https://www.technologyreview.com/the-download/612567/a-new-ai-method-can-train-on-medical-records-without-revealing-patient-data/\">MIT Technology Review</a></strong></p>", "people": ["raskar@media.mit.edu", "vepakom@media.mit.edu"], "title": "Distributed and collaborative learning", "modified": "2018-12-12T20:53:48.275Z", "visibility": "PUBLIC", "start_on": "2018-08-27", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": null, "slug": "distributed-learning-and-collaborative-learning-1"}, {"website": "", "description": "<p>As we move towards an increasingly IoT-enabled ecosystem, we find that it is easier than ever before to capture vast amounts of audio data.  However, there are many scenarios in which we may seek a \"compressed\" representation of an audio stream, consisting of an intentional curation of content to achieve a specific presentation\u2014a background soundtrack for studying or working; a summary of salient events over the course of a day; or an aesthetic soundscape that evokes nostalgia of a time and place.  In this work, we present a novel, automated approach to the task of content-driven \"compression,\" built upon the tenets of auditory cognition, attention, and memory.  We expand upon our previous experimental findings, which demonstrate the relative importance of higher-level gestalt and lower level spectral principles in determining auditory memory, to design corresponding computational implementations enabled by auditory saliency models, deep neural networks for audio classification, and spectral feature extraction.  We demonstrate the approach by generating a number of 30 second binaural mixes from eight-hour recordings captured in three contrasting locations at the Media Lab, and conduct a qualitative evaluation illustrating the relationship between our feature space and a user's perception of the resulting presentations.  Through this work, we suggest rethinking traditional paradigms of compression in favor of an approach that is goal-oriented and modulated by human perception.</p>", "people": ["dramsay@media.mit.edu", "ishwarya@media.mit.edu"], "title": "Cognition-driven audio summarization", "modified": "2019-04-16T20:15:50.983Z", "visibility": "PUBLIC", "start_on": "2019-03-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "compression-by-content-curation"}, {"website": "", "description": "", "people": [], "title": "Investigating Bodily Responses to Unknown Words: a Focus on Facial Expressions and EEG", "modified": "2018-08-15T02:19:41.640Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["fluid-interfaces"], "published": false, "active": false, "end_on": null, "slug": "unknown-words-eeg"}, {"website": "", "description": "<p>Functionally graded materials\u2014materials with spatially varying composition or microstructure\u2014are omnipresent in nature. From palm trees with radial density gradients, to the spongy trabeculae structure of bone, to the hardness gradient found in many types of beaks, graded materials offer material and structural efficiency. But in man-made structures such as concrete pillars, materials are typically volumetrically homogenous. While using homogenous materials allows for ease of production, improvements in strength, weight, and material usage can be obtained by designing with functionally graded materials. To achieve graded material objects, we are working to construct a 3D printer capable of dynamic mixing of composition material. Starting with concrete and UV-curable polymers, we aim to create structures, such as a bone-inspired beam, which have functionally graded materials. This research was sponsored by the NSF EAGER award: Bio-Beams: FGM Digital Design &amp; Fabrication.</p>", "people": ["stevenk@media.mit.edu", "neri@media.mit.edu"], "title": "3D printing of functionally graded materials", "modified": "2019-04-18T19:42:12.916Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "3d-printing-of-functionally-graded-materials"}, {"website": "", "description": "", "people": ["mdchurch@media.mit.edu", "alonsolp@media.mit.edu", "agrignar@media.mit.edu", "yasushis@media.mit.edu", "mcllin@media.mit.edu", "doorleyr@media.mit.edu", "kll@media.mit.edu", "heckbert@media.mit.edu", "noyman@media.mit.edu", "ptinn@media.mit.edu", "ryanz@media.mit.edu"], "title": "City Science Network", "modified": "2019-05-29T19:57:48.798Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "city-science-network"}, {"website": "", "description": "<p>dataVisionaRi is an exploration of big data visualizations&nbsp;techniques in VR. The research generates new taxonomy and&nbsp;structures around data visualization in the relatively&nbsp;unexplored space of Virtual Reality. The first demonstration,&nbsp;NodeitAll, is a Media Cloud link map that experiments with&nbsp;various ways to present edges and nodes, playing with&nbsp;interaction and manipulability. Developed in Unity, the&nbsp;scripting is open source so that anyone may input their own&nbsp;CSV files and have a randomly generated network graph.&nbsp;</p>", "people": ["mboya@media.mit.edu"], "title": "dataVisionaRi", "modified": "2019-04-22T18:11:31.980Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": null, "slug": "datavisionari"}, {"website": "", "description": "<p>Electronics are ubiquitous in today\u2019s world, with applications ranging from smart cities to health care, defense, economy, government, education, research, and entertainment. While the computing demands of these applications are ever increasing, the capabilities of electronics have hit fundamental limitations and have plateaued. In the past, the growth in computing capabilities of electronics has been sustained by scaling both the dimensions as well as power-supply voltage of the Field-Effect-Transistors (FETs), that form the building blocks of the integrated circuits. However,&nbsp;<b>as the scaling roadmap enters the sub-5 nm regime, today\u2019s transistor reaches its physical limits</b>&nbsp;in dimensional scalability and can no longer offer effective electrostatics, leading to exponential increase in static leakage power. In addition, the power supply voltage, which is the most effective way for lowering the dynamic power, can no longer be scaled effectively as in earlier technology generations. The&nbsp;<b>voltage in-scalability has fundamental roots in the thermal distribution of carriers</b>,&nbsp;<b>which limits the steepness of turn-on characteristics or subthreshold swing</b>&nbsp;of conventional FETs. These dimensional and voltage scalability issues, lead to exponential increase in power-density and has ushered in the dead end of the glorious growth episode of 'Information Society'. Moreover,&nbsp;<b>the fundamental nature of the problem suggests the inability of evolutionary solutions</b>&nbsp;to address this growing energy crisis and demands radically new innovations on multiple fronts.</p><p><span style=\"font-size: 18px; font-weight: 400;\">Our research involves a holistic approach towards solving this energy crisis, starting from exploration of beyond-Silicon nanomaterial technology, to in-depth understanding of the physics of fundamentally different device-working mechanisms and finally, experimental demonstration of novel, highly energy-efficient and scalable electronic devices.</span></p><p>We invented the&nbsp;<b>world\u2019s thinnest channel (6 atoms thick) quantum mechanical transistor&nbsp;</b>involving band-to-band-tunneling, which&nbsp;<b>overcomes the fundamental thermal limitations&nbsp;</b>in subthreshold swing&nbsp;and leads to record energy reduction by more than 75% [<a href=\"https://www.nature.com/articles/nature15387\"><b>D. Sarkar et.al., Nature, 526(7571),&nbsp;91-95, (2015)</b></a>] [Nature (News and Views)&nbsp;526, 51\u201352(2015)]. This atomically thin and layered semiconducting-channel tunnel FET (ATLAS-TFET), is based on the idea, that we conceived, of a unique tunneling heterojunction combining the best attributes of 3D (matured doping technology) and 2D (excellent electrostatics and ultra-low tunneling barrier) materials to achieve extremely efficient and controllable electron wave propagation through the energy barrier. This device is the&nbsp;<b>first and</b>&nbsp;<b>only tunneling-transistor till date, in any architecture and any material platform, to achieve ITRS prescription&nbsp;</b>of sub-thermal subthreshold swing over four decades of current at an ultra-low power-supply voltage of 0.1V(thus, allowing voltage scalability). Moreover, the atomically-thin 2D channel provides near-ideal device electrostatics, which&nbsp;<b>allows dimensional scalability to beyond Silicon scaling era</b><b>&nbsp;(sub 5nm)</b>. Thus, this device can crack the long-standing issue of simultaneous dimensional and power-supply voltage scalability.</p>", "people": ["deblina@media.mit.edu"], "title": "Ultra-Scalable and Energy-Efficient Nanoelectronics", "modified": "2018-09-10T21:04:38.259Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["nano-cybernetic-biotrek"], "published": true, "active": false, "end_on": null, "slug": "ultra-scalable-and-energy-efficient-nanoelectronics"}, {"website": "https://dci.mit.edu", "description": "<h1>Privacy-preserving auditing on distributed ledgers</h1><p>zkLedger is a project that combines techniques from modern cryptography to analyze private data, while at the same time ensuring the integrity of that analysis by committing to the private data on a blockchain that is verified by all participants.</p><p>zkLedger uses permissioned blockchains, zero-knowledge proofs, and additively homomorphic commitment schemes to create a tamper-resistant, verifiable ledger of transactions which hides the amounts, senders, and recipients of transactions, and still allows for rich auditing.</p><h2>Auditing complex systems increases confidence that said systems work as intended</h2><p>Lack of auditability or inaccurate results from auditing can have devastating effects, as demonstrated by the 2008 financial crisis. Traditionally, auditability for companies has been solved by the use of trusted third party auditors, such as the \u201cBig Four\u201d: Deloitte, PriceWaterhouseCoopers, Ernst and Young, and KPMG. Auditability for financial institutions and exchanges has been insured by federal and state government agencies such as the OCC, the FDIC, and SEC, to name just a few. Unfortunately, this type of auditing is a laborious, time-consuming process, that is far from real-time. Blockchain technology proposes an alternative, yet for that alternative to work, direct competitors would need to share information that they consider proprietary.</p><h2>Permissioned blockchains</h2><p>Recently, financial institutions have formed consortia to investigate the use of a different architecture for securities settlement, inspired by blockchain technology. Bitcoin\u2019s success has motivated institutions to consider upgrading their technical infrastructure by using permissioned blockchains, often maintained by participants with a consensus protocol. There are many strong players in this area that are making an impact, such as R3\u2019s Corda system and IBM\u2019s Hyperledger. With a large number of financial institutions already participating in these ledgers, what stands in the way of real-time auditing is a way to run computations on data while allowing participants to maintain the privacy of their data. This is where zkLedger can help.</p><h2>Zero-knowledge proofs</h2><p>Using zero-knowledge proofs, one party can prove that they know some secret information without revealing what that information is. One way to understand this is to look at an example: suppose that Alice has two billiard balls, one red and one green (they are otherwise identical). Bob, who is colorblind, cannot tell the difference between the balls, so he assumes that they are the same color. Alice wants to convince Bob that they are in fact different without revealing the colors of the balls to Bob, so Bob takes both balls, puts them behind his back, and either switches them or keeps them in the same hand. If Alice can correctly answer each time whether they have been switched or not, then she has some knowledge about the balls, but has never revealed what the color of either ball is. If she were to answer incorrectly once, then we know that she was guessing each time. We use zero-knowledge proofs in APL to ensure that transactions added to the ledger are consistent, and that auditing computations are performed correctly. Going further, zero-knowledge proofs can impact many aspects of the financial sector by providing both secrecy and accountability to financial institutions, and we\u2019re exploring new ways to leverage this technology.</p><h2>zkLedger uses and current status</h2><p>We are exploring non-financial uses cases for zkLedger.&nbsp; Our paper \"zkLedger: Privacy-Preserving Auditing on Distributed Ledgers\" will appear at&nbsp;<a href=\"https://www.usenix.org/conference/nsdi18/technical-sessions\">NSDI 2018</a>, and our prototype software will be released soon.</p><p><b>Read the paper below:</b></p>", "people": ["madars@media.mit.edu", "narula@media.mit.edu"], "title": "zkLedger: Privacy-Preserving Auditing", "modified": "2018-05-07T02:08:11.910Z", "visibility": "PUBLIC", "start_on": "2018-01-01", "location": "", "groups": ["digital-currency-initiative-dci"], "published": true, "active": false, "end_on": null, "slug": "zkledger-privacy-preserving-auditing"}, {"website": "", "description": "<p>Two-dimensional radiographs, while commonly used for evaluating sub-surface hard structures of teeth, have low sensitivity for early caries lesions particularly those on tooth occlusal surfaces. Radiographs are also frequently refused by patients over safety concerns. Translucency of teeth in the nearinfrared (NIR) range offers a non-ionizing and safe approach to detect dental caries. We report construction of a NIR (850 nm) LED imaging system, comprised of a NIR source and an intraoral camera for rapid dental evaluations. The NIR system was used to image teeth and successfully detected secondary, amalgam\u2013occluded and early caries lesions without supplementary image processing. The camera-wand system was also capable of revealing demineralized areas, deep and superficial cracks, and other clinical features of teeth usually visualized by X-rays. The NIR system\u2019s clinical utility, simplistic design, low cost and user friendliness makes it an effective dental caries screening technology in conjunction or in place of radiographs.</p>", "people": [], "title": "Near-Infrared Imaging for Detecting Caries and Structural Deformities in Teeth", "modified": "2018-05-03T17:47:20.946Z", "visibility": "PUBLIC", "start_on": "2017-04-12", "location": "", "groups": ["camera-culture"], "published": false, "active": false, "end_on": null, "slug": "near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth-1"}, {"website": "", "description": "<p>Two of the most important traits of environmental hazards today are their invisibility and the fact that they are experienced by communities, not just individuals. Yet we don't have a good way to make hazards like chemical pollution visible and intuitive.&nbsp; SeeBoat and the thermal fishing bob seek to visceralize rather than simply visualize data by creating a physical data experience that makes water pollution data present in communities.&nbsp;</p><p>SeeBoat is a remote control boat with sensors (temperature, turbidity, conductivity, pH) that measure local water quality and LEDs that&nbsp;display the data on site by changing color in real time. Data is also logged to be physically displayed elsewhere and can be further recorded using long-exposure photos.&nbsp;Making environmental data experiential and interactive will help both communities and researchers better understand pollution and its implications.</p><p>The Thermal Fishing Bob is an early version of this tool that has a spherical form factor and focuses on measuring water temperature as a marker for combined sewer overflows (CSOs) that may pollute rivers.&nbsp;&nbsp;</p><p>This project began in partnership with Sara Wylie (Northeastern University) in Spring 2015. Early work included Thermal Fishing Bob workshops, design iteration, prototyping, system testing with users in the Mystic River and Charles River, long exposure photography events, and further concept development. In Spring of 2017, Perovich and Wylie began a collaboration with Roseann Bongiovanni of GreenRoots, an&nbsp;environmental justice community group in Chelsea, MA, to test and iterate on the devices so they best suit the environmental and social context in the local community.&nbsp; As part of this process,&nbsp;Perovich continued to develop the technical side of the project to create SeeBoat, a remote control boat based system, including sensors for&nbsp;turbidity, conductivity, pH, radio based data communication, and designs for and early implementation of an Android app for collecting and viewing quantitative sensor data. Perovich, Wylie, and Bongiovanni are also pursuing related routes of research and community engagement around open access environmental data, the politics of space, community based data installations, and evaluating individual and group learning through extended participatory action research projects.&nbsp;<br></p><p>A publication describing their first year of collaboration can be found in their paper:&nbsp;&nbsp;</p><p>Laura J. Perovich, Sara Wylie, Roseann Bongiovanni (2018)&nbsp;Pok\u00e9mon Go, pH, and projectors: Applying transformation design and participatory action research to an environmental justice collaboration in Chelsea, MA,&nbsp;Cogent Arts &amp; Humanities,&nbsp;5:1,&nbsp;1-22. (<a href=\"https://www.tandfonline.com/doi/pdf/10.1080/23311983.2018.1483874\">Link to PDF</a>.)&nbsp;</p><p>In July of 2018, the team began to collaborate with high school students and staff at the Microsoft Garage Makerspace to test the ease of fabrication of SeeBoat in a more general audience and to continue development of the SeeBoat Android app for numeric data display.&nbsp;</p><p>Thanks to&nbsp;ECO, David Ortiz, Adela Gonzalez, Leo Martinez, GreenRoots staff, Don Blair, Catherine D\u2019Ignazio, the Boston University Law Clinic, and Dr. Sharon Harlan for their support and input on this project. Thanks to MIT undergraduates Sophia Struckman, Rod Bayliss, Robert Henning, and Claudia Chen who contributed to the technical aspect of these workshops and citizen science tool development, photographers Jorge Valdez and Shirin Adhami,&nbsp; the Wylie Lab at Northeastern University, Dr. V. Michael Bove and members of the Object-Based Media group at the MIT Media Lab, the MIT Arts Scholars, the Public Lab community,&nbsp; Mare Librum, the MIT Sailing Pavilion, and the Council for the Arts at MIT.</p>", "people": ["perovich@media.mit.edu", "vmb@media.mit.edu"], "title": "SeeBoat (Thermal Fishing Bob): In-place environmental data visualization", "modified": "2019-04-17T18:22:32.202Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "thermal-fishing-bob-in-place-environmental-data-visualization"}, {"website": "", "description": "<p class=\"\">Blockchain Certificates is a set of tools, software, and strategies to store and manage digital credentials. Certificates are registered on the bitcoin blockchain, cryptographically signed, and tamper-proof. They can represent or recognize many different types of achievements. After a number of prototypes (we issued digital credentials to Media Lab Director's Fellows and Media Lab alumni) we published our code under an open-source license to enable others to deploy the tools we developed. More information at&nbsp;<a href=\"http://blockcerts.org\" class=\"\">http://blockcerts.org</a>.&nbsp;</p>", "people": ["srishti@media.mit.edu", "ps1@media.mit.edu", "jnazare@media.mit.edu", "kamcco@media.mit.edu", "guyzys@media.mit.edu"], "title": "Digital Academic Credentials", "modified": "2019-01-31T17:23:57.299Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ml-learning"], "published": true, "active": false, "end_on": null, "slug": "media-lab-digital-certificates"}, {"website": "", "description": "<p>Games are a uniquely human endeavour, reducing stress and supporting mental well-being. Astronauts aboard the ISS have created their own games using materials at hand and pure creativity.  What if we could create games for them that took particular advantage of aspects of space, such as micro-gravity, and would help keep astronauts mentally engaged, socially connected, and physically relaxed?&nbsp; </p><p>Zero-G-ames is an ongoing series of workshops&nbsp;to explore, discuss, and design the history and future of games in constrained spaces and microgravity environments.</p><p>Let\u2019s play!</p>", "people": ["novysan@media.mit.edu", "jhaas@media.mit.edu"], "title": "Zero-G-ames", "modified": "2018-06-25T17:52:03.983Z", "visibility": "PUBLIC", "start_on": "2018-03-10", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "zero-g-ames"}, {"website": "", "description": "<p>Generative audio models based on neural networks have led to considerable improvements across fields including speech enhancement, source separation, and text-to-speech synthesis. These systems are typically trained in a supervised fashion using simple element-wise l1 or l2 losses. However, because they do not capture properties of the human auditory system, such losses encourage modeling perceptually meaningless aspects of the output, wasting capacity and limiting performance. Additionally, while adversarial models have been employed to encourage outputs that are statistically indistinguishable from ground truth and have resulted in improvements in this regard, such losses do not need to explicitly model perception as their task; furthermore, training adversarial networks remains an unstable and slow process.</p><p><br>In this work, we investigate an idea fundamentally rooted in psychoacoustics. We train a neural network to emulate an MP3 codec as a differentiable function. Feeding the output of a generative model through this MP3 function, we remove signal components that are perceptually irrelevant before computing a loss. To further stabilize gradient propagation, we employ intermediate layer outputs to define our loss, as found useful in image domain methods. Our experiments using an autoencoding task show an improvement over standard losses in listening tests, indicating the potential of psychoacoustically motivated models for audio generation.</p>", "people": ["ishwarya@media.mit.edu"], "title": "Towards a Perceptual Loss:  Using a neural network codec approximation as a loss for generative audio models", "modified": "2019-04-16T20:20:38.400Z", "visibility": "PUBLIC", "start_on": "2019-02-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "codec-perceptual-loss"}, {"website": "", "description": "<p>We are creating new communication interfaces for people with speech and language disorders, incorporating technologies like physiological sensing and personalized machine learning. The goal of our work is to identify gaps in the technologies that exist and what people need, and create and test devices that meet those needs.</p><p>Do you have a speech or language impairment, or have a close friend or family member who does? We would really appreciate your feedback on our <b>AAC device survey</b>, which will help us identify promising architectures for further development.</p><p>Click this<b>&nbsp;<a href=\"https://mit.co1.qualtrics.com/jfe/form/SV_6gmpNchtSf8vCLz\">link</a></b>&nbsp;to access the survey.</p><p>Sources for logo:</p><p><a href=\"https://svgsilh.com/image/40466.html\">Spell deaf talk speech</a></p><p>Information Computer Technology <a href=\"https://www.maxpixel.net/Information-Computer-Technology-Digital-Binary-3374456\">Digital Binary</a></p><p>Alternative Handicapped Accessible <a href=\"https://commons.wikimedia.org/wiki/File:Alternative_Handicapped_Accessible_sign.svg\">sign</a></p>", "people": ["jnarain@media.mit.edu"], "title": "User needs for augmentative communication interfaces", "modified": "2019-04-18T17:48:47.733Z", "visibility": "PUBLIC", "start_on": "2019-01-13", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "augmentative-communication-interfaces"}, {"website": "", "description": "<h1>Open-Source Autonomous Platform for Educational and Service Design Applications</h1><h2><i>How can new technologies respond to society\u2019s diverse industrial, socio-economic, and educational needs?</i></h2><p>Despite AI and robotics being widely trumpeted as keys to the new Industrial Revolution, access to their development remains largely restricted to companies and institutions that are rich in capital and/or data, potentially further deepening the socio-economic disparity observed across continents. As a likely result, these new technologies generate limited positive externalities. For instance, are automobiles really the most critical area in need of self-driving technology? Where else might AI and robotics be applied to lead to increased urban livability, socioeconomic equity, and the vibrancy of local businesses?</p><p>Building upon the architecture of MIT\u2019s open-source race car platform, the City Science group introduces&nbsp;a new open-ended and heavy-duty self-driving platform.&nbsp;&nbsp;Torque is intended to be used by educators and makers and is ideal for hackathons and classroom instruction. Torque will soon allow rapid prototyping of usage scenarios and services for various contexts and needs.&nbsp;</p>", "people": ["lukeji@media.mit.edu", "cq_zhang@media.mit.edu", "mcllin@media.mit.edu", "ptinn@media.mit.edu"], "title": "Torque", "modified": "2018-05-04T15:23:33.802Z", "visibility": "PUBLIC", "start_on": "2017-07-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "torque"}, {"website": "", "description": "<p>Biomarker imaging provides non-invasive indicators of disease and is used by human experts to augment disease diagnosis. It is, however, often expensive and reliant on experts to interpret the resulting images. We have developed a process for learning associations between standard white light images and both biomarkers and expert annotations of disease. Oral imaging is one particular example of biomarker imaging that can supplement expert knowledge; the biomarker porphyrin is associated with poor oral health and oral cancer. We report that our process learns to accurately predict the presence of porphyrin and expert-annotated conditions.</p><p><strong>Why is this work important?</strong></p><p>Biomarker imaging provides non-invasive indicators of disease and is used by human experts to augment disease diagnosis. Capturing biomarker images requires specialized and often expensive hardware, annotations, and analyses by experts, resulting in substantial diagnosis delays.</p><p><strong>What has been done before?</strong></p><p>Even when biomarker imaging is available, experts are often needed to interpret the resulting images. There is a rich literature on medical image segmentation, but many approaches\u2014especially deep learning\u2014require large amounts of images and operate on information from only a single given imaging modality.</p><p><strong>What are our contributions?</strong></p><p>We successfully learn assocations between images and union signatures of biomarker presence and expert disease annotations. By transforming the image-level segmentation problem into a region-based problem, we are able to learn from far fewer images than other approaches. We specifically test our approach on detecting the biomarker porphyrin and associated conditions in millions of image patches. Once trained, the classifiers predict the location of porphyrin in images without requiring specialized biomarker imaging devices or expert intervention.</p><p><strong>What are the next steps?</strong></p><p>We are developing processes incorporating numerous other biomarkers, conditions, and imaging modalities.</p><p><strong>Related projects</strong></p><ol><li><a href=\"https://www.media.mit.edu/projects/porphyrin-imaging/overview/\">Biomarker Imaging with Mobile Phones</a></li></ol>", "people": ["pratiks@media.mit.edu", "gyauney@media.mit.edu", "kla11@media.mit.edu"], "title": "Machine Learning for Combined Classification of Fluorescent Biomarkers and Expert Annotations Using White Light Images", "modified": "2018-08-15T19:45:24.252Z", "visibility": "PUBLIC", "start_on": "2017-02-01", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "machine-learning-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images"}, {"website": "https://inequality.media.mit.edu", "description": "<h2><b>Segregation is hurting our societies and especially our cities. But economic inequality isn't just limited to neighborhoods. The restaurants, stores, and other places we visit in cities are all unequal in their own way.&nbsp;</b></h2><p>The Atlas of Inequality &nbsp;shows the income inequality of people who visit different places in the Boston metro area. It uses aggregated anonymous location data from digital devices to estimate people's incomes and where they spend their time.&nbsp;Using that data, we've made our own <b>place inequality </b><b>metric</b> to capture how unequal the incomes of visitors to each place are. Economic inequality isn't just limited to neighborhoods; it's part of the places you visit every day.</p><p>Try it yourself here:</p><h2><a href=\"http://inequality.media.mit.edu\"><b>The Atlas of Inequality</b></a></h2><p>The Atlas of Inequality is a project from the Human Dynamics group at the <a href=\"https://www.media.mit.edu/\">MIT Media Lab</a> and the Department of Mathematics at <a href=\"http://www.uc3m.es/\">Universidad Carlos III de Madrid</a>.</p><p>It is part of a broader initiative to understand human behavior in our cities and how large-scale problems like transportation, housing, segregation, or inequality depend in part on the emergent patterns of people\u2019s individual opportunities and choices.</p>", "people": ["dcalacci@media.mit.edu", "sandy@media.mit.edu", "emoro@media.mit.edu", "xdong@media.mit.edu"], "title": "The Atlas of Inequality", "modified": "2019-03-20T17:07:49.005Z", "visibility": "PUBLIC", "start_on": "2019-03-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "the-atlas-of-inequality"}, {"website": "https://www.linkedin.com/in/johndelaparra/", "description": "<h1><b>OpenPharm</b></h1><h2><b>The Future of Pharmacy&nbsp;</b></h2><p>What if we could produce <b>new and complex plant-derived pharmaceuticals</b> with the same <b>reliability and reproducibility </b>that has been developed with traditional bioreactor technology\u2014without the time and expense of genetic manipulation and specialized cell culture protocols?&nbsp;</p><p><a href=\"http://openag.mit.edu\">MIT\u2019s Open Agriculture Initiative </a>is developing just such a solution for <b>whole medicinal plants</b> by precisely tuning the entire organism\u2019s metabolism in modified <a href=\"https://www.media.mit.edu/projects/food-server/overview/\">Food Servers</a>,&nbsp; highly controlled environments tied to an<b> integrated bioinformatics platform </b>and the <a href=\"https://www.media.mit.edu/projects/open-phenome-project/overview/\">Open Phenome</a>.&nbsp;</p><p>Data derived from these networked environments allows for <b>computer modeling, machine learning, and simulations</b>\u2014all working to predict drug production optimization within the plant. By embracing the complexity of big data we are able to discern the specifics of environmental stimuli and a plant\u2019s precise metabolic response.</p><p>This technology also has exciting <b>global health implications</b>. The WHO estimates that <b>over 80 percent of the world relies on plant-based medicine</b> for some part of primary healthcare. However, many of these plants can be rare, highly endemic, or endangered species with limited natural population ranges\u2014for which bulk collections would not be sustainable.&nbsp;</p><p>At OpenAg it becomes possible to <b>grow a wide variety of medicinal plants in precisely controlled environments</b>, elicit with environmental cues to spike and diversify metabolite levels, and then extract higher yields for further analysis and production.</p>", "people": ["delapa@media.mit.edu"], "title": "OpenPharm", "modified": "2018-10-20T16:49:29.218Z", "visibility": "LAB-INSIDERS", "start_on": "2018-08-27", "location": "", "groups": ["open-agriculture-openag"], "published": true, "active": false, "end_on": null, "slug": "openpharm"}, {"website": "", "description": "<p><i>\"The sense of freedom\u2026entails not simply the absence of frustration but the absence of obstacles to possible choices and activities\u2014absence of obstructions on roads along which a man can decide to walk.\"&nbsp;</i>\u2014Isaiah Berlin</p><p>Can we understand freedom as a subjective sensation?</p><p>The Freedom Simulator&nbsp;is a set of three experiences that aim to induce a feeling of freedom. These experiences question the manifestation and significance of freedom in our everyday lives.</p><p>Each experience is based on a modern political philosophy perspective on freedom:&nbsp;&nbsp;positive freedom, negative freedom, and freedom in light of ethical individualism. Various techniques such as spatial audio, motion tracking, and real-time video projection are utilized.</p>", "people": ["hanelee@media.mit.edu"], "title": "Freedom Simulator", "modified": "2019-04-23T16:53:35.179Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "freedom-simulator"}, {"website": "", "description": "<p>PARTICLES is a modular hardware system that can dynamically control attraction forces in-between, aiming to provide malleable tangible affordances (e.g. squishing, splitting, and scooping). Each module contains Spherical Gears + Switchable Magnet to variably control the attraction force on the surface. Having a number of these modules, we aim to dynamically tune perceived material property through the attraction force control. We imagine scalable applications such as a novel tangible display for data exploration in medical and geo-science. Another application can be an educational tool that people can learn nano-scale molecule structures and attraction forces through tangible interaction with particles.&nbsp;</p>", "people": ["ishii@media.mit.edu", "ken_n@media.mit.edu"], "title": "PARTICLES", "modified": "2018-05-23T19:46:16.346Z", "visibility": "LAB-INSIDERS", "start_on": "2017-10-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "particles"}, {"website": "", "description": "<h2>Algorithmic Fairness</h2><p>Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Substantial work in algorithmic fairness has surged, focusing on either post-processing trained models, constraining learning processes, or pre-processing training data.&nbsp;Recent work has proposed optimal post-processing methods that randomize classification decisions on a fraction of individuals in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concerns due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail.&nbsp;</p><h2>Active Fairness</h2><p>The present work proposes an alternative <b>active framework for fair classification</b>, where, in deployment, a decision maker adaptively acquires information according to the needs of different groups or individuals towards balancing disparities in classification performance. We propose two such methods where information collection is adapted to group- and individual-level needs, respectively. We show on real-world datasets that these can achieve: 1) <b>calibration and single error parity</b> (e.g., equal opportunity) and 2) <b>parity in both false positive and false negative rates</b> (e.g., equal odds). Moreover, we show that, by leveraging their additional degree of freedom, active approaches can <b>outperform randomization-based classifiers previously considered optimal</b>, while also avoiding limitations such as intra-group unfairness.</p>", "people": ["bakker@media.mit.edu", "sandy@media.mit.edu", "noriega@media.mit.edu"], "title": "Active Fairness in Algorithmic Decision Making", "modified": "2019-01-02T20:52:50.453Z", "visibility": "PUBLIC", "start_on": "2018-01-31", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "active-fairness"}, {"website": "", "description": "<h1>Cloud-Based Urban Data Platform</h1><p><br>cityIO (input/output) is a cloud- and database-driven platform which allows remote participation, database augmentation, and high-end complex visualization. cityIO operates anywhere, on multiple platforms and devices, using client-side apps or web-based interfaces.&nbsp;The cityIO platform is built for scale and to serve large volumes of end-users in real time, in order to augment multi-participant discussions and decision-making processes. Utilizing the mass adaptation of mobile and hand-held devices, cityIO promotes an equal and decentralized discussion for multiparty stakeholders.&nbsp;cityIO offers a suite of augmented reality data-visualization tools that utilize server-side data and analysis. cityIO allows client-side interactions in multiple forms:</p><p><b>AUGMENTED REALITY URBAN SIMULATION</b></p><p>cityIO is intended to reduce complexity in design and planning tools and to support  data-driven environments for planners, designers, and decision makers. cityIO uses modern simulation tools and employs cutting-edge AR applications in order to offer an immersive user experience for both planning professionals and the general public. These simulations can augment indoor and outdoor environments, physical models, and technical drawings. </p><p><b>REMOTE AND DECENTRALIZED PUBLIC PARTICIPATION</b></p><p>Using self-explanatory web and mobile apps with high-end visualization and user interfaces, cityIO offers cities, municipalities, and planning authorities the ability to better communicate complex planning processes and to aggregate the public's opinion in real time. cityIO's scalable server side allows multiple users to collaborate, participate, and voice their opinions on design and planning initiatives.  </p><p><b>CITYI/O HAMBURG<br></b></p><p>cityIO Hamburg augments cityMatrix table. This deployment allows design in the urban context of the Rothenburgsort neighborhood.</p>", "people": ["noyman@media.mit.edu"], "title": "cityIO", "modified": "2018-10-20T17:04:48.161Z", "visibility": "PUBLIC", "start_on": "2016-05-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "cityio"}, {"website": "", "description": "<p><strong>LeakyPhones</strong>&nbsp;is a public/private headset that was designed to encourage face-to-face interactions, curiosity, and&nbsp;healthier&nbsp;social skills by letting users \"peek\" into each other's music just by looking at one another.&nbsp;</p><p>Gaze is an important social signal in human interaction.&nbsp;Though its interpretation may vary across cultures, it is generally agreed that eye contact indicates interest&nbsp;and the point of attention in a conversation. Despite this, many common personal computing technologies, such as our smartphones and headphones, require significant visual and auditory attention thereby inhibiting our ability&nbsp;to interact with others. LeakyPhones offers a new approach for addressing this challenge.&nbsp;</p>", "people": ["amosg@media.mit.edu", "achituv@media.mit.edu", "ishii@media.mit.edu"], "title": "LeakyPhones", "modified": "2019-02-26T20:28:41.011Z", "visibility": "PUBLIC", "start_on": "2017-03-16", "location": "", "groups": ["tangible-media", "fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "leakyphones"}, {"website": "", "description": "<p>The outermost skin of a space-based structure is designed using materials known to protect against the harsh elements of space. Simultaneously, the skin provides a unique opportunity to characterize the environment proximate to a spacecraft and to perform real-time damage detection. Thus, we propose developing an aerospace-grade fabric that simultaneously senses and protects, emulating the dual protective and sensory capabilities of biological skin.&nbsp;Aerospace-grade sensory skins will serve a key role in&nbsp;next generation haptic feedback systems for spacesuits (see<a href=\"https://www.media.mit.edu/projects/spaceTouch/overview/\"> SpaceTouch application area</a>), as well as next generation thermal blankets for distributed detection of high velocity debris impact.&nbsp;</p><p>For example, Beta Cloth\u2014the outermost layer of the International Space Station\u2014is particularly resilient to atomic oxygen erosion and extended UV radiation exposure. It is also regularly exposed to high velocity debris impact. We draw from recent advances in functional fibers and electronic textiles in order to weave and coat sensors directly into the teflon-coated fiberglass that comprises Beta Cloth, enabling the skin to detect and characterize impact events. We seek to demonstrate that the well-characterized, protective properties of aerospace-grade woven materials can be preserved even when modified to include sensory functionality. </p><p>Our work begins by examining integrated piezoelectric yarns and piezoelectric ink coatings for vibration sensing. We will also consider state-of-the-art manufacturing methods (e.g., device-in-fiber technology [1]) and intriguing high velocity impact sensing modalities; e.g., detection of impact plasma RF emission [2]). Other possibilities include the use of free-flying external optical sensors that collaborate with the skin to assess damage (\"Skin and Eyes\"), as well as work towards making the sensory skin robust to cutting, sewing, and wrapping.&nbsp; &nbsp;&nbsp;</p><p>[1] Tao, Guangming, et al. \"Multimaterial fibers.\" Lab-on-Fiber Technology. Springer, Cham, 2015. 1-26.</p><p>[2] Lee, N., et al. \"Measurements of freely-expanding plasma from hypervelocity impacts.\" International Journal of Impact Engineering 44 (2012): 40-49.</p>", "people": ["cherston@media.mit.edu"], "title": "SpaceSkin: Aerospace-grade electronic textiles for distributed sensing on persistent orbital structures", "modified": "2019-04-18T14:52:40.026Z", "visibility": "PUBLIC", "start_on": "2018-05-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "SpaceSkin"}, {"website": "", "description": "<p>Virtual and augmented reality headsets are unique as they have access to our facial area, an area that presents an excellent opportunity for always-available input and insight into the user's state. Their position on the face makes it possible to capture bio-signals as well as facial expressions.&nbsp; The&nbsp; PhysioHMD platform&nbsp;introduces a software and hardware modular interface built for collecting affect and physiological data from users wearing a head-mounted display. The platform enables researchers and developers to aggregate and interpret signals in real-time and use them to develop novel, personalized interactions, as well as evaluate virtual experiences. Our design offers seamless integration with standard HMDs, requiring minimal setup effort for developers and those with less experience using game engines. The PhysioHMD platform is a flexible architecture that offers an interface that is not only easy to extend but also complemented by a suite of tools for testing and analysis. We hope that PhysioHMD can become a universal, publicly available testbed for VR and AR researchers.</p><p>To create a seamless experience, we have integrated several bio-signal sensors into the faceplate of an HTC VIVE VR headset and utilized the Shimmer3 sensor for emotion-sensing. For the collection of Galvanic Skin Response, dry electrodes were positioned on the forehead area due to the fact that it is one of the areas most dense with sweat glands. GSR data reflects emotional arousal, but in order to identify how arousal, valence, motivation, and cognition interact in response to physical or psychological stimuli, it becomes necessary to complement GSR with other biosensors. For the heart rate, a PPG (photoplethysmogram) sensor, which senses the rate of blood flow by utilizing light to monitor the heart\u2019s pumping action, was placed in the temple region of the user. This is done to get insights into the respondent's physical state, anxiety and stress levels (arousal), and to determine how changes in their physiological state relate to their actions and decisions.<br></p>", "people": ["gbernal@media.mit.edu", "yangtao@media.mit.edu", "pattie@media.mit.edu"], "title": "PhysioHMD", "modified": "2018-11-14T19:05:00.306Z", "visibility": "PUBLIC", "start_on": "2017-02-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "physiohmd"}, {"website": "", "description": "<p>Suitable for anywhere a \"Pepper's Ghost\" display could be deployed, this display adds 3D with motion parallax, as well as optically relaying the image into free space such that gestural and haptic interfaces can be used to interact with it. The current version is able to display a person at approximately full-size.  </p>", "people": ["novysan@media.mit.edu", "vmb@media.mit.edu"], "title": "Aerial Light-Field Display", "modified": "2019-02-18T02:25:17.276Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "ce-20", "future-storytelling", "object-based-media"], "published": true, "active": false, "end_on": null, "slug": "aerial-light-field-display"}, {"website": "", "description": "<p><i>Paper presented at the \"Connected Life 2019: Data &amp; Disorder\" conference at the Oxford Internet Institute.</i></p><p>High-resolution individual geolocation data passively collected from mobile phones is increasingly sold in private markets and shared with researchers.</p><p>This data poses significant security, privacy, and ethical risks: it\u2019s been shown that users can be re-identified in such datasets, and its collection rarely involves their full consent or knowledge. This data is valuable to private firms (e.g. targeted marketing) but also presents clear value as a public good. Recent public interest research has demonstrated that high-resolution location data can more accurately measure segregation in cities and provide inexpensive transit modeling. But as data is aggregated to mitigate its re-identifiability risk, its value as a good diminishes. How do we rectify the clear security and safety risks of this data, its high market value, and its potential as a resource for public good? We extend the recently proposed concept of a tradeoff curve that illustrates the relationship between dataset utility and privacy. We then hypothesize how this tradeoff differs between private market use and its potential use for public good. We further provide real-world examples of how high resolution location data, aggregated to varying degrees of privacy protection, can be used in the public sphere and how it is currently used by private firms.</p>", "people": ["dcalacci@media.mit.edu", "kll@media.mit.edu", "sandy@media.mit.edu", "aberke@media.mit.edu"], "title": "The Tradeoff Between the Utility and Risk of Location Data and Implications for Public Good", "modified": "2019-05-24T15:26:51.045Z", "visibility": "LAB", "start_on": "2019-03-01", "location": "", "groups": ["human-dynamics", "city-science"], "published": false, "active": false, "end_on": null, "slug": "the-trade-off-between-the-utility-and-privacy-risks-of-location-data-and-implications-for-data-as-a-public-good"}, {"website": "https://affectivenetwork.media.mit.edu", "description": "<p><a href=\"https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US\">Try Affective Network!</a></p><p><span style=\"font-size: 18px;\">Emotional contagion in online social networks has been of great interest over the past years. Previous studies have mainly focused on finding evidence of affection contagion in homophilic atmospheres. However, these studies have overlooked users' awareness of the sentiments they share and consume online. In this work, we present an experiment with Twitter users that aims to help them better understand which emotions they experience on this social network. We introduce&nbsp;</span><a href=\"https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US\" style=\"font-size: 18px;\"><b>Affective Network</b></a><span style=\"font-size: 18px;\">&nbsp;(Aff-Net), a Google Chrome extension that enables Twitter users to filter and make explicit (through colored visual marks) the emotional content in their news feed.</span><br></p><p>The extension is powered by machine learning algorithms that classify tweets into different sentiment categories: positive posts tend to use happy or surprising language; negative posts tend to use sad, angry, or disgusting language; and posts without strong emotional language are classified as neutral.</p><p>Affective Network aims to help social media users better understand which emotions they tend to consume on social media, and how these emotions can spread through their social networks. It was built by researchers at the <a href=\"https://www.media.mit.edu/groups/social-machines/overview/\">Laboratory for Social Machines&nbsp;</a>and the&nbsp;<a href=\"https://www.media.mit.edu/groups/affective-computing/overview/\">Affective Computing</a> group at the <a href=\"https://www.media.mit.edu/\">MIT Media Lab</a>.</p><p>Note that Affective Network does not necessarily reflect the official position of the MIT Media Lab regarding the benefits and drawbacks of filtering out specific emotional content.</p><p><a href=\"https://chrome.google.com/webstore/detail/affective-network/ockbdbdpdiabkkdpcbdhcmmkkbfhigcc?hl=en&amp;gl=US\">Try Affective Network!</a></p>", "people": ["dkroy@media.mit.edu", "picard@media.mit.edu", "belen@media.mit.edu"], "title": "Affective Network", "modified": "2019-05-17T19:51:18.500Z", "visibility": "PUBLIC", "start_on": "2018-12-01", "location": "", "groups": ["social-machines", "affective-computing"], "published": true, "active": false, "end_on": null, "slug": "affective-network"}, {"website": "", "description": "<p>The ability to automatically understand and infer characters' goals and their emotional states is key towards better narrative comprehension. Reasoning about mental representations of various characters in a narrative has been referred to as Theory of Mind (ToM) reasoning. In this work, we propose an unsupervised neural network that  exploits the personal stories on social media and incorporates commonsense knowledge about characters' motivations and reactions to generate interpretable trajectories of characters' mental states. We find that our model is capable of learning coherent mental representations from characters' actions and their affect states. We evaluate our model using a publicly available dataset for mental state tracking of characters in short commonsense stories.&nbsp;</p>", "people": ["dkroy@media.mit.edu", "bridgitm@media.mit.edu", "pralav@media.mit.edu"], "title": "Story Comprehension", "modified": "2019-04-18T15:04:09.320Z", "visibility": "PUBLIC", "start_on": "2019-02-10", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "story-comprehension"}, {"website": "", "description": "<p>Our initial experiments in spark electrosintering fabrication have demonstrated a capacity to solidify granular materials (35-88 micron soda ash glass powder) rapidly using high voltages and power in excess of 1 kW. The testbed high-voltage setup comprises a 220V 60A variable autotransformer and a 14,400V line transformer. There are two methods to form members using electrosintering: the one-electrode drag (1ED) and two-electrode drag (2ED) techniques. The 1ED leaves the first electrode static while dragging the second through the granular mixture. This maintains a live current through the drag path and increases the thickness of the member due to the dissipation of heat. Large member elements have been produced with a tube diameter of around 0.75\". The 2ED method pulls both electrodes through the granular mixture together, sintering the material between the electrodes in a more controlled manner.</p>", "people": ["stevenk@media.mit.edu", "j_klein@media.mit.edu"], "title": "Additive Manufacturing in Glass: Electrosintering and spark gap glass", "modified": "2019-04-18T19:43:07.523Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "additive-manufacturing-in-glass-electrosintering-and-spark-gap-glass"}, {"website": "", "description": "<p>OpenIdeo put out a challenge to create a book targeting children ages 0-3 that would inspire children and their caregivers to read together. The challenge encouraged writers to create stories that center underrepresented identities and reflect the lives of urban communities like Philadelphia.</p><p>We are creating a story that draws parallels between the life of a little Black girl and the life of an astronaut by juxtaposing typical activities like getting ready in the morning, taking the bus or having family gatherings with the activities that an astronaut is expected to conduct for their job requirements. The message of the story is that astronauts, while few in number, are still just normal human beings with a career that children of any identity should be able to see themselves in.</p>", "people": ["lizbethb@media.mit.edu", "ufuoma@media.mit.edu"], "title": "Early Childhood Book Challenge", "modified": "2019-04-22T18:25:19.612Z", "visibility": "LAB-INSIDERS", "start_on": "2019-04-12", "location": "", "groups": ["space-enabled"], "published": true, "active": false, "end_on": null, "slug": "children-s-book-ch"}, {"website": "http://www.drugastefania.com/", "description": "<p>Cognimates&nbsp;is  a platform where parents and children (7-10 years old) participate in creative programming activities in which they learn how to build games, program robots, and train their own AI models. Some of the activities are mediated by embodied intelligent agents which help learners scaffold learning and better collaborate. <a href=\"http://cognimates.me/\">Learn more about our research, projects, and learning guides</a>.</p><p>Conversational agents and connected toys are becoming common in homes. Increasing exposure to \"intelligent\" technology raises important questions about the ways that children understand it and how they could learn&nbsp;with and from it. Embodied intelligent agents, such as social robots, afford longer-term engagement in the home for children and their families .&nbsp;&nbsp;</p><p>Building on the prior experience in the <a href=\"http://www.media.mit.edu/groups/personal-robots\">Personal Robots</a> group of designing social robots for nurturing children's curiosity and learning, we built a platform where children and parents can learn to program with embodied intelligent agents which in turn become learning companions (Cognimates). The goal is to enable&nbsp; learners&nbsp; to interact with a social robot but also program it, train it to remember and learn things over time, and have&nbsp; reflective conversations with their peers prompted by it.&nbsp;</p><p>Why, how, and when can embodied intelligent agents support children and parents to learn via reflective teaching? What are the new intergenerational learning pathways that Cognimates could facilitate? How can these future learning companions be integrated into various learning applications and what are the generalizable design considerations?&nbsp; In this research project we are addressing these questions by allowing children and parents to use a visual programming interface to control and customize an embodied intelligent agent.&nbsp;</p><p><a href=\"https://drive.google.com/file/d/0B8pX8mypq8MsUnJNbHNFUjI0RmM/view?usp=sharing\">Demo video</a></p>", "people": ["cynthiab@media.mit.edu", "sdruga@media.mit.edu"], "title": "Cognimates: Collaborative creative learning with embodied intelligent agents", "modified": "2019-03-27T16:43:04.722Z", "visibility": "PUBLIC", "start_on": "2017-11-12", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "cognimates"}, {"website": "", "description": "<p>New technologies for recording neural activity, controlling neural activity, or building brain circuits, may be capable someday of serving in therapeutic roles for improving the health of human patients - enabling the restoration of lost senses, the control of aberrant or pathological neural dynamics, and the augmentation of cognition and empathy, through prosthetic means. High throughput molecular and physiological analysis methods may also open up new diagnostic possibilities. We are inventing new noninvasive methods for targetedly controlling brain dynamics in living human subjects, and also exploring novel ways of reading activity from the brain in noninvasive fashion. We are assessing, often in collaborations with other groups, the translational possibilities opened up by our technologies, exploring the safety and efficacy of our technologies in multiple animal models, in order to discover potential applications of our tools to various clinically relevant scenarios. New kinds of \"brain co-processor\" may be possible which can work efficaciously with the brain to augment its computational abilities, e.g. in the context of cognitive, emotional, sensory, or motor disability.</p>", "people": ["esb@media.mit.edu"], "title": "Prototype strategies for treating brain disorders", "modified": "2019-04-17T18:27:33.446Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": null, "slug": "prototype-strategies-for-treating-brain-disorders"}, {"website": "", "description": "<p>What it is.</p><p>Where it is going.</p><p>List the projects.</p>", "people": ["zive@media.mit.edu", "groh@media.mit.edu"], "title": "creAItivity", "modified": "2019-02-14T19:47:57.622Z", "visibility": "PUBLIC", "start_on": "2018-01-01", "location": "", "groups": ["scalable-cooperation"], "published": false, "active": false, "end_on": null, "slug": "creaitivity"}, {"website": "http://www.curiouslearning.org/", "description": "<p>Early literacy plays an important role in a child's future. However, the reality is that over 57 million children have no access to a school and another 100 million attend such inadequate schools that they will remain functionally non-literate.</p><p>Curious Learning is an open platform that addresses the deployment and learning challenges faced by under-resourced communities, particularly their limited access to literacy instruction.</p><p>We are developing a system of early literacy apps, games, toys, and robots that will triage how children are learning, diagnose literacy deficits, and deploy dosages of content to encourage app play using a mentoring algorithm that recommends an appropriate activity given a child's progress. Currently, over 200 Android-based tablets have been sent to children around the world; these devices are instrumented to provide a very detailed picture of how kids are using these technologies. We are using this big data to discover usage and learning models that will inform future educational development.&nbsp; The open-source software enables any Android device to be transformed into a literacy mentor. This platform is presently deployed in Ethiopia, Uganda, India, South Africa, and rural United States.</p><p>The open-source tablet software enables data collection across the deployment sites.  By employing a data-driven approach to understanding learning behaviors across cultures and contexts, this project seeks to design and develop a personalized, adaptive learning platform.&nbsp;</p>", "people": ["cynthiab@media.mit.edu", "nikhita@media.mit.edu", "pcuellar@media.mit.edu"], "title": "Curious Learning: Understanding learning behaviors for early literacy", "modified": "2019-04-17T18:41:03.019Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "curious-learning"}, {"website": "", "description": "<p>an antiracist, purposed-based, creative learning guide to engaging non-dominant youth in STEM activities</p>", "people": [], "title": "Uncovering Hidden Pathways", "modified": "2018-11-18T20:27:29.087Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": [], "published": false, "active": false, "end_on": null, "slug": "uncovering-hidden-pathways"}, {"website": "", "description": "<p>The <b>Places</b> dataset&nbsp;(<a href=\"http://places2.csail.mit.edu/\">website</a>)&nbsp;is designed following principles of human visual cognition. Our goal is to build a core of visual knowledge that can be used to train artificial systems for high-level visual understanding tasks, such as <b>scene context</b>, object recognition, action and event prediction, <b>emotion recognition, or theory-of-mind inference</b>.&nbsp;</p><p>You can try our <a href=\"http://places2.csail.mit.edu/demo.html\">online demo</a>.</p>", "people": ["agata@media.mit.edu"], "title": "Place Recognition and Categorization", "modified": "2018-10-22T19:53:47.491Z", "visibility": "PUBLIC", "start_on": "2014-10-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "place-recognition-and-categorization-1"}, {"website": "", "description": "<p>QR Websites serves websites without the need for an Internet connection. The approach uses a minified site, encoded as a Data URI on a QR code, to enable a smooth interaction: scan the QR code, and open the URI in the address bar of any browser. Applications that involve sending data, such as webforms or quizzes, work by using a response formatting engine to send the data as SMS messages. Using this approach, developers can create forms, quizzes, educational tools, games, and demonstrations without a connection to the Internet, all while using what people likely have in their pockets: a mobile phone with a browser, and any QR code scanning app.&nbsp;</p>", "people": ["lip@media.mit.edu"], "title": "QR Sites", "modified": "2019-04-17T14:21:29.384Z", "visibility": "PUBLIC", "start_on": "2019-04-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2019-09-30", "slug": "qr-sites"}, {"website": "https://plix.media.mit.edu", "description": "<p>Public libraries are one of most trusted public institutions in the US and increasingly provide a broad range of education services, ranging from early learning programs, to maker spaces, to adult training. Libraries are not storage places for books, but communities for social change and innovation. The Public Library Innovation Exchange connects Media Lab researchers with librarians to develop new creative learning programs together. We will host how-to materials that help public libraries deploy projects developed at the Media Lab, such as &nbsp;<a href=\"http://www.chicagotribune.com/bluesky/series/gadgets/ct-innovative-gifts-bsi-photos-20151210-001-photo.html\">Circuit Stickers</a> or <a href=\"https://www.youtube.com/watch?v=rfQqh7iCcOU\">Makey Makey</a>, and we will offer scholarships to support exchanges and residencies to foster collaborative research going forward. The project is funded by Knight Foundation.&nbsp;</p><p>Visit our project website to learn more: <a href=\"http://plix.media.mit.edu\">plix.media.mit.edu</a></p>", "people": ["hbailey@media.mit.edu", "lguterma@media.mit.edu", "ps1@media.mit.edu", "kamcco@media.mit.edu", "cohenm@media.mit.edu"], "title": "Public Library Innovation Exchange", "modified": "2019-05-30T18:56:51.688Z", "visibility": "PUBLIC", "start_on": "2017-03-15", "location": "", "groups": ["ml-learning"], "published": true, "active": false, "end_on": null, "slug": "public-library-innovation-exchange"}, {"website": "http://www.computerclubhouse.org/teensummit", "description": "<p>Teen Summit is a biennial week-long Youth Leadership event that brings Clubhouse youth together from each of the 100 Clubhouses internationally. Youth leaders explore issues relevant to them and propose solutions through the creative use of innovative, high-end technologies. The 2018 Teen Summit will take place in late July at Boston University, featuring a college and career fair, collaborative cross-cultural activities, and many other opportunities for educational, career, and personal growth.</p>", "people": ["jaleesat@media.mit.edu", "mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "Clubhouse Teen Summit", "modified": "2018-11-03T16:33:08.852Z", "visibility": "PUBLIC", "start_on": "2017-10-02", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "teen-summit"}, {"website": "", "description": "<p>Earlier studies proved that behavior is highly shaped and constrained by one's social networks, and demonstrated ways in which individuals can manipulate these networks to achieve specific goals. A great example is the much-studied \"strength of weak ties\" hypothesis, which states that the strength of a tie between A and B increases with the overlap of their friendship circles, resulting in an important role for weak ties in connecting communities. Mark Granovetter first proposed this idea in a study that emphasized the nature of the tie between job changers in a Boston suburb and the contacts who provided the necessary information for them to obtain new employment.  Basically, although people with whom the job seekers had strong ties were more motivated to provide information, the structural position of weak ties played a more important role. The implication is that those to whom one is weakly tied are more likely to move in different circles, and will thus have access to different information than the people to whom you are tied more strongly. </p><p>Much of our knowledge about how mobility, social networks, communication, and education affect the economic status of individuals and cities has been obtained through complex and costly surveys, with an update rate ranging from fortnights to decades. However, recent studies have shown the value of mobile phone data as an enabling methodology for demographic modeling and measurement.</p><p>Many of our daily routines are driven by activities either afforded by our economic status or related to maintaining or improving it, from our movements around the city, to our daily schedules, to our communication with others. As such, we expect to be able to measure passive patterns and behavioral indicators, using mobile phone data, that could describe local unemployment rates.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">To investigate this question, we examined anonymized mobile phone metadata combined with beneficiaries' records from an unemployment benefit program. We found that aggregated activity, social, and mobility patterns strongly correlate with unemployment. Furthermore, we constructed a simple model to produce accurate reconstructions of district-level unemployment from mobile communication patterns alone.</span></p><p>Our results suggest that reliable and cost-effective indicators of economic activity could be built based on passively collected and anonymized mobile phone data. With similar data being collected every day by telecommunication services across the world, survey-based methods of measuring community socioeconomic status could potentially be augmented or replaced by such passive sensing methods.</p>", "people": ["amaatouq@media.mit.edu", "sandy@media.mit.edu"], "title": "Social Physics of Unemployment", "modified": "2019-06-05T19:17:28.176Z", "visibility": "PUBLIC", "start_on": "2014-07-23", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": null, "slug": "social-physics-of-unemployment"}, {"website": "", "description": "<p>Adaptive Music for Affect Improvement (AMAI) is a music generation and playback system with the goal of steering the listener toward a state of more positive affect. AMAI utilizes techniques from game music in order to adjust elements of the music being heard; such adjustments are made adaptively in response to the valence levels of the listener as measured via facial expression and emotion detection.</p>", "people": ["picard@media.mit.edu", "davidsu@media.mit.edu"], "title": "Adaptive Music for Affect Improvement", "modified": "2019-01-22T15:44:58.691Z", "visibility": "PUBLIC", "start_on": "2017-10-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "adaptive-music-for-affect-improvement"}, {"website": "", "description": "<p>Rapid prototyping technologies speed product design by facilitating visualization and testing of prototypes. However, such machines are limited to using one material at a time; even high-end 3D printers, which accommodate the deposition of multiple materials, must do so discretely and not in mixtures. This project aims to build a proof-of-concept of a 3D printer able to dynamically mix and vary the ratios of different materials in order to produce a continuous gradient of material properties with real-time correspondence to structural and environmental constraints.</p>", "people": ["neri@media.mit.edu"], "title": "FABRICOLOGY: Variable-property 3D printing as a case for sustainable fabrication", "modified": "2019-04-18T19:48:48.656Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-001", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "fabricology-variable-property-3d-printing-as-a-case-for-sustainable-fabrication"}, {"website": "", "description": "<p>The LEGO Wayfinder project combines LEGO, robotics, and seawater into a playground of project-based learning and citizen science for budding engineers and explorers. As part of this outreach program, our team has developed a first prototype of a buildable LEGO marine exploration vehicle kit\u2014addressing some of the design challenges of building for the underwater context.</p><p>Our aim is to build an awareness of the state of the aquatic environment and instill a greater responsibility in shaping our interactions with the environment. To do so, young people will view underwater wonders of the world with their robots and get outside to explore their local waterway. Our approach embraces Seymour Papert\u2019s model of \"low floors\" (where getting started is easy), and \"high ceilings,\" where students can pour their time and collaborative work efforts into creative engineering solutions to carry out a marine science experiment of their own design in the field.</p>", "people": ["katybell@media.mit.edu", "ave@media.mit.edu", "ps1@media.mit.edu", "kamcco@media.mit.edu"], "title": "LEGO Wayfinder", "modified": "2019-04-22T17:15:27.304Z", "visibility": "PUBLIC", "start_on": "2018-04-01", "location": "", "groups": ["ml-learning", "open-ocean"], "published": true, "active": false, "end_on": null, "slug": "lego-wayfinder"}, {"website": "", "description": "<p>The esc-Pod&nbsp; (or Escape Pod) is an exploratory platform for researchers investigating moments of refuge within our bustling work lives.&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">The core of the esc-Pod consists of actuated work and rest surfaces. This allows for moments of productivity and relaxation to occur within a single space.&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">The outer skin provides variable transparency, enabling a spectrum of visibility settings according to privacy requirements.&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">The inner skin provides an infrastructure for the modulation of spatial experiences. Each panel is a pixel, connecting itself to the skin network, and can embody an array of senses.</span></p>", "people": ["kapeloni@media.mit.edu", "kll@media.mit.edu", "cassiano@media.mit.edu", "csmuts@media.mit.edu", "nlutz@media.mit.edu"], "title": "Escape Pod", "modified": "2019-04-08T17:01:14.555Z", "visibility": "PUBLIC", "start_on": "2016-08-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "escape-pod-1"}, {"website": "", "description": "<p>Prosthetic sockets belong to a family of orthoic devices designed for amputee rehabilitation and performance augmentation. Although such products are fabricated out of lightweight composite materials and designed for optimal shape and size, they are limited in their capacity to offer local control of material properties for optimizing load distribution and ergonomic fit over surface and volume areas. Our research offers a novel workflow to enable the digital design and fabrication of customized prosthetic sockets with variable impedance informed by MRI data. We implement parametric environments to enable the controlled distribution of functional gradients of a filament-wound carbon fiber socket.</p>", "people": ["cdgu@media.mit.edu", "neri@media.mit.edu"], "title": "Functionally graded filament-wound carbon-fiber prosthetic sockets", "modified": "2019-04-18T19:50:23.822Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "functionally-graded-filament-wound-carbon-fiber-prosthetic-sockets"}, {"website": "", "description": "<p>A web app that is a television with topical tuning rather than channel tuning. You can select the subject, the slant, and the emotional intensity of the people and the captions. Our question is whether these controls will broaden people's perspective and help create a common framework with which we can discuss our attitudes, sentiments, and positions on public issues. This project is a visualization of the infrastructure of Unspoken News and Superglue. These are an evolving resource for broadcast news understanding.</p>", "people": ["mhjiang@media.mit.edu", "eickhoff@media.mit.edu", "lip@media.mit.edu"], "title": "YouTune", "modified": "2019-04-18T16:50:41.771Z", "visibility": "LAB-INSIDERS", "start_on": "2018-10-15", "location": "", "groups": ["viral-communications"], "published": false, "active": false, "end_on": null, "slug": "youtune-1"}, {"website": "", "description": "<p>NeuroKnit is the interplay between the physical and virtual that is explored in response to the current lack of culture, expression, and emotions in VR  experiences; we propose a two-fold solution. First, the integration of bio-signal sensors into the HMD and techniques to detect aspects of the emotional state of the user. Second, the use of this data to generate expressive avatars.</p>", "people": ["gbernal@media.mit.edu", "pattie@media.mit.edu"], "title": "Emotional Beasts Parte Dos: NeuroKnit", "modified": "2019-06-07T12:59:19.489Z", "visibility": "PUBLIC", "start_on": "2017-07-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "neuroknit"}, {"website": "", "description": "<p>Space and space flight are extreme environments for the human body due to exposure to microgravity and high radiation levels. While the brain is neuroplastic and adapts to different habitats by learning over time, sudden changes in the environment and unpreparedness for it can totally hinder the functioning of the individual.&nbsp;The physiological changes caused by microgravity include vestibular problems causing space motion sickness, bone demineralization, skeletal muscle atrophy, cardiovascular problems, and more.&nbsp;The primary goal of this research project is to investigate vestibular system stimulation techniques to combat motion sickness and create more intuitive experiences when being in non-natural gravity environments.</p><p>Motion sickness is theorized to be either a cause of sensory mismatch between visual and vestibular afferent nerves(inter-sensory) or between semicircular and otolith nerve in the vestibular system (intra-sensory). The magnitude of alteration and the latency between the sensory inputs also contributes to the severeness of the motion sickness.&nbsp;To combat the non-congruent changes in sensory signals while transitioning into space, we propose to investigate vestibular neuromodulation techniques for facilitating adaptation in a more natural way, appeasing the effects of motion sickness and use the altered gravity to create novel experiences in virtual/augmented reality devices.</p><p>We built a prototype for multipole vestibular stimulation for simulating acceleration in roll and pitch axis. The prototype will be tested on the upcoming zero gravity flight for minimizing the effects of alterations between micro and hyper gravity phases.</p>", "people": ["abyjain@media.mit.edu", "patpat@media.mit.edu", "pattie@media.mit.edu"], "title": "Sensory Synchrony", "modified": "2019-05-31T13:37:03.535Z", "visibility": "LAB", "start_on": "2018-09-24", "location": "", "groups": ["fluid-interfaces", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "SensorySynchrony"}, {"website": "", "description": "", "people": [], "title": "Society-in-the-Loop", "modified": "2019-01-07T19:46:48.367Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["scalable-cooperation"], "published": false, "active": false, "end_on": null, "slug": "society-in-the-loop"}, {"website": "http://harpreetsareen.com", "description": "<p>Plants can sense the environment, other living entities and regenerate, actuate or grow in response.&nbsp;Our interaction and communication channels with plant organisms in nature are subtle - whether it be looking at their color, orientation, moisture, position of flowers, leaves and such. This subtlety stands in contrast to our interactions with artificial electronic devices&nbsp;that are centered in&nbsp;and around the screens, requiring full attention and induce cognitive load.&nbsp; We envision bringing such interaction out from the screens back into natural world around us.&nbsp;</p><p>Beyond external indicators, plants also have electrochemical signals and response mechanisms inside them that make them very similar to our electronic devices.&nbsp;To tap into such capacities already built in nature, we propose a new convergent view of interaction design. Our goal is to&nbsp;<i>merge and power our electronic functionalities with existing biological functions of living plant</i>s.&nbsp;Through Cyborg Botany, we re-appropriate some of these natural capabilities of plants for our interactive functions.&nbsp;</p>", "people": ["sareen@media.mit.edu", "pattie@media.mit.edu"], "title": "Cyborg Botany: Augmented plants as sensors, displays, and actuators", "modified": "2019-05-09T14:53:38.490Z", "visibility": "PUBLIC", "start_on": "2017-01-03", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "cyborg-botany"}, {"website": "", "description": "<p>Depressive disorders are ranked as the single largest contributor to non-fatal health loss (7.5% of all Years Lived with Disability), affecting an estimated 300 million people<a href=\"#_ftn1\">[1]</a>. Evidence-based treatments are available and measurement-based care has been described as the gold standard. Monitoring of depressive symptoms is currently performed with self-administered and interview-based assessment methods conducted by clinicians in their offices. However, the shortage of mental health specialists and the limited resources available to primary care physicians who often manage patients with depression prevent close monitoring of symptoms, delaying optimal treatment, and potentially prolonging suffering. Passive recording of behavioral data (gathering information without an individual's direct input) has been identified as a potentially feasible method for long-term monitoring of depression.&nbsp;</p><p>During the past decade, along with the development of wearable sensors, we have seen the progressive use of machine learning, which has allowed for the development of complex models. The combination of sensor technology and machine learning enables detailed measurement in real time of a wealth of behaviors predicting variation of depression. Our interdisciplinary team, including one of the leading labs on depression research at the Massachusetts General Hospital and the Affective Computing group at the MIT Media Lab, has conducted a study applying machine learning analytics to create a model combining wristband sensor data and phone-based passive measurements to assess depression. In our pilot study with chronically depressed patients monitored over eight weeks, we found that an algorithm based on biological and behavioral sensor data could estimate depression severity evaluated by a clinician with high accuracy, comparable to the inter-rater reliability. </p><p>\n\nIn this project, we build from our pilot study and develop an objective,\npassive, sensor-based algorithm able to detect depression and early response as\nwell as predict response. We will monitor&nbsp; for 12 weeks 100 adults with Major Depression Disorder\nwho just started treatment. The identification of\nreliable, objective, passive assessment of depression with biosensors will have\nsignificant ramifications for the monitoring of depression, early detection of\nresponse, and ultimately contribute to the advancement of precision medicine.\n\n<br>        </p><p><a href=\"#_ftnref1\">[1]</a> World Health Organization. \"Depression and other common mental disorders: global health estimates.\" (2017).</p>", "people": [], "title": "Leveraging artificial intelligence for the assessment of severity of depressive symptoms", "modified": "2019-04-03T16:56:14.282Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "leveraging-artificial-intelligence-for-the-assessment-of-severity-of-depressive-symptoms"}, {"website": "", "description": "<p>Social Mirror is a web application that helps Twitter users interactively explore the politically active parts of their social network.  Worsening political polarization over the past several years has exacerbated ideological echo chambers, which in turn have further fueled polarization by widening knowledge and empathy gaps between disparate groups.  We hope digital tools like Social Mirror can help inspire self-reflection, and ultimately, intellectual humility by providing people with a new view of their social media ecosystems and helping them form new network connections.</p>", "people": ["dkroy@media.mit.edu", "annyuan@media.mit.edu", "ngillani@media.mit.edu"], "title": "Social Mirror", "modified": "2018-11-15T19:09:55.909Z", "visibility": "PUBLIC", "start_on": "2017-06-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "social-media-mirror"}, {"website": "", "description": "<p>Today's students have grown up in a world of rapidly evolving technology, and they are natural-born experimenters, programmers, and tinkerers. By introducing OpenAg\u2122&nbsp; Food Computers into the education context, including in schools and museums, we hope to inspire and empower the high-tech farmers of the future.&nbsp;</p><p>Our latest testing in schools and libraries shows&nbsp;<b style=\"font-size: 18px;\">more than half of students </b><span style=\"font-size: 18px;\">who tested a PFC_EDU&nbsp;<b>think </b></span><b><span style=\"font-size: 18px;\"><i>more</i></span><span style=\"font-size: 18px;\"> about food production, hunger, and where their food comes from</span></b><span style=\"font-size: 18px;\"><b>.</b>&nbsp;</span></p><h2><b>Build a Food Computer</b></h2><ul><li>Visit the<a href=\"https://wiki.openag.media.mit.edu/pfc_edu_3.0\">&nbsp;<b>OpenAg Wiki page</b></a>&nbsp;for the latest version of <a href=\"https://www.media.mit.edu/projects/personal-food-computer/overview/\"><b>Personal Food Computer </b>-<b>&nbsp;the \"PFC_EDU\"</b></a>&nbsp;including activity guides, materials lists, and helpful tips on how to get parts manufactured.</li></ul><ul><li>Join&nbsp;<a href=\"https://forum.openag.media.mit.edu/c/education\">our forum and&nbsp;<b>meet other educators</b></a>, or follow along with <a style=\"font-size: 18px;\" href=\"https://forum.openag.media.mit.edu/t/libraries-food-computers-plix-build-public-library-innovation-exchange-at-mit-media-lab/2895/67\"><b>public libraries</b> who are integrating Food Computers into their activities</a><span style=\"font-size: 18px;\"> for students of all ages.</span></li><li>Download or contribute activities, materials, lesson plans, curricula, and other education resources on our <a href=\"https://wiki.openag.media.mit.edu/education\"><b>OpenAg EDU Wiki Page</b></a>.</li></ul>", "people": ["pcerqu@media.mit.edu", "calebh@media.mit.edu", "hildreth@media.mit.edu", "davidzac@media.mit.edu"], "title": "OpenAg EDU", "modified": "2019-03-25T17:22:50.561Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["open-agriculture-openag"], "published": true, "active": false, "end_on": null, "slug": "openag-education"}, {"website": "", "description": "<p>Scratch is built on the principle of \u201cwide walls,\u201d encouraging a wide array of diverse projects, reflecting the diverse interests of Scratchers. In order for Scratch to fulfill this promise, we&nbsp;propose a personalized Scratch project recommendation algorithm that will recommend projects for users to explore, while also taking into account their interests, preferences, and learning pathway. The objective is for such an algorithm to increase the distribution of project attention across the platform as well as broaden the experience of Scratchers. To that end, this project involves using machine learning to classify Scratch projects by type and complexity, making it easier for the project recommendation algorithm to pull specific types of projects to suggest to the user.</p>", "people": ["labdalla@media.mit.edu", "ascii@media.mit.edu"], "title": "Scratch Project Recommendation", "modified": "2019-04-30T15:14:49.042Z", "visibility": "LAB-INSIDERS", "start_on": "2018-09-05", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratch-project-recommendation"}, {"website": "", "description": "<p>City Science Lab Toronto was established in cooperation with Ryerson University in Toronto, Canada.&nbsp; It&nbsp; started in 2018 and is the newest city in the City Science Network. The lab will be embedded in&nbsp;the Faculty of Communications and Design and will be part of the University's Paradox Initiative.&nbsp;The two groups plan to build and work on&nbsp;the development and simulation of urban interventions, such as&nbsp; micro-units for young people, shared work and collaboration spaces, educational facilities, financial services innovations, and new mobility and parking systems. This information will be analyzed&nbsp;and visualized using different platforms including the CityScope. The two groups&nbsp; plan to&nbsp;&nbsp;define new parameters &nbsp;which may include financial modeling, design of innovation flow,&nbsp; public health, new mobility systems, and/or energy networks. A number of large financial institutions, telecommunications companies, and hospitality groups are the founding corporate supporters of the initiative. The lab's director is Professor Hossein Rahnama, who is also a visiting faculty member at the MIT Media Lab.&nbsp;</p>", "people": ["rahnama@media.mit.edu"], "title": "City Science Lab Toronto", "modified": "2018-11-16T14:46:12.537Z", "visibility": "PUBLIC", "start_on": "2018-03-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "city-science-lab-toronto"}, {"website": "", "description": "<p>Civic Entertainment is a project based at the Center for Civic Media that explores the intersection of civic engagement with film, television, radio, theatre, literature, and digital entertainment. The project aims to study the modes in which entertainment can create greater knowledge of public institutions, motivate citizens towards democratic duties, and present effective strategies of social and political change.</p><p>The research focuses on studying the ways in which fiction media can affect change in thought and behavior, develops case studies of past and existing films and television shows that reflect or carry elements of civic engagement, explores the representation of protest and activism in popular culture, and experiments with techniques to balance civic education with humor or drama within entertainment.</p><p>The project has a key focus on Indian entertainment and works with Civic Studios (www.civicstudios.com), a Mumbai-based production firm, on creating civic entertainment content for young people across India.</p>", "people": ["anushkas@media.mit.edu", "ethanz@media.mit.edu"], "title": "Civic Entertainment", "modified": "2019-03-15T19:38:40.415Z", "visibility": "PUBLIC", "start_on": "2018-03-20", "location": "", "groups": ["center-for-civic-media", "civic-media"], "published": true, "active": false, "end_on": null, "slug": "civic-entertainment"}, {"website": "", "description": "<p>We present a multimaterial voxel-printing method enabling the physical visualization of data sets commonly associated with scientific imaging. Leveraging voxel-based control of multimaterial 3D printing, our method enables additive manufacturing of discontinuous data types such as point cloud data, curve and graph data, image-based data, and volumetric data. By converting data sets into dithered material deposition descriptions, through modifications to rasterization processes, we demonstrate that data sets frequently visualized on screen can be converted into physical, materially heterogeneous objects.&nbsp;</p><p>Our approach alleviates the need to post-process data sets to boundary representations, preventing alteration of data and loss of information in the produced physicalizations. Therefore, it bridges the gap between digital information representation and physical material composition. We evaluate the visual characteristics and features of our method, assess its relevance and applicability in the production of physical visualizations, and detail the conversion of data sets for multimaterial 3D printing. We conclude with exemplary 3D printed datasets produced by our method pointing towards potential applications across scales, disciplines, and problem domains.</p>", "people": ["ssunanda@media.mit.edu", "bader_ch@media.mit.edu", "jpcosta@media.mit.edu", "limulus@media.mit.edu", "kolb@media.mit.edu", "neri@media.mit.edu"], "title": "Making Data Matter: Voxel-printing for the digital fabrication of data across scales and domains", "modified": "2019-04-18T19:51:47.144Z", "visibility": "PUBLIC", "start_on": "2018-05-30", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "making-data-matter"}, {"website": "", "description": "<p>Two-dimensional radiographs are commonly used for evaluating sub-surface hard structures of teeth, but they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces, and they are also frequently refused by patients over safety concerns. Our goal is to augment ultimately replace the prevalence of ionizing and expensive X-ray imaging and cone-beam computed tomography (CBCT) for dental care with near-infrared (NIR) imaging. Translucency of teeth in the NIR range offers non-ionizing and safe detection of dental features. NIR can be used in conjunction with multiple light sources to create three-dimensional images of teeth. By modeling the scattering of photons in teeth, we can effectively see several millimeters inside, providing additional diagnostic value.</p><p><strong>Why is this work important?</strong></p><p>Two-dimensional radiographs and cone-beam computed tomography are commonly used for evaluating sub-surface hard structures of teeth. While radiographs are the current standard of care for diagnostic dental imaging, they have low sensitivity for early caries lesions, particularly those on tooth occlusal surfaces. They are also frequently refused by patients over safety concerns about exposure to ionizing radiation. Medical image acquisition without ionizing radiation can expand the use of important diagnostic tools and decrease safety concerns.</p><p><strong>What has been done before?</strong></p><p>NIR light can be transmitted across healthy dental enamel with marginal scattering, allowing for imaging dental features. NIR light at 850 nm and 1310 nm, which strike a balance between enamel and water attenuation, have been shown to provide helpful diagnostics that visual examination alone lacks. Our previous work has demonstrated the sensitivity of 850 nm NIR images to early caries lesions and demineralization. For NIR is to synergistically augment or eventually replace ionizing radiation as the standard of care, we aim to expand its diagnostic potential to clinical features that exist beyond the surface of the tooth.</p><p><strong>What are our contributions?</strong></p><p>This is ongoing research. Preliminary work shows promise for augmenting the diagnostic power of NIR by modeling scattering.</p><p><strong>What are the next steps?</strong></p><p>Large-scale screenings can evaluate the effectiveness of our new imaging process.</p><p><strong>Related projects</strong></p><ol><li><a href=\"https://www.media.mit.edu/projects/near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth/overview/\">Near-Infrared Imaging for Detecting Dental Caries</a></li><li><a href=\"https://www.media.mit.edu/projects/near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging/overview/\">Near-Infrared Transillumination Guides Administration of Dental 2D Radiography and Cone Beam Computed Tomography Imaging</a></li></ol>", "people": ["pratiks@media.mit.edu", "gyauney@media.mit.edu", "arana@media.mit.edu", "kla11@media.mit.edu"], "title": "Non-Ionizing Imaging for Medical Diagnosis Instead of 2D Radiography and Cone-Beam Computed Tomography", "modified": "2018-10-26T16:23:18.913Z", "visibility": "PUBLIC", "start_on": "2017-05-08", "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis"}, {"website": "", "description": "<p>We drive faster than we can react, watch screens that are sharper than we can resolve, and are surrounded by architectural facades with greater depth and scale than we can perceive. When our focus is saturated by a demanding task, we can be blind to what is directly in front of us. What happens if we high-jack the biologically hard-coded mechanisms in the visual processing pipeline and present them with discrete, coded instructions to heighten and augment perceptual awareness toward a richer engagement in our everyday lives? Our visual mechanisms constitute a set of powerful, networked input controls responsible for parsing and encoding the natural world before us. If we can<br>interject parallel streams of information tailored to the subtle and sometimes preconscious mechanisms of vision, there is a potential to multiplex within our spatial and attentional bandwidth, to deliver more dynamic, content-rich environments.<br></p><p>The function and structure of early visual processes are well established, and technologies capable of delivering dynamic, tailored information exist; in this dissertation I propose to build systems capable of bridging the gap by translating functional building blocks of vision into real-world environments. These systems will serve as platforms to quantify and parameterize the potentials in new perceptual realities. The experiments will target visual mechanisms responsible for image motion, curve estimation, and perceived position shift to challenge the observer\u2019s proprioception, sensations of shape and depth, and to create digital media environments which appear to expand beyond the physical limitations of the display.<br></p><p>From how we navigate visually unpredictable environments with multifaceted restriction on visual attention, to perceiving the made environment through perceptual skins, to arguing for a deeper look into the way we generate media and display technologies, this dissertation will explore real world application for unattended visual potentials.</p>", "people": ["elawson@media.mit.edu"], "title": "Biologically Encoded Augmented Reality", "modified": "2019-04-23T01:11:54.681Z", "visibility": "LAB-INSIDERS", "start_on": "2019-01-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "biologically-encoded-augmented-reality"}, {"website": "https://scratch.mit.edu", "description": "<p>Scratch is a programming language and online <a href=\"http://scratch.mit.edu\">community</a> that makes it easy to create your own interactive stories, games, and animations\u2014and share your creations online. As young people create and share Scratch projects, they learn to think creatively, reason systematically, and work collaboratively, while also learning important mathematical and computational ideas. Young people around the world have shared more than 21 million projects on the Scratch website, with thousands of new projects every day. (For information on who has contributed to Scratch, see the <a href=\"http://scratch.mit.edu/info/credits/\">Scratch Credits page</a>.)</p>", "people": ["millner@media.mit.edu", "chrisg@media.mit.edu", "jmaloney@media.mit.edu", "kbrennan@media.mit.edu", "tmickel@media.mit.edu", "shanemc@media.mit.edu", "kaschm@media.mit.edu", "mataylor@media.mit.edu", "tarmelop@media.mit.edu", "jbuitrag@media.mit.edu", "jwzimmer@media.mit.edu", "shrutid@media.mit.edu", "khanning@media.mit.edu", "sleggss@media.mit.edu", "cwillisf@media.mit.edu", "christan@media.mit.edu", "otts@media.mit.edu", "ria@media.mit.edu", "sdruga@media.mit.edu", "silver@media.mit.edu", "rschamp@media.mit.edu", "jaleesat@media.mit.edu", "morant@media.mit.edu", "sdg1@media.mit.edu", "eschill@media.mit.edu", "colbygk@media.mit.edu", "hisean@media.mit.edu", "mres@media.mit.edu", "ericr@media.mit.edu", "ascii@media.mit.edu", "nrusk@media.mit.edu", "bowman@media.mit.edu", "champika@media.mit.edu"], "title": "Scratch", "modified": "2019-01-11T16:27:28.527Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "LEGO Learning Lab", "groups": [], "published": true, "active": false, "end_on": null, "slug": "scratch"}, {"website": "", "description": "<p>College of Design and Innovation of Tongji University, Shanghai, and the MIT Media Lab's City Science group are co-developing a version of the MIT CityScope platform to support the urban decision making that promotes urban vibrancy and innovation potential. </p><p>The \u201cNICE2035 LivingLine\u201d project in Shanghai, China, is a design-driven, community-based urban innovation initiated by Professor Yongqi Lou, Dean of College of Design of Innovation. LivingLine is a crowdsourcing and co-creation project aiming at building an ecosystem of innovation and entrepreneurship on the internal street of a typical gated residential neighborhood. By introducing radical programs such as living labs, co-working space, and startup-incubators into underutilized storefront space, LivingLine\u2019s goal is to revitalize the urban space and to prototype diverse future lifestyles.<br></p>", "people": ["thomassl@media.mit.edu", "kll@media.mit.edu", "chenhan@media.mit.edu", "liuymit@media.mit.edu", "ryanz@media.mit.edu"], "title": "CityScope LivingLine Shanghai", "modified": "2019-06-06T16:01:44.891Z", "visibility": "PUBLIC", "start_on": "2018-03-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "cityscope-livingline-shanghai"}, {"website": "", "description": "<p>A collaboration between Professor Christine Ortiz (project lead), Professor Mary C. Boyce, Katia Zolotovsky, and Swati Varshaney (MIT). Operating at the intersection of biomimetic design and additive manufacturing, this research proposes a computational approach for designing multifunctional scaled-armors that offer structural protection and flexibility in movement. Inspired by the segmented exoskeleton of Polypterus senegalus, an ancient fish, we have developed a hierarchical computational model that emulates structure-function relationships found in the biological exoskeleton. Our research provides a methodology for the generation of biomimetic protective surfaces using segmented, articulated components that maintain user mobility alongside full-body coverage of doubly curved surfaces typical of the human body. The research is supported by the MIT Institute for Soldier Nanotechnologies, the Institute for Collaborative Biotechnologies, and the National Security Science and Engineering Faculty Fellowship Program.</p>", "people": ["dumo@media.mit.edu", "neri@media.mit.edu", "j_duro@media.mit.edu"], "title": "Meta-Mesh: Computational model for design and fabrication of biomimetic scaled body armors", "modified": "2019-04-18T19:54:09.747Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": null, "slug": "meta-mesh-computational-model-for-design-and-fabrication-of-biomimetic-scaled-body-armors"}, {"website": "", "description": "<p>In this project, we are motivated to address clinical trials issues using Blockchain technology. We propose, OPEN TrialChain, a privacy-preserving Blockchain-based data sharing infrastructure that uses open algorithms between stakeholder of the federation in the clinical trials ecosystem. OPEN TrialChain balances between the sharing of clinical data and the need for subject\u2019s privacy protection by allowing queries on decentralized raw datasets from which it returns aggregated safe answers that are blinded (i.e. anonymized). Using Type II Diabetes as a case study, we study the adoption of OPEN TrialChain and how it can address clinical trials issues. Results show that OPEN TrialChain, first, encourages pharmaceutical companies to report their trial results with higher fidelity, as well as, federated ones to provide more detailed results in return for their peers\u2019 detailed results. Furthermore, it allows for multiple studies to be queried for results from underrepresented demographics, producing greater insight from previously ignored minorities. Finally, analysis done using OPEN TrialChain should illustrate meaningful results without violating the privacy of individuals. Eventually, OPEN TrialChain is expected to optimize the clinical trial ecosystem by improving patient safety, saving lives, cutting drug development costs, encouraging transparent results, preserving patients privacy, and maintaining pharmaceuticals integrity.</p>", "people": ["sandy@media.mit.edu", "shada@media.mit.edu"], "title": "OPEN TrialChain", "modified": "2019-04-01T17:38:57.336Z", "visibility": "PUBLIC", "start_on": "2018-01-01", "location": "", "groups": ["human-dynamics"], "published": false, "active": false, "end_on": null, "slug": "open-trialchain"}, {"website": "", "description": "<h2>Shrinking problems in 3D printing</h2><p>Although a range of materials can now be fabricated using additive manufacturing techniques, these usually involve assembly of a series of stacked layers, which restricts three-dimensional (3D) geometry. Oran&nbsp;et al.&nbsp;developed a method to print a range of materials, including metals and semiconductors, inside a gel scaffold (see the Perspective by Long and Williams). When the hydrogels were dehydrated, they shrunk 10-fold, which pushed the feature sizes down to the nanoscale.</p><p>Lithographic nanofabrication is often limited to successive fabrication of two-dimensional (2D) layers. We present a strategy for the direct assembly of 3D nanomaterials consisting of metals, semiconductors, and biomolecules arranged in virtually any 3D geometry. We used hydrogels as scaffolds for volumetric deposition of materials at defined points in space. We then optically patterned these scaffolds in three dimensions, attached one or more functional materials, and then shrank and dehydrated them in a controlled way to achieve nanoscale feature sizes in a solid substrate. We demonstrate that our process, Implosion Fabrication (ImpFab), can directly write highly conductive, 3D silver nanostructures within an acrylic scaffold via volumetric silver deposition. Using ImpFab, we achieve resolutions in the tens of nanometers and complex, non\u2013self-supporting 3D geometries of interest for optical metamaterials.</p>", "people": ["esb@media.mit.edu", "tillberg@media.mit.edu", "danoran@media.mit.edu", "chenf@media.mit.edu"], "title": "Implosion Fabrication", "modified": "2018-12-13T20:17:08.530Z", "visibility": "PUBLIC", "start_on": "2018-01-02", "location": "", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": null, "slug": "implosion-fabrication-1"}, {"website": "", "description": "<p>Our dream is a sci-fi future, where dreams are controllable.&nbsp;</p><p>We are working to build technology that interfaces with the sleeping mind.&nbsp;As the dreamer descends into sleep, we track different sleep-stages using brain activity, muscle tension, heart rate, and movement data.&nbsp;External stimuli in the form of scent, audio, and muscle stimulation affect the content of the dreams. We are working on integrating multiple projects&nbsp; developed at the Fluid Interfaces group towards a vision where sleep is controllable.</p><p><a href=\"http://dreams.media.mit.edu\">dreams.media.mit.edu</a><br></p>", "people": ["gbernal@media.mit.edu", "adamjhh@media.mit.edu", "tomasero@media.mit.edu", "amores@media.mit.edu", "rosello@media.mit.edu", "pattie@media.mit.edu"], "title": "Engineering Dreams", "modified": "2019-04-30T15:12:45.024Z", "visibility": "PUBLIC", "start_on": "2018-03-21", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "engineering-dreams"}, {"website": "", "description": "<p>How can we make sense of the expressed\u2014and latent\u2014interests of millions of TV show audience members online? We analyze millions of users on Twitter who follow a set of TV shows, discovering affinities between users and identifying clusters that reflect what they're interested in (e.g., soccer, hip hop, the environment). This \"interest map\" enables us to identify audience clusters that might be likely to engage with certain TV shows or genres, potentially supporting the creation, distribution, and promotion of more impactful stories.</p>", "people": ["russell5@media.mit.edu", "bcroy@media.mit.edu"], "title": "TV Audience Tribes", "modified": "2019-04-18T17:30:58.538Z", "visibility": "LAB-INSIDERS", "start_on": "2018-11-26", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "tv-audience-tribes"}, {"website": "", "description": "<p>How do regions acquire the knowledge they need to diversify their economic activities? How does the migration of workers among firms and industries contribute to the diffusion of that knowledge? Here we measure the industry-, occupation-, and location-specific knowledge carried by workers from one establishment to the next, using a dataset summarizing the individual work history for an entire country. We study pioneer firms\u2014firms operating in an industry that was not present in a region\u2014because the success of pioneers is the basic unit of regional economic diversification. We find that the growth and survival of pioneers increase significantly when their first hires are workers with experience in a related industry and with work experience in the same location, but not with past experience in a related occupation. We compare these results with new firms that are not pioneers and find that industry-specific knowledge is significantly more important for pioneer than for non-pioneer firms. To address endogeneity we use Bartik instruments, which leverage national fluctuations in the demand for an activity as shocks for local labor supply. The instrumental variable estimates support the finding that industry-specific knowledge is a predictor of the survival and growth of pioneer firms. These findings expand our understanding of the micromechanisms underlying regional economic diversification.</p>", "people": ["hidalgo@media.mit.edu", "crisjf@media.mit.edu", "bjun@media.mit.edu"], "title": "The role of industry, occupation, and location-specific knowledge in the survival of new firms", "modified": "2018-12-14T20:31:53.678Z", "visibility": "PUBLIC", "start_on": "2017-03-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": null, "slug": "industry-knowledge"}, {"website": "", "description": "<p><b>Paper Dreams explores the various modes of creativity that can be enabled by artificial intelligence.</b></p><p>A fair amount of current research has been focused on using machine intelligence to learn \u201ccreativity,\u201d from transfer learning of artistic styles to translation of languages. However, the application of current machine learning algorithms and multi-modal inputs to the augmentation of existing human creativity is a relatively unexplored area. By focusing on the dynamics of this human-machine interaction and working with the representations inside machine learning models, we can give people new tools for reasoning.</p><p>Our system at its current form provides a user with a canvas that it is more like a mirror, where we draw strokes and re-evaluate what our drawing looks like at each step in order to continue. We do this by building a neural network which takes a small number of input variables, called <i>latent variables</i>, and produces the entire sketch as output.</p><p>The machine can use analog modes of thinking to recognize what we are drawing and help us move forward. At the moment, Paper Dreams augments the drawing experience in three ways: by adding textures/colors, suggesting other elements/drawings for the scene, and introducing serendipity. To adjust the level of serendipity, the user has control of a dial that determines how \"predictable\" vs \"unpredictable/nutty\" you want these machine additions to be.</p><p><b>Some of the driving questions for this project are:</b></p><ul><li>To what extent are these new tools enabling creativity? </li><li>Can they be used to generate ideas which are truly surprising and new, or are the ideas clich\u00e9s, based on trivial recombinations of existing ideas?</li><li>Can such systems be used to develop fundamental new interface primitives? </li><li>How will those new primitives change and expand the way humans think?&nbsp;</li></ul>", "people": ["gbernal@media.mit.edu"], "title": "Paper Dreams", "modified": "2019-04-30T16:09:38.788Z", "visibility": "PUBLIC", "start_on": "2018-03-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "paper-dreams"}, {"website": "http://CharlesHolbrow.com", "description": "<p>Bauhaus artist Paul Klee considered a line to be&nbsp; one of the atomic elements of art and architecture. He famously described a line drawing as \"<a href=\"https://www.artsy.net/article/artsy-editorial-how-to-be-an-artist-according-to-paul-klee\">a dot that went for a walk</a>.\"&nbsp; What would a dot on a walk look like in three dimensions?&nbsp;</p><p>This project uses our custom high-resolution AR rig to find out.&nbsp; See the very first test footage:</p>", "people": ["holbrow@media.mit.edu"], "title": "Taking a line on a walk", "modified": "2019-04-18T14:10:20.018Z", "visibility": "PUBLIC", "start_on": "2018-12-02", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "taking-a-line-on-a-walk"}, {"website": "", "description": "", "people": ["zive@media.mit.edu", "groh@media.mit.edu"], "title": "AI Spirits", "modified": "2018-11-14T21:20:32.287Z", "visibility": "PUBLIC", "start_on": "2018-10-31", "location": "", "groups": ["scalable-cooperation"], "published": false, "active": false, "end_on": "2019-10-31", "slug": "ai-spirits"}, {"website": "", "description": "<p>Could a social robot collaboratively exchange stories with children as a peer and help improve their linguistic and storytelling skills? Tega uses machine learning algorithms to learn actions that improve children's storytelling and keep them engaged. &nbsp;We are also interested in how Tega can personalize its interaction with each child over multiple encounters, because every child learns and engages differently.&nbsp;</p><p>In Spring 2017, Tega went to twelve preschool classrooms in the Greater Boston area for&nbsp;three months, pioneering the field of long-term human-robot interaction.&nbsp;Using Q-learning, a policy was trained to tell stories optimized for each child\u2019s engagement and linguistic skill progression. Tega monitored children's affect signals and asked dialogic questions during storytelling to gauge their engagement. Tega also invited children to&nbsp;tell&nbsp;it stories, which Tega used to assess each child's linguistic skill development. Our results show robot's interaction policy indeed personalized to each child. At the end of the sessions, the policy significantly differed from one child to the other. Children who interacted and built relationships with a personalized robot showed higher engagement, learned and retained more vocabularies, and used more complex syntax structure in their speech compared to where they had started.</p>", "people": ["cynthiab@media.mit.edu", "hchen25@media.mit.edu", "randiw12@media.mit.edu", "igrover@media.mit.edu", "samuelsp@media.mit.edu", "nikhita@media.mit.edu", "pcuellar@media.mit.edu", "haewon@media.mit.edu"], "title": "Personalized Robot Storytelling Companion", "modified": "2019-01-22T17:42:46.144Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "collaborative-robot-storyteller"}, {"website": "", "description": "<p>Privacy-preserving mHealth application using Open Algorithm (OPAL) architecture to address urgent care challenges in Riyadh, Saudi Arabia.</p>", "people": ["alotaibi@media.mit.edu", "jobalbar@media.mit.edu", "hhao@media.mit.edu", "sandy@media.mit.edu", "shada@media.mit.edu"], "title": "SMART^2 OPAL", "modified": "2019-04-01T17:39:27.492Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": ["human-dynamics"], "published": false, "active": false, "end_on": null, "slug": "smart-2-opal"}, {"website": "https://ninalutz.github.io", "description": "<p>What if we could leverage technology and modern cosmetics for good? To empower people to explore identities? To give anyone, regardless of skill level, the tools to bend, express, and explore various gender identities and traits?</p><p>The goal of this project is to create a tool that will allow a user to add more feminine, masculine, or androgynous traits to themselves with cosmetics. After interacting with renders and the tool's preferences, the user will receive a list of cosmetics and then follow along with the tool in real time while they apply their makeup.</p><p>This tool is meant to be accessible, to be free and open source technology, and made for users of all skin tones, shapes, and preferences. To allow anyone to express their identities.</p><p>This work is ongoing and will have regular updates, beta releases, and user surveys coming soon.&nbsp;</p>", "people": ["vmb@media.mit.edu", "nlutz@media.mit.edu"], "title": "Cosmetics-Empowered Identities", "modified": "2019-02-07T16:07:17.285Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "cosmetics-empowered-identities"}, {"website": "", "description": "<p>We designed, implemented, and tested a proof of concept, wrist-based wearable object identification system which allows users to \u201chover\u201d their hands over objects of interest and gain access to contextual information that is tied to them, through an intelligent personal assistant. The system uses a fusion of sensors to be able to perform the identification of an object under a variety of conditions. Among these sensors, there is a camera (operating in the visible and infrared spectrum), a small solid-state radar, and multi-spectral light spectroscopy sensors. Users can interact with contextual information tied to an object through conversations with an intelligent assistant to permit a hands-free, non-obtrusive, and discreet experience. The system explores audio interfacing with augmented reality content without the hassle of phones or head-mounted devices.</p>", "people": ["pe25171@media.mit.edu"], "title": "Hover", "modified": "2019-03-05T18:50:14.679Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "hoverband"}, {"website": "", "description": "<h2>Should workers worry about automation and AI?</h2><p>Many workers, policy makers, and researchers are asking themselves exactly this question. But the answer has proven elusive using traditional tools and methods. While some fear the end of employment and rising wealth inequality, others celebrate rising productivity and new frontiers for innovation and investment. The confusion between these perspectives arises from several barriers that inhibit today's study of AI, technology, and the future of work.</p>", "people": ["alexis@media.mit.edu", "cebrian@media.mit.edu", "mrfrank@media.mit.edu", "hyoun@media.mit.edu", "groh@media.mit.edu", "emoro@media.mit.edu", "irahwan@media.mit.edu"], "title": "AI, Automation, Labor, and Cities: How to map the future of work", "modified": "2019-04-03T14:42:46.705Z", "visibility": "PUBLIC", "start_on": "2017-06-06", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "future-of-work-ai-automation-labor"}, {"website": "", "description": "<p>Achieving a safe privacy-preserving information sharing environment for individualized care using blockchain-based technology in multiple use cases in the healthcare space.</p>", "people": ["ecstll@media.mit.edu", "sandy@media.mit.edu", "shada@media.mit.edu"], "title": "Healthy Blockchain", "modified": "2019-04-01T17:40:08.489Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["human-dynamics"], "published": false, "active": false, "end_on": null, "slug": "healthy-blockchain"}, {"website": "http://www.harpreetsareen.com", "description": "<p>Argus is a plant with DNA nanosensors inside it that detects lead (Pb2+ Heavy Metal). Irregularities in industrial waste management has lead to depletion of water quality in rivers in many regions. Current processes of monitoring the water quality are not realtime (taking a week or longer) or require special systems that are not off-the-shelf.</p><p>Through Argus, we propose a novel water monitoring method where plants provide an optical readout of lead in water. A DNAZyme is used as sensor assay\u2014double stranded DNA that breaks into single stranded on contact with Pb2+ and binds to single walled carbon nanotubes. This sensor assay can be injected inside the leaf of a plant and stays within the intercellular space.</p>", "people": ["sareen@media.mit.edu"], "title": "Argus: Water monitoring through nanosensors inside plants", "modified": "2019-05-20T17:32:10.412Z", "visibility": "PUBLIC", "start_on": "2017-06-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "argus-sensors-inside-plants"}, {"website": "", "description": "<p>The 8K Data Manipulator is a Unity game engine application which harnesses the gesture recognition and limb tracking available through Microsoft Kinect and combines it with the multitouch capability of our 8K display. Using either gesture at a distance or close-up touch, a user can rotate, zoom, and slice a very large graphical dataset. The 8K Data Manipulator is currently being used to visualize seismic data; however, it is capable of loading an arbitrary point cloud or volume, supporting medical, terrestrial, biochemical, social network, design, and other applications.</p>", "people": ["pe25171@media.mit.edu"], "title": "The 8K Data Manipulator", "modified": "2019-03-05T18:52:23.376Z", "visibility": "PUBLIC", "start_on": "2016-09-15", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "the-8k-data-manipulator"}, {"website": "", "description": "<p>Autonomous vehicles (AVs), drones, and robots will revolutionize our way of traveling and understanding urban space. In order to operate, all of these devices are expected to collect and analyze a lot of sensitive data about our daily activities. However, current operational models for these devices have extensively relied on centralized models of managing these data. The security of these models unveiled significant issues.</p><p>This project&nbsp; proposes BASIC, the Blockchained Agent-based Simulator for Cities. This tool aims to verify the feasibility of the use of blockchain in simulated urban scenarios by considering the communication between agents through&nbsp;<i>smart contracts</i>. In order to test the proposed tool, we implemented a car-sharing model within the city of Cambridge (Massachusetts, USA). In this research, the relevant literature was explored, new methods were developed, and different solutions were designed and tested. Finally, conclusions about the feasibility of the combination between blockchain technology and agent-based simulations were drawn.</p><p>Developed using&nbsp;<a href=\"https://gama-platform.github.io/\">Gama Platform</a>.&nbsp;&nbsp;</p><p>Click <a href=\"https://github.com/mitmedialab/Basic\">here</a> for the Open Source Repository.</p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "ecstll@media.mit.edu", "kll@media.mit.edu", "sandy@media.mit.edu"], "title": "BASIC: Blockchained Agent-based Simulator for Cities", "modified": "2019-04-23T20:08:21.338Z", "visibility": "PUBLIC", "start_on": "2018-09-03", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "basic"}, {"website": "", "description": "<p>Mobility is a key issue for city planners. Being able to evaluate the impact of its evolution is complex and involves many factors including new technologies like electric cars, autonomous vehicles and also new social habits like vehicle sharing. We need a better understanding of different scenarios to improve the quality of long-term decisions. Computer simulations can be a tool to better understand this evolution, to discuss different solutions and to communicate the implications of different decisions. In this paper, we propose a new generic model that creates an artificial micro-world which allows the modeler to create and modify new mobility scenarios in a quick and easy way. This not only helps to better understand the impact of new mobility modes on a city, but also fosters a better-informed discussion of different futures. Our model is based on the agent-based paradigm using the GAMA Platform. It takes into account different mobility modes, people profiles, congestion and traffic patterns. In this paper, we review an application of the model of the city of Cambridge.</p>", "people": ["agrignar@media.mit.edu"], "title": "ABMobility", "modified": "2019-03-27T20:04:59.772Z", "visibility": "PUBLIC", "start_on": "2016-04-15", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "abmobility"}, {"website": "", "description": "<p><i>\u201cCan a man know the truth and tell it to the greatest number and still be misunderstood? Can one man be of the many and still be known?\u201d </i></p><br><p><i>Schoenberg in Hollywood </i>is the most recent opera by composer-inventor-professor Tod Machover that explores the complex relationship between uncompromising art and mass appeal, and of whether\u2014and how\u2014art can change the world. Arnold Schoenberg was a man of extraordinary contradictions: now considered one of the twentieth century\u2019s greatest composers, during much of his lifetime Schoenberg was known for\u2014and excelled at\u2014composing music hated by the public and critics; a man whose only compass was his pursuit of pure ideas, Schoenberg also yearned for popularity; Schoenberg\u2019s music absorbs tradition, but it is not hampered by it and always points forward. What happened\u2014and might have happened\u2014when such an uncompromising spirit settled in Hollywood, the epicenter of American popular culture, after he fled Hitler's Europe in 1935? <br></p><p><i>Schoenberg in Hollywood </i>begins with a meeting (one that did occur in history) between the legendary producer Irving G. Thalberg of Metro Goldwyn Mayer and Arnold Schoenberg. Thalberg asks Schoenberg to compose music for a film based on Pearl S. Buck\u2019s <i>The Good Earth</i>, a best-seller about the life of peasants in a Chinese village. Although Schoenberg disdained the idea of composing music to please the public, the prospect of writing a Hollywood film score that would reach millions appealed to him greatly. In reality, Schoenberg was not offered the job when he demanded a $50,000 fee\u2014an astronomical sum at the time\u2014from Thalberg. However, <i>Schoenberg in Hollywood</i> exploits\u2014and explores\u2014the hypothetical scenario of what would have happened if Schoenberg had indeed composed for Hollywood. In the opera, Schoenberg imagines the events of his life through the lens of different film genres: silents, noir mysteries, Disney cartoons, musicals, and Westerns, making the movie\u2014and projecting his vision well into the future\u2014that Hollywood never allowed him to do.</p><p>Commissioned and presented by Boston Lyric Opera, with much visionary  technology for sound, image, and staging created at the MIT Media Lab, <i>Schoenberg in Hollywood</i> is based on a scenario by the late Braham Murray, with a libretto by Simon Robson and directed by Karole Armitage. <i>Schoenberg in Hollywood</i> premiered at the Boston Lyric Opera in November 2018 and travels to the Vienna Volksoper in the 2019/2020 season.</p>", "people": ["tod@media.mit.edu"], "title": "Schoenberg in Hollywood: A new opera by Tod Machover", "modified": "2019-04-18T00:45:48.433Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": null, "slug": "schoenberg-in-hollywood"}, {"website": "http://www.ficmaker.org.br", "description": "<p>The Festival of Invention and Creativity (FIC) is a great celebration of the inventive, collaborative, and hands-on&nbsp; spirit&nbsp; of Brazilian education. In it, children, young people, their families and educators have&nbsp; opportunity to explore high and low tech tools and materials, participate in interactive workshops and learn in a stimulating and relaxed way.</p><p>The Festival aims to disseminate, inspire and facilitate the implementation of creative learning activities&nbsp; in schools and non-formal educational environments.&nbsp;</p><p>In 2018, the Brazilian Creative Learning Network, with support from the MIT Media Lab and the Lemann Foundation, facilitated the organization of more than 15 regional Festivals throughout the country.</p><p>----</p><p>O Festival de Inven\u00e7\u00e3o e Criatividade (FIC) \u00e9 uma grande celebra\u00e7\u00e3o do esp\u00edrito inventivo, colaborativo e m\u00e3o na massa da educa\u00e7\u00e3o brasileira. Nele, crian\u00e7as, jovens, seus familiares e educadores tem a oportunidade de explorar materiais e tecnologias high e low tech, participar de atividades interativas, e aprender de forma estimulante e descontra\u00edda.</p><p>O Festival tem como objetivos divulgar, inspirar e facilitar a implementa\u00e7\u00e3o de atividades de aprendizagem m\u00e3o na massa em escolas e ambientes de educa\u00e7\u00e3o n\u00e3o formal.</p><p>S\u00f3 em 2018, a Rede Brasileira de Aprendizagem Criativa, contando com o apoio do MIT Media Lab e da Funda\u00e7\u00e3o Lemann, facilitou a organiza\u00e7\u00e3o de mais de 15 Festivais regionais por todo o pa\u00eds.</p>", "people": ["leob@media.mit.edu"], "title": "Festival of Invention and Creativity", "modified": "2018-12-14T08:20:17.955Z", "visibility": "PUBLIC", "start_on": "2017-01-10", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "festival-de-invencao-e-criatividade"}, {"website": "", "description": "<p>An 85-inch television screen combined with 8K resolution provides a tool through which multiple users can perform quick, highly detailed, visual analysis of a dataset to identify anomalies.&nbsp; In order to test this, we developed a fully 3D application similar to the popular picture game Where\u2019s Waldo.&nbsp; In the Where\u2019s Waldo game, users must find a person (Waldo) within a densely-populated picture.&nbsp; In the developed application, users utilize touch and gestures to navigate a large 3D city in which a person of a varying size is placed randomly.&nbsp; The application serves as a proof of concept that multiple users can interact with one large, high resolution dataset to visually find anomalies.&nbsp; &nbsp;&nbsp;</p>", "people": ["pe25171@media.mit.edu"], "title": "Where's Pedro?", "modified": "2019-04-16T16:41:36.312Z", "visibility": "PUBLIC", "start_on": "2017-01-07", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "where-s-pedro"}, {"website": "", "description": "<p>kinetiX is a transformable material featuring a design that resembles a cellular structure. It consists of rigid plates or rods and elastic hinges. These modular elements can be combined in a wide variety of ways and assembled into multifarious forms.</p><p>This project describes a group of auxetic-inspired material structures that can transform into various shapes upon compression. While the majority of the studies of auxetic materials focus on their mechanical properties and topological variations, our work proposes a parametric design approach that gives auxetic structures the ability to deform beyond shrinking or expanding. To do so, we see the auxetic structure as a parametric four-bar linkage. We developed four cellular-based material structure units composed of rigid plates and elastic/rotary hinges. Different compositions of these units lead to a variety of tunable shape-changing possibilities, such as uniform scaling, shearing, bending and rotating. By tessellating those transformations together, we can create various higher level transformations for design. The simulation is validated by the 3D printed structures.&nbsp;</p><p>&nbsp;We hope this work will inspire research in metamaterials design, shape-changing materials, and transformable architecture.</p>", "people": ["jannik@media.mit.edu", "ishii@media.mit.edu", "nv2247@media.mit.edu", "jifei@media.mit.edu"], "title": "kinetiX", "modified": "2019-01-24T14:26:13.216Z", "visibility": "PUBLIC", "start_on": "2017-03-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "kinetix"}, {"website": "", "description": "<p>How can people learn advanced motor skills such as front flips and tennis swings without starting from a young age? The answer, following the work of Masters et. al., we believe, is implicitly. Implicit learning is associated with higher retention and knowledge transfer, but that is unable to be explicitly articulated as a set of rules. To achieve implicit learning is difficult, but may be taught using obscured feedback\u2014that is, feedback that does not directly describe the result of an action.<br></p><p>With AUFLIP , we sought to provide auditory feedback to help newcomers learn front flips. We created a wearable system with a simplified  model of a front flip that compares a user\u2019s time to peak rotation  against their ideal time. As the user approaches their ideal  performance, the system begins playing a chord, only completing the  chord if the user manages to rotate at their ideal peak time. We tested  this system by integrating it into an environment where professional  coaches teach novices how to perform front flips; we found preliminary  results suggesting that users wearing the device exhibited implicit  learning. </p>", "people": ["dvlevine@media.mit.edu"], "title": "AUFLIP", "modified": "2019-01-29T19:46:29.141Z", "visibility": "PUBLIC", "start_on": "2016-10-14", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": null, "slug": "AUFLIP_Feedback"}, {"website": "", "description": "<p>\"Distributed Personalization\", \"Humanization of and Algos and Recommendations\"</p><p>Your data can't be used against you.</p><p>Project 1: SIO</p><p>Project 2:&nbsp; Product personality</p><p>Get my data - portofolio of products,&nbsp;product personality test&nbsp;<br>https://www.sciencedirect.com/science/article/pii/S0142694X08000859<br>http://brandpersonalityquiz.com/take-the-quiz/<br>And your resultant product personality https://www.myersbriggs.org/my-mbti-personality-type/mbti-basics/home.htm?bhcp=1&nbsp; &nbsp;Then onto suggestions.&nbsp;Nice easy and fun.&nbsp;</p>", "people": [], "title": "Decentralized Self", "modified": "2019-04-05T20:23:07.136Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["viral-communications"], "published": false, "active": false, "end_on": null, "slug": "decentralized-self"}, {"website": "", "description": "<p>Machines powered by artificial intelligence (AI) increasingly                           mediate our social, cultural, economic, and                           political interactions. Understanding the                           behavior of AI systems is essential to our                           ability to control their actions, reap their                           benefits, and minimize their harms. We argue                           this necessitates a broad scientific research                           agenda to study machine behavior that                           incorporates but expands beyond the discipline                           of computer science and requires insights from                           across the sciences. Here we first outline a                           set of questions fundamental to this emerging                           field. We then explore the technical, legal,                           and institutional constraints facing the study                           of machine behavior.</p>", "people": ["cynthiab@media.mit.edu", "cebrian@media.mit.edu", "sandy@media.mit.edu", "irahwan@media.mit.edu", "nobradov@media.mit.edu"], "title": "Machine Behavior", "modified": "2019-04-30T19:56:54.922Z", "visibility": "PUBLIC", "start_on": "2019-04-24", "location": "", "groups": ["human-dynamics", "personal-robots", "scalable-cooperation"], "published": true, "active": false, "end_on": null, "slug": "machine-behavior"}, {"website": "", "description": "<p>Cities are becoming increasingly complex. As of&nbsp;2008, more than half of the world's population has been living in cities; this number is expected to be as high as 70 percent by 2050, with the most growth occurring in the southern hemisphere. According to the Word Bank, 85 percent of the global GDP is created in cites.&nbsp;We must find new solutions to meet the complex challenges of the future.&nbsp;</p><p>We pose an opportunity to rethink current models and invent new systems and strategies. We consider&nbsp;<i>cities without</i>, and the opportunities this can pose for a more livable, equitable, and resilient future.&nbsp;</p>", "people": ["mdchurch@media.mit.edu", "gba@media.mit.edu"], "title": "Cities Without", "modified": "2019-04-08T14:52:00.977Z", "visibility": "GROUP", "start_on": "2019-03-27", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "cities-without"}, {"website": "", "description": "<p>With advances in virtual reality (VR) and physiological sensing technology, even more immersive computer-mediated communication through life-like characteristics is possible. As a solution for the current lack of culture, expression, and emotions in VR avatars, we propose a two-fold solution. First, integrate bio-signal sensors into the head-mounted display (HMD) and implement techniques to detect aspects of the emotional state of the user. Second, connect the data collected to an expressive avatar: Emotional Beasts. The creation of Emotional Beasts allowed us to experiment with the manipulation of a user's self-expression in VR space and as well as the perception of others in it, with the goal of pulling the avatar design away from the uncanny valley and making it more expressive, more relatable to our own mannerisms. Based on this we have implemented a prototype system in which VR, human motion, and physiological signals are integrated to allow avatars to become more expressive in virtual environments in real time.</p>", "people": ["gbernal@media.mit.edu"], "title": "EmotionalBeasts", "modified": "2018-10-29T02:39:13.728Z", "visibility": "PUBLIC", "start_on": "2016-08-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "emotionalbeasts-1"}, {"website": "", "description": "<p>Major depressive disorder is the leading cause of disability in the US for ages 15-44 and affects 14.8 million American adults, or about 6.7 percent of the US population age 18 and older. However, individual psychotherapy is still not yet widely accessible to the majority of people who need psychological interventions, and are not available to patients on a daily basis.&nbsp;</p><p>In order to increase accessibility to therapeutic interventions, we are developing a social robot that provides daily positive psychology interventions to&nbsp;enhance people's psychological wellbeing at their home. Our system uses multi-modal data&nbsp;and a reinforcement learning approach to personalize the interventions for each user based on his/her preferences, affect, and engagement.&nbsp; In our one-month deployment study, we expect to find higher engagement and better wellbeing from participants who are provided personalized interventions than from participants with non-personalized interventions.&nbsp;&nbsp;</p>", "people": ["cynthiab@media.mit.edu", "sooyeon6@media.mit.edu"], "title": "MoodyBoost", "modified": "2019-05-20T18:58:40.920Z", "visibility": "LAB-INSIDERS", "start_on": "2018-06-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "moodyboost"}, {"website": "https://collection.cooperhewitt.org/exhibitions/2318794479/page1", "description": "<p>We are at the dawn of a mobility revolution, where autonomous vehicles will replace cars controlled by humans. We can imagine this developing along two possible divergent paths with very different implications for cities. Will we create an ownership society with private, driverless cars? Or a world of lightweight, shared social mobility robots available to all? In this CityScope project, extreme versions of these two futures are presented. In one, streets are dominated by machines and density leads to congestion and anxiety. The other is a more vibrant city where humans recapture streets and density increases equity and creative interactions. We propose that new mobility systems must be explored to meet the profound challenges of the future and to enable dynamic and evolving places that respond to the complexities of life.</p>", "people": ["mdchurch@media.mit.edu", "alonsolp@media.mit.edu", "agrignar@media.mit.edu", "yasushis@media.mit.edu", "kll@media.mit.edu", "gbabio@media.mit.edu", "maitanei@media.mit.edu", "noyman@media.mit.edu", "gba@media.mit.edu", "csmuts@media.mit.edu", "ryanz@media.mit.edu", "aberke@media.mit.edu"], "title": "CityScope Cooper Hewitt", "modified": "2019-04-02T21:27:39.368Z", "visibility": "PUBLIC", "start_on": "2018-11-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "cityscope-cooper-hewitt"}, {"website": "https://ninalutz.github.io", "description": "<p>From skin to light: an artistic dialog between cosmetics, lasers, and lighting between the human and microscopic scale.&nbsp;</p><p>Cosmetics essentially diffract and scatter light in a way that makes the wearer appear different to the naked eye. In this project we propose an illumination grammar between cosmetic projects and&nbsp;coherent light lasers, as a suggestion for alternative interior lighting as well as artistic effect and expression. From skin to light, we explore these artistic archetypes.</p>", "people": ["vparth@media.mit.edu", "nlutz@media.mit.edu"], "title": "Cosmetic Light", "modified": "2019-02-07T15:55:16.524Z", "visibility": "PUBLIC", "start_on": "2018-02-06", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "cosmetic-light"}, {"website": "", "description": "<p>&nbsp;</p>", "people": [], "title": "Social Robot for Medication Adherence", "modified": "2019-05-20T19:03:52.395Z", "visibility": "PUBLIC", "start_on": "2019-03-01", "location": "", "groups": ["personal-robots"], "published": false, "active": false, "end_on": null, "slug": "social-robot-for-medical-adherence"}, {"website": "", "description": "<p>&nbsp;This project aims to create a modular platform for exploring micro-kitchens that are culture specific. Cooking is a personal experience that has cultural attributes.&nbsp; This project explores new modes of cooking using robotically enabled cabinets and appliances to minimize the footprint of the kitchen, while maximizing the ability for users to cook large meals, socialize, and utilize the same space during non-meal times for work. Piccolo kitchen is one of the components of the micro-units that are currently under development as part of the CityHome 02 projects.</p>", "people": ["kll@media.mit.edu", "alhadidi@media.mit.edu", "maitanei@media.mit.edu", "nanaco@media.mit.edu", "apaolaza@media.mit.edu"], "title": "Piccolo Kitchen", "modified": "2019-04-19T13:39:55.600Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "piccolo-kitchen"}, {"website": "http://datalanterns.com/", "description": "<p>The Open Water Data project explores data physicalization as a path to community engagement and action on important environmental issues. For our 2018 installation&nbsp;<i>Chemicals in the Creek</i>, we released glowing lanterns representing water quality permit violations from local facilities onto the river as part of a performance of local environmental challenges that informed a community conversation on these issues.&nbsp;</p>", "people": ["perovich@media.mit.edu"], "title": "Open Water Data", "modified": "2019-04-23T13:14:44.647Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": null, "slug": "open-water-data"}, {"website": "", "description": "", "people": ["fadel@media.mit.edu", "zhluo@media.mit.edu"], "title": "TurboTrack", "modified": "2019-02-19T17:21:52.504Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["signal-kinetics"], "published": false, "active": false, "end_on": "2021-07-31", "slug": "turbotrack1"}, {"website": "", "description": "<p>This project delivers a series of Github-signal based metrics that would be useful to both the layman and investor, helping look beyond the rocky trends of the cryptocurrency market to the actual viability of the token as a utility or security from the perspective of developer dynamic. By doing a quantitative analysis of source code and developer activity within a community, we can provide a rich set of insights into the health of a token\u2019s foundation.<br></p><p><b>Assessment Methodology</b></p><p>Building upon&nbsp;<a href=\"https://github.com/manganese/alteramentum-repo-data/blob/master/repos.csv\">previous research</a>, we gathered a ranked list of 13,695 repositories that correlate with 1,011 tokens\u2014where we assess a token (i.e., Bitcoin) which has multiple repositories (i.e., bips, bitcoin, libbase58).&nbsp;The repository metadata information includes <i>url data, forks_count, subscribers_count, network_count, open_issues_count, watchers_count, stargazers_count, size, created_at, updated_at, pushed_at, has_wiki, has_downloads, has_issues, is_fork.</i> We have built a classifier that accepts this metadata as input features &nbsp;and returns a label to help us assess ranking qualities of the specified token.</p><p>Co-authored with <a href=\"https://www.linkedin.com/in/kokje/\">Yashashree Kokje</a></p>", "people": ["oceane@media.mit.edu"], "title": "Humanized Cryptoassets", "modified": "2019-04-19T03:41:54.685Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "depicting-token-quality"}, {"website": "", "description": "<p>Search Intent Optimization unwinds SEO by externalizing intent. Users select their intent during a search\u2019s initiation. We offer a set of example controls to distinguish between shopping and information access. Adding your intent suppresses irrelevant noise results\u2014no more \"I'm feeling lucky.\"</p>", "people": ["anderton@media.mit.edu", "lip@media.mit.edu"], "title": "Search Intent Optimization", "modified": "2019-04-18T17:12:22.057Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "search-intent-optimization"}, {"website": "", "description": "<p>TurboTrack is a 3D localization system for fine-grained robotic tasks, with unique capability to localize backscatter nodes with sub-centimeter accuracy without any constraints on their locations or mobility. We showed that TurboTrack can work in multiple collaborative applications with robotic arms and nanodrones including indoor tracking, packaging, assembly, and handover.</p><p>This research is partially funded by the National Science Foundation (NSF) and a Google Faculty Research Award.</p>", "people": ["fadel@media.mit.edu", "zhluo@media.mit.edu", "yunfeima@media.mit.edu"], "title": "TurboTrack: 3D backscatter localization for fine-grained robotics", "modified": "2019-04-17T18:08:41.636Z", "visibility": "PUBLIC", "start_on": "2017-06-01", "location": "", "groups": ["signal-kinetics"], "published": true, "active": false, "end_on": null, "slug": "turbotrack-3d-backscatter-localization-for-fine-grained"}, {"website": "", "description": "<p>SpeechBlocks is a medium that allows children (4-5 years old) to engage in open-ended play with writing. They can build arbitrary compositions out of words and associated images, which can become cards, signs, stories, and \"books.\" We hypothesize that such creative, self-expressive play can foster development of basic literacy skills, like phonological awareness. However, because users of SpeechBlocks are not yet in command of writing, it is necessary for the system to scaffold and guide them. We study a variety of ways to accomplish this.<br></p>", "people": ["snehapm@media.mit.edu", "jhgray@media.mit.edu", "isysoev@media.mit.edu"], "title": "SpeechBlocks", "modified": "2019-05-21T17:11:02.880Z", "visibility": "PUBLIC", "start_on": "2015-03-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "speech-blocks"}, {"website": "", "description": "<p>The Hol-Deep-Sense project (funded by the&nbsp;EU\u2019s Horizon 2020 Marie Sk\u0142odowska-Curie grant)&nbsp;aims at holistic machine perception of human phenomena such as personal attributes (e.g., age, gender), emotion, health, as well as cognitive and physical states. The machine learning methods developed in this project help personalize AI technologies and enable natural human-machine communication. In particular, this project addresses the shortcoming of today's recognition systems that regard affective states as isolated patterns. Using novel multi-task and transfer learning techniques, we aim to shed light on the interrelations between the facets of human phenomena.&nbsp;The overarching goal of the Hol-Deep-Sense project is to create an end-to-end, multi-input, and multi-output learning framework that learns from multi-modal sensory inputs (e.g., audio, visual, physiological signals), an acoustic model to jointly recognize multiple output targets.&nbsp;</p>", "people": ["yuefw@media.mit.edu"], "title": "HOL-DEEP-SENSE", "modified": "2019-05-31T19:54:27.551Z", "visibility": "PUBLIC", "start_on": "2018-10-15", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2020-04-15", "slug": "hol-deep-sense"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: 400;\">StoryBlocks aims to promote creative expression, literacy development, and social-emotional development through storytelling for children ages six to ten. In this app, children create personally generated, comic-style stories by inserting characters, setting emotions, typing dialogue, using words to insert images that customize scenes, and recording their voices to narrate their unique stories. With StoryBlocks, we can collect a corpus of children\u2019s stories in order to build analysis tools that&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">can document children\u2019s narrative development over time, and support coaches in providing personalized scaffolding for children\u2019s narratives.&nbsp;</span></p>", "people": ["anneli@media.mit.edu", "snehapm@media.mit.edu", "exposito@media.mit.edu", "sballing@media.mit.edu", "jnazare@media.mit.edu"], "title": "StoryBlocks", "modified": "2019-04-11T15:39:53.269Z", "visibility": "PUBLIC", "start_on": "2016-07-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": null, "slug": "storyblocks"}, {"website": "", "description": "<p>We have pioneered the development of fully genetically encoded reagents that, when targeted to specific cells, enable their physiology to be controlled via light. These reagents, known as optogenetic tools, enable temporally precise control of neural electrical activity, cellular signaling, and other high-speed physiological processes using light. Such tools are in widespread use in neuroscience and bioengineering, for the study of how specific neurons contribute to cognition, emotion, and movement, and to brain disorder states, or to the remedy thereof. These tools are also being evaluated as components of prototype optical neural control prosthetics for ultraprecise treatment of intractable brain disorders. Derived from the natural world, these tools highlight the power of ecological diversity, in yielding technologies for analyzing biological complexity and addressing human health. We distribute these tools as freely as possible, and routinely host visitors to learn optogenetics.</p>", "people": ["esb@media.mit.edu"], "title": "Optogenetics: Molecules enabling neural control by light", "modified": "2019-04-17T18:30:04.066Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-427", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": null, "slug": "optogenetics-and-synthetic-biology-tools"}, {"website": "", "description": "<p>The Huggable is a new type of robotic companion for health care, education, and social communication applications. The Huggable is much more than a fun, interactive robotic companion; it functions as an essential team member of a triadic interaction. Therefore, the Huggable is not meant to replace any particular person in a social network, but rather to enhance it.<br></p><p>Children and their parents may undergo challenging experiences when admitted for inpatient care at pediatric hospitals. While most hospitals make efforts to provide socio-emotional support for patients and their families during care, gaps still exist between human resource supply and demand. The Huggable project aims to close this gap by creating a social robot able to mitigate stress, anxiety, and pain in pediatric patients by engaging them in playful interactions. In collaboration with Boston Children's Hospital and Northeastern University, we are currently running an experimental study to compare the effects of the Huggable robot to a virtual character on a screen and a plush teddy bear. We demonstrated preliminarily that children are more eager to emotionally connect with and be physically activated by a robot than a virtual character, illustrating the potential of social robots to provide socio-emotional support during inpatient pediatric care.</p>", "people": ["cynthiab@media.mit.edu", "sooyeon6@media.mit.edu"], "title": "Huggable: A social robot for pediatric care", "modified": "2019-04-17T18:42:16.283Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["advancing-wellbeing", "personal-robots"], "published": true, "active": false, "end_on": null, "slug": "huggable-a-social-robot-for-pediatric-care"}, {"website": "", "description": "\u200b<p>Although significant progress is being made in identifying pH sensing materials&nbsp;and device configurations, a standard protocol for benchmarking performance&nbsp;of next-generation pH devices is still lacking. In particular, key properties&nbsp;of characterization systems, such as inherent component contributions, time plots for extended-gate field-effect transistor (EGFET) measurements,&nbsp;and the input resistance (Rin), often go unreported in studies of pH sensing&nbsp;systems. These properties strongly influence the characterization system and&nbsp;can lead to mistaken attribution of properties to the device. In this project, a series of essential characterization tests and parameters are reported to&nbsp;evaluate pH systems, such as the zinc oxide (ZnO) EGFET, in a standardized protocol.&nbsp;This EGFET ZnO sensor has a sensitivity of \u221258.1 mV pH\u22121, drift range from&nbsp;2.5 to 14.2 \u03bcA h\u22121, and response time of 136 s. By using a ZnO sensing&nbsp;electrode, it is demonstrated that i) intrinsic contributions of reference&nbsp;electrode and commercial transistor (for EGFET) are not negligible; ii) time&nbsp;plots for EGFET configuration and defining a critical point at the onset of drift&nbsp;are essential for accurate sensitivity, response time, and drift reporting; and&nbsp;iii) the results of the pH sensing system are strongly dependent on the input resistance of the used characterization instruments.</p>", "people": ["asadraei@media.mit.edu", "mghoneim@media.mit.edu", "canand@media.mit.edu"], "title": "A Protocol to Characterize pH Sensing Materials and Systems", "modified": "2019-04-17T18:50:52.506Z", "visibility": "PUBLIC", "start_on": "2018-08-23", "location": "", "groups": ["conformable-decoders"], "published": true, "active": false, "end_on": null, "slug": "a-protocol-to-characterize-ph-sensing-materials-and-systems"}, {"website": "", "description": "<p>Homes and offices are being filled with sensor networks to answer specific queries and solve pre-determined problems, but no comprehensive visualization tools exist for fusing these disparate data to examine relationships across spaces and sensing modalities. DoppelLab is a cross-reality virtual environment that represents the multimodal sensor data produced by a building and its inhabitants. Our system encompasses a set of tools for parsing, databasing, visualizing, and sonifying these data; by organizing data by the space from which they originate, DoppelLab provides a platform to make both broad and specific queries about the activities, systems, and relationships in a complex, sensor-rich environment. </p>", "people": ["gershon@media.mit.edu", "bmayton@media.mit.edu", "joep@media.mit.edu"], "title": "DoppelLab: Experiencing multimodal sensor data", "modified": "2019-04-19T14:22:34.749Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "doppellab-experiencing-multimodal-sensor-data"}, {"website": "", "description": "<p>Nearly all classes of wearable and implantable biomedical devices depend on battery power for continuous operation. However, the life span of batteries is limited, rarely exceeding a few hours for wearables and a few years for implants. Consequently, battery replacements and, often times, surgical procedures are required to change the depleted batteries of implants, exposing people to high risks of surgical complications and/or high financial costs. This project seeks to develop conformal piezoelectric patches integrated to personal garments to extract energy from body movements such as motion of arms, fingers, and legs. The completion of this project could improve quality life for people and potentially provide environmentally friendly power.</p>", "people": ["asadraei@media.mit.edu", "canand@media.mit.edu"], "title": "Conformal Piezoelectric Mechanical Energy Harvesters: Mechanically Invisible Human Dynamos", "modified": "2019-04-17T18:52:17.221Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["conformable-decoders"], "published": true, "active": false, "end_on": null, "slug": "conformal-piezoelectric-mechanical-energy-harvesters"}, {"website": "", "description": "<p>Artificial intelligence (AI) agents in an embodied form, such as Jibo, Amazon Alexa, and Google Home, are increasingly becoming part of our daily lives and our homes. While there have been numerous studies in lab settings documenting short-term individual interactions with intelligent agents, we are at a point where we need to be exploring the larger impact of these technologies in the world, living with real people over longer periods of time.</p><p>From a design research perspective, understanding and developing robots and AI that intersect with society is a \u201cwicked problem,\u201d a problem with many components that cannot be solved without interdisciplinary approaches. Design research within interdisciplinary applications has sought to develop approaches, methods, tools, and techniques to investigate the impact of technologies and inform future development. This work focuses on developing tools for exploring robots\u2019 and AI\u2019s impact on daily lives to better inform the&nbsp;development of these technologies by elucidating academia\u2019s and industry\u2019s requirements of tools for this domain.</p><p>For more information, please contact Anastasia Ostrowski (<a href=\"mailto:akostrow@media.mit.edu\">akostrow@media.mit.edu</a>).</p>", "people": ["cynthiab@media.mit.edu", "akostrow@media.mit.edu", "haewon@media.mit.edu"], "title": "Tools to investigate societal impacts of robots and AI", "modified": "2019-04-17T18:48:44.577Z", "visibility": "PUBLIC", "start_on": "2018-07-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "tools-to-investigate-societal-impacts-of-robot-ai"}, {"website": "http://news-graph.um-dokku.media.mit.edu", "description": "<p>This project aims to show a different picture of the data behind the news, looking at how we analyze, represent, and interact with it.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">Video content is constantly created and added to the public archives, but there is never time to watch it all. News Graph explores a new method for interacting with news media.&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">By analyzing the words that are said, extracting entities that appear, and finding the connections between them, we are able to map connections between video segments. Each connection represents two entities that were mentioned in the same video segment, and a video segment can be mapped to a number of connections.</span></p>", "people": ["lip@media.mit.edu", "jasrub@media.mit.edu"], "title": "News Graph", "modified": "2019-04-18T01:15:48.619Z", "visibility": "PUBLIC", "start_on": "2016-04-01", "location": "", "groups": ["ultimate-media", "viral-communications"], "published": true, "active": false, "end_on": "2019-09-30", "slug": "news-graph"}, {"website": "", "description": "<p>Doppelmarsh is a cross-reality sensor data browser built for experimenting with presence and multimodal sensory experiences. Built on evolving terrain data from a physical wetland landscape, the software integrates real-time data from an environmental sensor network with real-time audio streams and other media from the site. Sensor data is rendered in the scene in both visual representations and as 3D sonification. Users can explore this data by walking on the virtual terrain in a first person view, or flying high above it. This flexibility allows Doppelmarsh to serve as an interface to other research platforms on the site, such as Quadrasense, an augmented reality UAV system that blends a flying live camera view with a virtual camera from Doppelmarsh. We are currently investigating methods for representing subsurface data, such as soil and water temperatures at depth, as well as automation in scene and terrain painting.</p>", "people": ["gershon@media.mit.edu", "bmayton@media.mit.edu", "joep@media.mit.edu", "ddh@media.mit.edu", "sfr@media.mit.edu", "eflynch@media.mit.edu"], "title": "Doppelmarsh: Cross-reality environmental sensor data browser", "modified": "2019-04-19T14:24:11.949Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": null, "slug": "doppelmarsh-cross-reality-environmental-sensor-data-browser"}, {"website": "", "description": "<p>Understanding social-emotional behaviors in storytelling interactions plays a critical role in the development of interactive and educational technologies for children. A challenge when designing for such interactions using technologies like social robots, virtual agents, and tablets is understanding the social-emotional behaviors pertinent to the storytelling context\u2014especially when emulating a natural peer-to-peer relationship between the child and the technology.&nbsp;&nbsp;We present P2PSTORY, a dataset of young children (5-6 years old) engaging in natural peer-to-peer storytelling interactions with fellow classmates.&nbsp;The dataset contains 58 recorded storytelling sessions along with a diverse set of behavioral annotations as well as developmental and demographic profiles of each child participant.&nbsp;</p><p>The CHI 2018 paper presenting this dataset can be found here:&nbsp;<br><b><a href=\"https://www.media.mit.edu/publications/p2pstory-dataset-of-children-storytelling-and-listening-in-peer-to-peer-interactions/\">Nikhita Singh, Jin Joo Lee, Ishaan Grover, and Cynthia Breazeal (2018). P2PSTORY: Dataset of Children Storytelling and Listening in Peer-to-Peer Interactions. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems.</a></b></p><br><p>See below for instructions on how to access the dataset.</p>", "people": ["cynthiab@media.mit.edu", "igrover@media.mit.edu", "jinjoo@media.mit.edu", "nikhita@media.mit.edu"], "title": "P2PSTORY: Dataset of children as storytellers and listeners in peer-to-peer interactions", "modified": "2019-06-10T09:19:51.212Z", "visibility": "PUBLIC", "start_on": "2018-01-22", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "p2pstory"}, {"website": "", "description": "<p>QTY-designed, detergent-free chemokine receptors have been expressed in SF9 insect cells, as well as produced using a low cost and simple <i>E.coli&nbsp;</i>system with much higher throughput. These QTY-designed receptor variants exhibit remarkable heat stability in the presence of arginine additive, retaining ligand binding activity after 100\u00b0C treatment. New protein variants can also be designed using the same alpha-helical segments but switching the extracellular (EC) loop, e.g., using variant B\u2019s EC loop to directly attach to variant A\u2019s alpha-helical segments. This approach helps our understanding of the binding mechanism of QTY variants and natural membrane receptors, as well as enables a novel pathway for the design and production of multi-functional, water-soluble membrane receptors, with tunable properties for <i>in vitro&nbsp;</i>and <i>in vivo&nbsp;</i>applications.&nbsp;</p>", "people": ["shuguang@media.mit.edu", "ruiqing@media.mit.edu"], "title": "Water-soluble transmembrane protein receptors with exchangeable and tunable ligand affinity", "modified": "2019-04-17T02:42:22.369Z", "visibility": "LAB-INSIDERS", "start_on": "2019-04-01", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "water-soluble-transmembrane-protein-receptors-with-exchangeable-tunable-ligand-affinity"}, {"website": "", "description": "<p>The brain is a three-dimensional, densely-wired circuit that computes via large sets of widely distributed neurons interacting at fast timescales. In order to understand the brain, ideally it would be possible to observe the electrical activity, and other intra- and intercellular signaling pathways, of many neurons\u2014and ideally entire brains\u2014with as great a degree of precision as possible, so as to understand the neural codes and dynamics that are produced by the circuits of the brain. Our lab and our collaborators are developing a number of innovations\u2014such as new fluorescent reporters of cellular signals such as voltage, and new robotic and nanotechnological probes\u2014to enable such analyses of neural circuit dynamics. These tools will hopefully enable pictures of how neurons work together to implement brain computations, and how these computations go awry in brain disorder states. Such neural observation strategies may also serve as detailed biomarkers of brain disorders or indicators of potential drug side effects. These technologies may, in conjunction with optogenetics, enable closed-loop neural control technologies, which can introduce information into the brain as a function of brain state (\"brain co-processors\"), enabling new kinds of circuit characterization tool as well as new kinds of advanced brain-repair prosthetic. To build these tools, we are developing supporting approaches such as robots and molecular strategies for multidimensional directed evolution of protein-based tools in mammalian cells.</p>", "people": ["esb@media.mit.edu"], "title": "Tools for recording high-speed brain dynamics", "modified": "2019-04-17T18:33:14.786Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": null, "slug": "tools-for-recording-high-speed-brain-dynamics"}, {"website": "http://blakeleyhoffman.me/", "description": "<h2>How do we raise conscientious consumers and designers of AI?</h2><p>Children today live in the age of artificial intelligence. On average, US children tend to receive their first smartphone at age 10, and by age 12 over half of all children have their own social media account. Additionally, it's estimated that by 2022, there will be 58 million new jobs in the area of artificial intelligence. Thus, it's&nbsp;important that the youth of today are both conscientious consumers and designers of AI.&nbsp;</p><p>This project seeks to develop an open source curriculum for middle school students on the topic of artificial intelligence. Through a series of lessons and activities, students learn technical concepts\u2014such as how to train a simple classifier\u2014and the ethical implications those technical concepts entail, such as algorithmic bias.&nbsp;</p>", "people": ["cynthiab@media.mit.edu", "haewon@media.mit.edu", "blakeley@media.mit.edu"], "title": "AI + Ethics Curriculum for Middle School", "modified": "2019-04-17T18:36:01.110Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "ai-ethics-for-middle-school"}, {"website": "", "description": "<p>Empathy is a core human skill. From early stages of our lives, being able to understand and behave with empathy is fundamental to our social experience. Research in the field of social robotics suggests that given a set of behaviors from a social robot, a child can perceive this agent as empathic. In this project, we explore a novel approach to modeling empathy in children using a social robot. Two social robots were programmed to have conversations containing interactions depicting empathic and non-empathic behaviors. Children were provided with opportunities to act on these interactions as well as to comment on the robot's behavior afterward.</p>", "people": ["cynthiab@media.mit.edu", "pcuellar@media.mit.edu"], "title": "The role of social robots in fostering human empathy", "modified": "2019-04-17T18:49:27.482Z", "visibility": "PUBLIC", "start_on": "2017-09-14", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": null, "slug": "children-s-perception-of-empathy-in-robot-robot-scenarios"}, {"website": "https://sip.scratch.mit.edu/", "description": "<p>Scratch in Practice (SiP) shares ideas and resources from the Scratch Team and educators around the world. Each month, the SiP website features a new theme to explore and discuss, and the team hosts conversations on social media to connect with educators.</p><br><p>SiP is developed by the Scratch Team and the Lifelong Kindergarten group at the MIT Media Lab. The goal of SiP is to help more educators integrate Scratch into their practice in a way that supports a creative learning approach.&nbsp;</p>", "people": ["tarmelop@media.mit.edu", "calla@media.mit.edu", "christan@media.mit.edu", "jaleesat@media.mit.edu", "kosiecki@media.mit.edu", "kathywu@media.mit.edu", "yusufa@media.mit.edu", "carolcr@media.mit.edu", "eschill@media.mit.edu", "hisean@media.mit.edu", "gabaree@media.mit.edu", "sotts@media.mit.edu", "mres@media.mit.edu", "muthui@media.mit.edu", "nrusk@media.mit.edu", "bowman@media.mit.edu", "leob@media.mit.edu", "yumikom@media.mit.edu", "champika@media.mit.edu"], "title": "Scratch in Practice", "modified": "2019-04-02T15:46:36.454Z", "visibility": "PUBLIC", "start_on": "2019-02-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": null, "slug": "scratch-in-practice"}, {"website": "", "description": "<p>Wall of Now is a multi-dimensional media browser of recent news items. It attempts to address our need to know everything by presenting a deliberately overwhelming amount of media, while simplifying the categorization of the content into single entities. Every column in the wall represents a different type of entity: people, countries, states, companies, and organizations. Each column contains the top-trending stories of that type in the last 24 hours. Pressing on an entity will reveal a stream of video that relates to that specific entity. The Wall of Now is a single-view experience that challenges previous perceptions of screen space utilization towards a future of extremely large, high-resolution displays.</p>", "people": ["weller@media.mit.edu", "anderton@media.mit.edu", "lip@media.mit.edu"], "title": "Wall of Now", "modified": "2019-04-05T20:29:14.403Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["ultimate-media", "viral-communications"], "published": true, "active": false, "end_on": null, "slug": "wall-of-now"}, {"website": "", "description": "<p>Many people think their vote doesn't count\u2014that a single vote would not change an election's outcome\u2014and they stay home on Election Day. However, your vote is a public statement of beliefs, amplified because many people in your voting district may agree with you, but do not (and in many cases cannot) vote. Your vote gives a voice to those many others.</p><p>For example, congressional elections affect everyone in the district, regardless of whether they are eligible to vote. The same senators represent everyone in your state. The same ballot questions affect everyone in your community.</p><p>Using data we've gathered from the United States Census Bureau, we have calculated just how many people you're representing with your vote. We directly compare the number of votes in recent congressional elections (by district) to the total population of the district.</p><p>When you vote, you're not only voting for your own best interest, but you're also voting for the best interest of those around you.</p><p>Visit&nbsp;<a href=\"https://www.relativotey.org/\" style=\"font-size: 18px; font-weight: 400;\">https://www.relativotey.org</a>&nbsp;and<span style=\"font-size: 18px; font-weight: 400;\">&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">recognize your relative voting power!</span></p>", "people": ["lip@media.mit.edu"], "title": "RelatiVotey", "modified": "2019-04-18T01:18:25.012Z", "visibility": "PUBLIC", "start_on": "2018-09-04", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": null, "slug": "relativoty"}, {"website": "", "description": "<p>In collaboration with Prof. Horst Vogel and Dr. Horst Pick (EPFL, Lausanne, Switzerland).</p><p>G protein-coupled receptors (GPCRs) are vital for diverse biological functions, including vision, smell, and aging. They are also involved in a wide range of diseases, and are among the most important targets of medicinal drugs. Tools that facilitate GPCR studies or GPCR-based technologies or therapies are thus critical to develop. We used QTY code (glutamine, threonine, tyrosine) to systematically replace 29 membrane-facing leucine (L), isoleucine (I), valine (V), and phenylalanine (F) residues in the transmembrane alpha-helices of the GPCR CXCR4. This variant, CXCR4QTY29, became more water-soluble, while retaining the ability to bind its natural ligand CXCL12. When transfected CXCR4QTY29 gene into HEK293 cells, the translated CXCR4QTY29 receptor inserted into the cell membrane and retained its cellular signaling activity. This QTY code can significantly improve GPCR and membrane protein studies by making it possible to design functional hydrophilic receptors. The QTY code can be applied to diverse a-helical transmembrane proteins, and may aid in the development of other applications, including clinical therapies.&nbsp;</p>", "people": ["shuguang@media.mit.edu"], "title": "QTY CXCR4 becomes more hydrophilic and retains cell signaling activity", "modified": "2019-04-01T17:45:11.000Z", "visibility": "LAB-INSIDERS", "start_on": "2019-04-01", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": null, "slug": "cxcr4-designed-using-qty-code-becomes-more-hydrophilic-and-retains-cell-signaling-activity"}, {"website": "", "description": "<p>Building new techniques and applications for developing space food based on fermentation can have a remarkable influence on human experience, cognitive performance, and overall wellbeing. Fermentation can be harnessed to improve astronaut gut health, which is linked to behavior and cognitive performance; manage food waste by preserving the nutritional value and flavor of fresh ingredients and space food leftovers; provide new creative modes of preparing food to give agency to astronauts over what they eat; and enhance flavor and the sensory experience of food through umami.</p><p><span style=\"font-size: 18px; font-weight: 400;\">New flavors will evolve through the migration of ingredients to outer space and our ongoing research with Josh Evans, from Empirical Spirits&nbsp; and noma, aims to map the journey of a new space \"terroir\".&nbsp;</span><br></p><p>We are exploring new techniques and applications for developing space food based on fermentation with Blanca Del Noval from the Basque Culinary Lab.&nbsp;<br></p>", "people": [], "title": "Space Terroir", "modified": "2019-06-10T13:32:34.233Z", "visibility": "LAB", "start_on": "2019-04-01", "location": "", "groups": ["space-exploration"], "published": false, "active": false, "end_on": "2020-04-30", "slug": "fermentation"}, {"website": "", "description": "<p>All over Africa, experts use satellite Earth Observation (EO) data for applications such as monitoring crop health or assessing the risk of disease vectors. These applications are often done at a national scale meaning there is a challenge to ensure that end users such as small companies, rural communities or otherwise marginalized groups benefit from EO systems. This project explores an EO application with the enterprise Green Keeper Africa (GKA) based in Cotonou, Benin, that addresses the management of an invasive plant species that is threatening local economic activities such as fishing. GKA helps control the infestation of the water hyacinth on Lake Nokoue by repurposing the plant into a product that absorbs oil-based waste. The EO application is an online Environmental Observatory that utilizes satellite, aerial and ground data to map the location of the water hyacinth over time, providing valuable information for government, private and public users. The research outcomes presented in this project address processes that (i) outline the steps for a small company in Benin to setup and operate a new EO technological capability, and (ii) enable low cost data collection of parameters describing the coastal water ecosystem.&nbsp;</p><p>In the observatory, the technique Normalized Difference Vegetation Index (NDVI) is applied to free satellite data to identify likely locations of the hyacinth in the target region of Lake Nokoue.&nbsp;</p>", "people": ["drwood@media.mit.edu", "ufuoma@media.mit.edu"], "title": "Low-cost invasive species management in coastal ecosystems: A case study in Benin", "modified": "2019-04-22T18:27:41.358Z", "visibility": "LAB-INSIDERS", "start_on": "2018-08-01", "location": "", "groups": ["space-enabled"], "published": true, "active": false, "end_on": null, "slug": "technology-design-and-assessment-for-coastal-water-ecosystem-management-a-case-study-of-benin"}, {"website": "", "description": "<p>Beautiful Me&nbsp;has been designed to address the problem of&nbsp;<a href=\"https://en.wikipedia.org/wiki/Acid_throwing\">acid attacks</a>&nbsp;in southeast Asian countries and parts of Europe. This form of assault is done primarily to disfigure someone's body or in an attempt to kill them. As there are very few legal checks pertaining to the buying and selling of acid, we have developed a foundation (topical cream) which can act as a barrier against the acid. The foundation formula creates a thin film, which reacts with the acid to reduce its effects and prevents the acid from penetrating to the skin. This first layer of defense is oleophilic,&nbsp; hence the acid can be washed off easily.&nbsp;</p>", "people": ["manisham@media.mit.edu"], "title": "Beautiful Me", "modified": "2019-05-22T16:48:35.447Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "", "groups": [], "published": true, "active": false, "end_on": null, "slug": "beautiful-me"}, {"website": "", "description": "\u200b<p><b>Cube Puzzles: A tangible platform for dynamic assessment of cognitive and psychomotor skills</b></p><p>Cognitive and psychomotor assessments are a vital scientific tool; however, they are resource intensive, can be biased by examiner presence and interaction, and can disadvantage individuals with neurodevelopmental differences such as autism or ADHD. We present a new tangible user interface, Cube Puzzles, that dynamically tracks placement and orientation of colored wooden blocks. The system not only automates data acquisition and captures real-time learning progressions, but it also enables customizable, motivating feedback and synchronization with biosensors.&nbsp;</p><p>We highlight the potential of the system with two user studies involving 22 participants. The first study (n=10) validates and visualizes the dynamic data capture of real-time user activity. The second study (n=12) explores Cube Puzzles as a gamified cognitive assessment platform by examining the effects of reinforcement on user performance, engagement, and physiological arousal. Our work has broad applications within educational and assessment domains, particularly for neurodiverse and elderly individuals.</p><p><b>Part of the SPRING system</b></p><p>Cube Puzzles is a new game-play module for SPRING. In this module, children move and orient colored blocks in order to match a displayed design. This module is inspired by the Block Design activity in the Wechsler Intelligence Scales (e.g., WAIS), quantifying visual-spatial orientation and motor skills. In free-play mode, Cube Puzzles also encourages open-ended design and play with geometric shapes and colors.&nbsp;&nbsp;</p>", "people": ["ktj@media.mit.edu"], "title": "Cube Puzzles", "modified": "2019-04-22T18:36:42.408Z", "visibility": "PUBLIC", "start_on": "2017-01-09", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "cube-puzzles"}, {"website": "", "description": "<p>The diagnosis and tracking of mood disorders still rely on clinical assessments, originating more than 50 years ago, of self-reported depressive symptoms via surveys and interviews. Such methods are known to provide limited accuracy and reliability in addition to being costly to track and scale. Once a problem is detected, providing personalized daily intervention and support is also too costly and does not scale. The goal of this pilot project is to develop a proof of concept of personalized emotional wellness coach focusing on key technology modules to create an emotionally intelligent social robot for this targeted domain. We shall also conduct a pilot evaluation with a five-week user study to evaluate the robot coach with respect to its ability to successfully sustain a user long-term adherence (i.e., daily self-report and efficacious advice)\u2014with the expected result that it is more effective than state-of-the-art, gamified mobile apps currently used.&nbsp;</p>", "people": ["cynthiab@media.mit.edu", "picard@media.mit.edu", "agata@media.mit.edu", "aymerich@media.mit.edu", "sooyeon6@media.mit.edu", "haewon@media.mit.edu"], "title": "Personalized Emotional Wellness Coach", "modified": "2019-04-22T15:26:39.440Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2019-09-30", "slug": "personalized-emotional-wellness-coach"}, {"website": "", "description": "<p>Maiden Flight is an autonomous biological laboratory environment designed for studying the impact of space flight on the sole reproductive node of a bee colony: <b>the queen bee and her retinue.&nbsp;&nbsp;</b></p><p>It represents the first space module of its kind built specifically to cater to queen bees. The hybrid-ecology of the capsule was created to take into account the distributed and uniquely non-human nature of bee biology, in order to consider how to extend the bee reproductive system for environmental extremes. This aim is reflected in the structure of the capsule interior, which was assembled by humans and augmented by the bees\u2019 natural fabrication.&nbsp;</p><p>In May 2019, the Mediated Matter group traveled to Texas to launch two laboratory capsules on Blue Origin\u2019s sub-orbital rocket system, New Shepard. Each custom-designed&nbsp;<b>metabolic support capsule&nbsp;</b>comprised an experimental environment for one queen bee and an attending retinue of 10-20 nurse bees for a parabolic flight to a 100-kilometer micro-gravitational space apogee, and back.</p>", "people": ["rssmith@media.mit.edu", "fkraemer@media.mit.edu", "renri@media.mit.edu", "neri@media.mit.edu"], "title": "Maiden Flight", "modified": "2019-05-17T13:11:46.844Z", "visibility": "PUBLIC", "start_on": "2019-04-19", "location": "", "groups": ["mediated-matter", "space-exploration"], "published": true, "active": false, "end_on": null, "slug": "maiden-flight"}, {"website": "", "description": "<p>The Project on Affinities and Language (PAL) is designed to help us understand what happens in a child\u2019s brain when they engage with their interests, passions, or hobbies \u2013 also known as \u201caffinities.\u201d</p>", "people": ["ktj@media.mit.edu"], "title": "PAL: Project on Affinities & Language", "modified": "2019-04-22T20:25:25.031Z", "visibility": "PUBLIC", "start_on": "2018-03-01", "location": "", "groups": ["affective-computing"], "published": false, "active": false, "end_on": null, "slug": "pal-project-on-affinities-language"}, {"website": "", "description": "<p>Every day, data about our environment, education, routes and other aspects of our lives become available.&nbsp;&nbsp;Yet, most people often struggle to understand, use and relate to these data. Especially because the access to all this data is often in formats that only few can retrieve, read, and understand.</p><p>In this project, we seek to&nbsp;disrupt the way we present and interact with datasets, using art as a vehicle to tell our stories informed by datasets.&nbsp;&nbsp;<b>Data dancing is a data experience.</b> \u201c<i>A data experience takes data off the screen and puts it into the physical world</i>\u201d [<a href=\"http://lauraperovich.com/thesisPerovich.pdf\">Perovich L., 2015</a>].&nbsp;<br></p><p>This project explores&nbsp;the ways in which data can be physicalized through human expression.&nbsp;We are working with&nbsp;members of the community to co-compose dance performances informed by datasets and, validate the use of dance and acting as methods to enhance data literacy.&nbsp;</p>", "people": ["perovich@media.mit.edu", "ethanz@media.mit.edu", "mavipasi@media.mit.edu", "rahulb@media.mit.edu"], "title": "Data Dancing", "modified": "2019-06-07T20:04:38.751Z", "visibility": "PUBLIC", "start_on": "2019-05-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2020-05-01", "slug": "dancing-data"}, {"website": "", "description": "<h2><b><i>What is ELSA?</i></b></h2><p>ELSA is an AI-powered chatbot that acts as an empathetic companion, encouraging users to talk about their day through a form of interactive journaling.</p><p>You can try some of the current ELSA bots in this online&nbsp;<a href=\"http://elsaneural.net\">demo</a>.&nbsp;</p><h2><b><i>How does ELSA work?</i></b></h2><p>Our project goal is to build a more empathetic neural network conversational AI by incorporating a deeper understanding of both the affective content of the conversation and the topic.&nbsp; More specifically, we build hierarchical recurrent neural network models that can converse like people &nbsp;and use transfer learning of topic and emotional tone recognition models to improve our final model.</p><h2><b><i>What are the applications of ELSA?</i></b></h2><p>Beyond the development of chatbots that act as an empathetic companion, we have a more ambitious and longer term goal: deploy the empathetic companion bots to support mental health.&nbsp; In particular, &nbsp;we aim to make ELSA useful for:</p><ul><li>Eliciting journaling</li><li>Suggesting behavioura interventions</li><li>Using Cognition Behavioral Therapy</li><li>Detecting individuals at risk of depression or suicide</li></ul><h2><b><i>Work in progress</i></b></h2><p>ELSA is a recently started project in the Affective Computing group. You can see an example of ELSA bot conversations below. You can also try our online <a href=\"http://elsaneural.net\">demo</a>. &nbsp;&nbsp;</p>", "people": ["fergusoc@media.mit.edu", "jaquesn@media.mit.edu", "asma_gh@media.mit.edu", "picard@media.mit.edu", "judyshen@media.mit.edu", "agata@media.mit.edu", "ncjones@media.mit.edu"], "title": "ELSA: Empathy learning, socially-aware agents", "modified": "2019-04-22T19:06:31.702Z", "visibility": "PUBLIC", "start_on": "2019-03-03", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "elsa"}, {"website": "", "description": "<p>Andorra and the City Science research group at the MIT Media Lab are taking on the challenge of turning Andorra into an \u201cInternationally Recognized Intelligent Country.\u201d The Andorra Living Lab project combines different research topics (tourism, innovation, energy and environment, mobility, dynamic urban planning) for the future urban challenges of the country. We are collaborating on a unique initiative providing Andorrans with the research, knowledge, methods, and tools to carry out such transformation.<br></p><p>Learn more below about City Science Andorra research below.</p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "mcllin@media.mit.edu", "doorleyr@media.mit.edu", "devisj@media.mit.edu", "kll@media.mit.edu", "maitanei@media.mit.edu", "noyman@media.mit.edu", "ptinn@media.mit.edu", "csmuts@media.mit.edu", "ryanz@media.mit.edu"], "title": "City Science Lab Andorra", "modified": "2019-05-24T18:49:18.611Z", "visibility": "PUBLIC", "start_on": "2014-05-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "city-science-andorra"}, {"website": "", "description": "<p>Automatic emotion recognition has become a well-established machine learning task in recent years. The sensitive and subjective nature of emotions may give rise to societal challenges manifesting from incorrect or misinterpreted predictions. In this work, we make the argument that emotion recognition models have an obligation to quantify their uncertainty (or similarly, provide confidence bounds). We provide demonstrations of how classical network architectures can be altered to give measures of epistemic and aleatoric uncertainty using established probabilistic inference techniques. We also explore what these uncertainties explain about the data and predictions and how it can reveal a lack of diversity in training data. We demonstrate how difficult and subjective training samples can be identified using these learned uncertainty measures.</p>", "people": ["asma_gh@media.mit.edu"], "title": "UncNet: Modeling uncertainty in deep learning for inherently subjective tasks", "modified": "2019-04-23T13:45:34.012Z", "visibility": "LAB-INSIDERS", "start_on": "2019-02-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "uncnet"}, {"website": "http://www.harpreetsareen.com", "description": "<p>Biological neuronal networks are highly complex and interconnected with superior information processing capabilities. Such networks have previously served as a model for creating inorganic synapses [1] through silver compounds. However, efficiently generating such complex networks through conventional fabrication remains a challenge. Our mission objective was to grow nanometer/sub-micrometer scale silver dendrite networks in microgravity and characterize them for nanoscale manufacturing on Earth.&nbsp;</p>", "people": ["sareen@media.mit.edu"], "title": "Floral Cosmonauts: Self-assembling silver dendrite networks in microgravity", "modified": "2019-05-08T18:01:27.669Z", "visibility": "PUBLIC", "start_on": "2018-09-01", "location": "", "groups": ["fluid-interfaces", "space-exploration"], "published": true, "active": false, "end_on": "2019-06-30", "slug": "floral-cosmonauts"}, {"website": "", "description": "<p>More ocean data has been collected in the last two years than in all previous years combined, and we are on a path to continue to break that record. More than ever, we need to establish a solid foundation for processing this ceaseless stream of data. This is especially true for visual data, where ocean-going platforms are beginning to integrate multi-camera feeds for observation and navigation.&nbsp;Techniques to efficiently process and utilize visual datasets with machine learning exist and continue to be transformative, but have had limited success in the ocean world due to:</p><ul><li>Lack of data set standardization;</li><li>Sparse annotation tools for the wider oceanographic community; and&nbsp;</li><li>Insufficient formatting of existing, expertly curated imagery for use by data scientists.&nbsp;</li></ul><p>Building on successes of the machine learning community, we are developing a public platform that makes use of existing (and future) expertly curated data. Our efforts will establish a new baseline dataset, optimized to directly accelerate development of modern, intelligent, automated analysis of underwater visual data. This effort will ultimately enable scientists, explorers, policymakers, storytellers, and the public to know what\u2019s in the ocean and where it is for effective and responsible marine stewardship.</p>", "people": ["katybell@media.mit.edu", "gaikwad@media.mit.edu"], "title": "Big Ocean, Big Data", "modified": "2019-04-22T17:49:21.666Z", "visibility": "PUBLIC", "start_on": "2018-04-02", "location": "", "groups": ["open-ocean"], "published": true, "active": false, "end_on": null, "slug": "big-ocean-big-data"}, {"website": "", "description": "<p>The delivery of mental health interventions via ubiquitous devices has shown a lot of promise. A natural conversational interface that allows longitudinal symptom tracking and appropriate just-in-time interventions would be extremely valuable. However, the task of designing emotionally aware agents is still poorly understood. Furthermore, the feasibility of automating the delivery of just-in-time mHealth interventions via such an agent has not been fully studied. In this project, we explore the design and evaluation of EMMA (EMotion-Aware mHealth Agent).</p><p>EMMA conducts experience sampling in an empathetic manner and provides emotionally appropriate micro-activities. We show the system can be extended to detect a user's mood purely from smartphone sensor data.&nbsp;</p><p>We have conducted a three-week user study (N=58).&nbsp;Our results show that extroverts preferred EMMA significantly more, and that our personalized machine learning model was effective, as was relying on ground-truth emotion samples from users.</p>", "people": ["asma_gh@media.mit.edu"], "title": "EMMA: An emotionally intelligent personal assistant for improving wellbeing", "modified": "2019-04-23T13:43:42.840Z", "visibility": "PUBLIC", "start_on": "2017-05-26", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "emma-an-emotionally-intelligent-personal-assistant-for-improving-wellbeing"}, {"website": "", "description": "<p>One of the six research methods used by the Space Enabled research group is creating models of complex systems by drawing on techniques from systems engineering, social science, and earth science. The environment, human vulnerability and societal impact, human decision-making, and technology design are four domains with complex interactions that can be simulated in computer models. Environmental models use physics-based simulations to estimate the behavior of natural features in the atmosphere, water, or vegetation. Human vulnerability and societal impact models estimate how people are impacted by environmental hazards, such as hurricanes, or ecosystems services, such as the benefits of forests, using physics-based simulations or economic regressions. Human decision-making models, which can be decentralized agent-based&nbsp; or centralized decision-logic, simulate the actions taken by humans in response to environmental features. The technology design model allows humans to explore options for technical systems (such as earth observation satellites) to improve awareness of the state of the environment.</p><p>While significant benefit has come from addressing each domain individually in existing models, and yet more from considering certain groupings (such as the economic valuations that combine both decision-making and societal impact), capturing all four together in an integrated software model can enable us to overcome important challenges that lie at the intersections of these domains.<br></p><p>Work is underway to make such integrated models a reality. We are developing applications aimed at facilitating the targeted harvesting of water hyacinth in southern Benin and at assisting the city of Rio de Janeiro with the conservation of their mangrove forests (see the images below). Once complete, these models will accomplish the dual objectives of improving remote observation-informed decision-making and to enable the exploration of remote observation system options, including specifying the architecture of a custom satellite.</p><p>The Space Enabled research group aims to develop an open-source standard for software model interfaces and begin to build an open-source library of new and existing computational models for use in a variety of applications. Hopefully this library will be used to facilitate the use of remote sensing data, craft policy here on Earth, and design new remote sensing platforms. As we progress and work towards version 1.0, we hope others will join us in this endeavor.</p>", "people": ["jackreid@media.mit.edu"], "title": "Integrated Complex Systems Modeling", "modified": "2019-04-19T13:29:24.236Z", "visibility": "PUBLIC", "start_on": "2018-09-03", "location": "", "groups": ["space-enabled"], "published": true, "active": false, "end_on": null, "slug": "integrated-complex-systems-modeling"}, {"website": "", "description": "<p>City Science researchers are developing a slew of tangible and digital platforms dedicated to solving spatial design and urban planning challenges. The tools range from simulations that quantify the impact of disruptive interventions in cities to communicable collaboration applications. We develop and deploy these tools around the world and maintain open source repositories for the majority of deployments. \"CityScope\" is a concept for shared, interactive computation for urban planning.</p><p>All current CityScope development, tools, and software are open source <a href=\"https://cityscope.github.io/\">here</a>.&nbsp; &nbsp; &nbsp;&nbsp;</p>", "people": ["alonsolp@media.mit.edu", "agrignar@media.mit.edu", "yasushis@media.mit.edu", "doorleyr@media.mit.edu", "kll@media.mit.edu", "gbabio@media.mit.edu", "noyman@media.mit.edu", "ryanz@media.mit.edu"], "title": "Theme | CityScope", "modified": "2019-05-16T20:42:57.474Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": null, "slug": "cityscope"}, {"website": "http://www.tomasvega.com", "description": "<h2>Discreet Teeth Gestures for Mobile Device Interaction</h2><p>Byte.it is a miniaturized, discreet interface that uses teeth gestures for hands-free input for wearable computing.&nbsp;<br></p><p>As humans, we are constantly seeking to communicate and consume information, and mobile devices give us&nbsp; access the world wide web, our digital selves, and all our digital assets with the touch of our fingers . Context can temporarily reduce our abilities to perform certain activities, preventing us from having a fluid interaction with mobile computing. Hands are not always available, and sustained visual attention is often required for successful task performance and social norms. Current screen-based interfaces are not designed to be used by a person engaged in another attention demanding activity such as walking , talking, or driving, leading to ineffective interactions and even dangerous situations.</p><p>Audio interfaces are a potential solution as they can provide a high-bandwidth communication channel without requiring visual attention. Speech has been the predominant interaction modality for audio interfaces, but it can be ineffective in situations with loud environmental noise, or inappropriate in certain social or dynamic on-the-go contexts. Recent work has explored teeth gestures as a solution for interaction in these contexts, but these attempts are limited by the number of gestural primitives recognized (bandwidth) and the discreteness of the interfaces used to detect these gestures.</p><p>Byte.it expands on this work by exploring the use of a smaller and more unobtrusively positioned sensor (accelerometer and gyroscope) for detecting tooth clicks of different groups of teeth and bite slides for everyday human-computer interaction.&nbsp;Initial results show that an unobtrusive position on the lower mastoid close to the mandibular condyle can be used to classify teeth tapping of four different teeth groups&nbsp;(front, back, left, and right teeth click)&nbsp;with an accuracy of 89 percent, or an accuracy of 84 percent for seven different teeth clicking and bite sliding gestures (front,&nbsp;left, and right click, and front, back, left, and right slide).</p><p>The applications currently being explored are centered around dynamic, on-the-go, hands-and-eyes-free contexts. For example, (1) controlling the different commands of a media player, such as play/pause, volume, and current time of a song, podcast, or audiobook. Productivity-wise, being able to subtly (2) start and stop audio recordings of conversations or meetings, and tag relevant events that might be worth reviewing later. Teeth gestures could also allow for a discrete and rapid way to (3) accept or reject incoming &nbsp;alerts, notifications, and reminders, while minimizing task-switch time.&nbsp;A minimal set of teeth gestures could also enable the seamless&nbsp;(4)&nbsp;access of information streams such as &nbsp;messages, emails, news, or relevant notes about the person, place, and/or time of interest that could enhance the current interaction.</p><p>This research aims to investigate the following:</p><p>1) Understand how people could use teeth gestures to perform specific interaction commands in order to establish a standardized teeth gesture language.</p><p>2) Identify the optimal position of the sensor to achieve the highest gesture classification accuracy possible while ensuring a discreet form factor.</p><p>3) Measure the performance of our classification algorithm in the wild, while sitting, standing, walking, running, and cycling.</p><p>4) Assess the usability of the interface + applications in the wild.</p>", "people": ["tomasero@media.mit.edu", "pattie@media.mit.edu"], "title": "Byte.it", "modified": "2019-05-09T16:26:43.481Z", "visibility": "PUBLIC", "start_on": "2017-09-07", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": null, "slug": "byte_it"}, {"website": "", "description": "<p>Designing the atmosphere and sensorial qualities of physical space can have a remarkable influence on human experience and behavior. The<i>&nbsp;<a href=\"https://www.media.mit.edu/posts/mediated-atmosphere/\">Mediated Atmospheres</a>&nbsp;</i>project envisions a smart office that is capable of dynamically transforming to enhance occupants\u2019 work experience and cognitive ability, via both subtle and overt customizations tailored to bio-signal inputs.&nbsp;The workspace prototype is equipped with a modular real-time control infrastructure, integrating biosignal sensors, controllable lighting, image projection, heat, smell, and sound. This technology (from customizable individual elements to entire room or building scale) can be applied to a space exploration habitat or cabin context to address&nbsp;behavioral health risk reduction and cognitive performance improvement&nbsp;for human life in deep space. Manipulating light, sound, smell, and visual-field objects can have a powerful effect on cognitive performance, mood, and physiology\u2014in combination with robotic agents, we envision a holistic, data-driven approach to the \u201cenvirome,\u201d an optimization of built-environment space experiences meant to supplement and interact with the human microbiome. This research area will combine and integrate projects from within the Space Exploration Initiative\u2019s existing portfolio, including Spatial Flux, Social Robots in Space, VR Maze, and Sensory Synchrony.</p><p><b>Selected, Recent Mediated Atmospheres Publications:</b></p><p>Zhao, Nan, Asaph Azaria, and Joseph A. Paradiso. \"Mediated Atmospheres: A Multimodal Mediated Work Environment.\"&nbsp;<i>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</i>&nbsp;1, no. 2 (2017): 31. Available:&nbsp;<a href=\"https://dl.acm.org/citation.cfm?id=3090096\">https://dl.acm.org/citation.cfm?id=3090096</a></p><p>Richer, Robert, Nan Zhao, Judith Amores, Bjoern M. Eskofier, and Joseph A. Paradiso. \"Real-time Mental State Recognition using a Wearable EEG.\" In&nbsp;<i>2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</i>, pp. 5495-5498. IEEE, 2018. Available:<a href=\"https://ieeexplore.ieee.org/abstract/document/8513653\">https://ieeexplore.ieee.org/abstract/document/8513653</a></p><p>\"Image-based perceptual analysis of lit environments.\" Nan Zhao, Christoph F. Reinhart, and Joseph A. Paradiso in Lighting Research and Technology Journal 0: 1\u201321, 2018.&nbsp;<a href=\"https://resenv.media.mit.edu/pubs/papers/2018_Zhao_LRTJ.pdf\">https://resenv.media.mit.edu/pubs/papers/2018_Zhao_LRTJ.pdf</a></p><p>\"Real-time Mental State Recognition using a Wearable EEG.\" Robert Richer, Nan Zhao, Judith Amores, Bjoern M. Eskofier, and Joseph A. Paradiso in the International Conference of the IEEE Engineering in Medicine and Biology Society, July 17-21, 2018, Honolulu, HI, USA.&nbsp;<a href=\"https://resenv.media.mit.edu/pubs/papers/richer18_eeg_scores.pdf\">https://resenv.media.mit.edu/pubs/papers/richer18_eeg_scores.pdf</a></p>", "people": ["aekblaw@media.mit.edu"], "title": "Mediated Atmospheres in Space", "modified": "2019-05-24T19:08:37.923Z", "visibility": "LAB", "start_on": "2019-05-01", "location": "", "groups": ["space-exploration"], "published": true, "active": false, "end_on": null, "slug": "mediated-atmospheres-in-space"}, {"website": "", "description": "<p>Can we sonfiy calming breathing and passively influence a state of calm?</p><p>Deep breathing has been scientifically proven to affect the heart, brain, digestive system, and the immune system. We believe designing a technology to promote deep breathing can facilitate transition into a calm state.&nbsp;</p><p>Nowadays, many people are spending a significant amount of time listening to music while working or studying. This makes music a good means for providing auditory breathing cues. While it has been shown that liminal auditory cues can be effective in encouraging a healthy breathing pattern, we are examining the use of subliminal encouragement of &nbsp;breathing modulation using music. Auditory ambient feedback has long been studied and is proved to be effective. It has been explored in concert settings, interactive installations, and smartphone applications. However, our aim is to design an intervention that is unobtrusive and doesn't keep people from doing their primary work. </p><p>In order to find the best auditory feedback design, we have designed a controlled study comparing an interactive rhythmic ambient music track that responds to a user's current breathing patterns to a fixed rate music track whose speed of playback is pegged to a rate slightly below the user's natural resting breathing rate. A control condition with no music is also included. We will compare the resulting breathing patterns, heart rate, EEG signals, and self-reported measures to determine if the ambient music feedback has any effect on the user's state of mind and body. If successful, a musical system to subliminally encourage calming breathing patterns may be integrated into workplace environments, hospitals, and other places where it is necessary to promote less stressful and healthy environments.</p><p>Our preliminary results significant shift in multiple physiological measures that indicate a state of calmness.</p>", "people": ["asma_gh@media.mit.edu", "picard@media.mit.edu", "gleslie@media.mit.edu"], "title": "BrainBeat: Breath-based music therapy", "modified": "2019-04-23T04:44:31.353Z", "visibility": "PUBLIC", "start_on": "2016-12-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "brainbeat"}, {"website": "", "description": "<p><span style=\"font-size: 18px;\">Real World Data (RWD) and Real World Evidence (RWE) are playing an increasing role in healthcare decisions to support innovative point-of-care and at-home studies using mobile phones, sensors, wearables and Electronic Health Records to generate new treatment approaches.</span><span style=\"font-size: 18px; font-weight: 400;\">&nbsp;In a study published in the </span><i style=\"font-size: 18px; font-weight: 400;\">British Medical Journal</i><span style=\"font-size: 18px; font-weight: 400;\">, we evaluated the significance and efficacy of RWD generated from advanced technology-enabled, non-invasive diagnostic screening (TES) using smartphones and other point-of-care medical imaging devices vs conventional vital signs examinations [</span><a style=\"font-size: 18px; font-weight: 400;\" href=\"https://www.media.mit.edu/projects/technology-enabled-mobile-phone-screenings-augment-routine-primary-care/overview/\">1</a><span style=\"font-size: 18px; font-weight: 400;\">]. This study led to significant insights regarding strategies for developing technologies at MIT that are ready for deployment and designed for effective and scalable primary care and RWE generation. For example:&nbsp;</span></p><p>a) Construction and validation of low-cost, point-of-care, near-infrared imaging devices to diagnose dental caries, cracks, and demineralization without the use of ionizing X-rays have been published&nbsp;[<a href=\"https://www.media.mit.edu/projects/replacing-2d-radiography-and-cone-beam-computed-tomography-with-non-ionizing-imaging-for-medical-diagnosis/overview/\">2</a>, <a href=\"https://www.media.mit.edu/projects/near-infrared-transillumination-guides-administration-of-dental-2d-radiography-and-cone-beam-computed-tomography-imaging/overview/\">3</a>, <a href=\"https://www.media.mit.edu/projects/near-infrared-imaging-for-detecting-caries-and-structural-deformities-in-teeth/overview/\">4</a>]. <br></p><p>b) We have open-sourced the construction and the algorithm of porphyrin imaging devices and also created a cell phone clip that can be used on a mobile phone camera [<a href=\"https://www.media.mit.edu/projects/porphyrin-imaging/overview/\">5</a>].</p><p> c) In collaboration with MIT Chemical Engineering and other researchers we have developed and published protocols for a POC lateral flow diagnostic strip and bio-digital wearable devices to detect biomarkers in human saliva samples. Biomarkers such as matrix metalloproteinases-8 and -9, pH and nitric oxide linked to oral diseases, stress and human physiology have been successfully integrated with our platforms [<a href=\"https://www.media.mit.edu/projects/detecting-biomarkers-with-printable-paper-diagnostics/overview/\">6</a>]. <br></p><p>d) A study of a low-cost mask to screen for sleep apnea through physiological monitoring: respiratory activity (airflow and nasal air pressure) and sleep behavior (motion and noise) has been published [<a href=\"https://www.media.mit.edu/projects/at-home-sleep-apnea-screening/overview/\">7</a>].</p>", "people": ["gyauney@media.mit.edu", "ggbhatia@media.mit.edu", "kla11@media.mit.edu"], "title": "Research Area | Point-of-Care Medical Technologies for Real World Data and Evidence Generation to Improve Public Health", "modified": "2019-05-04T07:54:59.243Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["health-0-0"], "published": true, "active": false, "end_on": null, "slug": "research-area-point-of-care-medical-technologies-for-real-world-data-and-evidence-generation"}, {"website": "", "description": "<p>This project aims to improve the prediction accuracy of wellbeing (stress, mood, and health levels) using temporal machine learning models. We extend our previous approach using Long Short-Term Memory models and time series data from the <a href=\"https://www.media.mit.edu/projects/snapshot-study/overview/\">SNAPSHOT study</a>. In addition, we consider adaptive methods to fill in missing data with time series information. We also develop the model using modifiable behavioral features such as bedtime, and examine how these contribute to wellbeing, so that people can get better control over how to improve their personal well-being.</p>", "people": ["picard@media.mit.edu", "sataylor@media.mit.edu", "akanes@media.mit.edu", "terumi@media.mit.edu"], "title": "Improving wellbeing prediction performance using temporal machine learning models", "modified": "2019-05-24T18:28:52.701Z", "visibility": "PUBLIC", "start_on": "2018-04-02", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": null, "slug": "improving-well-being-prediction-performance-using-temporal-machine-learning-models"}, {"website": "", "description": "<p>&nbsp;\u201cMolecular Gastronomy in Zero G\u201d explores the artistic and technical aspect of preparing food in space.&nbsp;Studying how food is created and consumed in zero-gravity can help shed light on how our experiences of food extend far beyond mere nourishment. Food is not simply fuel\u2014it\u2019s part of what makes us human. Debriefs with astronauts tell us that food is one of the only creature comforts in spaceflight, and it will play an even more significant role on long duration spaceflight and space habitats. The current space food system offers some variability in menu items, but does little outside of sustenance. It\u2019s freeze-dried and pre-packaged in ways consistent with the demands of present day space travel. For longer trips these self-contained meals will be detrimental to astronauts\u2019 mental health. Promoting new culinary techniques and interactions between different sensory modalities will assist in improving the experience of food in space.</p><p><b>Phase 1: </b>Conduct a basic spherification experiment to explore Molecular Gastronomy as a possible method of creating food in zero-gravity. Spherification will be used to make Algae Caviar. This dish has been specifically made for space as its a closed loop food system\u2014algae can be grown, cooked and consumed in space. The experiment will also teach us about fluid dynamics in zero-gravity. \u200bThere is a long standing precedent for research on fluid dynamics in zero g, but less is known within the context of cooking and molecular gastronomy. Understanding the effects of zero g on spherification will help encourage a new food culture to emerge.</p><p><b>Phase 2:&nbsp;</b>Consume food to investigate the sensory and physiological experience of eating in zero gravity. Develop a framework for recording the senses in extreme and isolated environments. This is becoming increasingly important as humans embark on longer trips to space where they need to be not only physically strong, but mentally and emotionally equipped. The foods will be consumed sequentially through a sensing menu. This will include Chewable Champagne, Algae Caviar, Umami Bones and Chocolate.</p>", "people": [], "title": "Molecular Gastronomy in Zero G", "modified": "2019-06-10T17:00:46.806Z", "visibility": "LAB", "start_on": null, "location": "", "groups": ["space-exploration"], "published": false, "active": false, "end_on": null, "slug": "molecular-gastronomy-in-zero-g"}]
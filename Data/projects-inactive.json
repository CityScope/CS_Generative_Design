[{"website": "", "description": "<h2>We support computer scientists and engineers from the Middle East to create technologies for refugee learners.&nbsp;</h2><p>The Refugee Learning Accelerator<span style=\"font-size: 18px; font-weight: 400;\">&nbsp;provides training, mentorship, and funding to teams of computer scientists, engineers and designers.&nbsp;</span></p><p> </p><h2>Who can apply?&nbsp;</h2><p> </p><ul><li>University students and recent graduates with backgrounds in computer science, design, and electrical engineering,<br></li><li>coming from some of the places most impacted by the current refugee crisis: Jordan, Lebanon, Syria, Iraq, and Palestine.<br></li></ul><p> </p><h2>More information available <a href=\"http://refugeelearning.media.mit.edu/\">here</a>.&nbsp;</h2>", "people": ["barrons@media.mit.edu", "ps1@media.mit.edu"], "title": "Refugee Learning Accelerator", "modified": "2017-08-09T18:32:49.146Z", "visibility": "PUBLIC", "start_on": "2017-05-01", "location": "", "groups": ["ml-learning"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "refugee-learning-accelerator"}, {"website": "", "description": "<p>The majority of today's communication technology focuses on communicating over distance, yet there are many ways technology can enrich face-to-face conversations. \"Second Messenger\" is a suite of interfaces, designed to augment a face-to-face interaction, that reveals simple observations about group dynamics. The goal of this project is to provide ways for a group to reflect upon its current interaction and improve upon it by considering a more diverse set of viewpoints in the discussion. The core focus of this project is evaluating the impact of these interfaces on a group. What is a group member's emotional reaction to the interfaces and the information revealed? How do these interfaces change the behavior of a group? </p>", "people": ["walter@media.mit.edu"], "title": "Second Messenger", "modified": "2016-12-05T00:16:05.718Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-309", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "second-messenger"}, {"website": "", "description": "<p>This work uses combined speech input and output to converse with a user seeking to control a computer, or to access information from it. Conversational techniques allow the computer gracefully to limit the vocabulary that is likely to be spoken, facilitating speech recognition. Discourse techniques aid error detection and correction. Domain knowledge and learning about user preferences allow for determining a user\ufffds needs more efficiently. Listening for various forms of back-channel response from the listener permits a talking computer to gauge user interest better, and determine at what level of detail to describe the requested information.</p>", "people": ["geek@media.mit.edu"], "title": "Conversational Interfaces", "modified": "2016-12-05T00:16:05.765Z", "visibility": "PUBLIC", "start_on": "1984-12-31", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "conversational-interfaces"}, {"website": "", "description": "<p>To maintain our relationships with family and friends, it'<span style=\"font-size: 18px; font-weight: 400;\">s important to share our daily lives. TimelessWords is an integrated, calendar-based life-sharing system to enhance emotion-driven daily communication. Users can easily pre-record voice messages via a handheld device, and send the messages to other community members. TimelessWords users have a shared calendar. Messages will be delivered at scheduled moments to express timeless care.</span></p>", "people": ["geek@media.mit.edu"], "title": "TimelessWords", "modified": "2017-11-15T17:28:00.712Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-384C", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "timelesswords"}, {"website": "", "description": "<p>About Us examines technology workplaces from a feminist perspective. The objective is to mobilize workers to collect simple gender-specific data, such as the male-female ratio of their workplace. This data will be made available to others to inform performances, wearables, and games. The idea is to create collaboratively a data infrastructure to support projects and perspectives that shed light on the complicated issue of gender imbalance in spaces of high-tech labor.</p>", "people": ["csik@media.mit.edu"], "title": "About Us", "modified": "2016-12-05T00:16:05.922Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-020A", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "about-us"}, {"website": "", "description": "<p>People often say that we live in a small world. In a brilliant experiment, legendary social psychologist Stanley Milgram proved the six degrees of separation hypothesis: that everyone is six or fewer steps away, by way of introduction, from any other person in the world. But how far are we, in terms of time, from anyone on Earth? Our team won the Tag Challenge, a social gaming competition, showing it is possible to find a person, using only his or her mug shot, within 12 hours.</p>", "people": ["dsouza@media.mit.edu", "irahwan@media.mit.edu"], "title": "Crowdsourcing a Manhunt", "modified": "2016-12-05T00:16:05.988Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": "2016-09-01", "slug": "crowdsourcing-a-manhunt"}, {"website": "", "description": "<p>AboutFace is a user-dependent system that is able to learn patterns and  discriminate the different facial movements characterizing confusion and  interest. The system uses a piezoelectric sensor to detect eyebrow  movements and begins with a training session to calibrate the unique values  for each user. After the training session, the system uses these levels to  develop an expression profile for the individual user.  The system has many  potential uses, ranging from computer and video-mediated conversations to  interactions with computer agents.  This system is an alternative to using  camera-based computer vision analysis to detect faces and expressions.  Additionally, when communicating with other people, users of this system  also have the option of conveying their expressions anonymously by wearing  a pair of glasses that conceals their expressions and the sensing device.\n</p>", "people": ["picard@media.mit.edu"], "title": "AboutFace", "modified": "2016-12-05T00:16:06.081Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "aboutface"}, {"website": "", "description": "<p>We are studying how social accountability information about companies affects consumer behavior. We are also developing Web-based tools to help consumers learn about and act on such information.</p>", "people": ["aithpao@media.mit.edu"], "title": "Account-Ability", "modified": "2016-12-05T00:16:06.230Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2010-09-01", "slug": "account-ability"}, {"website": "", "description": "<p>We are developing a new set of movement experiences titled Crenulations and Excursions. This work consists of two aspects: a public installation that allows visitors to explore a rich sonic space through their expressive movement, and a short dance performance that allows a trained performer to explore the expressive capabilities of the installation environment. With a tiny, energetic gesture or with a fluid and sweeping movement, a performer can create and shape layers of sound around herself. Both the installation and performance will explore the body as a subtle and powerful instrument, providing continuous control of continuous expression. This piece is related to Elena Jessop's doctoral work on new high-level analysis frameworks for recognition and extension of expressive physical performance.</p>", "people": ["ejessop@media.mit.edu", "tod@media.mit.edu"], "title": "Crenulations and Excursions", "modified": "2016-12-05T00:16:06.254Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "crenulations-and-excursions"}, {"website": "", "description": "<p>Technical devices are generally considered to be compliant instruments that are fully controllable tools, and from this perspective, humans have total power to operate machines at will. However, technologies often act back on humans, reversing the intended command direction. The Acoustic Chase project examines processes in which machines are intentionally provided with \ufffddegrees of freedom\ufffd not fully controllable by people. This will partially invert the relationship of human and tool, and create a system of balanced power. Expanding on the Haptic Opposition project, which combined touch and vision to create a human-machine interaction wherein the apparatuses could push back against human motion, Acoustic Chase will extend the concept of machines with a \ufffdfree\ufffd acting will, but use sound as the primary output. The sensation of sound is direct; it offers quick information flow and a great variety of expressions. It is also difficult to escape: people can\ufffdt \"hear away\" or \"close their ears.\" Tracking human body movements will provide an input channel. In combination with spatial sound, Acoustic Chase will enable intuitive human reactions, like discovering, playing, or fleeing.</p>", "people": ["csik@media.mit.edu"], "title": "Acoustic Chase", "modified": "2016-12-05T00:16:06.272Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-001", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "acoustic-chase"}, {"website": "", "description": "", "people": [], "title": "NetWorld", "modified": "2017-03-29T19:05:05.139Z", "visibility": "PUBLIC", "start_on": "2016-12-07", "location": "", "groups": [], "published": false, "active": false, "end_on": "2017-04-30", "slug": "networld"}, {"website": "", "description": "<p>Varied important problems can be solved using surprisingly approximate arithmetic.  We've designed a co-processor for such arithmetic that provides 100,000 cores on a single standard chip, or 1,000 cores in a sub-watt mobile device.  We are exploring applications of such machines in image and video processing.  Cost can be under a penny per core, and compared to CPUs, improvements in speed and energy use can exceed 10,000x.  </p>", "people": ["dkroy@media.mit.edu"], "title": "10,000x More Efficient Computing", "modified": "2016-12-05T00:16:06.336Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "10000x-more-efficient-computing"}, {"website": "", "description": "<p>The project explores how to best engage design collaborators in an ongoing dialogue following a design charette process. A video record of the charette is recorded, parsed, posted, and transformed into an active web of meaning so that salient ideas and themes can be followed through the many phases of discussion. Initial processing is human intensive; however the intention is to apply common-sense technology in the future.</p>", "people": ["gid@media.mit.edu"], "title": "3D: Digital Dialogs for Design", "modified": "2016-12-05T00:16:06.459Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-368", "groups": [], "published": true, "active": false, "end_on": "2008-09-01", "slug": "3d-digital-dialogs-for-design"}, {"website": "", "description": "<p>Typically, 1-teacher schools are viewed as relatively backwards due to lack of resources, but research shows that teachers at such schools can have stronger, fuller, and deeper relationships with their students, and that the students can be more autonomous and in control of their own learning while still more supportive of and cooperative with their peers. Technology can provide connectivity, eliminating some resource constraints. We go further, using new learning methodologies and technologies to build upon the strengths of 1-teacher schools, creating models of learning environments to serve as exemplars not only for rural education, but also for large urban environments.</p>", "people": ["cavallo@media.mit.edu", "papert@media.mit.edu", "calla@media.mit.edu"], "title": "1-Teacher Schools", "modified": "2016-12-05T00:16:06.400Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2000-01-01", "slug": "1-teacher-schools"}, {"website": "", "description": "<p>Time perception is a fundamental component in our ability to build mental models of our world. Without accurate and precise time perception, we might have trouble understanding speech, fumble social interactions, have poor motor control, hallucinate, or remember events incorrectly. Slight distortions in time perception are commonplace and may lead to slight dyslexia, memory shifts, poor eye-hand coordination, and other relatively benign symptoms, but could a diminishing sense of time signal the onset of a serious brain disorder? Could time perception training help prevent or reverse brain disorders? This project is a series of experimental tools built to assist and increase human time perception. By approaching time-perception training from various perspectives, we hope to find a tool or collection of tools to increase time perception, and in turn discover what an increase in time perception might afford us.</p>", "people": ["slavin@media.mit.edu", "cwwang@media.mit.edu"], "title": "Tools for Super-Human Time Perception", "modified": "2016-12-05T00:16:06.546Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2015-07-01", "slug": "tools-for-super-human-time-perception"}, {"website": "", "description": "<p>(void*) is a virtual lounge that unites the physical and digital, allowing people and a cast of virtual characters to interact with each other. In this novel gathering place, as in a nightclub, the interaction focuses mostly on group dynamics and body language, and little on verbal communication. Building on SWAMPED!, this installation continues our work in \ufffdIntentional Characters,\ufffd \ufffdSympathetic Interfaces\ufffd and \ufffdAutonomous Cinematography.\ufffd It also adds \ufffdDynamic Music Composition\ufffd as an integral part of the system. (void *) will premiere this summer at Siggraph \ufffd99. Today we will be showing a video and demonstrating the technology. \n</p>", "people": [], "title": "(void *): A Cast of Characters", "modified": "2016-12-05T00:16:06.566Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "Pond", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "1999-09-01", "slug": "void-a-cast-of-characters"}, {"website": "", "description": "<p>Multi-layer 3-D microfluidic circuits are the driving platform for micro-total analysis systems in the current era. The traditional fabrication for these\nfluidic circuits is planar. For building multi-layer structures a precise\nalignment for multiple layers is required, which makes the fabrication very difficult. We have invented a flow-based extrusion process for multi-layer microfluidic circuits in polymer substrates. The channels can be as small as 100nm with multi-materials embedded in the substrate. Such a direct-write 3-D process will enhance testing and prototyping of microfluidic circuits and enable fabrication of true 3-D structures. </p>", "people": ["neilg@media.mit.edu"], "title": "3-D Rapid Microfluidic Circuit Fabrication", "modified": "2016-12-05T00:16:06.627Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "3-d-rapid-microfluidic-circuit-fabrication"}, {"website": "", "description": "<p>The 1998 Everest Expedition was the first major scientific expedition to  Mount Everest.  We joined forces with the Boston Museum of Science, NASA,  Yale, the Explorer's Club, and a world-class climbing team, to investigate  questions of geology, climatology, and human physiology.\n.</p>", "people": ["mike@media.mit.edu"], "title": "1998 Everest Expedition", "modified": "2016-12-05T00:16:06.645Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "1998-09-01", "slug": "1998-everest-expedition"}, {"website": "", "description": "<p>The AmEC Infrastructure is an attempt to mitigate the well-known problem of software reuse in the domain of agent-mediated electronic commerce. Most of the systems developed by the AmEC initiative share a set of requirements that, once met, could serve as the foundation of future electronic commerce systems. The AmEC Infrastructure will define both a framework for the development of new systems and a set of pre-built subsystems that could be incorporated into new electronic commerce agent systems.</p>", "people": [], "title": "Agent Mediated Electronic Commerce (AmEC) Infrastructure", "modified": "2016-12-05T00:16:06.712Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "agent-mediated-electronic-commerce-amec-infrastructure"}, {"website": "", "description": "<p>With the Attentional Mixer we are investigating how attentional cues can be used to selectively amplify events in a person's environment. Audio streams coming from various sources are mixed and delivered to the user according  to what the person is actively paying attention to. In a noisy cocktail party the Attentional Mixer can improve a person's ability to hear their conversational partner, or in a rock band it can allow the musicians to change their personal \"monitor mix\" without skipping a beat, just by looking at the band member to be amplified. Opportunities for collaborative filtering arise when the number of devices grows.</p>", "people": [], "title": "Attentional Mixer", "modified": "2016-12-05T00:16:06.821Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "attentional-mixer"}, {"website": "", "description": "<p>Agents (human or computer) provide their users with recommendations. This project investigates whether the type of agent making the recommendations (human vs. computer, collaborative vs. individual) influences the decision strategy that people utilize. Specifically, we examine whether the type of agent used influences decision makers' price/quality tradeoffs, risk attitude, willingness to compromise or to follow their gut feeling, or their likelihood of becoming more sensitive to social influences.</p>", "people": [], "title": "Acting on Advice from Computer Agents", "modified": "2016-12-05T00:16:06.880Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-441", "groups": ["erationality"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "acting-on-advice-from-computer-agents"}, {"website": "", "description": "<p>The 3DprintedClock project is the result of ready-assembled 3D printed computational mechanisms, and is related to research in the fields of rapid prototyping and digital fabrication. The clock was modeled in CAD software after an existing clock, and uses a weight and a pendulum to keep track of time. The CAD model was created according to the specifications of the 3D printer, assuring sufficient gaps and clearances for the different parts. In addition, support material, drainage, and perforations were added to allow for excess support material being removed after printing. The 3DprintedClock is intended to demonstrate the superior capabilities of 3D printing as a fabrication process. It should contribute toward the future use of 3D printers to replace injection molding and expensive tooling processes, and allow for on demand, customized, and \ufffdgreener\ufffd consumer products.</p>", "people": ["cynthiab@media.mit.edu", "rswartz@media.mit.edu"], "title": "3DprintedClock", "modified": "2016-12-05T00:16:06.934Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-001", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "3dprintedclock"}, {"website": "", "description": "<p>Throughout the ages bioluminescence has inspired myths. Long ago, sailors in the Indian Ocean encountered massive bioluminescent blooms as they sailed through the water, lighting the wakes of their ships like the spokes of a wheel carrying them to their destination in a chariot of wind and water.&nbsp; They called this phenomenon \u201cThe Wheels of Poseidon.\" Our goal is to harness the beauty of bioluminescence to create a new medium for artistic expression. We will generate a living, programmable bioluminescent display, with pixels and voxels built of bioluminescent plankton (<i>Pyrosystis fuciformis</i>) floating freely in the water column and stimulated to glow by a programmable pattern of pressure waves (acoustic waves) in the water. <br></p>", "people": ["rssmith@media.mit.edu", "danoran@media.mit.edu", "novysan@media.mit.edu"], "title": "Wheels of Poseidon", "modified": "2019-04-22T18:07:56.239Z", "visibility": "PUBLIC", "start_on": "2018-04-02", "location": "", "groups": ["open-ocean"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "wheels-of-poseidon"}, {"website": "", "description": "<p>This project presents a new approach for the design and fabrication of the acoustic instrument, using digital fabrication technologies, and specifically 3D printing. A concert flute was 3D printed, including (almost) all of its mechanisms, without the need to assemble the moving parts. In order to fulfill this challenge a new design of the instrument was made. Instead of proposing the machine can easily replace the human craft - we use this research to examine the possible contributions of rapid prototyping to the design and fabrication of traditional instruments.</p>", "people": ["joep@media.mit.edu"], "title": "3D Printed Flute", "modified": "2016-12-05T00:16:06.907Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "3d-printed-flute"}, {"website": "", "description": "<p>Beyond is a collapsible input device for direct 3D manipulation. When pressed against a screen, Beyond collapses in the physical world and extends into the digital space of the screen, so that users have an illusion that they are inserting the tool into the virtual space. Beyond allows users to interact directly with 3D media without having to wear special glasses, avoiding inconsistencies of input and output. Users can select, draw, and sculpt in 3D virtual space, and seamlessly transition between 2D and 3D manipulation.</p>", "people": ["ishii@media.mit.edu"], "title": "Beyond: A Collapsible Input Device for 3D Direct Manipulation", "modified": "2016-12-05T00:16:07.044Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "beyond-a-collapsible-input-device-for-3d-direct-manipulation"}, {"website": "", "description": "<p>MEMS have been built by patterning nanoparticle colloids of metals, insulators, and release materials. Nanoparticle colloids permit greater solubility and far-suppressed melting temperature compared to the bulk material, which allows liquid to be \"printed\" on plastic substrates and then sintered at plastic-compatible temperatures to form fully functional devices. Using two patterning techniques, ink-jet deposition and liquid embossing, we have created thermal actuators (heatuators), linear and rotary electrostatic motors, and mechanical test structures with characteristic feature sizes ranging from 200 nm to 20 mm.</p>", "people": ["jacobson@media.mit.edu"], "title": "Building MEMS with Nanoparticles", "modified": "2016-12-05T00:16:07.111Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-015", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "building-mems-with-nanoparticles"}, {"website": "", "description": "<p>The 8D Display combines a glasses-free 3D display (4D light field output) with a relightable display (4D light field input). The ultimate effect of this extension to our earlier BiDi Screen project will be a display capable of showing physically realistic objects that respond to scene lighting as we would expect. Imagine a shiny virtual teapot in which you see your own reflection, a 3D model that can be lighted with a real flashlight to expose small surface features, or a virtual flashlight that illuminates real objects in front of the display. As the 8D Display captures light field input, gestural interaction as seen in the BiDi Screen project is also possible.</p>", "people": ["holtzman@media.mit.edu"], "title": "8D Display", "modified": "2016-12-05T00:16:07.170Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "8d-display"}, {"website": "", "description": "<p>CircuiTUI is an application for circuit design and modeling, intended to aid novice circuit designers or students in gaining intuition into the behavior of circuit components. Projected images of circuit components can be bound to physical objects and reorganized in a virtual circuit as the physics of the circuit is modeled and overlaid quantitatively on the components.</p>", "people": ["ishii@media.mit.edu"], "title": "circuiTUI", "modified": "2016-12-05T00:16:07.244Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "circuitui"}, {"website": "", "description": "<p>The automatic classification of marine mammal sounds is very attractive as a means of assessing massive quantities of recorded data, freeing humans and offering rigorous and consistent output. Calculations on a set of vocalizations of Northern Resident killer whales using dynamic time warping have been reported recently. Since this method requires the time-consuming pre-processing measurement of frequency contours, we have explored the use of Gaussian Mixture Models (GMM) and Hidden Markov Models (HMM). These methods can be applied directly to time-frequency decompositions of the recorded signals. Calculations have been made on a set of 75 calls previously classified perceptually into seven call types. With cepstral coefficients as features both HMM\ufffds and GMM\ufffds give over 90% agreement with the perceptual classification, with the\nHMM over 95% for some cases.</p>", "people": ["brown@media.mit.edu", "bv@media.mit.edu"], "title": "Classification of Killer Whale Sounds with GMM and HMM", "modified": "2016-12-05T00:16:07.280Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "--Choose Location", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "classification-of-killer-whale-sounds-with-gmm-and-hmm"}, {"website": "", "description": "<p>The SEC is an extension to the free open-source program EyesWeb\ufffdthat contains a large number of machine learning and signal processing algorithms that have been specifically designed for real-time pattern and gesture recognition.  All the algorithms within the SEC are encapsulated as individual blocks, allowing the user to connect the output of one block to the input of another to create a signal flow chain.  This allows a user to quickly build and train their own custom gesture recognition system, without having to write a single line of code or explicitly understand how any of the machine learning algorithms within their recognition system work.</p>", "people": ["joep@media.mit.edu"], "title": "A Machine Learning Toolbox for Musician Computer Interaction", "modified": "2016-12-05T00:16:07.479Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "a-machine-learning-toolbox-for-musician-computer-interaction"}, {"website": "", "description": "<p>CREATE research proposes a holistic model for learning environments drawing from experiences in technologically saturated environments. The pilot experience of the research program, which implemented a one-to-one computer infrastructure in a small rural community in Costa Rica, is the first to consider concretely such an infrastructure in a developing country, and analyze it from different dimensions. The case study of this suggests that \"appropriation\" is the most powerful theoretical lens for studying the effects of the computer presence in learning environments. The details of the model, the case study, and some examples that illustrate the students\ufffd appropriation of technology will be presented during the open house. </p>", "people": ["papert@media.mit.edu", "calla@media.mit.edu"], "title": "CREATE: Opportunities for Technology Appropriation", "modified": "2016-12-05T00:16:07.583Z", "visibility": "LAB", "start_on": "2005-01-01", "location": "E15-388", "groups": [], "published": true, "active": false, "end_on": "2005-01-01", "slug": "create-opportunities-for-technology-appropriation"}, {"website": "", "description": "<p>We are exploring innovative use of cell-phone Bluetooth technologies for consumer research and customer measurement.  We have developed a small, portable, Bluetooth base station that can monitor consumer activity in a retail space and also enable new interactive services.  This Bluetooth hub also serves as a network gateway for other wireless sensors in the local area.</p>", "people": ["fletcher@media.mit.edu", "picard@media.mit.edu"], "title": "Customer Measurement Using Bluetooth", "modified": "2016-12-05T00:16:07.608Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-449", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "customer-measurement-using-bluetooth"}, {"website": "", "description": "<p>A picture is more than a thousand words. As cameras become smarter, imagery will be enhanced by meta-data of all sorts. Some of our early field projects, like the exploratory use of the dataCam on Mount Everest, pointed out the acute need to fuse image and sensor data, and perhaps more importantly, to construct a new image bank that can really leverage these new collections of data-enhanced images.  Deep View addresses this latter need. As a first step, we are digitizing a large backlog of photographs (two years of expeditionary images) and building the new file system and relational framework to leverage this and future data-rich imagery.  With this basis, the system will be able to intelligently create overview presentations or slide shows.</p>", "people": ["mike@media.mit.edu"], "title": "Deep View: An Image and Data Bank", "modified": "2016-12-05T00:16:07.642Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-468", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "deep-view-an-image-and-data-bank"}, {"website": "", "description": "<p>We are working with sponsor Schneider Electric to deploy a dense, low-power wireless sensor network aimed at environmental monitoring for smart energy profiling. This distributed sensor system measures temperature, humidity, and 3D airflow, and transmits this information through a wireless Zigbee protocol. These sensing units are currently deployed in the lower atrium of E14. The data is being used to inform CFD models of airflow in buildings, explore and retrieve valuable information regarding the efficiency of commercial building HVAC systems, energy efficiency of different building materials, and lighting choices in novel architectural designs.</p>", "people": ["nanwei@media.mit.edu", "joep@media.mit.edu"], "title": "Dense, Low-Power Environmental Monitoring for Smart Energy Profiling", "modified": "2016-12-05T00:16:07.663Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "dense-low-power-environmental-monitoring-for-smart-energy-profiling"}, {"website": "", "description": "<p>We are using consumer-grade inkjet printers with shrinkable print media in order to enable the rapid printing of extremely inexpensive holograms and diffractive optics.</p>", "people": ["vmb@media.mit.edu"], "title": "Desktop Printed Holograms", "modified": "2016-12-05T00:16:07.688Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "Garden Conference Room", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "desktop-printed-holograms"}, {"website": "", "description": "<p>Some auction sites have \"deterministic-ending\" rules, which means the auction ends at a known time. Other auction sites have \"contingent-ending\" rules, which means they end once a certain criterion has been met (e.g., when no one has increased a bid for 30 minutes). In this project, we examine the role of ending rules on bidders' competitiveness and final bids.</p>", "people": [], "title": "Deterministic vs. Contingent Ending Rules in Online Auctions", "modified": "2016-12-05T00:16:07.707Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "--Choose Location", "groups": ["erationality"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "deterministic-vs-contingent-ending-rules-in-online-auctions"}, {"website": "", "description": "<p>Exploring path diversity via multipath routing adds more degrees of freedom and reliability for wireless communications, in particular in wireless ad hoc networks. We present approaches based on smart traffic allocation and novel scheduling algorithms to achieve distributed security in wireless communication systems, where the reception from one path at the receiver gets a reasonable result and the receptions from more paths lead to better results. </p>", "people": ["lip@media.mit.edu"], "title": "Distributed Security via Multi-path Routing in Wireless Networks", "modified": "2016-12-05T00:16:07.751Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "distributed-security-via-multi-path-routing-in-wireless-networks"}, {"website": "", "description": "<p>Sleepiness is known to decrease driving performance, resulting in one of the major causes of fatal car accidents. An effective countermeasure to drowsiness is to increase activation, as activity and moderate exercise increase alertness. We are currently using redesigned accelerator pedals to integrate some form of physical activity while driving to maintain alertness levels in drivers.</p>", "people": [], "title": "Active Driver", "modified": "2016-12-05T00:16:07.885Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "active-driver"}, {"website": "", "description": "<p>Active Essays were forms of narrative expression in which computational objects were integrated with text, graphics, and video. For example, a simulation of a scientific concept could be integrated with text descriptions of the concept, so that \"readers\" actively explore the concept.</p>", "people": ["mres@media.mit.edu", "bss@media.mit.edu"], "title": "Active Essays", "modified": "2016-12-05T00:16:07.932Z", "visibility": "PUBLIC", "start_on": "1996-01-01", "location": "E15-489", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "active-essays"}, {"website": "", "description": "<p>This project builds a real-time, scalable network where each node cooperates in the distribution of data. This cooperation frees the mobile nodes from any fixed base stations and enables advances in efficient use of power and bandwidth without adding latency.  Effectively it operates like a cellular network where the cells are created on-the-fly instead of by base stations. The protocol is being developed via an infrared simulation on an air hockey table where each node is a puck.</p>", "people": ["lip@media.mit.edu"], "title": "Dynamic Cellularization", "modified": "2016-12-05T00:16:08.002Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-483A", "groups": ["media-and-networks"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "dynamic-cellularization"}, {"website": "", "description": "<p>Many of us enjoy making, changing, and sharing digital media, but we are often unsure how to use or present our collections. This project allows users to focus on the higher-level aspect of media manipulation: controlling the structure by which images and sounds come together. Working with its users, the Emonic Environment structures audio, video, and text into a network, while continuously providing suggestions for manipulations that can be applied to elements of this network. It operates in real time, is capable of operating with or without human guidance, and participants can contribute and edit media, or interact solely on the structural network level, leaving the low-level control to the system's algorithms. The system and its content are controllable by mouse and keyboard, microphones, cameras, cell phones, MIDI controllers, sensors, and third-party interfaces.</p>", "people": ["gid@media.mit.edu"], "title": "Emonic Environment", "modified": "2016-12-05T00:16:08.063Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "emonic-environment"}, {"website": "", "description": "<p>You can test whether a website is usable by making wire frames, but how do you know if that site, product, or store is emotionally engaging? We build quick, iterative environments where emotions can be tested and improved. Emphasis is on setting up the right motivation (users always have to buy what they pick), pressures (can you buy the laptop in 10 minutes?), and environment (competitors\ufffd products better be on the shelf too). Once we see where customers are stressed or miss the fun part, we change the space on a daily, iterative cycle. Within two to three weeks, we can tell how to structure a new offering for a great experience. Seldom do the emotions we hope to create happen on the first try; emotion prototyping delivers the experience we want. We hope to better understand the benefits of emotion prototyping, especially while using the skin conductance sensor.</p>", "people": ["picard@media.mit.edu", "hedman@media.mit.edu"], "title": "Emotion Prototyping: Redesigning the Customer Experience", "modified": "2016-12-05T00:16:08.083Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "emotion-prototyping-redesigning-the-customer-experience"}, {"website": "", "description": "<p>In the EME project we worked with a quadraplegic individual with severe cerebral palsy to develop the hardware and software necessary for an individual with extremely limited movement to fully interpret\ufffdas a soloist in a live performance\ufffdhis own music composed in Hyperscore. Our technological development has incorporated an adaptive input device and accompanying software to allow the user\ufffds symptomatic movement to provide sufficient accuracy, precision, and expressivity for the demands of a performance. By working with the subject as a collaborator, we have designed an interface that not only meets his very specific physical needs, but also unveils novel insights into the design of expressive interfaces that can be customized and personalized for any user.</p>", "people": ["tod@media.mit.edu"], "title": "Enabling Musical Expression", "modified": "2016-12-05T00:16:08.109Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "enabling-musical-expression"}, {"website": "", "description": "<p>What really divides Cambridge?</p><p>Who can occupy certain spaces in Cambridge? What are&nbsp;the less obvious social and cultural factors at play? When&nbsp;moving to a new neighborhood, one of the first things you&nbsp;might think about is whether you can afford to live there.&nbsp;But for many people, housing is not only a matter of&nbsp;affordability but also a matter of access to community.&nbsp;For others, private housing may not even be an option,&nbsp;so the accessibility of homeless shelters and public&nbsp;indoor spaces become important factors.</p><p>This project is in the prototype phase. It uses census data&nbsp;and augmented reality (AR) to examine the livability of&nbsp;three areas of Cambridge: Harvard Square, Central&nbsp;Square, and Kendall Square. In doing so, we hope to&nbsp;create an intersectional map that helps us understand&nbsp;housing accessibility.</p>", "people": ["mboya@media.mit.edu"], "title": "AR Neighbor/hood", "modified": "2019-04-22T18:08:48.885Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2018-12-12", "slug": "ar-neighbor-hood"}, {"website": "", "description": "<p>The Additive Lathe is a 3D printer that prints onto rotating objects. It has two degrees of freedom: the first degree is the x-axis that moves with a rack and pinion mechanism. The second degree is the rotation of the object. Unlike regular lathes, in the Additive Lathe the object can spin in both directions and can spin in steps due to the stepper motor that spins the spindle. The additive lathe can produce objects that are difficult to make with regular 3D printer like threads and springs.</p>", "people": ["neri@media.mit.edu"], "title": "Additive Lathe", "modified": "2016-12-05T00:16:08.168Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "additive-lathe"}, {"website": "", "description": "<p>String instruments are typically close-miked, to capture qualitative presence in recordings. This has been impossible for woodwind instruments, since each note comes from a different place on the instrument. We are developing microphones that can overcome this difficulty for woodwind instruments. By embedding the microphone with proximity detection, pitch detection, and tonal analysis, a microphone can continually adapt the audio signal. The instrument\ufffds natural voicing will be preserved when close-miked, making it possible to record woodwind instruments with more detail.</p>", "people": ["bv@media.mit.edu"], "title": "Adaptive Microphones", "modified": "2016-12-05T00:16:08.431Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "adaptive-microphones"}, {"website": "", "description": "<p>Can I borrow your network? Shout! is a marketplace for retweets that allows people to exchange micro-contracts for future retweets. Shout! facilitates the coordination of social media diffusion efforts by groups.</p>", "people": ["hidalgo@media.mit.edu"], "title": "Shout!", "modified": "2018-05-07T18:22:08.055Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2016-06-01", "slug": "shout"}, {"website": "", "description": "<p>Many researchers have considered probabilistic models of music structure representation. However, these models have yet to provide a truly generative framework, capable of elucidating the cognitive processes of induction, concept formation, or categorization. Feature centrality examines dependency networks of harmonic units within short phrases as a possible foundation for this type of inference. We have found that features of music structures do not adhere to the same feature centrality principles as artifacts or essentialized categories. We are continuing to develop this model through subject testing and computational modeling.</p>", "people": ["tod@media.mit.edu"], "title": "Feature Centrality in Western Harmonic Structure", "modified": "2016-12-05T00:16:08.304Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "feature-centrality-in-western-harmonic-structure"}, {"website": "", "description": "<p>Because common-sense knowledge differs culturally, misunderstandings frequently occur. Because differences can be subtle, there has been little work in trying to detect places in text where cultural differences might arise. We explicitly represent the common-sense knowledge of each culture in separate knowledge bases. By analyzing a text, we can find differences between each culture's knowledge concerning its subject. For example, given an invitation to a party, the system is able to infer that in an American cultural context, hip-hop dancing might be expected, but in a Mexican context, salsa dancing might be the norm. We are building an email client that suggests knowledge from multiple cultures that might be relevant, while watching the user's typing.</p>", "people": ["lieber@media.mit.edu"], "title": "Finding Cultural Differences in Text", "modified": "2016-12-05T00:16:08.340Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "finding-cultural-differences-in-text"}, {"website": "", "description": "<p>An environment for creative collaboration is significant for enhancing human communication and expressive activities. We introduce Second Surface, a novel multi-user Augmented reality system that fosters a real-time interaction for user-generated contents on top of the physical environment. This interaction takes place in the physical surroundings of everyday objects such as trees or houses. Our system allows users to place three dimensional drawings, texts, and photos relative to such objects and share this expression with any other person who uses the same software at the same spot. Second Surface explores a vision that integrates collaborative virtual spaces into the physical space. Our system can provide an alternate reality that generates a playful and natural interaction in an everyday setup.</p>", "people": ["ishii@media.mit.edu", "heun@media.mit.edu"], "title": "Second Surface", "modified": "2016-12-05T00:16:08.404Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "second-surface"}, {"website": "", "description": "<p>This project explores the contribution of advanced audio systems to live performance, their design and construction, and their integration into the theatrical design process. We look specifically at innovative input and control systems for shaping the analysis and processing of live performance; and at large-scale output systems which provide a meaningful virtual abstraction to DSP in order to create flexible audio systems that can both adapt to many environments and achieve a consistent and precise sound field for large audiences. </p>", "people": ["tod@media.mit.edu", "benb@media.mit.edu"], "title": "Advanced Audio Systems for Live Performance", "modified": "2016-12-05T00:16:08.466Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "advanced-audio-systems-for-live-performance"}, {"website": "", "description": "<p>Devices to facilitate gene therapy will be of increasing importance in years to come.  We are developing fluidic systems to facilitate viral delivery in complex tissues.</p>", "people": ["esb@media.mit.edu", "jberns@media.mit.edu"], "title": "Gene Therapy Devices", "modified": "2016-12-05T00:16:08.489Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "gene-therapy-devices"}, {"website": "", "description": "<p>There have been many attempts at developing generative music systems. Rule-based methods generate a limited range of consistent but repetitive music. Probabilistic models are more flexible, allowing the generation of diverse music but lacking the clear causal structures and hierarchies found in most music. We are developing a generative system that induces the qualitative characteristics of motion in music by hierarchically analyzing and modeling changes of momentum simultaneously along multiple parameters, and subsequently transforming our model to give rise to variety and surprise.</p>", "people": ["bv@media.mit.edu"], "title": "Generative Music Systems", "modified": "2016-12-05T00:16:08.515Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "generative-music-systems"}, {"website": "", "description": "<p>Glowdoodle is free software for painting with light.  In front of your webcam, just move a a glowing object, or anything brightly colored, and see the traces appear on the screen in real time. Then participate in the worldwide Glowdoodle community by sharing your creations on the web.</p>", "people": ["mres@media.mit.edu", "ericr@media.mit.edu"], "title": "Glowdoodle", "modified": "2016-12-05T00:16:08.568Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "glowdoodle"}, {"website": "", "description": "<p>In this project we are seeking to define a unique new approach to the challenges of artificial intelligence, one that draws its inspiration from the biological phenomenon of development. Our central contention is that by understanding the processes by which animals \"grow up\" cognitively, we will be able to come much closer to our ultimate goal of creating virtual creatures that are as clever and adaptable as their real-life counterparts. After a careful analysis of the biological literature, we have implemented a novel behavior architecture for our virtual creatures, one that is grounded in what we see as the key computational lessons of development. Foremost amongst these lessons is the concept of Specialized Learning Tools, a domain-specific means of organizing the learning of our creatures so as to efficiently manage and integrate diverse competencies in a unified behavioral context.</p>", "people": [], "title": "Growing Up Virtual: The Computational Lessons of Development", "modified": "2016-12-05T00:16:08.650Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-468", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "growing-up-virtual-the-computational-lessons-of-development"}, {"website": "", "description": "<p>Affect as Index is a tool that takes group physiological data as input, aggregates it across different demographic dimensions, and attaches them to media content. Users can review videotaped or prerecorded events by clicking on points of interest in a physiological graph. This software addresses two challenges: the difficulty of expressing and sharing emotions with others, and the laborious task of monitoring interpersonal interactions within natural settings. For the former, groups interested in discussing shared and dissimilar emotions evoked during experiences can use this tool to place context around their dialogue. For the latter, \"meaningful moments\" observed within natural interactions can be marked and superimposed on the physiological data collected. In this way, affect and observations of affect can be used to index group-level significant moments that occur within volumes of video data.</p>", "people": ["picard@media.mit.edu"], "title": "Affect as Index", "modified": "2016-12-05T00:16:08.720Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-448", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "affect-as-index"}, {"website": "", "description": "<p>Predicting media effectiveness from automatically measured affective responses collected \"in-the-wild\" over the Internet. This project shows that it is possible to predict powerful measures of media effectiveness automatically from natural and spontaneous facial responses captured via the web.</p>", "people": ["picard@media.mit.edu"], "title": "Affective Storytelling", "modified": "2016-12-05T00:16:08.772Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2014-01-01", "slug": "affective-storytelling"}, {"website": "", "description": "<p>HeadLock is a semi-automated system for head pose annotation that explores how human-computer interfaces can be combined with computer vision technologies to efficiently extract behavioral information from video recordings.  For images with limited resolution, the orientation of a head is often the best approximation for gaze direction, a crucial component to analyzing the rich interactions and behaviors of humans.  The goal of HeadLock is to reduce the cost of extracting head pose from video by several orders of magnitude by developing machine-perception technologies that can perform robust head pose estimation with minimal constraints on resolution and camera angle.</p>", "people": ["dkroy@media.mit.edu", "decamp@media.mit.edu"], "title": "HeadLock: Video Analysis for the Human Speechome Project", "modified": "2016-12-05T00:16:08.795Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-441", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "headlock-video-analysis-for-the-human-speechome-project"}, {"website": "", "description": "<p>The \"Affective Carpet\" is a soft, deformable surface made of cloth and  foam, which detects continuous pressure with excellent sensitivity and  resolution.  It is being used as an interface for projects in affective expression, including as a controller to measure a musical performer's direction and intensity in leaning and weight-shifting patterns. \n</p>", "people": ["picard@media.mit.edu"], "title": "Affective Carpet", "modified": "2016-12-05T00:16:08.751Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "affective-carpet"}, {"website": "", "description": "<p>Holographic optics and novel rendering techniques allow for a real-time, off-axis, full-parallax autostereoscopic display based on the fly's-eye lens array. Holographic optical elements (HOEs) allow us to create nested, square arrays. Applications for use include medical imaging, video conferencing, and 3-D survey data.</p>", "people": [], "title": "Holographic Fly's-Eye Autostereoscopic Display", "modified": "2016-12-05T00:16:08.821Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-441", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "holographic-flys-eye-autostereoscopic-display"}, {"website": "", "description": "<p>Full-color, wide-angle, and large-size computer-generated hard-copy holograms take considerable time to create. A \"holographic laser printer\" will allow hard-copy holograms to be generated in minutes instead of hours, automatically and without wet processing. Research topics include recording materials and processing, optical design, image processing and LCD display, and optical techniques for image noise reduction.</p>", "people": [], "title": "Holographic Laser Printer", "modified": "2016-12-05T00:16:08.843Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "holographic-laser-printer"}, {"website": "", "description": "<p>By coupling damped semi-acoustic physical objects with virtual percussion acoustics, we have developed highly controllable hybrid digital/acoustic percussion instruments that can respond not just to the velocity of the hit, but also to how they are struck.  Scraping, stirring with brushes, and very subtle sounds are all possible, greatly extending the richness of digital percussion systems.</p>", "people": ["tod@media.mit.edu"], "title": "Hybrid Percussion", "modified": "2016-12-05T00:16:08.888Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "hybrid-percussion"}, {"website": "", "description": "<p>Affective Circles combines the Chat Circles 2 interface with affective information, including skin conductivity data, to enhance online communication.</p>", "people": ["judith@media.mit.edu"], "title": "Affective Circles", "modified": "2016-12-05T00:16:09.030Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "affective-circles"}, {"website": "", "description": "<p>The AFK cookset is designed for the hungry role-playing gamer who can connect her food items, e.g., Spicy Wolf Dumplings, to her online cooking habits. By scanning in the food items, the video game physically adjusts a hot plate to cook the item for the correct amount of time. The virtual character then jubilantly announces the status of the meal to both the gamer and the other individuals playing online: \ufffdO la la my roasted raptor is about to be done!\ufffd When the food is ready, the system automatically puts the character in AFK (\ufffdAway From Keyboard\ufffd) mode to provide the gamer with a moment to eat. When the player resumes playing, he or she might just discover his or her character\ufffds behavior is affected by the food consumed in real life\ufffdsluggish from overeating or alternately exuberant and energetic.</p>", "people": ["ishii@media.mit.edu"], "title": "AFK Cookset", "modified": "2016-12-05T00:16:09.133Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "afk-cookset"}, {"website": "", "description": "<p>Building on our Expressive Footwear project, we have constructed a low-cost, compact, wireless inertial measurement unit, featuring three axes of both gyroscopes and accelerometers. We have used this device in applications that determine rotational and translational gestural features and estimated orientation in 3-D with fairly good accuracy.  The cubical package measures 1.25\" on a side and the system will run continuously for two days on embedded batteries. More recently, we built a framework for gesture recognition and feedback through this device, enabling users to easily assemble a gesture-recognition front-end for a wide variety of applications. This system can recognize both the type and parameters of a number of atomic gestures, which can then by combined to create composite gestures whose execution can be tied to various outputs. A simplified hand-held version of this framework is now on display.</p>", "people": ["joep@media.mit.edu"], "title": "Inertial Gesture Recognition and Compact Inertial Sensor Packages", "modified": "2016-12-05T00:16:09.167Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-357", "groups": ["responsive-environments", "health", "toys-of-tomorrow", "personal-fabrication"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "inertial-gesture-recognition-and-compact-inertial-sensor-packages"}, {"website": "", "description": "<p>AffQuake is an attempt to incorporate signals that relate to a player's affect into ID Software's Quake II in a way that alters game play. Several modifications have been made that cause the player's avatar within Quake to alter its behaviors depending upon one of these signals. In StartleQuake, when a player becomes startled, his or her avatar also becomes startled and jumps back. Quake changes the size of the player's avatar in relation to the user's response as well, representing player excitement by average skin conductance level, and growing the avatar's size when this level is high.  A taller avatar means the player can see further; however, it also makes him or her an easier target. \n</p>", "people": ["picard@media.mit.edu"], "title": "AffQuake", "modified": "2016-12-05T00:16:09.071Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-383", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "1999-09-01", "slug": "affquake"}, {"website": "", "description": "<p>Installation consists of a viewing window and a stylus with which users can create virtual forms and install them permanently into real space. By tracking the position and orientation of the stylus and the window itself, we are able to calibrate virtual coordinates with real viewing position. Virtual objects created in the system respond as though they were physically in the space of the room. Once objects are placed in the environment, they will stay there in perpetuity, changing and growing like plants in the air.</p>", "people": [], "title": "Installation", "modified": "2016-12-05T00:16:09.212Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-305A", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "installation"}, {"website": "", "description": "<p>This project takes advantage of our group's Ubiquitous Media Portals platform, which enables a large suite of research around the broad theme of what we call Dynamic Ubiquitous Media. This will include relevant, personalized information delivered ambiently to Lab visitors, with intuitive non-contact gestural input for interacting with this information. This project will build a framework for implementing dynamic media, and demonstrate it running throughout our building through a variety of applications.</p>", "people": ["nanwei@media.mit.edu", "joep@media.mit.edu"], "title": "Interaction with Ubiquitous Dynamically Responsive Media", "modified": "2016-12-05T00:16:09.242Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "interaction-with-ubiquitous-dynamically-responsive-media"}, {"website": "", "description": "<p>The Affective Tigger is a plush toy designed to recognize and react to certain emotinal behaviors of its playmate.  For example the toy enters a state of \"happy,\" moving its ears upward and emitting a happy vocalization when it recognizes that the child has postured the toy upright and is bouncing it along the floor.  Tigger has five such states, involving recognizing and responding with an emotional behavior.  The resulting behavior Tigger demonstrates allows it to serve as an affective mirror for the child's expression.  This work involved designing the toy, and evaluating sessions of play with it with dozens of kids.  The toy was shown to successfully communicate some aspects of emotion, and to prompt behaviors that are interesting to researchers trying to learn about the development of human emotional skills such as empathy.  </p>", "people": ["picard@media.mit.edu"], "title": "Affective Tigger", "modified": "2016-12-05T00:16:09.272Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-384", "groups": ["affective-computing", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "affective-tigger"}, {"website": "", "description": "<p>We are building a new kind of agent that acts as a user's assistant in browsing the World Wide Web. Many current Web tools perform searches for the user, but our approach is to consider the search for information as a cooperative venture between the human user and an intelligent software agent. Rather than search a pre-indexed portion of the Web according to user-stated keywords, the agent, Letizia, infers interest implicitly from observing user actions and tries to stay just a few steps ahead of the user, searching the user's immediately accessible links dynamically. </p>", "people": ["lieber@media.mit.edu"], "title": "An Advisory Agent for Web Browsing", "modified": "2016-12-05T00:16:09.300Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "an-advisory-agent-for-web-browsing"}, {"website": "", "description": "<p>What would it mean to browse collaboratively? In business meetings or family situations, several people may be attending a browsing session, even if only one \"controls\" the browser. The success of browsing depends on satisfying the interests of the passive participants as well. We are working on an agent that will provide assistance to group browsing, representing the interests of the entire group, and performing \"reconnaissance\" in anticipation of what pages are likely to be of common interest. It features automatic detection of the presence of users, acquisition of information about the interests of users, automated \"channel surfing,\" and dynamic display of the user profiles and explanation of recommendations.</p>", "people": ["lieber@media.mit.edu"], "title": "Agents for Collaborative Browsing", "modified": "2016-12-05T00:16:09.324Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-305A", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2002-01-01", "slug": "agents-for-collaborative-browsing"}, {"website": "", "description": "<p>How could we enhance the experience of video-conferencing by utilizing an interactive display? With a Kinect camera and sound sensors, we explore how expanding a system's understanding of spatially calibrated depth and audio alongside a live video stream can generate semantically rich three-dimensional pixels, containing information regarding their material properties and location. Four features have been implemented: Talking to Focus, Freezing Former Frames, Privacy Zone, and Spacial Augmenting Reality.</p>", "people": ["raskar@media.mit.edu", "liningy@media.mit.edu", "ishii@media.mit.edu"], "title": "Kinected Conference", "modified": "2016-12-05T00:16:09.418Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "kinected-conference"}, {"website": "", "description": "<p>The ability of shape displays to move and manipulate objects enables the assembly, disassembly, and reassembly of different forms and structures using a set of fundamental building blocks (cubes). We present different assembly techniques such as stacking, scaffolding, and catapulting, which allow us to create 3D structures on a shape display.</p>", "people": ["ishii@media.mit.edu"], "title": "Kinetic Blocks", "modified": "2016-12-05T00:16:09.457Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "kinetic-blocks"}, {"website": "", "description": "<p>AgI connects isolated, rural farmers to markets, to knowledge, and to each other. Farmers will have access to market price information, weather data, agricultural extension services, buyers and intermediaries, and relevant experts. Importantly, a synchronous communication module enables farmers to establish voice and data communication with peer farmers from other rural communities. The system supports a geography-based user interface that respects a range of written literacy levels. For instance, all market price information includes text representations and machine-generated, spoken audio.</p>", "people": [], "title": "AgI: Agricultural Information System", "modified": "2016-12-05T00:16:09.537Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-391", "groups": ["edevelopment", "e-markets"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "agi-agricultural-information-system"}, {"website": "", "description": "<p>Humans and animals display a flexible and rich array of social competencies, demonstrating the ability to interpret, predict, and react appropriately to the behavior of others, to engage others in a variety of complex social interactions, and to enrich their understanding of the environment by observing other's behavior. We believe that developing systems that have these same sorts of social abilities is a critical step in designing animated characters, robots, and other computer agents, who appear intelligent and capable in their interactions with humans (and each other), and who are intuitive and engaging for humans to interact with. Towards this end, we have begun adding a variety of social competencies to our character architecture, such as imitation, gaze following, and simple empathy (the ability to recognize other's emotions), as well taking steps towards creating characters who can form complex beliefs and expectations about other's behavior (such as Max T. Rat's ability to recall people with whom he has previously interacted, and to react appropriately to them). </p>", "people": [], "title": "Learning From and About Others: Social Learning for Synthetic Characters", "modified": "2016-12-05T00:16:09.645Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-441", "groups": [], "published": true, "active": false, "end_on": "2006-01-01", "slug": "learning-from-and-about-others-social-learning-for-synthetic-characters"}, {"website": "", "description": "<p>Miniature sensors are so sophisticated that a wristwatch-mounted inertial measurement system can differentiate subtle actions such as shaking hands or turning a doorknob. Presently, sensors placed on objects in the environment can detect location, movement, sound, and temperature; however, interpretation of this data into human language remains challenging. Previous attempts at activity recognition force all descriptions into small, formal catagories specified in advance. For example, location could be at home or at the office\ufffdthese models have not been adapted to the wider range of complex, dynamic, and idiosyncratic human activities. LifeNet constructs a mapping between sensor streams and partially ordered sequences of events in human language. We believe that mapping sensor data into LifeNet will act as a \"semantic mirror\" to meaningfully interpret sensory data into cohesive patterns in order to understand and predict not only human action, but give clear direction toward understanding human thought and motivation.</p>", "people": ["joep@media.mit.edu", "minsky@media.mit.edu"], "title": "LifeNet: Learning Common Sense from Sensors", "modified": "2016-12-05T00:16:09.680Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-309", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "lifenet-learning-common-sense-from-sensors"}, {"website": "", "description": "<p>As information takes on new physical forms in the environment around us, there is a need for new types of instrumentation to detect and to interrogate the smart materials used to embody the information, and to sense the environment. We are currently working on low-cost, ultra-wideband methods of probing physical structures and decoding the information they contain. In addition, we are exploring further applications of such instrumentation, with particular interest in the frequency-dependent dielectric constant analysis of materials. Preliminary studies have been done in analyzing mail content, with applications in postal mail security, as well as in analyzing milk-fat content, with applications in the collection and distribution of milk in rural India. This project builds on work done by Richard Fletcher, Joe Paradiso, Olufemi Omojola and Matthew Reynolds.</p>", "people": ["fletcher@media.mit.edu", "neilg@media.mit.edu"], "title": "Low-Cost RF Analyzer", "modified": "2016-12-05T00:16:09.844Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2002-01-01", "slug": "low-cost-rf-analyzer"}, {"website": "", "description": "<p>Inspired by the success of the fusible alloy clutch utilized in the digitally reconfigurable surface actuation system, we have been looking into the possibility of abstracting this concept into three dimensions, using fusible alloy to attach spheres or other particles together.  In a simple case this involves plating micro-milli spheres (metal, plastic, glass, etc.) in a solder wetting material (tin, silver, gold, copper, etc.) and then plating that coating with a low temperature solder alloy so that it can be reversibly \ufffdsintered\ufffd to adjacent particles.  In a more complex case, particles would have internal electronics that turn on or off (by heating) bond plates, resulting in a more \ufffdatom-like\ufffd particle that could self-assemble or self-disassemble.</p>", "people": ["neri@media.mit.edu"], "title": "Macro Atom Additive Manufacturing", "modified": "2016-12-05T00:16:09.919Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "macro-atom-additive-manufacturing"}, {"website": "", "description": "<p>aireForm is a dress of many forms that fluidly morph from one to another, animated by air, reflecting the shifting of our own personas. Pneumatic pillows transform the shape of sections of the dress, revealing new forms and evoking classic feminine silhouettes, from sleek to supple to striking.</p>", "people": ["pip@media.mit.edu", "x_x@media.mit.edu", "ishii@media.mit.edu", "holtzman@media.mit.edu", "jacobsj@media.mit.edu", "mres@media.mit.edu"], "title": "aireForm", "modified": "2016-12-05T00:16:09.894Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["information-ecology", "tangible-media", "lifelong-kindergarten"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "aireform"}, {"website": "", "description": "<p>Even the most open content system is only as transparent as its interface. Taking cues from the timeless activity of annotating the margins of books, Marginalia provides a visual overlay for analyzing Wikipedia articles. Employing a number of different visualizations, users can browse Wikipedia with a critical eye for who authored each section, how contentious an article or topic is, or the geographic diversity of the authors. Marginalia pulls back the curtains on the collaborative authorship process, and has broader applications in the critical reading of online works.</p>", "people": ["jkestner@media.mit.edu", "dsmall@media.mit.edu", "holtzman@media.mit.edu"], "title": "Marginalia: Critical Lenses for Reading Wikipedia ", "modified": "2016-12-05T00:16:09.941Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["design-ecology", "information-ecology"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "marginalia-critical-lenses-for-reading-wikipedia"}, {"website": "", "description": "<p>A pair of glasses that darken whenever a television is in view. The user should wear the glasses whenever she feels that she is likely to encounter unpleasant media. The glasses are intended to be used as a therapy system, relieving the stress and despondency associated with heavy television viewing.</p>", "people": ["csik@media.mit.edu"], "title": "Media-Sensitive Sunglasses", "modified": "2016-12-05T00:16:10.009Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-020D", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "media-sensitive-sunglasses"}, {"website": "", "description": "<p>Gait research on trans-femoral prosthesis users has shown that the metabolic costs for these individuals are significantly higher than those of able-bodied individuals for level-ground walking.  Additionally, trans-femoral amputees exhibit a much higher degree of gait asymmetry between the affected and non-affected sides, leading to reduced walking speeds and increased hip and back pain compared to non-amputees.  This project consists of a clinical study of five to ten unilateral trans-femoral amputees using either a conventional or a powered knee prosthesis and height-weight matched able-bodied individuals. This work will compare the metabolic cost of transport and biomechanics of conventional standard of care prosthetic knees with a novel powered knee.  It is hypothesized by using a powered prosthetic knee both the metabolic and biomechanical aspects of amputee gait can be improved.</p>", "people": ["hherr@media.mit.edu"], "title": "Metabolic and Biomechanical Effects of Using a Powered Prosthetic Knee", "modified": "2016-12-05T00:16:10.044Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "metabolic-and-biomechanical-effects-of-using-a-powered-prosthetic-knee"}, {"website": "", "description": "<p>In the spirit of Midnight Basketball, which was created to give inner-city youths positive nighttime activities in the summer, we have created Midnight Computing, to give the option of non-athletic, positive activities. To our surprise, rather than only attracting teenagers, we have attracted whole families, ranging in age from 6 to 78. This has given rise to a whole set of community-based learning activities, taking place in technology centers in public housing projects. Beginning in the fall of 1988, graduates from the vocational training are working with this group at several of the tech centers.</p>", "people": ["cavallo@media.mit.edu"], "title": "Midnight Computing/Community Computing", "modified": "2016-12-05T00:16:10.078Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "", "groups": ["future-of-learning-2"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "midnight-computingcommunity-computing"}, {"website": "", "description": "<p>The scientific community is making marked progress in the area of Alzheimer's disease (AD) treatment: memory-related pharmaceuticals are available, the neurobiology of AD is fairly well understood, and the genetic underpinnings of the disease continue to be unraveled. However, despite these advances, it has been shown that individuals often present the symptoms of AD years before they seek a diagnosis. The barrier to treatment is the lack of structure with which to obtain a diagnosis or even predict the onset of disease in a stigmatized environment. With technology, we can build clinically valid assessment into the tools we use every day\ufffdthe tools we care about. We are developing music tools to detect cognitive performance in the memory domains at risk of decline in the earliest stages of AD. These tools are mobile, longitudinal, and the patient is the first point of feedback.</p>", "people": ["tod@media.mit.edu"], "title": "Mobile Music Diagnostics: Targeting Alzheimer's Disease", "modified": "2016-12-05T00:16:10.106Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "mobile-music-diagnostics-targeting-alzheimers-disease"}, {"website": "", "description": "<p>We have developed a simple modular platform for wireless sensing. This system allows for easy prototyping and testing of embedded sensor applications, and encapsulates much of the wireless system design and makes it reusable. All boards in the system are 1.4 inch square; the master board contains a 22 MIPS microcontroller with 12-bit ADC and a 115.2 kBps wireless link with TDMA channel sharing. A flash memory board is also available for local storage. Sensor boards can be attached to the master via a 26-pin fixed link, which provides for both direct and multiplexed connections, as well as a variety of other data protocols and power distribution. So far, constructed sensor boards include a six degree of freedom inertial measurement unit, a tactile (e.g., bend and pressure) sensor signal processing board, and a promixity sensing board based on both sonar and capacitve sensors. An ambient sensor (camera, IR, heat, sound) board is currently in testing. This platform has been used in a number of lab projects, including a wearable gait laboratory, a novel musical controller and an instrument plush bear.</p>", "people": ["joep@media.mit.edu"], "title": "Modular Platform for High-Density Wireless Sensing", "modified": "2016-12-05T00:16:10.138Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "modular-platform-for-high-density-wireless-sensing"}, {"website": "", "description": "<p>The goal of this project is to develop tools to sense and adapt to a user's affective state based on his or her mouse behavior. We are developing algorithms to detect frustration level for use in usability studies. We are also exploring how more permanent personality characteristics and changes in mood are reflected in the user\ufffds mouse behavior. Ultimately, we seek to build adaptive relational agents that tailor their interactions with the user based on these sensed affective states.</p>", "people": ["picard@media.mit.edu"], "title": "Mouse-Behavior Analysis and Adaptive Relational Agents", "modified": "2016-12-05T00:16:10.161Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "mouse-behavior-analysis-and-adaptive-relational-agents"}, {"website": "", "description": "<p>This project explores how we can visually compare multiple newsgroups. We represent some of the implicit information about group communication that takes place within a newsgroup by tracking how much people respond to each others' comments, the frequency of supportive or angry words used, or the frequency with which people return to the group and post follow-up messages. Mapping this sort of information to visual characteristics such as size, shape, color, and nesting of objects will give each representation of a newsgroup a distinct visual appearance.</p>", "people": ["judith@media.mit.edu"], "title": "Multiples of News", "modified": "2016-12-05T00:16:10.187Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-450", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "multiples-of-news"}, {"website": "", "description": "<p>Using creative software for piano learning, we train non-musicians to play short melodies on a piano. We are conducting an fMRI study to look at the neurobehavioral changes associated with listening to, learning, and playing music. The software has been designed to automate all aspects of subject training and behavioral testing.</p>", "people": ["tod@media.mit.edu"], "title": "Music-Training Software for fMRI", "modified": "2016-12-05T00:16:10.221Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "music-training-software-for-fmri"}, {"website": "", "description": "<p>About a decade ago, University of Edinburgh and its partners put together a collection of transcribed human/human dialogues in which both participants were given similar maps. The person giving instructions had to navigate the other particpant along a path only on the giver's map. Since then, this corpus has been used for various tasks. To our knowledge, there have been no attempts to model actual understanding on the instruction receiver end; this study aims to fill this gap. We introduce Navigational Information Units (NIUs) describing individual intervals of the paths, drawn from several categories (such as \"moves\" and \"positions\"). Our first aim is to decompose these NIUs into a number of possibly independent constituents and ground the latter in terms of the objects on the maps. Having such models at hand, we can further employ the notion of context and try and replicate the entire paths as a sequence of extracted NIUs.</p>", "people": ["dkroy@media.mit.edu"], "title": "Natural Language Understanding for MapTask", "modified": "2016-12-05T00:16:10.242Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-469", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "natural-language-understanding-for-maptask"}, {"website": "", "description": "<p>Despite use in treating depression, and promise in treating stroke, Parkinson's, tinnitus, and other disorders, non-invasive brain stimulation technology is bulky, power-hungry, non-focal, and requires precision alignment with neural structures. We are applying modern engineering techniques to create a portable, focal, non-invasive brain stimulator that will enable a new platform for therapeutic neuromodulation.</p>", "people": ["esb@media.mit.edu", "henninge@media.mit.edu"], "title": "Non-Invasive, Focal, and Portable Brain Stimulators", "modified": "2016-12-05T00:16:10.295Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-435", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "non-invasive-focal-and-portable-brain-stimulators"}, {"website": "", "description": "<p>Computing in Context is a distributed computing environment tailored to host agile, opprtunistic processes on a collection of heterogeneous computer systems and devices. The environment lets each user leverage the diversity of resources made available in a given neighborhood to provide rich computational abilities for distributed applications. Processes deployed on the substrate opportunistically aggregate and consume ambient resources at runtime. Applications are written in a high-level language providing a simple and extensible semantic to express complex distributed behaviors.\n</p>", "people": ["lip@media.mit.edu"], "title": "Computing in Context", "modified": "2016-12-05T00:16:10.345Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "computing-in-context"}, {"website": "", "description": "<p>In this project we explore forms of representing that emphasize notions such  as discovery, embodiment, and fluidity. We are currently working on new techniques for shape matching and object recognition, with applications in generic object detection in natural scenes, pose discrimination, and active vision. This work provides an important link between the physical world and the situation-aware communication systems designed by other members of the Cognitive Machines group.</p>", "people": ["dkroy@media.mit.edu"], "title": "Object Vision for Language", "modified": "2016-12-05T00:16:10.452Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "object-vision-for-language"}, {"website": "", "description": "<p>Air Mobs creates a local mobile community to allow users to freely share Internet access among diverse carrier 3G and 4G data accounts. We have built an app where anyone can advertise that they have bits and battery to spare and are willing to let other Air Mob members tether to them. They might do this if they are near their data cap and either need a little more data, or have some they are willing to let others use before it expires. A website tracks the evolution of the community and posts the biggest donators and users of the system. To date, this app works on Android devices. It is designed to be open and community-based.  We may experiment with market credits for sharing airtime and adding other devices and features. </p>", "people": ["holtzman@media.mit.edu", "lip@media.mit.edu"], "title": "Air Mobs", "modified": "2016-12-05T00:16:10.372Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["viral-communications", "information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "air-mobs"}, {"website": "", "description": "<p>Protocols for Alternative Sexuality and Sensibility (PASS) is a wireless, networked system designed to function with a multidisciplinary description of a network in mind, incorporating conceptual implications and technical implementation of networking. PASS visibly tracks connections in public space based on the embodiment of protocols associated with sexual identity. These user-configured devices exchange information with other devices in order to uncover the often hidden interconnections created by the internalization of sexual identity. Sexual identity is represented by several alternative paradigms in addition to the culturally predicated homosexual/heterosexual binary. </p>", "people": ["csik@media.mit.edu"], "title": "PASS: Protocols for Alternative Sexuality and Sensibility", "modified": "2016-12-05T00:16:10.518Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-020A", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "pass-protocols-for-alternative-sexuality-and-sensibility"}, {"website": "", "description": "<p>This project is about embodying the user interface for a call-handling agent in a small animatronics device. The embodied agent\ufffds primary function is to interact socially, with both the user and other co-located people. In order for an agent to be understandable by humans, it must have a naturalistic embodiment and must interact with its environment as living creatures do by sending out readable social cues that convey its internal state. Our conversational agent has physical presence through interactive \ufffdstuffed animals\ufffd of different shapes and sizes, remotely controlled by a computer. These creatures interact with a combination of pet-like and human-like behaviors, such as waking up, waving for attention, or eye contact. These subtle non-verbal cues are socially intuitive to us, and therefore could be an ideal platform for unobtrusive interruption by mobile communication devices.</p>", "people": ["geek@media.mit.edu", "stefanm@media.mit.edu"], "title": "Physical Embodiments for Mobile Communication Agents", "modified": "2016-12-05T00:16:10.605Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "physical-embodiments-for-mobile-communication-agents"}, {"website": "", "description": "<p>We have developed a novel market game, Prediction Game and Experience Sharing (PreGES, pronounced PreGuess), that harnesses people's collective prediction and experience sharing to forecast success or failure of new items (e.g., products, services, UI designs). Companies can register their new items on this market (as a testbed) to ask for collective opinions. In each PreGES trial session, participants makes their own best predictions on other people's overall opinions about the new items to get incentives (e.g., real opportunities to experience the items) and have fun in gambling-like games. As a participant\ufffds guess (or portfolio) approaches the collective guess of all participants, he or she has a greater chance of winning an incentive. Participants improve the accuracy of their next prediction by sharing experiences. As participants have more trial sessions, their collective prediction converges into one common opinion (forecasting the success or failure of new items).</p>", "people": ["rmorris@media.mit.edu", "picard@media.mit.edu"], "title": "Prediction Game and Experience Sharing Market for Forecasting Marketplace Success", "modified": "2016-12-05T00:16:10.716Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "prediction-game-and-experience-sharing-market-for-forecasting-marketplace-success"}, {"website": "", "description": "<p>Neurological and psychiatric disorders afflict over one billion people worldwide, presenting annual costs exceeding $1 trillion. What are the principles of controlling neural circuits, in order to improve their functions and overcome intractable neurological and psychiatric disorders? We have invented cell-type-specific optical neural control technologies, and with them we are seeking to parse out the methods with which to fix activity in aberrant neural circuits, correcting the computational dynamics within, in order to discover new principles of treating neural disease. </p>", "people": ["esb@media.mit.edu", "bdallen@media.mit.edu", "geggio@media.mit.edu", "henninge@media.mit.edu", "jberns@media.mit.edu"], "title": "Principles of Controlling Neural Circuits", "modified": "2016-12-05T00:16:10.749Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-435", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "principles-of-controlling-neural-circuits"}, {"website": "", "description": "<p>Touch sensing has become established for a range of devices and systems both commercially and in academia. In particular, multi-touch scenarios based on flexible sensing substrates are popular for products and research projects. We leverage recent developments in single-layer, off-the-shelf, inkjet-printed conductors on flexible substrates as a practical way to prototype the necessary electrode patterns, and combine this with our custom-designed PrintSense hardware module which uses the full range of sensing techniques. Not only do we support touch detection, but in many scenarios also pressure, flexing, and close proximity gestures.</p>", "people": ["nanwei@media.mit.edu", "joep@media.mit.edu"], "title": "PrintSense: A Versatile Sensing Technique to Support Flexible Surface Interaction", "modified": "2016-12-05T00:16:10.790Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "printsense-a-versatile-sensing-technique-to-support-flexible-surface-interaction"}, {"website": "", "description": "<p>The von Neumann architecture is the foundation of all modern microprocessor design. What is the analogue which will drive quantum computation, and enable efficient quantum processors? We have built a large-scale system for simulating and testing architectural designs for quantum computers, and are evaluating fault-tolerant quantum computation schemes.</p>", "people": ["ike@media.mit.edu"], "title": "Quantum Computer Architecture", "modified": "2016-12-05T00:16:10.866Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-427", "groups": [], "published": true, "active": false, "end_on": "2008-01-01", "slug": "quantum-computer-architecture"}, {"website": "", "description": "<p>Complex data\ufffdsuch as neurophysiological recordings, or measures of human behavior, Internet, and general network data\ufffdare extremely difficult to analyze because of the dynamic nature of the high-dimensional set of interacting processes that generates the data. Accordingly, traditional statistical and data analysis methods\ufffdclustering, correlation, and so forth\ufffdcan rarely create models sophisticated enough to explain the data without trying to explain noise, demanding astronomically sized datasets, or requiring enormous amounts of hand-tuning by insightful labor. We propose to design and develop a system that continuously generates novel data-modeling hypotheses and evaluates them in real time, testing models of ever-increasing complexity on data as it comes in.</p>", "people": ["esb@media.mit.edu", "bdallen@media.mit.edu", "cwentz@media.mit.edu", "jkinney@media.mit.edu", "caromk@media.mit.edu"], "title": "Real-Time Data Mining and Perturbation", "modified": "2016-12-05T00:16:10.893Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-435", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "real-time-data-mining-and-perturbation"}, {"website": "", "description": "<p>Increasing numbers of networked appliances are bringing about new opportunities for control and automation. At the same time, an increase in multifunctional appliances is creating a complex and often frustrating environment for the end-user. Motivated by these opportunities and challenges, we are exploring the potential for sensor fusion to increase usability and improve user experience while retaining the user in the control loop. We have developed a novel, camera-less, multi-sensor solution for intuitive gesture-based indoor lighting control, called RElight. Using a wireless handheld device, the user simply points at a light fixture to select it and rotates his hand to continuously configure the dimming level. Pointing is a universal gesture that communicates one\ufffds interest in or attention to an object. Advanced machine learning algorithms allow rapid training of gestures and continuous control that supplements gesture classification.</p>", "people": ["bmayton@media.mit.edu", "joep@media.mit.edu", "nanzhao@media.mit.edu"], "title": "RElight: Exploring pointing and other gestures for appliance control", "modified": "2016-12-05T00:16:10.942Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "relight-exploring-pointing-and-other-gestures-for-appliance-control"}, {"website": "", "description": "<p>A robotic computer that moves its monitor \"head\" and \"neck,\" but that has no explicit face, is being designed to interact with users in a natural way for applications such as learning, rapport-building, interactive teaching, and posture improvement. In all these applications, the robot will need to move in subtle ways that express its state and promote appropriate movements in the user, but that don't distract or annoy. Toward this goal, we are giving the system the ability to recognize states of the user and also to have subtle expressions.</p>", "people": ["picard@media.mit.edu"], "title": "RoCo: A Robotic Desktop Computer", "modified": "2016-12-05T00:16:11.022Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-468", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "roco-a-robotic-desktop-computer"}, {"website": "", "description": "<p>Although we designed Scratch primarily as a means for personal expression, a growing number of high schools and colleges (including Harvard and Berkeley) are using Scratch as an introduction to computer science and programming. The Scratch4CS project explores the question: \"Can we extend Scratch so that it is suitable for a full-semester introduction to programming and computational thinking?\" This question is particularly relevant now since there are several initiatives underway to rethink introductory computer science courses and advanced-placement exams.</p>", "people": ["jmaloney@media.mit.edu", "mres@media.mit.edu"], "title": "Scratch for Computer Science", "modified": "2016-12-05T00:16:11.068Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "scratch-for-computer-science"}, {"website": "", "description": "<p>The Atmosphere installation presents a large cloud of information on a wide presentation screen that can be manipulated by three handheld devices mounted on plinths within the gallery space. The visitor takes one of the devices and physically moves it within a designated area, thereby manipulating the information presented both on the screen and the larger projection. A simple magnetic coupling system was developed to track the position and orientation of the handheld device. Signals are magnetically coupled into these tags from transmitting coils on each plinth. The magnitude of the coupling is inversely proportional to the distance from the coils and proportional to the sine of the angle between the tag and the normal of the coil. Using the magnitude of the recovered signal from each tag for each coil, it is possible to calculate both the position and (unsigned) orientation of the device relative to the reference frame formed by the coils. Atmosphere is a collaboration of the Aesthetics and Computation group, the Responsive Environments group, and Steelcase Inc.</p>", "people": ["geppetto@media.mit.edu", "joep@media.mit.edu"], "title": "Simple, Low-Cost Tracking with Active Magnetic Tags", "modified": "2016-12-05T00:16:11.134Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-357", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "simple-low-cost-tracking-with-active-magnetic-tags"}, {"website": "", "description": "<p>SMPL is a software architecture framework aimed at supporting rich synchronous and asynchronous collaboration among people across the Internet. The architecture allows different groups of users to collaborate through a wide range software, hardware, and data platforms. Applications powered by SMPL will not only be able to share processes and data, but also to harvest efficiently the information generated by the interaction of their users. SMPL presents a hybrid topology that simplifies the creation of new functionalities by facilitating the aggregation of different Internet services across a wide range of computing devices, in what is termed \"functionality composition.\"</p>", "people": [], "title": "SMPL", "modified": "2016-12-05T00:16:11.164Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "smpl"}, {"website": "", "description": "<p>People are experts in social intelligence. Most know what is socially appropriate in a given situation, especially how to gauge if something is important enough to interrupt a conversation. People usually also know what kind of social situation they are in and, for example, if it is appropriate to take phone calls. We have built a system that allows an intelligent call-handling agent to \"ask\" the participants in a face-to-face conversation with the user in a very subtle way\ufffda wirelessly actuated, vibrating finger ring\ufffdif an interruption from a mobile communication device would be appropriate. All involved people are given the possibility to \"veto\" an incoming communication anonymously in an equally subtle way, by touching their finger ring. This shifts the burden of deciding to interrupt or not away from the agent and toward the humans actually involved in a conversation.</p>", "people": ["geek@media.mit.edu", "stefanm@media.mit.edu"], "title": "Social Polling of Immediate Surroundings", "modified": "2016-12-05T00:16:11.226Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "social-polling-of-immediate-surroundings"}, {"website": "", "description": "<p>Recently, multi-view display hardware has made compelling progress in graphics. Soundaround is a multi-viewer interactive audio system, designed to be integrated into unencumbered multi-view display systems, presenting localized audio/video channels with no need for glasses or headphones. Our technical work describes a framework for the design of multi-viewer interactive audio systems that is general and supports optimization of the system for multiple observation planes and room responses.</p>", "people": ["raskar@media.mit.edu", "olwal@media.mit.edu", "holtzman@media.mit.edu"], "title": "Soundaround", "modified": "2016-12-05T00:16:11.280Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["camera-culture", "information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "soundaround"}, {"website": "", "description": "<p>We have seen an explosion of data, but the tools necessary to understand it have not kept pace. Humans have innate spatial memory and spatial organization abilities, but our interfaces into this ever-growing world of data rarely take into account a learned behavior of space. SpaceMarks changes that, creating an intuitive and consistent method to organize, learn, and collaborate with spatially aware resources: it provides a much-needed method for offloading the processing of our information into the world around us.</p>", "people": ["pattie@media.mit.edu"], "title": "SpaceMarks: Brain Offloading through Spatial Thinking", "modified": "2016-12-05T00:16:11.333Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "Pond", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "spacemarks-brain-offloading-through-spatial-thinking"}, {"website": "", "description": "<p>AlphaWolf presents a synthetic wolf pack comprised of autonomous and semi-autonomous wolves who interact with each other much as real wolves do: forming dynamic social relationships based on their past experience. Several people can interact with this installation at the same time. Each participant affects the emotional state of her given wolf by howling, growling, or whimpering into a microphone. In addition, participants may encourage their wolves to interact with specific other wolves. The dynamics of how the wolves interact is determined by the wolves' internal state, their social position in the pack, and their previous experiences with their pack-mates. By letting participants \"get inside the mind and body\" of a wolf, we hope to give them a compelling opportunity to explore the meaning of social behavior. The installation features a suite of supporting technology, including evocative real-time computer graphics, autonomous cinematography, and dynamic scoring and sound design.</p>", "people": [], "title": "AlphaWolf", "modified": "2016-12-05T00:16:11.369Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-441", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "alphawolf"}, {"website": "", "description": "<p>Conventional perception on wireless communications is that as more nodes are added, there could be more transmission interference. We demonstrate spectrum sharing with TV channels in a multicast setting, where more added nodes lead to less transmission power and less interference via cooperations among nodes. This demonstration will show that as more nodes are added in the network, the interference with TV signals due to spectrum sharing becomes almost imperceptible.</p>", "people": ["lip@media.mit.edu"], "title": "Spectrum Sharing with TV Channels", "modified": "2016-12-05T00:16:11.451Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "spectrum-sharing-with-tv-channels"}, {"website": "", "description": "<p>We are using intelligent interfaces to spatially organize streams of incoming information into coherent themes. Theme Stream helps us to deal with the ever-increasing streams of information generated around us by applying both supervised and unsupervised machine-learning techniques to the layout of information in the user interface.</p>", "people": ["pattie@media.mit.edu"], "title": "Theme Stream: Visualizing Complex Time-Based Information", "modified": "2016-12-05T00:16:11.893Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "theme-stream-visualizing-complex-time-based-information"}, {"website": "", "description": "<p>Joysticks are a natural interface for controlling F-11s. Mice and keyboards suffice for document preparation. But what is the right interface to \"direct\" the actions of an interactive character, for example, a chicken with a mind of its own? In this case, an ordinary plush toy chicken, augmented with wireless sensing technology, may provide a more iconic and natural interface. In this project we are exploring novel interfaces that allow users to direct the behavior of interactive animated characters who have minds of their own. We are particularly interested in exploring how the innate intelligence of the characters can be used to facilitate the interpretation of the user's actions. Drawing on experience in mechanical and electrical engineering, gesture recognition, visual arts, sculpture, and animation, our group designs new toys to match our interactive worlds. In Swamped! we demonstrate the use of a wireless, plush-toy chicken to direct the actions of an animated chicken.</p>", "people": [], "title": "Squish: Using Toys to Direct Interactive Characters", "modified": "2016-12-05T00:16:11.524Z", "visibility": "PUBLIC", "start_on": "1995-12-31", "location": "", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "squish-using-toys-to-direct-interactive-characters"}, {"website": "", "description": "<p>Talking In Circles is an audioconferencing chat environment. Unlike traditional text-based chat environments, Talking In Circles allows participants simply to speak to each other. Participants are represented as colored circles which accompany their speech with real-time graphical feedback.  In order to allow for 'mingling' and subgroup conversations, audio volume is regulated according to the the circles' location within the system, so that faraway speech is softer. Participants can also draw in their circles, permitting pictorial and gestural communication in addition to speech. The system also contains \"audio booths\" which play music and news to supplement conversation.</p>", "people": ["judith@media.mit.edu"], "title": "Talking In Circles", "modified": "2016-12-05T00:16:11.735Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "talking-in-circles"}, {"website": "", "description": "<p>In this project, learners construct computational models of how they would like to improve their communities. The basic premise is that students will perform a critical inquiry into the life, culture, and functioning of their cities, and create new models of how they would like some aspect to be. They can either address something they perceive as problematic (waste recycling, transportation, energy generation and consumption, employment, crime), or propose a model for a grand new idea to provide some elements desired but not previously possible or conceived (interactive public entertainment and art spaces for community, dynamic customizable clean transportation, instant playgrounds, responsive environments). They work in a variety of computational and traditional media. We are augmenting computational tools for learning, and building new support technologies for distance support and collaboration. The project also serves as a concrete model for alternative-learning environments and for teacher development.</p>", "people": ["cavallo@media.mit.edu", "ronmac@media.mit.edu", "edith@media.mit.edu"], "title": "The City that We Want/A Cidade que a Gente Quer", "modified": "2016-12-05T00:16:11.814Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-443", "groups": ["future-of-learning"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "the-city-that-we-wanta-cidade-que-a-gente-quer"}, {"website": "", "description": "<p>In the tradition of baseball cards, Pogs, and Beanie Babies, we created a new generation of toys that let children trade bits instead of atoms. Imagine a child teaching her robotic toy a new dance step. Then, when she is playing with a friend, her toy can \"teach\" this new dance to her friend's toy. Later, her friend can modify this dance a little, and pass it on to another friend. The creator of the dance can check the Internet to see how far her dance has spread (\"150 toys know my dance\" or \"It spread all the way to Japan!\"). Trading digital (rather than physical) objects has distinct advantages: kids can more easily author, copy, modify, and trace them. We  studied how such toys can give children a richer understanding of their social network, and help them play a more active role in constructing \"kid culture.\" We explored a wide range of Tradable Bits, including pieces of music, animations, stories, and digital creatures. </p>", "people": ["borovoy@media.mit.edu", "mres@media.mit.edu", "bss@media.mit.edu"], "title": "Tradable Bits", "modified": "2016-12-05T00:16:11.966Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-120B", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2001-09-01", "slug": "tradable-bits"}, {"website": "", "description": "<p>The concept of story, or narrative, is closely associated with the understanding of self. Human beings use the narrative thread to interpret reality: what is happening to us in events, in our interaction with others, and in relation to a thing we construct\ufffdan object, a dialogue, a story. Interactive networks provide a means of connecting and communicating remotely. They also provide an opportunity for maintaining and comparing records of use or transactions. By embedding transactional hooks in digital environments, we can enhance the power of the communication, opening the door to porous story-scapes, collectibles and tradeables, affectively enhanced presentations, or economic pro-rating of responsive narrative.</p>", "people": ["barbara@media.mit.edu", "gid@media.mit.edu"], "title": "Transactional Storytelling", "modified": "2016-12-05T00:16:12.097Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "", "groups": ["interactive-cinema"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "transactional-storytelling"}, {"website": "", "description": "<p>This project explores the use of physical fixtures and objects to display temporal information within an architectural space in a subtle and aesthetically pleasing way. Ambient displays present and communicate information through their changes in form, movement, sound, color, and light. Ambient displays are persistently connected to digital information sources, continually displaying changing information within the physical environment.</p>", "people": ["ishii@media.mit.edu"], "title": "Ambient Displays", "modified": "2016-12-05T00:16:12.032Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2000-09-01", "slug": "ambient-displays"}, {"website": "", "description": "<p>We are developing a system for inferring safety context on construction sites by fusing data from wearable devices, distributed sensing infrastructure, and video. Wearable sensors stream real-time levels of dangerous gases, dust, noise, light quality, precise altitude, and motion to base stations that synchronize the mobile devices, monitor the environment, and capture video. Context mined from these data is used to highlight salient elements in the video stream for monitoring and decision support in a control room. We tested our system in a initial user study on a construction site, instrumenting a small number of steel workers and collecting data. A recently completed hardware revision will be followed by further user testing and interface development.</p>", "people": ["gershon@media.mit.edu", "bmayton@media.mit.edu", "joep@media.mit.edu"], "title": "TRUSS: Tracking Risk with Ubiquitous Smart Sensing", "modified": "2016-12-05T00:16:12.131Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "truss-tracking-risk-with-ubiquitous-smart-sensing"}, {"website": "", "description": "<p>uniMorph is an enabling technology for thin-sheet shape-changing interfaces. It affords actuation powered by environmental temperature changes, as well as computational control of actuation. Additionally, the composite allows a seamless integration of electronic components. We developed a digital fabrication method to enable easy creation of composites with actuation patterns of high accuracy.</p>", "people": ["heibeck@media.mit.edu", "basheer@media.mit.edu", "ishii@media.mit.edu", "clarkds@media.mit.edu"], "title": "uniMorph: Thin-Film Shape-Changing Interfaces", "modified": "2016-12-05T00:16:12.153Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "unimorph-thin-film-shape-changing-interfaces"}, {"website": "", "description": "<p>When computers automatically generate video game environments, the result is often as simplistic as a randomly generated maze. Using the Open Mind Common Sense knowledge base, we are exploring ways to automatically generate virtual environments that are immersive and intelligently designed. By allowing video games to dynamically create their own environments, developers will be able to reduce the time they currently spend crafting environments by hand and focus on higher-level design issues. More importantly, games that are capable of changing their own environments will feel less static, resulting in a more believable experience and an increased replay value.</p>", "people": ["lieber@media.mit.edu"], "title": "Using Common-Sense Reasoning in Video Game Design", "modified": "2016-12-05T00:16:12.189Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "using-common-sense-reasoning-in-video-game-design"}, {"website": "", "description": "<p>Audio conferences have a number of striking downsides\ufffdwe lose many of the familiar physical signals from other people that help us judge what other people want to do. In this project, we explore how a new currency for conversation might be added to audio conferences to provide a new channel for non-verbal communication. We're experimenting with different uses for this currency, such as managing the agenda, conversation turn-taking, or length of speaking. Adding this currency provides participants with meaningful, non-verbal ways to communicate through giving currency to people in the meeting you want to support, or spending that currency to guide the meeting in directions in which you want it to move. Mediated environments typically lack these rich backchannels, and we hope that adding new kinds of channels will change the power structures in meetings in a positive way.</p>", "people": ["geek@media.mit.edu"], "title": "Conch", "modified": "2016-12-05T00:16:12.337Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "conch"}, {"website": "", "description": "<p>We wish to address two aspects of vending machine design. The first is the paying system and thus the role of the machine in the market. The second is the interface and interaction with the customer. We are starting with four interface projects: 1) make the vending machine glass a semitransparent display that is touch enabled; 2) use a \"magic wand\" to point and click to purchase items and view information about them; 3) interface the vending machine with a PDA; and 4) introduce speech recognition to the machine.</p>", "people": ["mike@media.mit.edu"], "title": "Vending Machine Challenge", "modified": "2016-12-05T00:16:12.432Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-068", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "vending-machine-challenge"}, {"website": "", "description": "<p>Shape-changing materials have been part of sci-fi literature for decades. But if tomorrow we invent them how are we going to use them? Amphorm is a kinetic sculpture that resembles a vase. Since Amphorm is a dual citizen between the digital and the physical worlds, its shape can be altered both in the  physical world through hand gestures and in the digital realms through an iPad app. Through this project, we explore how the physical world could be synchronized to the digital world and how tools from both worlds can jointly alter dual-citizens.</p>", "people": ["dlakatos@media.mit.edu", "ishii@media.mit.edu"], "title": "Amphorm", "modified": "2016-12-05T00:16:12.387Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "amphorm"}, {"website": "", "description": "<p>This project presents a multimodal speech recognition system for real world scene description tasks. Given a visual scene, the system dynamically biases its language model based on the content of the visual scene and visual attention of the speaker. Visual attention is used to focus on likely objects within the scene. Given a spoken description the system then uses the visually biased language model to process the speech. The system uses head pose as a proxy for the visual attention of the speaker. Readily available standard computer vision algorithms are used to recognize the objects in the scene and automatic real-time head pose estimation is done using depth data captured via a Microsoft Kinect. The system was evaluated on multiple participants. Overall, incorporating visual information into the speech recognizer greatly improved speech recognition accuracy. The rapidly decreasing cost of 3D sensing technologies such as the Kinect allows systems with similar underlying principles to be used for many speech recognition tasks where there is visual information.</p>", "people": ["dkroy@media.mit.edu", "soroush@media.mit.edu"], "title": "Visually grounding speech recognition systems using Kinect ", "modified": "2016-12-05T00:16:12.500Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "visually-grounding-speech-recognition-systems-using-kinect"}, {"website": "", "description": "<p>In this project we demonstrate an application of our work in understanding natural language about spatial scenes. All 3-D modeling applications face the problem of letting their users interact with a 2-D projection of a 3-D scene. Rather than the common solutions that include multiple views, and selective display and editing of the scene, we employ our language learning and understanding research to allow for speech-based selection and manipulation of objects in the 3-D scene. We demonstrate such an interface based on our Bishop project for the 3-D modeling application Blender.</p>", "people": ["dkroy@media.mit.edu"], "title": "Visually-Grounded Language Understanding for the Interface of an Interactive Design System", "modified": "2016-12-05T00:16:12.534Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "visually-grounded-language-understanding-for-the-interface-of-an-interactive-design-system"}, {"website": "", "description": "<p>Microsoft Kinect has popularized the use of body gestures to control games and animations, but only expert programmers can create applications for the Kinect. With our new video-motion extensions to Scratch, it is possible for everyone, even novice programmers, to create Kinect-like games and animations using only a standard webcam. Our extension uses computer-vision algorithms based on optical flow to track motion from real-time video, allowing Scratch programmers to access the amount and direction of motion across the whole scene and also under individual objects. </p>", "people": ["jmaloney@media.mit.edu", "mres@media.mit.edu"], "title": "Watch Me Move!", "modified": "2016-12-05T00:16:12.583Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "watch-me-move"}, {"website": "", "description": "<p>WaterBot is a persuasive water conservation device that presents \ufffdjust-in-time\" context-sensitive feedback to users. Its goal is to motivate behavior change regarding water conservation while using the bathroom sink. The system collects water usage information from flow sensors and presents subtle prompts at the point of behavior. These prompts are designed to be unobtrusive in the form of illumination, graph bars, and auditory feedback. WaterBot uses several persuasive techniques, such as law of contrast, positive reinforcement, variable schedule of reinforcement, social validation, scarcity, curiosity, and challenge. The demonstration includes a video  showing its different behaviors while installed on a sink.</p>", "people": [], "title": "WaterBot: A Persuasive Technology to Motivate Water Conservation", "modified": "2016-12-05T00:16:12.618Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-320", "groups": ["context-aware-computing", "counter-intelligence"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "waterbot-a-persuasive-technology-to-motivate-water-conservation"}, {"website": "", "description": "<p>Information that is spatially or temporarily detached from us is hard to relate to our own personal lives. In addition, we are confronted with more and more information, and we need tools to simplify and abstract this vast amount of data. Using Facebook as an example, we have created a tool for personalized data visualization. Your Facebook friends and acquaintances become stand-ins for the entire world population, and serve as a more easily comprehensible representation for this vast data set. It generates a sense of empathy and connection to what is usually a bodiless and abstract set of numbers. This project is part of the Face the Facts series, which experiments with different ways to make abstract statistical data more tangible using unusual, personalized data-visualization techniques.</p>", "people": ["dsmall@media.mit.edu", "pattie@media.mit.edu"], "title": "What If the World Were Your n Facebook Friends?", "modified": "2016-12-05T00:16:12.666Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "what-if-the-world-were-your-n-facebook-friends"}, {"website": "", "description": "<p>Amphibian allows users to easily imprint digital functions onto common everyday physical objects. Amphibian is a low cost, low infrastructure system that enables users to choose their own physical objects and imprint onto them almost any standard interface functions that take place on a GUI desktop. The goal of Amphibian is to create a system that the common user can implement and operate so that we may learn more about the digital-physical object relationships people will form. </p>", "people": ["ishii@media.mit.edu"], "title": "Amphibian", "modified": "2016-12-05T00:16:12.747Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-331", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "amphibian"}, {"website": "", "description": "<p>We are examining autonomic sleep patterns using a wrist-worn biosensor that enables comfortable measurement of skin conductance, skin temperature, and motion. The skin conductance reflects sympathetic arousal. We are looking at sleep patterns in healthy groups, in groups with autism, and in groups with sleep disorders. We are looking especially at sleep quality and at performance on learning and memory tasks.</p>", "people": ["picard@media.mit.edu", "akanes@media.mit.edu"], "title": "Analysis of Autonomic Sleep Patterns", "modified": "2016-12-05T00:16:12.769Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "analysis-of-autonomic-sleep-patterns"}, {"website": "", "description": "<p>We are experimenting with how the Web can augment the research process, focusing on Web software as vehicles for experimentation, design, and learning. We believe that software tools can be constructed to achieve intrinsic empirical research goals and that the open sharing and dissemination of these results is a key component of the design process. Recycled research software is intended to be sharable, embeddable, and complimentary to explicit research goals. Some key areas of interest for this investigation include: viral experimentation, explicit research licensing, open data sharing and analysis, and pseudo genetic adjustment.</p>", "people": [], "title": "An Experiment in Recycled Research", "modified": "2016-12-05T00:16:12.829Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "an-experiment-in-recycled-research"}, {"website": "", "description": "<p>The goal of Animal Blocks is to provide a sketch book for children's collaborative storytelling by collecting and connecting their narratives. Each animal in Animal Blocks can tell the stories that were told by children who played with it in the past. When a child picks up an animal and puts it on a play scene, a colorful shadow of the toy appears on the scene. Then the shadow walks up to the story book placed next to the play scene, and illustrates its story in the book with text and pictures. The child can write her own stories in the book by using the physical animal blocks in addition to the keyboard. The goal is to encourage children to transfer their oral storytelling, using toys such as Animal Blocks, into written representations, making a connection between children's oral storytelling play and their literacy activities.</p>", "people": [], "title": "Animal Blocks", "modified": "2016-12-05T00:16:12.878Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "animal-blocks"}, {"website": "", "description": "<p>Andante is a representation of music as animated characters walking along the piano keyboard that appear to play the physical keys with each step. Based on a view of music pedagogy that emphasizes expressive, full-body communication early in the learning process, Andante promotes an understanding of the music rooted in the body, taking advantage of walking as one of the most fundamental human rhythms. </p>", "people": ["x_x@media.mit.edu", "ishii@media.mit.edu"], "title": "Andante", "modified": "2016-12-05T00:16:12.904Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "andante"}, {"website": "", "description": "<p>Can animated playground props support and possibly enhance open-ended and physically active play in playgrounds? This project expands the repertoire of objects conceived specifically for children\ufffds outdoor play environments. A category of playground prop called space explorer suggests new opportunities for children to experience their outdoor play environment.</p>", "people": ["susanne@media.mit.edu"], "title": "Animated Playground Props", "modified": "2016-12-05T00:16:13.017Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "animated-playground-props"}, {"website": "", "description": "<p>JaJan! is a telepresence system wherein remote users can learn a second language together while sharing the same virtual environment. JaJan! can support five aspects of language learning: learning in context; personalization of learning materials; learning with cultural information; enacting language-learning scenarios; and supporting creativity and collaboration. Although JaJan! is still in an early stage, we are confident that it will bring profound changes to the ways in which we experience language learning and can make a great contribution to the field of second language education.</p>", "people": ["kevinw@media.mit.edu", "pattie@media.mit.edu"], "title": "JaJan!: Remote Language Learning in Shared Virtual Space", "modified": "2017-09-05T13:19:06.022Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2015-09-30", "slug": "jajan-remote-language-learning-in-shared-virtual-space"}, {"website": "", "description": "<p>Anthropomorphic visualization is a new approach to presenting historical information about participants in online spaces using the human form as the basis for the visualization. We use several pieces of data from the messages an author writes in a Usenet newsgroup to create a depiction that looks somewhat like a person. We are currently investigating users' understanding and reactions to the visualization to assess the advantages and disadvantages of this visual approach.</p>", "people": ["judith@media.mit.edu"], "title": "AnthroViz", "modified": "2016-12-05T00:16:12.995Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-391", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "anthroviz"}, {"website": "", "description": "<p>Animated Vines is an interactive paper installation that comes to life in the presence of a viewer. Normally a static wall hanging, as the viewer approaches the vines begin to curl and slither up and down.  Each vine is made up of eight units, and each unit actuated to curl using muscle wire sewn directly to the paper. While a single unit can only curl slightly, cascading the units sums the individual movements to create a lifelike dance. The muscle wires\ufffd movements are silent so that the interaction is accompanied only by the sound of gently crackling and creaking paper.</p>", "people": ["leah@media.mit.edu", "jieqi@media.mit.edu"], "title": "Animated Vines", "modified": "2016-12-05T00:16:13.037Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "animated-vines"}, {"website": "", "description": "<p>Most story representation and annotation focuses on low-level annotations such as parts of speech. We are trying to annotate English problem-solving narratives with mental processes and ways to think used in the story. This would help for an automated common-sense reasoning system to activate similar mental processes and ways to think in analogy problems.</p>", "people": ["lieber@media.mit.edu", "minsky@media.mit.edu"], "title": "Annotating Stories with Mental Processes", "modified": "2016-12-05T00:16:13.055Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-385", "groups": ["society-of-mind", "software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "annotating-stories-with-mental-processes"}, {"website": "", "description": "<p>Arabiia is a caricature of media stereotypes typically associated with arab women. Aside from exotic Daisy Duck with her dance of the seven veils, and the mute abiding second class citizen in a black burka, not many images spring to mind when thinking of an Arab woman. The convertible outfit is equipped with two servo motors and a switch. It enables its wearer to voluntarily choose which of two extreme representations fits her mood and audience.</p>", "people": ["csik@media.mit.edu"], "title": "Arabiia", "modified": "2016-12-05T00:16:13.076Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "Cube", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "arabiia"}, {"website": "", "description": "<p>AR-Jig is a new, handheld, tangible user interface for 3-D digital modeling. AR-Jig has a pin array that displays a 2-D physical curve coincident with a contour of a digitally displayed 3-D form. It allows physical interaction with a portion of a 3-D digital representation, allowing 3-D forms to be touched and modified directly. In contrast to traditional tangible interfaces, which physically embody the entirety of data, this project leaves the majority of the data in the digital domain but gives physicality to any portion of the larger digital dataset via a handheld tool. This intersection of tangible and digital tools results in the ability to flexibly manipulate digital artifacts both tangibly and virtually.</p>", "people": ["ishii@media.mit.edu", "dick@media.mit.edu"], "title": "AR-Jig", "modified": "2016-12-05T00:16:13.141Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "ar-jig"}, {"website": "", "description": "<p>Concrete Financial Sim aims to anticipate probable outcomes of different decisions across time. Life consistently presents choices that require a rational balance between instant gratification and long-term consequences. Should I buy the sunglasses now or should I save? Should I buy a house, or should I rent a room? What if I do it next year instead of next month? Intertemporal components of choices complicate the decision-making process. The complexity comes not in just a one-to-one immediate tradeoff decision, but in its long-term implications. Based on one\ufffds past financial behavior and current plans, we are designing a decision environment that visualizes the future values of present choices. The goal is to create a reality-based model that informs decision makers of their probable rewards and penalties over time, and will serve as a \ufffdcognitive prosthesis\ufffd for people to externalize their mental model of intertemporal choices.</p>", "people": ["dkroy@media.mit.edu", "aithpao@media.mit.edu"], "title": "Concrete Financial Sim", "modified": "2016-12-05T00:16:13.202Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "concrete-financial-sim"}, {"website": "", "description": "<p>While instant messaging clients are frequently and widely used for interpersonal communication, they lack the richness of face-to-face conversations. Without the benefit of eye contact and other non-verbal \"back-channel feedback,\" text-based chat users frequently resort to typing \"emoticons\" and extraneous punctuation in an attempt to incorporate contextual affect information in the text communication. Conductive Chat is an instant messenger client that integrates users' changing skin conductivity levels into their typewritten dialogue. Skin conductivity level (also referred to as galvanic skin response) is frequently used as a measure of emotional arousal, and high levels are correlated with cognitive states such as high stress, excitement, and attentiveness. On an expressive level, Conductive Chat communicates information about each user's arousal in a consistent, intuitive manner, without needing explicit controls or explanations. On a communication-theory level, this new communication channel allows for more \"media rich\" conversations without requiring more work from the users.</p>", "people": ["judith@media.mit.edu", "walter@media.mit.edu"], "title": "Conductive Chat", "modified": "2016-12-05T00:16:13.264Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-320", "groups": ["affective-computing", "electronic-publishing", "sociable-media"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "conductive-chat"}, {"website": "", "description": "<p>We often have 'idle' time during which we consume various types of audio media (radio, audio books, MP3s, CDs, podcasts)\ufffdwhile driving to work, or waiting at a cafe for a friend. Are We There Yet? helps an audio-playing device put together a listening program that will neatly fit within, for example, your bus ride home. It has audio time-compression techniques to match the playback duration to the user's available listening time.</p>", "people": ["geek@media.mit.edu"], "title": "Are We There Yet?", "modified": "2016-12-05T00:16:13.230Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "are-we-there-yet"}, {"website": "", "description": "<p>We are developing a wireless system of devices to monitor the condition of patients with chronic diseases. The patient will use these devices at home to measure relevant medical parameters. Measurements will be relayed by wireless links to a base station in the patient's home. The base station can be used to store the data, perform initial analysis, and relay the data to a central caregiver for further analysis, charting, and alert scanning, as well as reporting to both the physician and the patient.</p>", "people": ["joep@media.mit.edu"], "title": "At-Home Chronic Health Care Monitoring", "modified": "2016-12-05T00:16:13.356Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-441", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "at-home-chronic-health-care-monitoring"}, {"website": "", "description": "<p>Askii is an SMS-based system that allows adult learners to study for a certification exam while on their commute. When learners have a spare five minutes, they can simply text Askii to begin their customized lessons. Askii will respond with a curated set of questions and links to content that learners can study on the go. We have begun building this prototype for learners to study for the US Naturalization Exam and plan to expand to other certification courses. Askii is a prototype within the larger Making Learning Work project.</p>", "people": ["ps1@media.mit.edu", "jnazare@media.mit.edu", "mres@media.mit.edu"], "title": "Askii", "modified": "2016-12-05T00:16:13.398Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "askii"}, {"website": "", "description": "<p>AskMobi is a tool for adolescents to engage in social science inquiry about issues important to them. The system has two components: a cell-phone application for collecting data and a web-based system for creating studies, analyzing data, and presenting results in a public forum where they can be critiqued. The system scaffolds adolescents\ufffd learning experiences by taking them through basic social science methodology step-by-step, while allowing the users to investigate issues they choose.\n</p>", "people": ["sylvan@media.mit.edu", "mres@media.mit.edu"], "title": "AskMobi", "modified": "2016-12-05T00:16:13.434Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "Cube", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "askmobi"}, {"website": "", "description": "<p>An upgrade to the ubiquitous iRx board, atmex is an Atmel \"digital glue\" microcontroller experimentation system (hardware, firmware, and software). It is low cost ($10), does not require a programmer, and eliminates many of the annoyances that are associated with iRx.</p>", "people": ["csik@media.mit.edu"], "title": "Atmex", "modified": "2016-12-05T00:16:13.451Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-020D", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "atmex"}, {"website": "", "description": "<p>This project redefines the future performance vehicle by utilizing an articulating chassis that is activated by using fluid air muscles and a hybrid powerplant. The body morphs, conforms to the road, and is complimented by a soft body skin. Traditional driver controls are replaced by a sensate and wearable seat that allows the driver to steer, accelerate, and brake by body movement. The simplest version of the athlete car consists of only the \"Interactive Seat\" (or \"Wearable Seat\") and the \"Wheel Robot,\" creating a vehicle that provides an exhilarating, sensual driving experience with high driver involvement, similar to non-motorized sports, such as skiing, surfing, and ice skating.</p>", "people": [], "title": "Athlete Car", "modified": "2016-12-05T00:16:13.467Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "athlete-car"}, {"website": "", "description": "<p>How do you get from a TV channel to a rich video archive, how do you get there automatically, and how can you use that content to create augmented media experiences? ATTN-SPAN converts C-SPAN into a series of overlapping video segments that are identified in terms of state, politician, topic, party, action, and legislative item.  Those clips are then used to augment and personalize media experiences, providing information layers that have been crafted to fit the context and consumer.</p>", "people": ["holtzman@media.mit.edu"], "title": "ATTN-SPAN", "modified": "2016-12-05T00:16:13.528Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "attn-span"}, {"website": "", "description": "<p>We have developed a technique by which devices like Pocket PCs, phones, or laptop computers can use audio processing to estimate their locations relative to one another.  This will enable the creation of ad hoc speaker or microphone arrays from heterogeneous collections of mobile devices.</p>", "people": ["vmb@media.mit.edu"], "title": "Audio-Based Self-Localization", "modified": "2016-12-05T00:16:13.589Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-368", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "audio-based-self-localization"}, {"website": "", "description": "<p>We have a limited range of hearing, defined primarily by volume and distance. As one moves further away from a constant sound, it becomes quieter. What if it didn't have to? Audiograph looks at how positional and orientation information can help us bridge distance barriers for audio, and create seamless audio interactions between individuals, places, and information. Applications include social interactions, serendipitous encounters, navigation, and a re-evaluation of phones and remote audio-based communication.</p>", "people": ["holtzman@media.mit.edu"], "title": "Audiograph: Superhero Hearing", "modified": "2016-12-05T00:16:13.555Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "audiograph-superhero-hearing"}, {"website": "", "description": "<p>AudioFile overlays imperceptible tones on standard audio tracks to embed digital information that can be decoded by standard mobile devices. AudioFile lets users explore their media more deeply by granting them access to a new channel of communication. The project creates sound that is simultaneously meaningful to humans and machines. Movie tracks can be annotated with actor details, songs can be annotated with artist information, or public announcements can be infused with targeted, meaningful data. </p>", "people": ["lip@media.mit.edu", "trich@media.mit.edu"], "title": "AudioFile", "modified": "2016-12-05T00:16:13.620Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "audiofile"}, {"website": "", "description": "<p>We have developed Audiopad, an interface for musical performance that aims to combine the modularity of knob-based controllers with the expressive character of multidimensional tracking interfaces. Audiopad uses a series of electromagnetically tracked objects called \ufffdpucks\ufffd as input devices. The performer assigns each puck to a set of samples that he wishes to control. Audiopad determines the position and orientation of these objects on a tabletop surface and maps this data into musical cues such as volume and effects parameters. Graphical information is projected onto the tabletop surface from above, so that information corresponding to a particular physical object on the table appears directly on and around the object. Our exploration suggests that this seamless coupling of physical input and graphical output can yield a musical interface that has great flexibility and expressive control.</p>", "people": ["ishii@media.mit.edu", "jpatten@media.mit.edu"], "title": "Audiopad", "modified": "2016-12-05T00:16:13.654Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "audiopad"}, {"website": "", "description": "<p>We have built a configurable infrastructure to protect users' dynamic levels of\ufffdprivacy in a pervasive sensor network. Our system is based around a badge that can alert the user to the presence of participating sensor networks, plus emit an RF beacon with which the network can gauge the level of privacy desired. Badges either periodically emit an \"opt out\" signal, blocking sensing within their RF (and sensor perceptual) range, or allow the users' desired level of privacy to be preconfigured online. This privacy level depends on user location, and can eliminate or \"blur\" the data and calculated features available from various sensors and sensor nodes. The privacy level can also be dependent on the status of the client browsing the sensor network\ufffdthe badge user can assign different levels of privacy to different groups of people.</p>", "people": ["nanwei@media.mit.edu", "joep@media.mit.edu"], "title": "Configurable Dynamic Privacy for Pervasive Sensor Networks", "modified": "2016-12-05T00:16:13.682Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-331", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "configurable-dynamic-privacy-for-pervasive-sensor-networks"}, {"website": "", "description": "<p>In the Living Observatory installation at the Other Festival, we invite participants into a transductive encounter with a wetland environment in flux. Our installation brings sights, smells, sounds, and a bit of mud from a peat bog undergoing restoration near Plymouth, MA to the MIT Media Lab. As part of the Living Observatory initiative, we are developing sensor networks that document ecological processes and allow people to experience the data at different spatial and temporal scales. Small, distributed sensor devices capture climate and other environmental data, while others stream audio from high in the trees and underwater. Visit at any time from dawn till dusk and again after midnight, and check the weather report on our website (tidmarsh.media.mit.edu) for highlights; if you\ufffdre lucky you might just catch an April storm.</p>", "people": ["gershon@media.mit.edu", "bmayton@media.mit.edu", "joep@media.mit.edu"], "title": "Living Observatory Installation: A Transductive Encounter with Ecology", "modified": "2019-04-24T17:33:21.560Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "living-observatory-installation-a-transductive-encounter-with-ecology"}, {"website": "", "description": "<p>Augmented Physicality aims to explore the causal relationship between virtual information and its physical counterpart. We look at the explicit coupling between tangible XYZ coordinates and digital XYZ coordinates,  focusing on adaptive rendering and perspective based on the factors of proximity and orientation. Ultimately, Augmented Physicality hopes to provide a multi-user, multi-perspective environment where both physical and digital objects can be collaboratively explored and manipulated.</p>", "people": ["ishii@media.mit.edu", "ndepalma@media.mit.edu"], "title": "Augmented Physicality", "modified": "2016-12-05T00:16:13.741Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "augmented-physicality"}, {"website": "", "description": "<p>This work consists of two projects. The first, a hypersonic sound system, collects outdoor impressions and transforms them into lush and soothing indoor atmospheres. The second, Aural Augmentation, pipes in natural environment sounds into an interior space.</p>", "people": [], "title": "Aural Ambience", "modified": "2016-12-05T00:16:13.766Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": [], "published": true, "active": false, "end_on": "2009-01-01", "slug": "aural-ambience"}, {"website": "", "description": "<p>Broadcasting and recording bird vocalizations through cell-phone networks is a simple yet powerful idea with strong repercussions in conservation projects, born out of the need to automate and facilitate bird population surveys. Of concern is the broadcast quality bird-song analysis and synthesis that can leverage the cell channels for transmission. This work aims to extract environmental information from vocalization recordings through cell-phone networks.</p>", "people": [], "title": "Automatic Polling of Wildlife Creatures", "modified": "2016-12-05T00:16:13.885Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2009-09-01", "slug": "automatic-polling-of-wildlife-creatures"}, {"website": "", "description": "<p>An Awareness Portal turns unused wall space into a community news resource. It responds to viewers' presence and maintains a trace of community browsing activity.</p>", "people": ["nitin@media.mit.edu", "geek@media.mit.edu"], "title": "Awareness Portal", "modified": "2016-12-05T00:16:13.954Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-344", "groups": ["living-mobile", "e-markets"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "awareness-portal"}, {"website": "", "description": "<p>Connectibles are small \"keepsake\" objects that are exchanged by individuals, establishing a communication portal between them. As an ecology of objects, the Connectibles system physically embodies a social network; as an ecology of \"smart\" objects, it can automatically build rich representations of social networks. Any gift from one person to another contains information about the relationship between the giver and the receiver, but this information remains locked away in each object. If we could capture this information, we might expose social networks that have both more detail and greater consonance with a user's \"reality-based\" social network than current virtual, Internet-based social network applications. We might also open up a new, tangible way to interact with social networks, using as phicons the very same sentimental objects that represent people in the user's social network.</p>", "people": ["vmb@media.mit.edu"], "title": "Connectibles", "modified": "2016-12-05T00:16:13.904Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "Garden Conference Room", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "connectibles"}, {"website": "", "description": "<p>&nbsp;</p>", "people": [], "title": "Proof of dance", "modified": "2018-05-22T23:19:54.990Z", "visibility": "LAB-INSIDERS", "start_on": "2018-03-08", "location": "", "groups": ["viral-communications"], "published": false, "active": false, "end_on": "2018-03-08", "slug": "proof-of-dance"}, {"website": "", "description": "<p>Projected augmented reality in the manufacturing plant can increase worker productivity, reduce errors, gamify the workspace to increase worker satisfaction, and collect detailed metrics. We have built new LuminAR hardware customized for the needs of the manufacturing plant and software for a specific manufacturing use case.</p>", "people": ["nfarve@media.mit.edu", "pattie@media.mit.edu", "kubat@media.mit.edu", "linder@media.mit.edu"], "title": "MARS: Manufacturing Augmented Reality System", "modified": "2017-09-05T13:24:16.601Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2015-09-30", "slug": "mars-manufacturing-augmented-reality-system"}, {"website": "", "description": "<p>The living room is the heart of social and communal interactions in a home. Often present in this space is a screen: the television. When in use, this communal gathering space brings together people and their interests, and their varying needs for company, devices, and content. This project focuses on using  personal devices such as mobile phones with the television; the phone serves as a controller and social interface by offering a channel to convey engagement, laughter, and viewer comments, and to create remote co-presence.</p>", "people": ["geek@media.mit.edu"], "title": "Back Talk", "modified": "2016-12-05T00:16:14.008Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "back-talk"}, {"website": "", "description": "<p>A loop engine that records a voice-generated drum pattern. It can instantly and automatically loop it, quantize it, slow it down or speed it up without distording the sound. Or it can turn your voice into a real-sounding drum kit.</p>", "people": ["tristan@media.mit.edu", "tod@media.mit.edu"], "title": "B-Box", "modified": "2016-12-05T00:16:14.052Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-484", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "b-box"}, {"website": "", "description": "<p>Babble Baubles, wearable bracelet-like devices for kids that send secret messages composed of changing patterns of color, let children program different display patterns and respond to other people's Baubles. In using the Baubles, kids learned how to construct language conventions and how to encrypt and decrypt codes.</p>", "people": ["sylvan@media.mit.edu", "mres@media.mit.edu"], "title": "Babble Bauble", "modified": "2016-12-05T00:16:14.086Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-020A", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "babble-bauble"}, {"website": "", "description": "<p>Industrial development is the process by which economies learn how to produce new products and services. But how do economies learn? And who do they learn from? &nbsp;<span style=\"font-size: 18px; font-weight: normal;\">The literature on economic geography and economic development has emphasized two learning channels: inter-industry learning, which involves learning from related industries; and inter-regional learning, which involves learning from neighboring regions.&nbsp;</span></p><p><span style=\"font-size: 18px; font-weight: normal;\">Here we use 25 years of data describing the evolution of China's economy between 1990 and 2015--a period when China multiplied its GDP per capita by a factor of ten--to explore how Chinese provinces diversified their economies.</span><span style=\"font-size: 18px; font-weight: normal;\">&nbsp;</span></p><p><span style=\"font-size: 18px; font-weight: normal;\">First, we show that the probability that a province will develop a new industry increases with the number of related industries that are already present in that province, a fact that is suggestive of inter-industry learning. Also, we show that the probability that a province will develop an industry increases with the number of neighboring provinces that are developed in that industry, a fact suggestive of inter-regional learning. Moreover, we find that the combination of these two channels exhibit diminishing returns, meaning that the contribution of either of these learning channels is redundant when the other one is present.&nbsp;</span><br></p><p><span style=\"font-size: 18px; font-weight: normal;\">Further, we address endogeneity concerns by using the introduction of high-speed rail as an instrument to isolate the effects of inter-regional learning. Our differences-in-differences (DID) analysis reveals that the introduction of </span>high speed-rail<span style=\"font-size: 18px; font-weight: normal;\"> increased the industrial similarity of pairs of provinces connected by high-speed rail. Also, industries in provinces that were connected by rail increased their productivity when they were connected by rail to other provinces where that industry was already present. These findings suggest that inter-regional and inter-industry learning played a role in China's great economic expansion.</span><br></p>", "people": ["gaojian@media.mit.edu", "hidalgo@media.mit.edu", "bjun@media.mit.edu"], "title": "Collective Learning  in China's Regional Economic Development", "modified": "2017-03-31T00:31:45.552Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2017-09-01", "slug": "collective-learning"}, {"website": "", "description": "<p>This project developed efficient versions of Bayesian techniques for a variety of inference problems, including curve fitting, mixture-density estimation, principal-components analysis (PCA), automatic relevance determination, and spectral analysis. One of the surprising methods that resulted is a new Bayesian spectral analysis tool for nonstationary and unevenly sampled signals, such as electrocardiogram (EKG) signals, where there is a sample with each new (irregularly spaced) R wave. The new method outperforms other methods such as Burg, Music, and Welch, and compares favorably to the multitaper method without requiring any windowing. The ability to use unevenly spaced data helps avoid problems with aliasing.  The method runs in real time on either evenly or unevenly sampled data.</p>", "people": ["picard@media.mit.edu"], "title": "Bayesian Spectral Estimation", "modified": "2016-12-05T00:16:14.256Z", "visibility": "PUBLIC", "start_on": "1996-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "bayesian-spectral-estimation"}, {"website": "", "description": "<p>Instead of ordering produce or groceries through the Web, is there a radically different future built on distributed food production? Working with advanced hydroponic technology in the kitchen, we are inventing and exploring methods for bringing efficient microfarming directly into the home. The thesis here is that networked home gardening can enable greater engagement, empowerment, and knowledge about the foods we eat. An essential enabler for this is a new home web of intelligent appliances\ufffdfor instance, radically improved tools for heating and cooling food. The refrigerator generally uses more electricity than any other appliance in the home. We are devising ways to make these and other appliances much more efficient by tracking patterns of use in homes and dorms and using fuzzy logic algorithms to modify appliance behavior. Likewise, the stove can be one of the most dangerous appliances in the home, and most stoves allow only the crudest control over temperature. By looking at the fundamental relationships between the food and the pot, the pot and the stove, and the users, we will invent safer, easier, more accurate and enjoyable ways to cook.</p>", "people": ["mike@media.mit.edu"], "title": "Better Home Freezing, Cooking, and Farming", "modified": "2016-12-05T00:16:14.326Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "better-home-freezing-cooking-and-farming"}, {"website": "", "description": "<p>The standard model of of human behavior is a model of rational actors. This model is widely accepted by individuals, policy makers, and businesses. In this work we are attempting to challenge some of the basic assumptions of economics and rationality. The primary focus of the project is consumer preference, trying to understand better the forces that define the demand in the marketplace.</p>", "people": [], "title": "Behavioral Economics", "modified": "2016-12-05T00:16:14.349Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-314", "groups": ["erationality"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "behavioral-economics"}, {"website": "", "description": "<p>Creative and productive information interchange in organizations is often stymied by a perverse incentive setting among the members. We transform that competition into a positive exchange by using market principles. Specifically, we apply innovative market mechanisms to construct incentives while still encouraging pro-social behaviors. Barter includes means to enhance knowledge sharing, innovation creation, and productivity. Barter provides managerial capability by using economic tools to stimulate activities and modify behaviors. We will measure the results and test the effectiveness of an information market in addressing organizational challenges. We are learning that transactions in rich markets can become an organizing principle among people, potentially as strong as social networks.</p>", "people": ["lip@media.mit.edu"], "title": "Barter: A Market-Incented Wisdom Exchange", "modified": "2016-12-05T00:16:14.223Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "barter-a-market-incented-wisdom-exchange"}, {"website": "", "description": "<p>Between the Bars is a blogging platform for one out of every 142 Americans\ufffdprisoners\ufffdthat makes it easy to blog using standard postal mail. It consists of software tools that make it easy to upload PDF scans of letters, crowd-sourced transcriptions of the scanned images. Between the Bars includes the usual full-featured blogging tools including comments, tagging, RSS feeds, and notifications for friends and family when new posts are available. </p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "Between the Bars", "modified": "2016-12-05T00:16:14.539Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "between-the-bars"}, {"website": "", "description": "<p>We propose a system that reveals hidden physical properties of objects and the environment, and that reveals objects and aspects of the environment via their hidden physical properties.  The display of these revealed properties should be coincident with their sources, both in space and in time.  This coincidence serves to simplify the mapping of property to source, and to facilitate collaborative evaluation of these properties.  The display should also be a simultaneous field rather than a sequentially revealed series of points, so that adjacent entities may be instantly compared.  To demonstrate the strengths of such a system, we will build and evaluate a thermographic projector.</p>", "people": ["ishii@media.mit.edu"], "title": "Better Thermal Understanding", "modified": "2016-12-05T00:16:14.582Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-331", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "better-thermal-understanding"}, {"website": "", "description": "<p>A wind-up energy prototype that utilizes an ergonomic, resonant mechanism to efficiently harvest human power. In the developed world, the Bettery can serve to power consumer electronics. In the developing world where power is scarce, the Bettery can power low-cost computing such as the Pengachu Linux Server.</p>", "people": ["jacobson@media.mit.edu", "neilg@media.mit.edu"], "title": "Bettery", "modified": "2016-12-05T00:16:14.613Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-015", "groups": ["molecular-machines", "physics-and-media", "personal-fabrication"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "bettery"}, {"website": "", "description": "<p>We developed new computational tools that allowed children to create their own scientific instruments\ufffdenabling today's children, like scientists of earlier eras, to become engaged in scientific inquiry not only through observing and measuring, but also through designing and building. We found that when students build their own scientific instruments, they not only become more motivated in science activities, but also develop critical capacities in evaluating scientific measurements and knowledge, make stronger connections to the scientific concepts underlying their investigations, and develop deeper understandings of the relationship between science and technology.</p>", "people": ["calla@media.mit.edu", "mres@media.mit.edu", "bss@media.mit.edu"], "title": "Beyond Black Boxes", "modified": "2016-12-05T00:16:14.637Z", "visibility": "PUBLIC", "start_on": "1996-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "beyond-black-boxes"}, {"website": "", "description": "<p>The Big Thing is a massive, child-oriented music construction kit that allows multiple children to experiment with creating musical structures, and then to manipulate and to perform with them. Big Things are made up of a two-tiered, functional base about three feet in diameter, which can be turned and twisted using sensing elastic chords. On top of such a base, children use a collection of hand-sized, soft, sensing objects, with a variety of functions: chunks each contain a musical fragment or parameter description; connectors establish functional and time-based relationships between chunks; poking areas allow musical data to be created within each chunk; sensors can be attached to the Big Thing at will, allowing body gesture or touch to control the entire Big Thing. Up to twelve Big Thing \"islands\" can be interconnected at a time, forming a stage-sized musical sculpture that invites group creative collaboration, and demonstrates through its physical form the basic principles of musical expression.</p>", "people": ["tod@media.mit.edu"], "title": "Big Thing", "modified": "2016-12-05T00:16:14.700Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-483", "groups": ["opera-of-the-future", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "big-thing"}, {"website": "", "description": "<p>Bibliodoptera is an installation commissioned for the MIT 150th Anniversary Celebration FAST Festival of Art, Science, and Technology. A cloud of vellum butterflies, newly emerged from the chrysalis of MIT\ufffds diverse library pages, floats above in the corridor between the Lewis and Hayden Libraries on MIT\ufffds campus. Trajectories through the cloud illuminate to guide passersby along the length of the corridor. This installation is an unobtrusive but strikingly beautiful symbol of the guiding knowledge of the arts and humanities that have been developed and pursued at MIT over the last 150 years. The butterflies, printed with text from books, sheet music and pages of MIT theses, are interactively illuminated by small lights from within.</p>", "people": ["ejessop@media.mit.edu", "patorpey@media.mit.edu", "tod@media.mit.edu"], "title": "Bibliodoptera", "modified": "2016-12-05T00:16:14.804Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "bibliodoptera"}, {"website": "", "description": "<p>The BiDi Screen is an example of a new type of thin I/O device that possesses the ability both to capture images and display them. Scene depth can be derived from BiDi Screen imagery, allowing for 3D gestural and 2D multi-touch interfaces. This bidirectional screen extends the latest trend in LCD devices, which has seen the incorporation of photo-transistors into every display pixel. Using a novel optical masking technique developed at the Media Lab, the BiDi Screen can capture light field-like quantities, unlocking a wide array of applications from 3D gesture and touch interaction with CE devices, to seamless video communication.</p>", "people": ["raskar@media.mit.edu", "holtzman@media.mit.edu"], "title": "BiDi Screen", "modified": "2016-12-05T00:16:14.823Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-301", "groups": ["camera-culture", "information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "bidi-screen"}, {"website": "", "description": "<p>Constant Crit encourages Media Lab researchers to post their work in its earliest form, a concise one- to two-sentence statement. The system then displays these ideas throughout the Media Lab, offering others a chance to critique the work by suggesting readings and comments. It also offers a way for others to simply 'like' a project, or to go further and follow it or collaborate with the author.</p>", "people": ["holtzman@media.mit.edu"], "title": "Constant Crit", "modified": "2016-12-05T00:16:15.100Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "constant-crit"}, {"website": "", "description": "<p>BioMod is an integrated interface for users of mobile and wearable devices, monitoring various physiological signals such as the electrocardiogram, with the intention of providing useful and comfortable feedback about medically important information. The first version of this system includes new software for monitoring stress and its impact on heart functioning, and the ability to wirelessly communicate this information over a Motorola cell phone. One application under development is the monitoring of stress in patients who desire to stop smoking: the system will alert an \"on-call\" trained behavior-change assistant when the smoker is exhibiting physiological patterns indicative of stress or likely relapse, offering an opportunity for encouraging intervention at a point of weakness. Challenges in this project include the development of an interface that is easy and efficient to use on the go, is sensitive to user feelings about the nature of the information being communicated, and accurately recognizes the patterns of physiological signals related to the conditions of interest.</p>", "people": ["amohan@media.mit.edu", "picard@media.mit.edu"], "title": "BioMod", "modified": "2016-12-05T00:16:15.219Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "biomod"}, {"website": "", "description": "<p>Millions of humans share their homes with pet parrots. Parrots are popular pets not only because of their intelligence, but also because of their ability to communicate vocally with their owners. But this vocal ability evolved in the wild for interparrot communication (i.e., for maintaining connections with other members of its flock). This connection with the flock is crucial to survival: a lone parrot cannot eat and search for predators simultaneously. What happens when a parrot, brought into a human home, adopts its human family as its flock and its flock members go to school, to work, or to do chores? Many times birds left alone engage in loud raucous calling that the owners--or neighbors--find unacceptable. Often parrots are given up for adoption because the owners cannot find ways of bringing this calling down to an acceptable decibel level. Our device is an attempt to engage the parrot and shape its behavior through positive rewards and mildly negative experiences that are completely under its own control.</p>", "people": ["impepper@media.mit.edu"], "title": "BirdSitter", "modified": "2016-12-05T00:16:15.251Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-358", "groups": ["pet-projects", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "birdsitter"}, {"website": "", "description": "<p>A collage of face pictures provides a bird's-eye-view over the Media Lab community, displayed on a public touch-screen. Contrary to hierarchical directories, this collage provides one-click access to personal information for anyone in the community, lending itself more towards browsing than specific  searching. We connect to individual Twitter accounts and mobile phone numbers, allowing the visitor to view a person's status and place a call to them. We also highlight the people that happened to walk by the screen up to five minutes ago by detecting their RFID tags.</p>", "people": ["ypod@media.mit.edu", "holtzman@media.mit.edu"], "title": "Bird's-Eye-View", "modified": "2016-12-05T00:16:15.323Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "birds-eye-view"}, {"website": "", "description": "<p>A microworld for learning about ecology: every action has a consequence! Whether planning a mission to Mars or imagining improvements to their own environments, kids can change conditions inside the Biosphera and observe perturbations in the system. The microworld enables changes to environmental parameters such as light, humidity, temperature, chemical conditions, and populations of micro-organisms. A computer interface works with sensors and actuators to control the physical world of the Biosphera. This interplay of perceptual accessibility and virtual representations supports experimentation and learning about biology, chemistry, and dynamic systems.</p>", "people": [], "title": "Biosphera", "modified": "2016-12-05T00:16:15.442Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "MLE", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "biosphera"}, {"website": "", "description": "<p>Active Messenger filters email messages according to dynamic rules, and automatically routes messages to a variety of wired and wireless delivery channels. It observes traffic from a user by knowing which devices have been used to originate or to respond to messages, recent log-ins, and caller ID when checking voice or email over the phone. Its goal is to make sure that desired messages always reach the subscriber, while paring messages down when the user is less reachable. Active Messenger also acts as a proxy, hiding the identity of the multiple device addresses at which the subscriber may be found. More than just a router, Active Messenger is a dynamic process which monitors a message's progress through various channels over time.</p>", "people": ["geek@media.mit.edu", "stefanm@media.mit.edu"], "title": "Active Messenger", "modified": "2016-12-05T00:16:15.612Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-344", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "active-messenger"}, {"website": "", "description": "<p>People cannot type as fast as they think, especially when faced with the constraints of mobile devices. The focus of this project is developing an alternative approach to predictive text entry using Open Mind Common Sense.</p>", "people": ["lieber@media.mit.edu"], "title": "A Common-Sense Approach to Predictive Text Entry", "modified": "2016-12-05T00:16:15.736Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "a-common-sense-approach-to-predictive-text-entry"}, {"website": "", "description": "<p>We propose cutting as a novel paradigm for ad hoc customization of printed electronic components. As a first example we created a printed, capacitive, multi-touch sensor, which can be cut by the end-user to modify its size and shape. This very direct manipulation allows the end-user to easily make real-world objects and surfaces touch-interactive, to augment physical prototypes and to enhance paper craft. We contribute a set of technical principles for the design of printable circuitry that makes the sensor more robust against cuts, damages, and removed areas. This includes novel physical topologies and printed forward error correction. A technical evaluation compares different topologies and shows that the sensor remains functional when cut to a different shape.</p>", "people": ["nanwei@media.mit.edu", "joep@media.mit.edu"], "title": "A Cuttable Multi-Touch Sensor", "modified": "2016-12-05T00:16:15.574Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "a-cuttable-multi-touch-sensor"}, {"website": "", "description": "<p>Blendie is an interactive, sensitive, intelligent, voice-controlled blender with a mind of its own. Materials are a 1950s Osterizer blender altered with custom-made hardware and software for sound analysis and motor control. Machines influence self-conception, expression, social perception, and perception of responsibility or action. By accessing and vitalizing the interplay of people and machines through custom interaction design and psychotherapeutic techniques, a social awareness is brought out and individuals are invited to reinvent their own existence.</p>", "people": ["monster@media.mit.edu", "csik@media.mit.edu"], "title": "Blendie", "modified": "2016-12-05T00:16:15.777Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-020D", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "blendie"}, {"website": "", "description": "<p>\"Weblogs\" are a method of distributing personal news, essentially an individual's log of activities, news, and thoughts presented in a public manner on the Web. As a publishing medium, Weblogs are ultimately democratic, often as timely as traditional news sources, and have a potential distribution much greater than print media. One problem with these personal information sources is the inability to find an audience. Blogdex is a system built to harness the power of personal news, amalgamating and organizing personal news content into one navigable source, moving this democratic media to a wider audience.</p>", "people": ["walter@media.mit.edu"], "title": "Blogdex", "modified": "2016-12-05T00:16:15.822Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-320", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "blogdex"}, {"website": "", "description": "<p>Current experiments in agent software rely mostly on domain-specific applications that either have been programmed from scratch, or explicitly modified with an agent in mind. Is it possible to make a tool kit or protocol that would allow an agent to communicate and to control applications that have been constructed more conventionally? Can the agent \"take the place\" of the user in the interface? Can the agent have access to the application's data and behavior? Will commercial \"inter-application communication\" mechanisms suffice? What is the division of labor between the agent and the application? This work will explore these questions.</p>", "people": ["lieber@media.mit.edu"], "title": "Agent-Application Communication", "modified": "2016-12-05T00:16:16.050Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "agent-application-communication"}, {"website": "", "description": "<p>Block Exchange is a website where Scratch users can share data sets and data sources in the form of Scratch programming blocks. For example, a soccer enthusiast can share a block that retrieves data about the number of goals scored by different players in the English Premier League. Other blocks in the Exchange can retrieve information from web-based dynamic data sources. For example, a Scratch user can share a block that dynamically retrieves meta-data on the books that are currently on the New York Times bestsellers list. With Block Exchange, you can create a large variety of projects, ranging from interactive data visualizations to stories and animations which incorporate information from the real world dynamically and in real-time.</p>", "people": ["jmaloney@media.mit.edu", "sdg1@media.mit.edu", "mres@media.mit.edu"], "title": "Block Exchange", "modified": "2016-12-05T00:16:16.084Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "block-exchange"}, {"website": "", "description": "<p>BodyChat is a prototype of a graphical chat system that allows users to communicate via text while their avatars automatically animate attention, salutations, turn-taking, back-channel feedback, and facial expression, according to the rules of conversational behavior.</p>", "people": [], "title": "BodyChat", "modified": "2016-12-05T00:16:16.113Z", "visibility": "PUBLIC", "start_on": "1995-12-31", "location": "E15-320", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "bodychat"}, {"website": "", "description": "<p>Bosu is a design tool offering kinetic memory\ufffdthe ability to record and play back motion in 3-D space\ufffdfor soft materials. It is used for motion prototyping and digitally augmented form finding, combining dynamic modeling with coincident sensing and actuation to create transformable structures. The system consists of varying modular units of bend sensors paired with shape memory alloy (nitinol) actuators woven into a bendable plastic frame and embedded in fabric. Each module can actuate between two positions and together form three-dimensional motion pixels.</p>", "people": ["ishii@media.mit.edu"], "title": "Bosu", "modified": "2016-12-05T00:16:16.138Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "bosu"}, {"website": "", "description": "<p>We are developing \"Constructionism\" as a theory of learning and education. Constructionism is based on two different senses of \"construction.\" It is grounded in the idea that people learn by actively constructing new knowledge, rather than by having information \"poured\" into their heads. Moreover, constructionism asserts that people learn with particular effectiveness when they are engaged in constructing personally meaningful artifacts (such as computer programs, animations, or robots).</p>", "people": ["cavallo@media.mit.edu", "papert@media.mit.edu", "mres@media.mit.edu"], "title": "Constructionism", "modified": "2016-12-05T00:16:16.199Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2007-01-01", "slug": "constructionism"}, {"website": "", "description": "<p>Glass bottles have been a part of human culture for thousands of years, serving both practical and aesthetic functions. The \"bottles\" project explores the transparency of an interface that weaves itself into the fabric of everyday life. Seamless extension of physical affordances and metaphors into the digital domain is a key principle for the design. The \"bottlogues\" piece explores narrative contents for our bottles. A set of three bottles is filled with a story told by three characters. Upon opening each bottle, the man, eagle, and stag start telling their part of the narrative.  Physical manipulation of the bottles - opening and closing - is the primary mode of interaction with digital contents. This project grew out of the \"musicBottles\" project to explore a wider variety of contents as well as both artistic and practical applications of the idea: bottles as containers for bits.</p>", "people": ["ishii@media.mit.edu"], "title": "bottlogues", "modified": "2016-12-05T00:16:16.348Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "bottlogues"}, {"website": "", "description": "<p>The Brain Opera is an attempt to bring expression and creativity to everyone, in public or at home, by combining an exceptionally large number of interactive modes into a single, coherent experience. The project connects a series of hyperinstruments designed for the general public with a performance and a series of real-time music activities on the Internet. Audiences explore the hands-on instruments (i.e., Harmonic Driving, Melody Easel, Gesture Wall, Rhythm Tree, Speaking and Signing Trees, etc.) as preparation for the performance, creating personal music that makes each performance unique. The Brain Opera attempts to redefine the nature of collective interaction in public places, as well as to explore the possibilities of expressive objects and environments for the workplace and home. Since its 1996 premiere and world tour through 1998 (United States, Europe, Asia, and South America), much has been learned about interactive software and interfaces, Internet music systems, and intelligent, evolving environments for creativity and expression. We are currently incorporating these new ideas, and designing significant upgrades and new features for the final version of the Brain Opera, which was permanently installed at the new House of Music in Vienna, Austria in July 2000.</p>", "people": ["tod@media.mit.edu"], "title": "Brain Opera Vienna", "modified": "2016-12-05T00:16:16.384Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-483", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "brain-opera-vienna"}, {"website": "", "description": "<p>Universal Serial Bus (USB) enables true plug-and-play for  peripherals. Simply plug a device in and the computer recognizes it and  downloads the drivers--and that's it. Oddly enough, while there are  millions of PC's out there with USB ports, many of the ports will never get  used, since there are few USB devices. To counter this, we wanted to make a  non-traditional object USB compatible, and chose a 16 million color LED  lamp that has no moving parts. USB has great potential for making toys easy  to hook up to a computer, and the lamp has great potential for changing the  way kids play with their toys by introducing entertainment lighting to the  home. \n</p>", "people": ["mike@media.mit.edu"], "title": "Bright Light", "modified": "2016-12-05T00:16:16.446Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "bright-light"}, {"website": "", "description": "<p>The Constructopedia was a browsable, interactive database, designed to help children build structures, mechanisms, and computer programs as well as to make connections to the mathematical and scientific ideas underlying those constructions. Whereas encyclopedias focus on \"what is,\" the Constructopedia focused on \"how to\" and \"what if.\" The Constructopedia was intended as a communal resource in which children not only get access to building tips, but also contribute their own ideas and designs.</p>", "people": ["mres@media.mit.edu"], "title": "Constructopedia", "modified": "2016-12-05T00:16:16.488Z", "visibility": "PUBLIC", "start_on": "1996-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "constructopedia"}, {"website": "", "description": "<p>We created a computational construction kit for the blind and visually impaired by modifying the user interface of our Cricket programmable bricks and adding capabilities such as speech recognition and speech synthesis. We provided this \"Bricket\" system to a small population of visually-impaired children to use in their homes, and studied how and what the children create with this new technology.</p>", "people": ["rahulb@media.mit.edu", "mres@media.mit.edu"], "title": "Brickets", "modified": "2016-12-05T00:16:16.517Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-020D", "groups": ["lifelong-kindergarten", "gray-matters"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "brickets"}, {"website": "", "description": "<p>We have developed an extremely low-power and low-cost wireless sensor network aimed at applications in asset tracking and ubiquitous activity monitoring. In addition to identifying an object, these nodes (termed active radio-frequency identification [RFID] tags) employ sensors to monitor its state, enabling new applications in fields like security, home automation, and supply-chain management. Although they contain a battery, these tags are not limited by it: by minimizing power consumption and quasi-passively waking on diverse stimuli (changes in light, RF carrier presence, shock or acceleration, and sound), they can last for years. Furthermore, their low cost and small size make them a good candidate for large-scale experiments at the intersection of RFID and wireless sensor networks.</p>", "people": ["nanwei@media.mit.edu", "joep@media.mit.edu"], "title": "Active RFID Tags for Security and Supply-Chain Management", "modified": "2016-12-05T00:16:16.630Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "active-rfid-tags-for-security-and-supply-chain-management"}, {"website": "", "description": "<p>Time-lapse photographs reveal how an observed physical object sustains itself in an environment. Captured as two-dimensional images over time, they reveal facets of how man-made artifacts, such as buildings, stand with and against natural wonders. We are capturing photographically Frank O. Gehry's  Stata Center on the MIT campus, exploring facets that reveal the interaction of the building with light and human encounters, as well as observing any characterization of the rhythm and flow of spaces that may resist a human story. From this data collection, we aim to incorporate technological means to extend attitude, so as to realize a building's expression of attitude.</p>", "people": ["gid@media.mit.edu"], "title": "Building with an Attitude", "modified": "2016-12-05T00:16:16.661Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "building-with-an-attitude"}, {"website": "", "description": "<p>We developed a new approach to computer programming, creating new languages with low thresholds but high ceilings, that enabled kids (and other novice programmers) to make a smooth transition from simple commands to complex programs. Using our LogoBlocks languages, kids created programs by snapping virtual LEGO bricks together on the screen.</p>", "people": ["mres@media.mit.edu", "bss@media.mit.edu"], "title": "Building-Block Programming", "modified": "2016-12-05T00:16:16.678Z", "visibility": "PUBLIC", "start_on": "1996-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "building-block-programming"}, {"website": "", "description": "<p>Natural language processing researchers currently have access to a wealth of information about words and word senses. Unfortunately, it is often difficult to search through and coordinate lexical information across data sources, each of which has its own seperate interface and viewing software. We have approached this problem by creating a unified, flexible interface for various natural- language processing resources. This modular browser, BULB (Brandeis Unified Lexical Browser), and its accompanying front-end provide the NLP researcher with a coordinated display from many of the available lexical resources focusing on the interaction and display of modules from existing NLP tools. This work builds on a collaboration between the OpenMind Project and Brandeis University.</p>", "people": ["havasi@media.mit.edu"], "title": "BULB", "modified": "2016-12-05T00:16:16.717Z", "visibility": "LAB", "start_on": "2006-01-01", "location": "E15-383", "groups": [], "published": true, "active": false, "end_on": "2007-09-01", "slug": "bulb"}, {"website": "", "description": "<p>Experiments in animating butterflies and moths with Shape Memory Metal, as part of a collaboration with the Punch Drunk theater group.</p>", "people": ["leah@media.mit.edu", "tod@media.mit.edu", "jieqi@media.mit.edu"], "title": "Butterflies Sleep No More", "modified": "2016-12-05T00:16:16.804Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["high-low-tech", "opera-of-the-future"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "butterflies-sleep-no-more"}, {"website": "", "description": "<p>BYOB is a computationally enhanced modular textile system that makes available a new material from which to construct \"smart\" fabric objects (bags, furniture, clothing). The small modular elements are flexible, networked, input/output capable, and interlock with other modules in a reconfigurable way. The object built out of the elements is capable of communicating with people and other objects, and of responding to its environment. </p>", "people": ["vmb@media.mit.edu"], "title": "BYOB: Build Your Own Bag", "modified": "2016-12-05T00:16:16.870Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-368", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "byob-build-your-own-bag"}, {"website": "", "description": "<p>C2 (and its derivatives) is the core software tool kit being developed by Synthetic Characters to support all of our work. C2 is an integrated tool kit for building autonomous animated characters that \"do what they ought to do\" \"learn what they ought to learn\" and \"move the way they ought to move.\" The tool kit provides an integrated approach to perception, motivation and emotion, learning, action-selection, and motor control.</p>", "people": [], "title": "C2-and-Beyond", "modified": "2016-12-05T00:16:16.896Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-320", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "c2-and-beyond"}, {"website": "", "description": "<p>CADcast is a system for projecting instructions into 3-D workspaces. It supports users in constructing three-dimensional structures with greater efficiency and more accuracy. The system also supports improved coordination between the design and construction teams involved in architectural scale building projects.</p>", "people": ["ishii@media.mit.edu"], "title": "CADcast", "modified": "2016-12-05T00:16:17.000Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "cadcast"}, {"website": "", "description": "<p>Call to Action is an open-source web platform for creating telephone-based services such as hotlines, voice petitions, and phone blogging. The platform, currently under development, provides an easy-to-use graphical interface that enables the user to plan the flow of calls, record custom audio, and make use of all the input and output features offered by a regular telephone. The service requires no software programming experience, and users can build a service in under half an hour.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "Call to Action", "modified": "2016-12-05T00:16:17.050Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "call-to-action"}, {"website": "", "description": "<p>CAMIT is a web-based front-end to milling machines like the Model-A and for our own in-house $75 milling machine, the Mantis Machine. It allows drag-and-drop \"printing\" of circuit boards in seconds. It also captures all uploaded designs and allows anyone to optimize, modularize, change, or simply re-print them.  CAMIT is a circuit-board design anyone can use.</p>", "people": ["holtzman@media.mit.edu"], "title": "CAMIT", "modified": "2016-12-05T00:16:17.114Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "camit"}, {"website": "", "description": "<p>PAF (paroxysmal atrial fibrillation) is a dangerous form of cardiac arrhythmia that poses severe health risks, sometimes leading to heart attacks, the recognized number-one killer in the developed world. The technical challenges for detecting and predicting PAF include accurate sensing, speedy analysis, and a workable classification system. To address these issues, electrocardiogram (ECG) data from the PhysioNet Online Database will be analyzed using new spectrum estimation techniques to develop a program able to predict, as well as recognize, the onset of specific cardiac arrhythmias such as PAF. The system could then be incorporated into wearable/mobile medical devices, allowing for interventions before cardiac episodes occur, and potentially saving many lives.</p>", "people": ["picard@media.mit.edu"], "title": "Cardiac PAF Detection and Prediction", "modified": "2016-12-05T00:16:17.177Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "cardiac-paf-detection-and-prediction"}, {"website": "", "description": "<p>We are designing interfaces to enhance driver self-awareness through subliminal visual feedback and shape-changing materials. Advancements in sensing technologies make it possible to measure physiological data in the car environment, opening up the possibility of harnessing such data for just-in-time feedback to drivers. </p>", "people": ["changzj@media.mit.edu", "rboldu@media.mit.edu", "amores@media.mit.edu", "pattie@media.mit.edu"], "title": "CarDio", "modified": "2016-12-05T00:16:17.267Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["fluid-interfaces", "advancing-wellbeing"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "cardio"}, {"website": "", "description": "<p>Cardiocam is a low-cost, non-contact technology for measurement of physiological signals such as heart rate and breathing rate using a basic digital imaging device such as a webcam. The ability to perform remote measurements of vital signs is promising for enhancing the delivery of primary healthcare.</p>", "people": ["picard@media.mit.edu", "zher@media.mit.edu"], "title": "Cardiocam", "modified": "2016-12-05T00:16:17.291Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "cardiocam"}, {"website": "", "description": "<p>A study in the roles of different facial features as used by social groups for communication. The goal is to be able to render faces as caricatured against the average face of different societies.</p>", "people": ["judith@media.mit.edu"], "title": "Caricatures", "modified": "2016-12-05T00:16:17.319Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "caricatures"}, {"website": "", "description": "<p>Carousel was inspired by Robert Morris's \"The Finch College Project.\" In his installation, a film camera was placed on a platform that rotated at one revolution per minute. One side of the room had a life-sized, black and white panorama of an audience. On the opposite side, a crew of people were constructing and deconstructing a wall of mirrored tiles. The sequence of activities in the room was recorded by the camera. The room was then emptied, leaving only dots from the mirror grid alignment. The projector (playing the footage captured by the camera) was placed on a platform rotating at the same rate as the original film camera. The  images of the audience appeared and disappeared as the mirror wall was built and unbuilt, projected onto the walls of the same room. Carousel uses this same visual technique to create a communication link between two public spaces. The center of each room will have a rotating podium. A camera, microphone, speakers, and a projector sit on each podium. The podiums rotate slowly at the same rate. As they rotate, the images captured as the camera rotates in one room are projected around the space in the other room and vice versa. The result is a moving snapshot-in-time along the periphery of the room as people, both physically present and projected, are moving about the space.</p>", "people": ["judith@media.mit.edu"], "title": "Carousel", "modified": "2016-12-05T00:16:17.354Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "carousel"}, {"website": "", "description": "<p>Cartagen is a set of tools for mapping, enabling users to view and configure live streams of geographic data in a dynamic, personally relevant way. Today's mapping software is largely based on static data sets, and neither incorporates the time dimension in its display nor provides for real-time data streams.</p>", "people": ["ethanz@media.mit.edu"], "title": "Cartagen", "modified": "2016-12-05T00:16:17.405Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "cartagen"}, {"website": "", "description": "<p>An understanding of the historical development of the car is critical to a \"reinvention\" of the car. This research project tracks the different chronologies and typologies that make up the genealogy of the car. An investigation of the early beginnings of the car such as the transition between horse and carriage and the early automobile reveals many clues about the different forces that have shaped the car. Ships, bicycles, and airplanes have all influenced this development in terms of aesthetics, technologies, manufacturing, and new materials. Other stylistic and technological developments are also documented in images and text in the genealogy in order to compare design from different eras and cultures. The genealogy spans from the ancient horse to the chariots of Greek and Roman times to the horse and carriage of the 18th and 19th centuries to the early automobiles of Henry Ford and Gottlieb Daimler. The genealogy continues with military vehicles of the World Wars to the automobiles of the 1930s and 1940s and eventually arrives at modern cars.</p>", "people": ["rchin@media.mit.edu"], "title": "Genealogy of the Car", "modified": "2016-12-05T00:16:17.426Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "genealogy-of-the-car"}, {"website": "", "description": "<p>In collaboration with the Early Childhood Cognition Center at MIT BCS, we are developing sensor-enabled toys and infant affect sensors with the goal to understand how children on the autism spectrum use patterns of evidence to learn causal relationships and the extent with which this is state-dependent. We investigate in what respects, if any, causal learning is different in comparison to typically developing children. The results of this research will inform the design of new object-based technologies for language and communication learning.</p>", "people": ["picard@media.mit.edu"], "title": "Causal Learning and Autism", "modified": "2016-12-05T00:16:17.452Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-450", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "causal-learning-and-autism"}, {"website": "", "description": "<p>You leave for work in the morning saluting your dog resting on the floor, wondering what he might be up to in the hours you don't see him. Returning home later, you find him where you left him\ufffdbut your sofa tells a story of playful traces of paw prints and body prints! Chairs and other seating are a large part of our daily lives, from car seats to desk chairs. What \"sense of chairness\" can a chair hold beyond its formal and functional qualities? This research explores the idea of anthropocentric illusions, which enable a chair\ufffdor any object\ufffdto reflect, share, and reveal its stories of experience. The system accumulates a multifaceted log of occurrences and encounters with humans and their contexts, and formulates unique expressions that reflect \"attitude\" back to the human. We are realizing a chair's expression by designing and engineering an electronic fabric called TechStyle, a next-generation electronic textile that incorporates input and output capability in the woven structure. The fabric was developed in collaboration with Collins & Aikman, whose expertise includes textile and chemical engineering. We are currently developing the system board that drives both the input and output capability of the fabric. The design and engineering is driven by the vision that electronic input/output will enhance the design of many everyday objects. </p>", "people": ["gid@media.mit.edu"], "title": "Chair with an Attitude", "modified": "2016-12-05T00:16:17.554Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "chair-with-an-attitude"}, {"website": "", "description": "<p>The Chameleon Car is a vehicle that extends its wheelbase width and length as it increases its speed. The project demonstrates an approach that allows a car to transform from a small, maneuverable vehicle to a longer, more aerodynamic vehicle. Included are baffled airbags for passenger safety and a longer, wider wheelbase for added efficiency, safety, and style.</p>", "people": [], "title": "Chameleon Car", "modified": "2016-12-05T00:16:17.606Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "chameleon-car"}, {"website": "", "description": "<p>We describe a study of the impact of visual traces on the behavior of groups of users in Chat Circles, an abstract graphical chat room. The results indicate that traces, which appear in the chat space when users move or speak, change the way people position themselves relative to others and the way they move around the space. Traces also facilitate more expressive uses of participants\ufffd avatars, such as intimidation or showing agreement. Additionally, we consider differences in positioning and communicating between dyadic and larger groups in the chat room.</p>", "people": ["judith@media.mit.edu"], "title": "Chat Circles User Study", "modified": "2016-12-05T00:16:17.718Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "chat-circles-user-study"}, {"website": "", "description": "<p>We are studying the communication capacities of unitary two-party unitary quantum interactions.  We have found expressions for their one-way classical and quantum capacities assisted by an arbitrary amount of entanglement per use of the gate.  More importantly, classical communication using unitary methods has turned out to be a useful abstraction for studying other problems in quantum information theory.  This abstraction, called \"coherent classical communication,\" has led to short proofs of many of the central results in the field, and allowed the unification of several problems in quantum information theory.</p>", "people": ["ike@media.mit.edu"], "title": "Chaniltonians", "modified": "2016-12-05T00:16:17.677Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-427", "groups": ["quanta"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "chaniltonians"}, {"website": "", "description": "<p>Chatterprint creates visual and textual fingerprints of one or more instant message conversations. These sorts of visualization techniques are typically used for browsing archives and exposing behavioral patterns. Instead, we are working to build static visualizations that capture a facet of someone's IM identity. Chatterprints will be easy to create and share, communicating someone's interests and style in a concise visual way. Using Chatterprints as a tool, we can explore what makes someone interested in sharing a personal visualization, what people see in other people's personal visualizations, and what kinds of information people are comfortable sharing from their IM histories.</p>", "people": ["judith@media.mit.edu"], "title": "Chatterprint", "modified": "2016-12-05T00:16:17.800Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "--Choose Location", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "chatterprint"}, {"website": "", "description": "<p>ChessMates is a collaborative chess game. Building on the TeleAction framework, ChessMates allows two communities of players to play chess against one another. This project focuses on supporting the social aspects of playing chess by providing a social interaction space for players to talk about the game, as well as a voting system to allow community-based decisions about which moves should be made.</p>", "people": ["judith@media.mit.edu"], "title": "ChessMates", "modified": "2016-12-05T00:16:17.875Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-450", "groups": ["sociable-media", "broadercasting"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "chessmates"}, {"website": "", "description": "<p>The Chit Chat Club is an experiment in bringing people together in a mixed physical and virtual environment. Online chatrooms and real world cafes are both venues for social interaction, but with significant differences, e.g., the participants' knowledge of each other's expressions and identity and the more governing introductions and turn-taking. Our goal is to create, through careful design of the physical environment and computer interface, a place that gracefully combines these two cultures. The analysis of how well this space actually functions will further our understanding of social interaction, both online and in person.</p>", "people": ["judith@media.mit.edu", "monster@media.mit.edu"], "title": "Chit Chat Club", "modified": "2016-12-05T00:16:17.897Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-449", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "chit-chat-club"}, {"website": "", "description": "<p>Build in Progress is a platform for sharing the story of your design process. With Build in Progress, makers document as they develop their design processes, incorporating iterations along the way and getting feedback as they develop their projects.</p>", "people": ["ttseng@media.mit.edu", "mres@media.mit.edu"], "title": "Build in Progress", "modified": "2016-12-05T00:16:17.917Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2016-08-31", "slug": "build-in-progress"}, {"website": "", "description": "<p>Cinemaware is a custom-built capture, annotation, editing, and compression tool for preparing video content for integration with multiple display and distribution applications. Implemented in Java, the application provides four modes of operation: live capture, keyword annotation, rudimentary editing, and batch processing. The capture mode provides an interface for capturing video through a USB port, while the annotation mode presents an extensible interface for adding freeform text annotations, time-code associated key-words, and general movie properties such as title, character, location, and time. The annotation schema can be modified to incorporate appropriate input fields depending on the needs of external retrieval and display applications. Captured video clips can be refined using a simple editing interface, and the batch-processing mode enables the export of multiple video, image, or audio files according to a specified compression output.</p>", "people": ["gid@media.mit.edu"], "title": "Cinemaware", "modified": "2016-12-05T00:16:17.979Z", "visibility": "PUBLIC", "start_on": "2002-09-01", "location": "E15-368", "groups": ["interactive-cinema"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "cinemaware"}, {"website": "", "description": "<p>Digital music libraries are constantly expanding, thus making it increasingly difficult to recall a particular song in the library, let alone create a playlist for a specific event. By using context-aware data, one can automatically generate a playlist to suit one's current activity or mood. Songs corresponding to a certain tempo can be selected from one's music library by converting a rate of step to a tempo value. Alternatively, a musical playlist can be generated based on the affective value of text entered by the user. Thus, music can be tailored to fit a specific context by using alternative classification methods and by monitoring a user\ufffds actions.</p>", "people": ["bv@media.mit.edu"], "title": "Context-Aware Playlist Generation", "modified": "2016-12-05T00:16:18.035Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "context-aware-playlist-generation"}, {"website": "", "description": "<p>Cicadence is an interactive soundscape. Created through a bio-mimetic process, it examines and is inspired by the auditory experience produced by a cicada.</p>", "people": ["tod@media.mit.edu"], "title": "Cicadence", "modified": "2016-12-05T00:16:17.934Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "cicadence"}, {"website": "", "description": "<p>Spin is a photography turntable system that lets you capture how your DIY projects come together over time. With Spin, you can create GIFs and videos of your projects that you can download and share on Twitter, Facebook, or any other social network.</p>", "people": ["ttseng@media.mit.edu", "mres@media.mit.edu"], "title": "Spin", "modified": "2016-12-05T00:16:18.135Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2016-08-31", "slug": "spin"}, {"website": "", "description": "<p>We are interested in building systems which can learn to connect natural language to objects and actions in video. In our current experiment, we have created videos of a person moving objects around on a table top. Naive subjects have been asked to describe verbally the video sequences. Our goal is to create a learning system which is able to \"ground\" the meaning of words and phrases in terms of observations found in the video. Applications of this work include verbal control of robots and natural-language-based access to video archives.</p>", "people": ["dkroy@media.mit.edu"], "title": "Acquiring Verbs from Speech and Video", "modified": "2016-12-05T00:16:06.376Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "acquiring-verbs-from-speech-and-video"}, {"website": "", "description": "<p>Touch screen systems allow users to interact directly with content displayed on a 2-D surface. However, current systems have severe limitations in usability because they do not preserve the familiar mechanical response of traditional writing and drawing tools. Furthermore, current touch-screen technologies can often require extensive modifications to the drawing/writing substrate, which make them expensive and impractical for large drawing surfaces or entire desktops. We are developing a touch-screen system based on an effect called Frustrated Total Internal Reflection (FTIR). This technique involves coupling light into a pane of glass and then detecting contact to the glass by measuring the absorption of the light by the object in contact, using a linear array of photodetectors adhered to the glass. An LCD screen or other video display can then be fitted behind the glass to produce an appropriate response. All hardware is inside the glass\ufffdthere is no need to mount anything on the side where the interaction takes place. We believe this system will be a practical and economical way to detect contact with brushes, felt pens, and fingers across large interactive surfaces.</p>", "people": ["joep@media.mit.edu"], "title": "2-D Input Device Based on Frustrated Total Internal Reflection", "modified": "2016-12-05T00:16:06.424Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-441", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "2-d-input-device-based-on-frustrated-total-internal-reflection"}, {"website": "", "description": "<p>With the proliferation of cheap video cameras that are always on, and always ready to record, distinctions between the amateur and professional videographer become blurred. When we can record at any moment, how will we decide what we want to record, and when and where we need to be in order to capture what we want? How can we best index the recorded material to support later communication needs? In this research, we use a common-sense approach to knowledge and inference to support a partnership between camera and human videographer. We present a new paradigm for producing common-sense video metadata and we show how the metadata can have a positive impact on video content capture, representation, and presentation.</p>", "people": ["barbara@media.mit.edu", "gid@media.mit.edu"], "title": "Cinematic Common Sense", "modified": "2016-12-05T00:16:18.113Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "cinematic-common-sense"}, {"website": "", "description": "<p>DJ I, Robot is the world's first fully robotic hip-hop DJ, able to cut, scratch, and mix records with three high-speed servo-controlled motors; it functions both as a tool for composing original collaged music and as a novel performance interface, the latter being the current focus of our research. The cultural phenomenon of DJing began in the mid-1970s as a ghetto misappropriation of a delicate consumer technology; this project attempts to intervene in this history by creating a novel technology for the field, and has been met by professional DJs with a mixture of respect, envy, and loathing. DJ I, Robot premiered in 2001 at the Berlin Film Festival, and has performed from Boston to Spliit, Croatia, while appearing in publications from Artbyte to Wired News.</p>", "people": ["csik@media.mit.edu"], "title": "DJ I, Robot", "modified": "2016-12-09T19:40:33.814Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-001", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "dj-i-robot"}, {"website": "", "description": "<p>Gestures [Natasha Sinha and Hugo Sol\ufffds] is composed for six Music Shapers, two trumpets, two trombones, two violas, and one double bass. In this piece, the children use the Shapers to improvise and create audio gestures in response to the sound environment produced by the acoustic instruments. The Shaper's sounds are in turn derived from the acoustic. Gestures is a celebration of collaborative improvisation that conveys excitement, uncertainty and joy, and takes players and listeners on a journey to discover the complex yet simple roots of creativity. The compositional process of Gestures was itself quite unusual in at least two ways. First, the piece is the result of close collaboration between a graduate student specialized in electronic music composition and a 12-year-old composer. Second, the process and techniques involved in its creation were diverse and enjoyable, designed to encourage both creators to think in new ways. Drawing, singing, improvising and formalizing audio gestures were some of the methods employed to build the score. In fact, the unique experiences of two composers from different backgrounds, working and learning from each other, served as a basis for imagining the piece itself. Shared interests were passion to explore non-traditional techniques for the acoustical instruments, the desire to make Shapers an idiomatic instrument, the use of space as compositional element, and ultimately the desire to create an acoustic frame where freedom was the fundamental spirit for composers, musicians, young participants, and audience alike.</p>", "people": ["tod@media.mit.edu"], "title": "Gestures: An Interactive Music Piece for Acoustical Ensemble and Shapers", "modified": "2016-12-05T00:16:08.542Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-443C", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "gestures-an-interactive-music-piece-for-acoustical-ensemble-and-shapers"}, {"website": "", "description": "<p>This installation features the animated character Max T. Rat (Dobie's cousin). Max is the first autonomous, animated character that can be teased, that changes its behavior in response to the teasing, and that remembers the participant and behaves appropriately should the participant return to the installation at a later time. The user teases Max with a piece of physical \"cheese,\" and can also verbally tease him. Based on the participant\ufffds actions, Max forms expectations\ufffdcan he trust the participant to give him the cheese?  is the participant a good person or not?\ufffdand ultimately modifies his behavior to find the best strategy for getting the cheese. The installation demonstrates a number of technologies: Max is built using the next generation of our behavioral toolkit that includes a new framework for learning in a long-term behavioral context, and specialized learning mechanisms for performing social learning and expectation-based learning. In addition, the system incorporates a new Java-based graphics library and novel applications of sensing technology. The \ufffdcheese\ufffd emits IR, allowing us to track, precisely and robustly, the position of the cheese via a stereo-vision system. An RF link allows us to change the state of the cheese in response to Max\ufffds actions. Participants are identified using retro-reflective markers on their badges as well as via acoustic pattern-matching. Pitch tracking is employed to detect if the participant is verbally teasing Max.</p>", "people": [], "title": "Don't Tease the Animals", "modified": "2016-12-05T00:16:18.581Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-489", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "dont-tease-the-animals"}, {"website": "", "description": "<p>In November 2014, Twitter granted Women, Action, and the Media (WAM!) authorized status to report harassment to the company. In three weeks, WAM! reviewers assessed 811 incoming reports of harassment and escalated 161 reports to Twitter, ultimately seeing Twitter carry out 70 account suspensions, 18 warnings, and one deleted account. This document presents quantitative and qualitative findings from this three-week project. Our findings focus on the people reporting and receiving harassment, the kinds of harassment that were reported, Twitter's response to harassment reports, the process of reviewing harassment reports, and challenges for harassment reporting processes.</p>", "people": ["jnmatias@media.mit.edu"], "title": "Reporting, Reviewing, and Responding to Harassment on Twitter", "modified": "2016-12-05T00:16:10.973Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2015-01-01", "slug": "reporting-reviewing-and-responding-to-harassment-on-twitter"}, {"website": "", "description": "<p>For various reasons, some communities have good connectivity amongst themselves, but limited or inadequate contact with a broader community. The CLAP (Cross-Layer Algorithms for Phealth) project is exploring ways to monitor quality-of-life parameters. We are developing low-cost, low-bandwith sensor networks that support applications for capturing, interpreting, and communicating various aspects of both community data (e.g., water, air, or soil quality), and individual data (e.g., heath, hygiene, nutrition), in settings where there is better connectivity among group members than to a core network. Examples of such communities may be found, for example, in OLPC deployment scenarios.</p>", "people": ["pantelis@media.mit.edu", "mbletsas@media.mit.edu"], "title": "CLAP", "modified": "2016-12-05T00:16:18.664Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2010-01-01", "slug": "clap"}, {"website": "", "description": "<p>A musical piece typically has a repetitive structure. Analysis of this structure will be useful for music segmentation, indexing, and thumbnailing. This project attempts to analyze automatically the repetitive structure of musical signals. Strategies for obtaining thumbnails of music based on the structual analysis results are also proposed.</p>", "people": ["bv@media.mit.edu"], "title": "Structural Analysis of Musical Signals for Indexing and Thumbnailing", "modified": "2016-12-05T00:16:11.604Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "structural-analysis-of-musical-signals-for-indexing-and-thumbnailing"}, {"website": "", "description": "<p>In the real world, people clothe themselves in garments whose cut and design encodes information about the social identity of the wearer. This encoding changes temporally as the design spreads throughout a population; this is the basis of the cultural phenomenon known as \"fashion.\" On the Web, people embellish their homepages with links, pictures and other items that exhibit similar patterns of dispersion. We are developing tools to track, analyze and visualize these patterns; our goal with this research is to further understanding of how the Web functions as a social environment.   \n</p>", "people": ["judith@media.mit.edu"], "title": "Virtual Fashion: Cultural Dispersion on the World Wide Web", "modified": "2016-12-05T00:16:12.466Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "virtual-fashion-cultural-dispersion-on-the-world-wide-web"}, {"website": "", "description": "<p>The Beat Browser is a music and sound browser designed to give users fast feedback while browsing arbitrarily large collections of audio. The goal of Beat Browser is to give the user a sense of exploring \ufffdlive\ufffd and continuous audio, while rapidly moving between sources by mouse. The primary design goal is to facilitate rapid browsing of music by genre, artist, and album in order to interactively build playlists. Beat Browser is a product of larger design strategies identified to build efficient browsing engines, applicable beyond sound.</p>", "people": ["geek@media.mit.edu"], "title": "Beat Browser: Browsing Audio Libraries and Playlist Construction", "modified": "2016-12-05T00:16:14.375Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "beat-browser-browsing-audio-libraries-and-playlist-construction"}, {"website": "", "description": "<p>Mood classification is very subjective. There have been many psychological studies performed throughout the past century relating to music and emotion, and as a result there exist many different representations of human emotion. This work proposes to utilize the best possible psychological model of emotion by incorporating the findings of these studies into an innovative front-end for a digital music library where music can be queried, browsed, and explored by mood, rather than artist, album, or genre. With the use of state-of-the-art audio and textual analysis tools this work proposes to automatically and accurately classify music by mood.</p>", "people": ["bv@media.mit.edu"], "title": "A Mood-Based Music Classification and Exploration System", "modified": "2016-12-05T00:16:15.283Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "a-mood-based-music-classification-and-exploration-system"}, {"website": "", "description": "<p>A camera for exploring and experimenting with the urban environment. The camera doesn\ufffdt sense light, but rather things that the eye can\ufffdt see like C02, temperature, or wind.</p>", "people": ["silver@media.mit.edu", "mres@media.mit.edu"], "title": "Camera for the Invisible", "modified": "2016-12-05T00:16:17.097Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "camera-for-the-invisible"}, {"website": "", "description": "<p>Conventional Web interfaces respond to and consider only mouse clicks when defining a user model. We have extended this and take into account all mouse movements on a page as an additional layer of information for inferring user interest. We have developed a straightforward way to record all mouse movements on a page, and conducted a user study to analyze and investigate mouse behavior trends. We have found certain mouse behaviors, common across many users, which are useful for content providers in increasing the effectiveness of their interface design.</p>", "people": [], "title": "Cheese", "modified": "2016-12-05T00:16:17.824Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "cheese"}, {"website": "", "description": "<p>This project explores how merging ubiquitous consumer electronics and the sociable Web can improve the user experience of these devices, increase the functionality of both, and help distribute content in a more sociable way.  Through custom software for digital video recorders and a Facebook application acting as a hub, we connect a community of television viewers via their televisions. By connecting these two technologies, the user can now see what her friends think of the shows available on her DVR, and automatically record her friends' favorites; in return, the user contributes her own viewing data back to the social network.</p>", "people": ["holtzman@media.mit.edu"], "title": "Television Meets Facebook: Social Networking via Consumer Electronics", "modified": "2016-12-05T00:16:29.826Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "television-meets-facebook-social-networking-via-consumer-electronics"}, {"website": "", "description": "<p>Affordable, easy-to-use digital video equipment, widely available high-bandwidth connectivity, and the variability of distributed networks and messaging protocols provide an opportunity to expand the ways people utilize video-based storytelling. The Shareable Media project includes tool design, exploration in aesthetic structures, and methods for developing Shareable Media communities. The research infrastructure supports applications concerned with contextual browsing, context-sensitive display, and the formation of contextual communities. Four demonstrations highlight different aspects of the Shareable Media project: Individeo, developed by James Jung-Hoon Seo; PlusShorts, developed by Aisling Kelliher; M-Views, developed by Pengkai Pan; and Scaleable Architecture, developed by Cathy Lin and Pengkai Pan.</p>", "people": ["gid@media.mit.edu"], "title": "Shareable Media", "modified": "2016-12-05T00:17:02.273Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-368", "groups": ["interactive-cinema", "gray-matters"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "shareable-media"}, {"website": "", "description": "<p>People keep their contacts, such as email addresses and phone numbers, in their computer address books and cell phones. Unless someone asks you for a contact, you probably don\ufffdt share the information. Is there a way to share this personal information that will benefit people in the same social circles? This research looks for opportunities to utilize and share contacts, based on user-defined contact groups, through P2P and Group2Group protocols on wireless/Internet networks.</p>", "people": ["geek@media.mit.edu"], "title": "Sharing Contacts Based on Social Networks", "modified": "2016-12-05T00:17:02.353Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-384C", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "sharing-contacts-based-on-social-networks"}, {"website": "", "description": "<p>Tensor displays are a family of glasses-free 3D displays comprising all architectures employing (a stack of) time-multiplexed LCDs illuminated by uniform or directional backlighting. We introduce a unified optimization framework that encompasses all tensor display architectures and allows for optimal glasses-free 3D display. We demonstrate the benefits of tensor displays by constructing a reconfigurable prototype using modified LCD panels and a custom integral imaging backlight. Our efficient, GPU-based NTF implementation enables interactive applications. In our experiments we show that tensor displays reveal practical architectures with greater depths of field, wider fields of view, and thinner form factors, compared to prior automultiscopic displays.</p>", "people": ["raskar@media.mit.edu", "gordonw@media.mit.edu", "naik@media.mit.edu"], "title": "Tensor Displays: High-Quality Glasses-Free 3D TV", "modified": "2016-12-05T00:17:03.939Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2015-09-15", "slug": "tensor-displays-high-quality-glasses-free-3d-tv"}, {"website": "", "description": "<p>We have built several handheld devices that combine grasp and orientation sensing with pattern recognition in order to provide highly intelligent user interfaces. The Bar of Soap is a handheld device that senses the pattern of touch and orientation when it is held, and reconfigures to become one of a variety of devices, such as a phone, camera, remote control, PDA, or game machine. Pattern-recognition techniques allow the device to infer the user's intention based on grasp. Another example is a baseball that determines a user's pitching style as an input to a video game.</p>", "people": ["vmb@media.mit.edu"], "title": "The \"Bar of Soap\": Grasp-Based Interfaces", "modified": "2016-12-05T00:17:03.965Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "Garden Conference Room", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "the-bar-of-soap-grasp-based-interfaces"}, {"website": "", "description": "<p>Click fraud is an important problem plaguing online advertisers. A click fraud is an attempt by an agency (such as a competitor) to increase the amount charged to an advertiser. Motivations for click fraud range from the urge to deplete a competitor's budget (to ensure that the competitor's advertisement is not aired to consumers) to an attempt to receive cheaper advertising. We are looking at statistical methods of measuring click fraud in order to control this problem.</p>", "people": [], "title": "Click-Fraud Detection", "modified": "2016-12-05T00:16:18.758Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-313", "groups": ["erationality"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "click-fraud-detection"}, {"website": "", "description": "<p>Collaborating and media creation are difficult tasks, both for people and for network architectures. CoCam is a self-organizing network for real-time camera image collaboration. Like with all camera apps, just point and shoot; CoCam then automatically joins other media creators into a network of collaborators. Network discovery, creation, grouping, joining, and leaving is done automatically in the background, letting users focus on participation in an event. We use local P2P middleware and a 3G negotiation service to create these networks for real-time media sharing. CoCam also provides multiple views that make the media experience more exciting\ufffdsuch as appearing to be in multiple places at the same time. The media is immediately distributed and replicated in multiple peers; thus if a camera phone is confiscated or lost, other users have copies of the images.</p>", "people": ["holtzman@media.mit.edu", "lip@media.mit.edu"], "title": "CoCam", "modified": "2016-12-05T00:16:18.896Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["viral-communications", "information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "cocam"}, {"website": "", "description": "<p>Coco is an alternative applications development system for PDAs based on PalmOS and PocketPC. Using Coco's development environment on a desktop machine, users will be able to write programs in a specialized Logo language and download them to run on PDAs. The goal is to increase the range of people who can develop solutions for PDAs, including standard PalmOS and PocketPC applications such as Calendar or games such as Ms. Pacman. In particular, we plan to enable PDALogo communication with the serial port, thereby allowing integration with the Tower prototyping system.</p>", "people": [], "title": "Coco", "modified": "2016-12-05T00:16:18.926Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-344", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "coco"}, {"website": "", "description": "<p>The practical demands of modern life obligate people to spend time physically separated from the people they care about. To cope, people turn to communications technology like the telephone, email, and instant messaging (IM) to maintain a connection with social contacts. These communication modalities are limited in their ability to provide social-connectedness by their failure to balance expressiveness, overhead, and social obligation. Clique Here is a mobile communications platform that attempts to address this limitation and support a higher degree of social-connectedness by complementing mobile telephone capability with media-rich awareness and multiple lightweight communication modes. The Clique Here system consists of a mobile client (implemented on a camera-embedded mobile handset) and a home client (implemented on a wireless web tablet); communication between clients is facilitated by an application server.</p>", "people": ["geek@media.mit.edu"], "title": "Clique Here", "modified": "2016-12-05T00:16:18.955Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "clique-here"}, {"website": "", "description": "<p>Users don\ufffdt want to browse the Web on mobile devices, they want to complete actions. Adeo is an application that streamlines accessing the Web on mobile devices. Users can teach a computer how to interact with their favorite Web sites by demonstrating the actions in a Web browser, and then play these actions back with a single click on their mobile device. Adeo reduces complex procedures to the minimal amount of required input and output, improving both the efficiency and usability of mobile browsing.</p>", "people": ["lieber@media.mit.edu"], "title": "Adeo: A Streamlined User Interface for Accessing the Web on Mobile Devices", "modified": "2016-12-05T00:17:05.891Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "adeo-a-streamlined-user-interface-for-accessing-the-web-on-mobile-devices"}, {"website": "", "description": "<p>As part of a Google-sponsored Glass developer event, we created a Glass-enabled improv comedy show together with noted comedians from ImprovBoston and Big Bang Improv. The actors, all wearing Glass, received cues in real time in the course of their improvisation. In contrast with the traditional model for improv comedy, punctuated by \"freezing\" and audience members shouting suggestions, using Glass allowed actors to seamlessly integrate audience suggestions. Actors and audience members agreed that this was a fresh take on improv comedy. It was a powerful demonstration that cues on Glass are suitable for performance: actors could become aware of the cues without having their concentration or flow interrupted, and then view them at an appropriate time thereafter.</p>", "people": ["swgreen@media.mit.edu", "pattie@media.mit.edu"], "title": "GlassProv Improv Comedy System", "modified": "2017-09-05T19:36:08.241Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2015-09-30", "slug": "glassprov-improv-comedy-system"}, {"website": "", "description": "<p>We propose Coded Lens, a novel system for lensless photography. The system does not require highly calibrated optics, but instead, utilizes a coded aperture for guiding lights. Compressed sensing (CS) is used to reconstruct scene from the raw image obtained through the coded aperture. Experimenting with synthetic and real scenes, we show the applicability of the technique and also demonstrate additional functionality such as changing focus programmatically. We believe this will lead to cheaper, more compact, and even more versatile imaging systems.</p>", "people": ["ishii@media.mit.edu"], "title": "Coded Lens", "modified": "2016-12-05T00:16:19.062Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "coded-lens"}, {"website": "", "description": "<p>A broad series of studies is being executed in the domain of computational machinery with intrinsically visible properties. Such experiments seek to culminate in a superior redesign of the programming paradigm as a process more oriented toward the visual thinker.</p>", "people": [], "title": "codefocus", "modified": "2016-12-05T00:16:19.086Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-443", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "codefocus"}, {"website": "", "description": "<p>This research examines the use of new music technologies as clinical interventions for individuals with autistic spectrum disorders. The technological application mediates and supports group music making. The intervention is designed to affect the domains where autistic individuals show symptomatic behaviors: communication, psychosocial interaction, and repetitive behavior. Furthermore, through working with the interface, a user generates quantifiable information pertaining to their cognitive performance on domain-specific cognitive tasks. This information is valuable for correlation with emergent social behavior as measured by the system. The primary contribution of the work will be to develop the design principles and methodology to track cognitive features over the course of a complex music interaction.</p>", "people": ["tod@media.mit.edu"], "title": "Cognitive Composing", "modified": "2016-12-05T00:16:19.131Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "cognitive-composing"}, {"website": "", "description": "<p>We are crowdsourcing the creation of socially rich, interactive characters by collecting data from thousands of people interacting and conversing in online multiplayer games, and mining recorded gameplay to extract patterns in language and behavior. The tools and algorithms we are developing allow non-experts to automate characters who can play roles by interacting and conversing with humans (via speech or typed text), and with each other. The Restaurant Game recorded over 16,000 people playing the roles of customers and waitresses in a virtual restaurant. Improviso is recording humans playing the roles of actors on the set of a sci-fi movie. This approach will enable new forms of interaction for games, training simulations, customer service, and HR job applicant screening systems.</p>", "people": ["dkroy@media.mit.edu"], "title": "Crowdsourcing the Creation of Smart Role-Playing Agents", "modified": "2016-12-05T00:17:10.118Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "crowdsourcing-the-creation-of-smart-role-playing-agents"}, {"website": "", "description": "<p>In DoppelLab, we are developing tools that intuitively and scalably represent the rich, multimodal sensor data produced by a building and its inhabitants. Our aims transcend the traditional graphical display, in terms of the richness of data conveyed and the immersiveness of the user experience. To this end, we have incorporated 3D spatialized data sonification into the DoppelLab application, as well as in standalone installations. Currently, we virtually spatialize streams of audio recorded by nodes throughout the physical space. By reversing and shuffling short audio segments, we distill the sound to its ambient essence while protecting occupant privacy. In addition to the sampled audio, our work includes abstract data sonification that conveys multimodal sensor data. </p>", "people": ["gershon@media.mit.edu", "bmayton@media.mit.edu", "joep@media.mit.edu"], "title": "DoppelLab: Spatialized Sonification in a 3D Virtual Environment", "modified": "2016-12-05T00:17:10.977Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "doppellab-spatialized-sonification-in-a-3d-virtual-environment"}, {"website": "", "description": "<p>Collections is an application for the management of digital pictures according to their intended audiences. The goal is to create a graphical interface that supports the creation of fairly complex privacy decisions concerning the display of digital photographs. Simple graphics are used to enable the collector to create a wide range of audience arrangements for her digital photographs. The system allows users to express their preferences in sharing their personal pictures over a disembodied environment such as the Web. The system also introduces an original approach to the presentation interface of photographic collections on the Web: a viewing application that takes into account the viewing history of the photographs and the integration of text comments to images. \n</p>", "people": ["judith@media.mit.edu"], "title": "Collections", "modified": "2016-12-05T00:16:19.202Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-449", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "collections"}, {"website": "", "description": "<p>This project explores the use of passive acoustic sensing in automobiles. We are building acoustic sensing equimpment suitable for automotive use, and developing algorithims and heuristics for dynamically extracting driver-relevant information from the acoustic data stream (i.e. location, trajectory, and acceleration of nearby vehicles).</p>", "people": ["joep@media.mit.edu"], "title": "Dynamic Acoustic Tracking and Characterization of Powered Vehicles", "modified": "2016-12-05T00:17:11.049Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-023", "groups": ["responsive-environments", "cc"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "dynamic-acoustic-tracking-and-characterization-of-powered-vehicles"}, {"website": "", "description": "<p>Story understanding is a notoriously difficult problem in AI. Broad-spectrum, common-sense knowledge about the world is a good resource, but current common-sense knowledge bases are far from human-level story understanding. We examine affective story understanding in order to perceive the broad emotional overtones of a story at the sentence level, using both a common-sense perspective and the observation that much of the way we emote in response to everyday situations is part of a shared human experience and therefore a part of common sense. With a corpus of common-sense knowledge, we create a semantic network of everyday situations and the emotions associated with them, which, when combined with our linguistic processing, lets our system classify story sentences into six primitive emotions. We then explore how this technology enables innovations in emotional UIs such as EmpathyBuddy, or in prosody, emotional TTS, gaming, story evaluation, and emotional indexing of documents.</p>", "people": ["lieber@media.mit.edu"], "title": "Emotus Ponens: Affective Story Understanding for Agents", "modified": "2016-12-05T00:17:11.444Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "emotus-ponens-affective-story-understanding-for-agents"}, {"website": "", "description": "<p>ContextController is a second screen social TV application that augments linear broadcast content with related contextual information. By utilizing existing closed-captioning data, ContextController gathers related explanatory video content, displaying this in real-time synchronized to the original content.</p>", "people": ["holtzman@media.mit.edu"], "title": "ContextController", "modified": "2016-12-05T00:16:19.293Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "contextcontroller"}, {"website": "", "description": "<p>This work builds on our earlier work with FaceSense, created to help automate the understanding of facial expressions, both cognitive and affective. The FaceSense system has now been made available commercially by Media Lab spinoff Affectiva as Affdex. In this work we present the first project analyzing facial expressions at scale over the Internet. The interface analyzes the participants' smile intensity as they watch popular commercials. They can compare their responses to an aggregate from the larger population. The system also allows us to crowd-source data for training expression recognition systems and to gain better understanding of facial expressions under natural at-home viewing conditions instead of in traditional lab settings.</p>", "people": ["picard@media.mit.edu"], "title": "Facial Expression Analysis Over the Web", "modified": "2016-12-05T00:17:11.889Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "facial-expression-analysis-over-the-web"}, {"website": "", "description": "<p>Comm.unity is a platform that implements a wireless, device-to-device information system that bypasses the need for any centralized servers, coordination, or administration. A key feature of the platform is that it combines knowledge, awareness, and learning of the user's social relationships and integrates this information into the communication protocols and network services. Comm.unity is designed to work on as many devices as possible, and with as many different radios as possible (WiFi, Bluetooth, IR). It is designed as a platform over which many different networked applications could be developed with ease. SnapN\ufffdShare and additional applications in development are intended to be used in upcoming field studies to collect information about user behavior and their social interactions, and aid in fine-tuning the platform\ufffds learning capabilities.</p>", "people": ["lip@media.mit.edu", "nadav@media.mit.edu"], "title": "Comm.unity", "modified": "2016-12-05T00:16:19.417Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "community"}, {"website": "", "description": "<p>Collective Power  employs a new type of human-computer interaction by visualizing data in a socially and psychologically aware way. The typical display of energy usage where a customer is informed, via various data visualization means, of their own use, is broadened to include a public element. The information for physically adjacent buildings is posted on nearby lamp posts in a vertical meter-like light display that can be viewed for long distances on the urban scale at night. The poles read a comparison to a global standard so that the energy collective participants can tell their collective standing in energy use, percapita, at a glance. More detailed information about individual participants in the energy collective is provided through an iPhone application accessible through a projection from each pole, enabling members of the public and the collective to improve their energy use habits.</p>", "people": ["holtzman@media.mit.edu"], "title": "Collective Power", "modified": "2016-12-05T00:16:19.344Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "collective-power"}, {"website": "", "description": "<p>With Color Code, you can create computer programs that respond to colors of objects in the physical world. You can stack up LEGO bricks to form an obstacle in a video game, integrate a crayon-drawn picture into a virtual story, or use M&Ms to create a musical score. </p>", "people": ["silver@media.mit.edu", "mres@media.mit.edu", "ericr@media.mit.edu"], "title": "Color Code", "modified": "2016-12-05T00:16:19.391Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "color-code"}, {"website": "", "description": "<p>CommenTV is a social commenting system for audiovisual content, able to take and display texts, images, and related videos as social comments. </p>", "people": ["holtzman@media.mit.edu"], "title": "CommenTV", "modified": "2016-12-05T00:16:19.500Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "commentv"}, {"website": "", "description": "<p>This project aims to develop an intelligent personal-finance advisory agent that bridges the gap between the novice user and the expert model of the finance domain. The agent uses common-sense reasoning and inference for associating the user's personal life, financial situation, and goals with the attributes of the expert domain model and vice versa. The agent interface provides a natural-language interface for elicitation and explanations of design and process rationale. The architecture of the system is domain-independent and consequently can be used for any novice-expert domain model.</p>", "people": ["lieber@media.mit.edu"], "title": "Common-Sense Investing", "modified": "2016-12-05T00:16:19.530Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "common-sense-investing"}, {"website": "", "description": "<p>The Jukebox understands its listeners and attempts to select music based on past performances. A user model is created by looking at a history of responses. A positive response is generated by dropping a coin into the jukebox, and a negative response can be given by a kick.</p>", "people": [], "title": "Contextual Jukebox", "modified": "2016-12-05T00:16:19.554Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2002-01-01", "slug": "contextual-jukebox"}, {"website": "", "description": "<p>Understanding spatial language is a challenging problem that requires the ability to map between language and real-world situations. We are building a spatial language understanding system that bridges this representational gap by computationally modeling the semantics of spatial prepositions. Our model enables a system to retrieve video clips that match natural language queries such as \"Show me people going across the kitchen.\" We are also applying it to build robots that can follow natural-language directions such as \"Go through the door near the elevators.\" By using corpus-based machine learning techniques, our model is robust to real-world noise and linguistic variation. Exploring the connection between language and the real world in concrete domains enables us to make progress towards computers that understand language in human-like ways.</p>", "people": ["dkroy@media.mit.edu"], "title": "Grounding Spatial Language for Video Retrieval and Robotic Direction Following", "modified": "2016-12-05T00:17:13.561Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-441", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "grounding-spatial-language-for-video-retrieval-and-robotic-direction-following"}, {"website": "", "description": "<p>The meanings of simple verbs such as \"pick up\" convey more information than meets the eye. In order to pick up an object, a person much reach towards it, touch it, grasp it, and then lift it. To know what \"pick up\" means is to know all of these things. From this kind of word meaning comes common-sense knowledge such as \"you can't pick something up without touching it.\" Machines today lack this depth of understanding of verbs. In this project, we are exploring new ways to represent the semantics of verbs so that machines can process and understand sensory-grounded meanings of natural language in human-like ways. Our basic approach is to connect verbs to non-linguistic representations based on the sensory and motor systems of physical robots. Using Ripley, our 7-degree-of-freedom manipulator robot, we have designed a system that learns to recognize gestures and the relations between gestures. Data is recorded by allowing a human operator to move the compliant robot through motions such as \"pick up\" and \"move toward.\" A Hidden Markov Model learning algorithm is then trained on the data generated by these gestures, which results in a structured sequential representation of each motion. Each sequential component can then be related to components of other gestures, thus enabling the system to acquire such relations as \"[pick up] is composed of [move toward], [close gripper], and [retract].\" We are thus able to derive relations between word meanings from relations of underlying sensory-motor structures. We believe that this kind of connection between language and non-linguistic knowledge is an essential step toward intelligent language processing and understanding by machines.</p>", "people": ["dkroy@media.mit.edu"], "title": "Grounding the Meaning of Verbs through Structured Motor Control Representations", "modified": "2016-12-05T00:17:13.609Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "grounding-the-meaning-of-verbs-through-structured-motor-control-representations"}, {"website": "", "description": "<p>We explore secure, cooperative storage mechanisms in peer-to-peer networks, where a group of peer nodes form a contributory storage infrastructure. In this collective storage platform, each peer node can access a large pool of information by only contributing a small portion of the total required storage. The basic idea is that we fragment a given data-object into several segments and store them in the peer nodes based on DHT (distributed hash table). Each segment is associated with a partial key, and the master key can be retrieved if the number of assembled partial keys reaches a given threshold. In practice, this means one can enlist one's peer group to help with one's storage needs, and vice versa; it is also secure even when one loses one's password.</p>", "people": ["lip@media.mit.edu"], "title": "Community Storage", "modified": "2016-12-05T00:16:19.629Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "community-storage"}, {"website": "", "description": "<p>What you own can sometimes tell what you need and how you feel. We have built a complete portable inventory of the possessions of one person and we are making that available in place and at the moment. Our system extracts affect information from our context (what we're wearing, what we're doing, where we're doing it), and matches it with a music playlist to provide a soundtrack, which makes it easy to enjoy and discover music that fits your lifestyle. It can also inform our financial interactions with the world and serve as guidance for our community.</p>", "people": ["lip@media.mit.edu"], "title": "CoMo", "modified": "2016-12-05T00:16:19.658Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-491", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "como"}, {"website": "", "description": "<p>Can we recognize stress, mood, and health conditions from wearable sensors and mobile-phone usage data? We analyze long-term, multi-modal physiological, behavioral, and social data (electrodermal activity, skin temperature, accelerometer, phone usage, social network patterns) in daily lives with wearable sensors and mobile phones to extract bio-markers related to health conditions, interpret inter-individual differences, and develop systems to keep people healthy.</p>", "people": ["picard@media.mit.edu", "akanes@media.mit.edu"], "title": "Mobisensus: Predicting Your Stress/Mood from Mobile Sensor Data", "modified": "2016-12-05T00:17:17.965Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["affective-computing", "advancing-wellbeing"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "mobisensus-predicting-your-stressmood-from-mobile-sensor-data"}, {"website": "", "description": "", "people": [], "title": "Programmable Body Extensions", "modified": "2017-06-05T22:52:20.111Z", "visibility": "PUBLIC", "start_on": "2017-04-01", "location": "", "groups": ["lifelong-kindergarten"], "published": false, "active": false, "end_on": "2017-09-01", "slug": "programmable-body-extensions"}, {"website": "", "description": "<p>A socially intelligent communication agent that assists the user in her telecommunication tasks needs \"communication intelligence.\" One way to acquire such intelligence is to rely on the human social intelligence of co-located people to decide on whether, when, and how to alert a user of an incoming communication. As a prerequisite, the agent needs to know who is part of an ongoing conversation; a network of small, body-worn, wireless sensors helps the agent to determine this. A novel, fully distributed decision-making process is used to detect conversations; these nodes have binary speech detectors and low-range radio transceivers, and communicate asynchronously with each other on a single channel. They independently come to a conclusion about who is part of the user's current conversation by looking at alignment and non-alignment of the speaking parties, and can be queried wirelessly for this list of people.</p>", "people": ["geek@media.mit.edu", "stefanm@media.mit.edu"], "title": "Conversation Finder for Socially Aware Communication", "modified": "2016-12-05T00:16:19.728Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "conversation-finder-for-socially-aware-communication"}, {"website": "", "description": "", "people": ["lajv@media.mit.edu", "djfitz@media.mit.edu", "ishii@media.mit.edu"], "title": "inFORCE 1.0", "modified": "2017-03-31T00:09:40.562Z", "visibility": "LAB-INSIDERS", "start_on": "2015-09-25", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2017-05-01", "slug": "inforce"}, {"website": "", "description": "<p>Walter's Helmet is a bike helmet project. The project seeks to establish mediated information with the minimum of user distraction. As a hands-free interface, the helmet will listen to the audio, imagery, and motion of the environment. The bicyclist can make phone calls or give dictation, or listen to music or other information. The helmet attempts to provide the user with the most information and the least interruptions. It uses a horn, and front, back, and side lighting for visibility and signaling. The helmet is designed to intrude slightly with unusual external information; for example, it will tell you when a car is approaching.</p>", "people": [], "title": "Walter's Helmet", "modified": "2016-12-05T00:16:19.831Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "walters-helmet"}, {"website": "", "description": "<p>CONVIVO Communicator is a synchronous voice over IP (VoIP) communication system designed to respond to poor or varying network performance. CONVIVO adapts to a network's performance both by configuring in real-time voice sampling parameters as well as by provisioning a variety of innovative user interfaces. User interface modalities facilitate turn-taking given a very high latency connection and basic messaging given an extremely low bandwidth connection. CONVIVO is designed to work given the network realities of many rural communities in the developing world.</p>", "people": [], "title": "CONVIVO Communicator", "modified": "2016-12-05T00:16:19.856Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-391", "groups": ["edevelopment"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "convivo-communicator"}, {"website": "", "description": "<p>CopyCAD is an augmented milling machine that allows users to design new objects directly on the material they wish to cut using projected feedback. CopyCAD empowers users to become creators by allowing them to copy geometry from existing objects, directly and tangibly, and remix it by adding geometry from other objects, or by sketching new geometry. CopyCAD attempts to bring remix culture to the design of objects.</p>", "people": ["ishii@media.mit.edu"], "title": "CopyCAD", "modified": "2016-12-05T00:16:19.907Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "copycad"}, {"website": "", "description": "<p>\"Logo Blocks\" met the challenge of creating a  programming language for kids that is both powerful and easy to use. Simply put, Logo Blocks puts the Logo code into graphical blocks that may be dragged around the screen and \"clicked\" together. A palette of blocks shows the user what commands are available, and users may choose different options (e.g., which motor port to turn on) simply by clicking the \"ON\" block.</p>", "people": ["mres@media.mit.edu"], "title": "Logo Blocks", "modified": "2016-12-05T00:16:19.942Z", "visibility": "PUBLIC", "start_on": "1995-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "1999-09-01", "slug": "logo-blocks"}, {"website": "", "description": "<p>Pulp-Based Computing is a series of explorations that combine smart materials, papermaking, and printing. By integrating electrically active inks and fibers during the papermaking process, it is possible to create sensors and actuators that behave, look, and feel like paper. These composite materials not only leverage the physical and tactile qualities of paper, but can also convey digital information, spawning new and unexpected application domains in ubiquitous and pervasive computing at extremely affordable costs.</p>", "people": ["marcelo@media.mit.edu", "pattie@media.mit.edu"], "title": "Pulp-Based Computing: A Framework for Building Computers Out of Paper", "modified": "2016-12-05T00:17:21.207Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "pulp-based-computing-a-framework-for-building-computers-out-of-paper"}, {"website": "", "description": "<p>The LogoChip is a simple-to-use microcontroller that runs a special version of the Logo programming language. It serves as a versatile building block for constructing a wide range of electronic projects. Playing with LogoChips is a great way for people to get started learning about electronics; with LogoChips you can easily build your own customized version of the Cricket or new layers for the Tower system.</p>", "people": ["bss@media.mit.edu"], "title": "LogoChip", "modified": "2016-12-05T00:16:20.003Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["personal-fabrication"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "logochip"}, {"website": "", "description": "<p>CoSync builds the ability to create and act jointly into mobile devices. This mirrors the way we as a society act both individually and in concert. CoSync device ecology combines multiple stand-alone devices and controls them opportunistically as if they are one distributed, or diffuse, device at the user\ufffds fingertips. CoSync includes a programming interface that allows time-synchronized coordination at a granularity that will permit watching a movie on one device and hearing the sound from another. The open API encourages an ever-growing set of such finely coordinated applications.</p>", "people": ["holtzman@media.mit.edu", "lip@media.mit.edu"], "title": "CoSync", "modified": "2016-12-05T00:16:20.101Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["viral-communications", "information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "cosync"}, {"website": "", "description": "<p>Coterie is an emerging series of experiments in the dynamic visualizations of online communities. Coterie reintroduces a human element to an abstract data source by creating visual representations that evoke the underlying activity and interactions among the members of an online community. The first experiment in this series portrays individuals as ovals, using a bouncing motion to show current individual activity. A person's saturation and opacity reflect their participation, over time, in the group.</p>", "people": ["judith@media.mit.edu"], "title": "Coterie", "modified": "2016-12-05T00:16:20.138Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-450", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "coterie"}, {"website": "", "description": "<p>Imagine a 3-D animated character who can pass multiple toys back and forth through the seemingly immutable boundary of a projection screen, and thus tell stories and share experiences together with a child. Bringing together our work in story-listening systems for children and our work on embodied conversational agents, we continue to develop a conversational character, Sam, who can act as a peer playmate to children and can tell stories with them by sharing physical objects across the real and virtual worlds, and by engaging in turn-taking story behaviors. The character and child share a play space and a set of story-evoking toys that can magically exist in both participants' worlds. The background of Sam is the real-time video of the child's environment, so that Sam actually exists in the child's play space. Recent testing of the system shows the extent to which this toy engages children in creative and collaborative full-body play.</p>", "people": [], "title": "Sam: An Embodied Conversational Storyteller for Children", "modified": "2016-12-05T00:17:22.590Z", "visibility": "LAB", "start_on": "1999-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "sam-an-embodied-conversational-storyteller-for-children"}, {"website": "", "description": "<p>What is the relationship between the input children hear and the words they acquire? We are investigating the role of variables such as input word frequency and prosody in one child's lexical acquisition using the Human Speechome Project corpus. We are analyzing data from ages nine to 24 months, including the child's first productive use of language at about 11 months, ending at the child\ufffds active use of a vocabulary with more than 500 words.</p>", "people": ["dkroy@media.mit.edu", "soroush@media.mit.edu"], "title": "Study of Child Language Acquisition in the Human Speechome Project ", "modified": "2016-12-05T00:17:22.945Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "study-of-child-language-acquisition-in-the-human-speechome-project"}, {"website": "", "description": "<p>How can the kitchen of the future help you instead of replace you?  That's the idea behind the CounterActive recipe counter.  It may look like an ordinary Formica kitchen surface, but this counter has been augmented with a overhead projection system and a field sensing array under the counter.  With it, we provide a new medium for teaching people how to cook. The project as a whole explores how moving the computer to the kitchen fundamentally changes the style of interaction.</p>", "people": ["mike@media.mit.edu"], "title": "CounterActive", "modified": "2016-12-05T00:16:20.172Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-068", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "counteractive"}, {"website": "", "description": "<p>Crickets are small programmable devices that can make things spin, light up, and play music. With Crickets, kids can create musical sculptures, interactive jewelry, dancing creatures, and other artistic inventions\ufffdand learn important math, science, and engineering ideas in the process. Lifelong Kindergarten researchers previously collaborated with LEGO on the development of the LEGO MindStorms robotics kits, now used by millions of people around the world. Crickets grow out of this same tradition, but with greater emphasis on artistic expression. Crickets are now sold as a product through the Playful Invention Company (www.picocricket.com).</p>", "people": ["mres@media.mit.edu", "bss@media.mit.edu", "nrusk@media.mit.edu"], "title": "Crickets", "modified": "2016-12-05T00:16:20.268Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "crickets"}, {"website": "", "description": "<p>The cry detector running on the iPAQ under Linux is an implementation of the baby monitor, one of the applications announced in the Impromptu project. Unlike the other Impromptu applications, the baby monitor doesn't need the bandwidth for sound transmission all the time. It opens a bi-directional audio channel only when the baby starts crying. One can think of detecting different types of interesting sound patterns. In many cases, the detection requires few computational resources. Simple sound-pattern detectors of this kind are highly scalable and can be used in a variety of applications.</p>", "people": ["geek@media.mit.edu"], "title": "CryBaby", "modified": "2016-12-05T00:16:20.290Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "crybaby"}, {"website": "", "description": "<p>Using the Eye Society network of robotic cameras, this project seeks to capture three-dimensional representations of static interior spaces for use with holographic displays. The project directs robotic cameras to gather images needed to generate novel 2-D renderings of the scene, directing cameras to move so that they can look behind objects that might be obstructing parts of their view.</p>", "people": ["vmb@media.mit.edu"], "title": "View Planning for Camera Networks", "modified": "2016-12-05T00:17:25.352Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-368", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "view-planning-for-camera-networks"}, {"website": "", "description": "<p>CrossTalk is an interactive keyboard designed to allow two or more people to play with language and communication in a social setting. It was designed as part of the emergent Literary Salon project, in which cafe tables function both as traditional tables and as ways to communicate with  other people in the same physical space, enhancing and enabling face-to-face social interactions, rather than detracting from them. A primary goal of this project is to encourage people to play with language; another goal is to create a feeling of community within the cafe.</p>", "people": [], "title": "Crosstalk", "modified": "2016-12-05T00:16:20.380Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "crosstalk"}, {"website": "", "description": "<p>The city of the future will provide a high degree of connectivity to its inhabitants. Targeting the fast-paced urban traffic environment, the smart meter revolutionizes curbside parking by integrating digital network technologies with existing parking meter infrastructure. Sensors integrated into existing parking meters detect occupancy; parking meters talk with each other in a multihope scheme; drivers will be able to \ufffdlocate\ufffd and \ufffdreserve\ufffd vacant parking spaces; and the parking space can email you or post a vacancy on a Web page, reducing traffic and saving time and fuel. These new meter functions will lead to new driver interfaces. This project re-thinks the relationship between the users of transit systems (mass-transit and private vehicles) and their surrounding context. A system of communication nodes throughout the urban landscape and within the car forms our vision for a networked city. (Based on previous work by Jeffrey Tsui, Joseph Ho, and Edwin Lau.)</p>", "people": ["rchin@media.mit.edu"], "title": "Curbside Sensor", "modified": "2016-12-05T00:16:20.352Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "Cube", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "curbside-sensor"}, {"website": "", "description": "<p>This project illustrates the shared spaces and resources between urban wildlife and citizens. It presents interactive tools to enhance human knowledge of our urban wildlife surroundings, as well as methods for mapping ambient devices to outdoor urban-wildlife-generated events.</p>", "people": ["labrune@media.mit.edu"], "title": "WildUrban: Shared Spaces Between Wildlife and Citizens ", "modified": "2016-12-05T00:17:26.507Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "Upper Atrium", "groups": [], "published": true, "active": false, "end_on": "2009-01-01", "slug": "wildurban-shared-spaces-between-wildlife-and-citizens"}, {"website": "", "description": "<p>In robotics, the emerging field of electronic textiles and fiber-electronics represents a shift in morphology from hard and rigid mechatronic components toward a soft-architecture\ufffdand more specifically, a flexible planar surface morphology. It is thus essential to determine how a robotic system might actuate flexible surfaces for donning and doffing actions. Zipperbot is a robotic continuous closure system for joining fabrics and textiles. By augmenting traditional apparel closure techniques and hardware with robotic attributes, we can incorporate these into robotic systems for surface manipulation. Through actuating closures, textiles could shape-shift or self-assemble into a variety of forms.</p>", "people": ["cynthiab@media.mit.edu"], "title": "Zipperbot: Robotic Continuous Closure for Fabric Edge Joining", "modified": "2016-12-05T00:17:26.789Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "zipperbot-robotic-continuous-closure-for-fabric-edge-joining"}, {"website": "", "description": "<p>The CurlyCart is a modification of a Power Wheels toy that can record and play back your motions.</p>", "people": ["judith@media.mit.edu", "mike@media.mit.edu"], "title": "CurlyCart", "modified": "2016-12-05T00:16:20.403Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-450", "groups": ["sociable-media", "cc"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "curlycart"}, {"website": "", "description": "<p>Curtains blends the aesthetics of everyday home design with sensing technology. Whereas today a sensor commonly augments a device, with Curtains, the sensing is inherent in the nature of the material. The metallic threads woven into the fabric allow for the sensing of touch and proximity. The curtain then becomes both sensor and ornament.\n</p>", "people": ["judith@media.mit.edu"], "title": "Curtains", "modified": "2016-12-05T00:16:20.429Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-449", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "curtains"}, {"website": "", "description": "<p>curlybot is an autonomous two-wheeled toy that can record and play back physical motion. As one plays with it, it remembers its change in position and can replay its movement forward or backward with all the intricacies of the original gesture. Every pause, and even the shaking in the user's hand, is recorded.</p>", "people": ["ishii@media.mit.edu"], "title": "curlybot", "modified": "2016-12-05T00:16:20.494Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "curlybot"}, {"website": "", "description": "<p>When we analyzed self-reported relationship surveys from several experiments around the world (from human subjects, not hobbits!), we found that while most people assume friendships to be two-way, only about half of friendships are indeed reciprocal. In itself this may seem like an interesting but minor finding, but this large proportion of asymmetric friendships translates to a major effect on the ability of individuals to persuade others to cooperate or change their behavior.</p><p>For example, when we examined the properties of friendship networks and how the directionality of ties can impact the level of influence that individuals exert on one another (based on analysis of a fitness and physical activity intervention where information about physical activity was collected passively by smartphones), we found that the program was more effective when a unilateral friendship tie existed from the buddy (the person applying peer pressure) to the subject (the person receiving the pressure) than when the friendship tie was from the subject to the buddy. In this example, reciprocal friendships are best, but having a buddy who thinks of the subject as a friend is the next best relationship. We attribute the difference to our peer-to-peer incentive mechanism\u2014as buddies were rewarded based on the progress of the subject, there are likely to be differences in communication when the buddy believes the subject to be their friend versus when they do not.</p><p>The findings of this work have significant consequences for designing interventions that seek to harness social influence:</p><p>Intervention designers, whether with fitness programs, smoking cessation programs, or any other attempt to change a subject\u2019s behavior, can't rely on how the subject perceives the relationship with the buddy to create effectiveness.</p><p>Also, we shouldn\u2019t assume people with a high number of social ties are \u201cinfluencers.\u201d Such people are no better and often are worse than average at exerting social influence. Our results suggest that this is because many of those ties either are not reciprocal or go in the wrong direction, and therefore won\u2019t lead to effective persuasion.</p><p>We demonstrate that an assumption common in previous studies of social influence, namely that friendships are created equal or reciprocal by default, is erroneous, which may have significantly biased the research results.</p><p>We hope that by understanding the factors and network properties that impact the level of social influence individuals exert on one another, we can be more effective at promoting behavioral change, disseminating new ideas, and even promoting products.</p>", "people": ["amaatouq@media.mit.edu", "sandy@media.mit.edu"], "title": "Friendship Reciprocity and Behavioral Change", "modified": "2016-12-05T00:17:27.755Z", "visibility": "PUBLIC", "start_on": "2015-05-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": "2016-08-18", "slug": "friendship-reciprocity-behavioral-change"}, {"website": "", "description": "<p>This project was a weekend exploration of gyroscopic stabilization with application to vehicle control and user interface.  Using the well known inverted pendulum drive system, a unicycle scooter was made from low-cost components. It's like a Segway, but more dangerous/fun!</p>", "people": ["neri@media.mit.edu"], "title": "Cyclopscooter", "modified": "2016-12-05T00:16:20.573Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "cyclopscooter"}, {"website": "", "description": "<p>Dance is an expressive activity that combines music and movement. We are interested in both encouraging people to dance, and in measuring the emotional experience of musical expression through movement. The Dance Remixer is a program that transforms any piece of music into something that people can dance to. The program remixes music to add customizable rhythmic elements\ufffdfor instance, those typical to Latin dance music. The Dance Remixer is a first step toward personalizing how people interact with music, by giving the user the ability to modify its function and emotional content. We are also interested in quantitatively measuring the emotional experience of dancing. What makes dancing enjoyable? How is our enjoyment of music reflected in dance? Specifically, we examine the psychological factors behind Argentine Tango, an improvisational dance that prioritizes the interpretation of music.</p>", "people": ["tod@media.mit.edu"], "title": "Dance, Emotion, and Expression", "modified": "2016-12-05T00:16:20.603Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "dance-emotion-and-expression"}, {"website": "", "description": "<p>Cristiano Ronaldo can famously volley a corner kick in total darkness. The magic behind this remarkable feat is hidden in Ronaldo's brain, which enables him to use advance cues to plan upcoming actions. Darkball challenges your brain to do the same, distilling that scenario into its simplest form\ufffdintercept a ball in the dark. All you see is all you need.</p>", "people": ["slavin@media.mit.edu", "cwwang@media.mit.edu"], "title": "Darkball", "modified": "2016-12-05T00:16:20.623Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": "2015-07-01", "slug": "darkball"}, {"website": "", "description": "<p>Providing people with information about their own physical state has been used for a long time under the general term \"bio-feedback.\" In this project, we are trying to expand the general ideas of bio-feedback to include the monitoring of day-to-day health information such as diets, exercise, and stress. We are particularly interested in compliance, and in health improvements as a function of both the hardware used (personal computers vs. PDAs) and the type of information and feedback that is given back to the users.</p>", "people": [], "title": "Day-to-Day Monitoring for e-Health", "modified": "2016-12-05T00:16:20.712Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "--Choose Location", "groups": ["erationality"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "day-to-day-monitoring-for-e-health"}, {"website": "", "description": "<p>People express and communicate their mental states\u2013such as emotions, thoughts, and desires\u2013through facial expressions, vocal nuances, gestures, and other non-verbal channels. We have developed a computational model that enables real-time analysis, tagging, and inference of cognitive-affective mental states from facial video. This framework combines bottom-up, vision-based processing of the face (e.g., a head nod or smile) with top-down predictions of mental-state models (e.g., interest and confusion) to interpret the meaning underlying head and facial signals over time. Our system tags facial expressions, head gestures, and affective-cognitive states at multiple spatial and temporal granularities in real time and offline, in both natural human-human and human-computer interaction contexts. A version of this system is being made available commercially by Media Lab spin-off Affectiva, indexing emotion from faces. Applications range from measuring people's experiences to a training tool for autism spectrum disorders and people who are nonverbal learning disabled.</p>", "people": ["mgoodwin@media.mit.edu", "micahrye@media.mit.edu", "picard@media.mit.edu", "mehoque@media.mit.edu"], "title": "FaceSense: Affective-Cognitive State Inference from Facial Video", "modified": "2018-04-04T19:58:17.407Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "facesense-affective-cognitive-state-inference-from-facial-video"}, {"website": "", "description": "<p>The Department of Play (DoP) is a working group of researchers, students, and community practitioners who share a common passion: designing appropriate technology and methods to empower youth and their communities. In particular, the Department of Play initiative aims to develop an easy-to-use, open-source digital toolkit to foster youth participation, social inclusion and local civic engagement. Among other things, we are implementing a multi-channel neighborhood communication system that combines email, SMS and regular touchtone phones to help young people organize and promote block parties, games, performances and other events in the places where they live. </p>", "people": ["ethanz@media.mit.edu", "leob@media.mit.edu"], "title": "Department of Play", "modified": "2016-12-05T00:16:20.806Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "department-of-play"}, {"website": "", "description": "<p>deFORM is a deformable input device that supports 2.5D touch gestures, tangible tools, and arbitrary objects through real-time structured light scanning of a malleable surface of interaction. deFORM captures high-resolution surface deformations and 2D grey-scale textures of a gel surface through a three-phase structured light 3D scanner. This technique can be combined with IR projection to allow for invisible capture, providing the opportunity for co-located visual feedback on the deformable surface</p>", "people": ["ishii@media.mit.edu"], "title": "deFORM", "modified": "2016-12-05T00:16:20.867Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "deform"}, {"website": "", "description": "<p>DesignBlocks is a derivative of the Scratch project that focuses on 2-dimensional digital design. With DesignBlocks, artists control lines, shapes, colors and images to create generative and interactive artworks.  DesignBlocks uses the same visual grammar as Scratch, but uses a vocabulary more suited for graphic design. Inspired by Processing, DesignBlocks aims to make programming more accessible and suited to artists.</p>", "people": ["mres@media.mit.edu", "ericr@media.mit.edu", "bss@media.mit.edu"], "title": "DesignBlocks", "modified": "2016-12-05T00:16:20.967Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "designblocks"}, {"website": "", "description": "<p>We are at last approaching a time when low-cost computers will make it technically feasible to provide real education to ALL the children of the world. It will be absurd if content and pedgagogy copies the past. This project aims to mobilize people, ideas, and resources to develop a radical new methodology and content. </p>", "people": ["cavallo@media.mit.edu", "papert@media.mit.edu"], "title": "Design for Global Education", "modified": "2016-12-05T00:16:20.928Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "--Choose Location", "groups": ["future-of-learning"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "design-for-global-education"}, {"website": "", "description": "<p>This project explores software architectures and user interfaces to voice as a computer data-type, as well as a command channel. Its goal is to make speech ubiquitous to a range of applications (for instance, editing a telephone message to include annotation of a text document). Related issues include object-oriented manipulation of multiple media \"selection\" (or \"clipboard\") data between processes, and a client-server architecture allowing multiple applications to share audio resources.</p>", "people": ["geek@media.mit.edu"], "title": "Desktop Audio", "modified": "2016-12-05T00:16:20.994Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "desktop-audio"}, {"website": "", "description": "<p>Dialup Radio is an audiocentric, mobile-phone-based independent media service. It has been designed and tested with activists in sub-Saharan Africa, and is expected to be deployed in several locations in 2008.</p>", "people": [], "title": "Dialup Radio", "modified": "2016-12-05T00:16:21.101Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "dialup-radio"}, {"website": "", "description": "<p>This project aims to reliably spot digit strings in voicemail messages. This can be very useful since it allows easy access to telephone numbers that have been left within a long message, eliminating the need to listen to the message in its entirety. The project involves running speech recognition in dictation mode on incoming voicemail messages, and further text processing to determine the boundaries of digit strings. This project can plug into voicemail systems seamlessly.</p>", "people": ["geek@media.mit.edu"], "title": "Digit Spotter", "modified": "2016-12-05T00:16:21.125Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "digit-spotter"}, {"website": "", "description": "<p>Low-resolution printing has many applications in the countries of the developing world. Just as the onset of digital desktop printers gave rise to a revolution in desktop publishing, we are concieving similar \"tools for tomorrow\" for handicraft workers in parts of rural India. Since the devices need to be locally fabricated and affordable, we are exploiting open-source hardware and software tool kits for the same.</p>", "people": ["neilg@media.mit.edu"], "title": "Digital Stamp", "modified": "2016-12-05T00:16:21.249Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "digital-stamp"}, {"website": "", "description": "<p>This project explores ways to use images as qualitative data for health maintenance/improvement. Currently, many health patients (such as diabetics) use handheld devices to get, for example, quantitative readings about blood sugar or heart rates. We are developing imaging tools to help people photograph their daily behaviors, and use these images to interpret these quantitative data. The goal is to use imagery as another diagnostic tool for patients and their physicians.</p>", "people": [], "title": "Digital Mirror", "modified": "2016-12-05T00:16:21.299Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-309", "groups": ["erationality"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "digital-mirror-3"}, {"website": "", "description": "<p>Perspectives was an interactive-art construction kit, designed specifically to help high-school students explore and document the different individual, social, and cultural perspectives that make up human experience. Students  created a new kind of interactive documentary, programming sensor input to control multiple video streams\ufffdand, in the process, explored multiple points-of-view of a single entity.</p>", "people": ["mres@media.mit.edu"], "title": "Perspectives LLK", "modified": "2016-12-05T00:16:21.389Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-120B", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "perspectives-llk"}, {"website": "", "description": "<p>The Digital Seed is a microworld for learning about plant growth, life cycles, and the origin of seeds. The Digital Seed is a virtual alter-ego of a real seed, he lives in a cube. To grow up he needs help from outside, the user must take care of the seed: watering the cube, keeping the cube the right temperature, and exposing it to the right amount of light. The physical actions on the cube will affect the inner virtual world where the seed lives and grows. The user must care for the seed throughout its life cycle until the end, with the birth of a new seed.</p>", "people": [], "title": "DigitalSeed", "modified": "2016-12-05T00:16:21.415Z", "visibility": "PUBLIC", "start_on": "2002-09-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "digitalseed"}, {"website": "", "description": "<p>With The Diorama Project we are putting virtual objects in real world  spaces. Our goal is to create a system that allows people to build an  imaginative parallel universe superimposed on their everyday space.  The  research challenges are technical (precise location sensing), perceptual  (creating a coherent hybrid space) and social (developing new interface  metaphors for physically located virtual objects). \n</p>", "people": ["judith@media.mit.edu"], "title": "Diorama", "modified": "2016-12-05T00:16:21.439Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "diorama"}, {"website": "", "description": "<p>A human disruption model is used to develop a disruption manager that mediates instant message interruptions as people navigate the Web. The system is designed to reduce disruption and increase overall user satisfaction. The disruption manager is based on research investigating people\ufffds reactions to interruptions and factors involved in the interruption process, such as interruption relevancy and task priority. The manager supports monitoring ongoing behaviors using implicit metrics to control possible disruptive outcomes given the user and system state (mouse and keyboard behaviors, interruption type, task request or notification, concepts surrounding the user\ufffds goals, interruption relevancy, and concept priority). The disruption manager selects appropriate interruption timing and selects whether or not instant message interruptions should be presented to the user.</p>", "people": [], "title": "Disruption Manager", "modified": "2016-12-05T00:16:21.501Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "disruption-manager"}, {"website": "", "description": "<p>Disruption is an important issue in the design of self-adaptive interfaces. This project attempts to make a computer intelligent enough to select the appropriate sensorial modality. This project investigates which modality is the most efficient and at the same time, the least disruptive. Two interruption modalities are studied: heat and light. This research adds to previous research by showing that there is an effect on performance caused by interruption modalities: for example, thermal modality produced a larger decrease in performance than visual modality; this modality has a greater disruptive effect on interrupted tasks than light. Disruptiveness and performance measures agree that heat causes more of a detrimental effect than light when used as an interruption.</p>", "people": [], "title": "Disruptive Interruptions", "modified": "2016-12-05T00:16:21.525Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "disruptive-interruptions"}, {"website": "", "description": "<p>Almost any communications system makes best use of scarce bandwidth by making the messages sent appear to be as random as possible. This is optimally done with spread-spectrum techniques based on Linear Feedback Shift Registers; this entails a significant amount of processing for a receiver to acquire and track a signal. We have found that simple physical systems can be designed that perform these functions through their dynamics, promising to make spread-spectrum communications significantly cheaper and faster.</p>", "people": ["neilg@media.mit.edu"], "title": "Dissipative Communications", "modified": "2016-12-05T00:16:21.549Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-023", "groups": ["physics-and-media", "toys-of-tomorrow", "personal-fabrication"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "dissipative-communications"}, {"website": "", "description": "<p>An exploration into the possibilities for individual construction and customization of the most ubiquitous of electronic devices, the cellphone. By creating and sharing open-source designs for the phone's circuit board and case, we hope to encourage a proliferation of personalized and diverse mobile phones. Freed from the constraints of mass production, we plan to explore diverse materials, shapes, and functions. We hope that the project will help us explore and expand the limits of do-it-yourself (DIY) practice. How close can a homemade project come to the design of a cutting-edge device? What are the economics of building a high-tech device in small quantities? Which parts are even available to individual consumers? What's required for people to customize and build their own devices?</p>", "people": ["mellis@media.mit.edu", "leah@media.mit.edu", "mres@media.mit.edu"], "title": "DIY Cellphone", "modified": "2016-12-05T00:16:21.623Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "diy-cellphone"}, {"website": "", "description": "<p>Using digital fabrication and embedded computation to allow individuals to make their own devices. This effort started by creating open-source DIY versions of common devices (speakers, radios, mice, and cellphones) each combining a custom electronic circuit board and digitally-fabricated enclosure. The current focus is on creating devices with unique functionality, aesthetics, or production processes. One early prototype is of a special-purpose internet-connected device, whose behavior can be customized by the person creating it. Another experiment explores the possibilities of automated circuit board assembly services and their implications for open-source hardware. Most importantly, we're beginning to develop resources to enable others to design and build custom devices through meaningful and educational creative processes. These efforts are still in an early stage, but we're interested in finding ways to transition from reproducing existing devices to helping people create a diverse set of new ones.</p>", "people": ["mellis@media.mit.edu", "mres@media.mit.edu"], "title": "DIY Devices", "modified": "2016-12-05T00:16:21.735Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "diy-devices"}, {"website": "", "description": "<p>We are surrounded by digital information that we can't see or touch. &nbsp;The most common computer interfaces, 2D screens, are devoid of any inherent spatial context. Virtual reality lets us immerse ourselves visually in digital worlds, but requires cumbersome head-mounted displays. Meanwhile, regaining a sense of touch in VR with current haptic feedback systems is even more obtrusive and disappointing. Worse, we can only experience these virtual worlds to the exclusion of the real world, cutting ourselves off from real objects and people around us. &nbsp;</p><p>ReVeal is a mobile tangible interface that serves as an unobtrusive intermediary between coincident virtual and real physical worlds. Operating within a room-scale virtual reality system, the shape display takes the form of virtual objects \"under\" it, so the same objects can exist in the same place at the same time in both the real and virtual worlds. A handheld projector, tracked by a virtual camera, lets users reveal the virtual objects, like shining a flashlight to illuminate the invisible.&nbsp;</p>", "people": ["djfitz@media.mit.edu"], "title": "ReVeal", "modified": "2017-05-02T05:52:32.865Z", "visibility": "LAB", "start_on": "2016-09-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2017-04-08", "slug": "ReVeal"}, {"website": "", "description": "<p>In Family Creative Learning, we engage parents and children in workshops to design and learn together with creative technologies, like the Scratch programming language and the MaKey MaKey invention kit. Just as children's literacy can be supported by parents reading with them, children's creativity can be supported by parents creating with them. In these workshops, we especially target families with limited access to resources and social support around technology. By promoting participation across generations, these workshops engage parents in supporting their children in becoming creators and full participants in today's digital society.</p>", "people": ["ria@media.mit.edu", "mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "Family Creative Learning", "modified": "2018-06-20T19:53:55.066Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten", "ml-learning"], "published": true, "active": false, "end_on": "2016-08-31", "slug": "family-creative-learning"}, {"website": "", "description": "<p>Dollars & Scents explores ways to display abstract information using scent. A device is installed above the lower-lobby rotating door that periodically outputs scent depending on the state of the NASDAQ index. The smell of mint indicates that the market has gone up since the day's opening, and the smell of lemon indicates it has gone down. While these scents have convenient verbal mnemonics (you're either \"making money\" or have \"picked lemons\"), they are abstract smell icons, or \"smicons,\" an olfactory equivelant of the \"earcons\" that have been developed in the field of auditory research.</p>", "people": ["mike@media.mit.edu"], "title": "Dollars & Scents", "modified": "2016-12-05T00:16:21.816Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-068", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "dollars-scents"}, {"website": "", "description": "<p>LuminAR reinvents the traditional incandescent bulb and desk lamp, evolving them into a new category of robotic, digital information devices. The LuminAR Bulb combines a Pico-projector, camera, and wireless computer in a compact form factor. This self-contained system enables users with just-in-time projected information and a gestural user interface, and it can be screwed into standard light fixtures everywhere. The LuminAR Lamp is an articulated robotic arm, designed to interface with the LuminAR Bulb. Both LuminAR form factors dynamically augment their environments with media and information, while seamlessly connecting with laptops, mobile phones, and other electronic devices. LuminAR transforms surfaces and objects into interactive spaces that blend digital media and information with the physical space. The project radically rethinks the design of traditional lighting objects, and explores how we can endow them with novel augmented-reality interfaces.</p>", "people": ["pattie@media.mit.edu", "kubat@media.mit.edu", "linder@media.mit.edu"], "title": "LuminAR", "modified": "2018-10-11T18:29:50.008Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2013-09-30", "slug": "luminar"}, {"website": "", "description": "<p>The \"Nominal Group Technique\" is a popular way to brainstorm, often executed with Post-it notes and voting stickers. We're reimagining and reimplementing this technique for online use, for things such as hackathons, design workshops, and brainstorms across multiple geographies. The best part: everyone can take the results of the brainstorm with them, and embed it in blogs or websites.</p>", "people": ["geek@media.mit.edu"], "title": "Dotstorm", "modified": "2016-12-05T00:16:21.859Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "dotstorm"}, {"website": "", "description": "<p>The Foodome addresses how to create deeper understanding and predictive intelligence about the relationships between how we talk and learn about food, and what we actually eat. Our aim is to build a food learning machine that comprehensively maps, for any given food, its form, function, production, distribution, marketing, science, policy, history, and culture (as well as the connections among all of these aspects). We are gathering and organizing a wide variety of data, including news/social content, recipes and menus, and sourcing and purchase information. We then use human-machine learning to uncover patterns within and among the heterogeneous food-related data. Long term, the Foodome is meant to help improve our understanding of, access to, and trust in food that is good for us; find new connections between food and health; and even predict impacts of local and global events on food.</p>", "people": ["dkroy@media.mit.edu", "lukeglw@media.mit.edu", "mmv@media.mit.edu", "soroush@media.mit.edu", "russell5@media.mit.edu", "pralav@media.mit.edu"], "title": "The Foodome: Building a Comprehensive Knowledge Graph of Food", "modified": "2017-10-16T15:28:49.377Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2017-05-31", "slug": "the-foodome-building-a-comprehensive-knowledge-graph-of-food"}, {"website": "", "description": "<p>Drawdio is a pencil that draws music. You can sketch musical instruments on paper and play them with your finger. Touch your drawings to bring them to life\ufffdor collaborate through skin-to-skin contact. Drawdio works by creating electrical circuits with graphite and the human body. </p>", "people": ["silver@media.mit.edu", "mres@media.mit.edu"], "title": "Drawdio", "modified": "2016-12-05T00:16:21.896Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "drawdio"}, {"website": "", "description": "<p>The Drum Network provides players with a collaborative playing experience where participants can manipulate, share, and shape each others' music in real time. The drums in the network serve as controllers, sensing hitting and pressure that is then sent via a central system to other players. The drums also serve as speakers by using an attached actuator, which provides acoustic and tactile feedback.</p>", "people": ["tod@media.mit.edu"], "title": "Drum Network", "modified": "2016-12-05T00:16:22.061Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-483", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "drum-network"}, {"website": "", "description": "<p>Dual-space drawing supports creative drawing and reflective learning experiences using dual layers: a screen display and a transparent display. Dual-space drawing users can reflect themselves and embody their ideas while designing scenes and drawing objects.</p>", "people": ["holtzman@media.mit.edu"], "title": "Dual-Space Drawing ", "modified": "2016-12-05T00:16:21.943Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "dual-space-drawing"}, {"website": "", "description": "<p>Email is a tool that people use daily, making an implicit statement about their relationships with other people, and providing an opportunity for a computer to learn about their social network. While people have come to depend on email in their daily lives, the tool has hardly changed to help people deal with an overwhelming amount of information. Many of the social cues that allow people to function naturally within their social network are not inherent or obvious in Computer Mediated Communication (CMC). This work uses automatic social network analysis to bring these cues to CMC and foster the user\ufffds coherent understanding of the people and resources of their communication network. The goal of this work is to demonstrate that Artificial Intelligence can help people in the realm of social decisions. Using AI of Social Networks, this work improves human-human communication, recognizing the social characteristics of human relations in order to achieve a more natural online communication interface.</p>", "people": [], "title": "DriftCatcher", "modified": "2016-12-05T00:16:21.967Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "driftcatcher"}, {"website": "", "description": "<p>Droplet is a tangible interface that explores the movement of information between digital and physical representations. Through light-based communication, the project allows information to be easily extracted from its digital form behind glass and converted into mobile, tangible representations,  altering its form and our perception of the information.</p>", "people": ["holtzman@media.mit.edu"], "title": "Droplet", "modified": "2016-12-05T00:16:21.994Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "droplet"}, {"website": "", "description": "<p>Media Lab Virtual Visit is intended to open up the doors of the Media Lab to people from all around the world. The visit is hosted on the <a href=\"https://www.media.mit.edu/projects/unhangout/overview/\">Unhangout</a> platform, a new way of running large-scale unconferences on the web that was developed at the Media Lab. It is an opportunity for students or potential collaborators to talk with current researchers at the Lab, learn about their work, and share ideas. </p>", "people": ["srishti@media.mit.edu", "ps1@media.mit.edu", "kamcco@media.mit.edu"], "title": "Media Lab Virtual Visit", "modified": "2018-06-20T19:58:01.122Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["ml-learning"], "published": true, "active": false, "end_on": "2016-08-01", "slug": "media-lab-virtual-visit"}, {"website": "", "description": "<p>E15:oGFx is a dynamic openGL texture engine. It provides an interface to a dynamic procedural texture generation context that can be modified at runtime. Using Python as the scripting language, E15:oGFx can be used for procedural animations and data visualizations. One goal of E15:oGFx is to increase the level of end user engagement with existing programs and foster additional creativity on top of scripts written by others within the E15 community. In contrast to traditional 2D graphics environments, E15:oGFx leverages openGL to reveal more than the standard 2D view of the script execution. In particular, the 2D canvas can be scaled in vector graphics sense, and the history of the script execution can be visualized. In addition to many graphics environments, E15:oGFx also supports dynamic loading of GLSL-based shaders, as well as procedural manipulation of shader parameters.</p>", "people": ["holtzman@media.mit.edu"], "title": "E15:oGFx", "modified": "2016-12-05T00:16:22.408Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "e15ogfx"}, {"website": "", "description": "<p>E15 is an experimental architecture that places the power of the presentation of Web content into the hands of those that use it. Based on a dynamic, interactive, OpenGL-based scripting engine, E15 exposes an entirely new face to Web content, freely modifiable by each individual user.</p>", "people": ["holtzman@media.mit.edu"], "title": "E15", "modified": "2016-12-05T00:16:22.353Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "e15"}, {"website": "", "description": "<p>To celebrate the construction of the new building, E14, the public installation E14 1/2 seeks to capture the Media Lab community's vision for our new space and our future directions via audience participation with a large projection on the facade of the new building. We consider the Lab's role within the School of Architecture and Planning, as well as MIT, and how the new community of E14-E15 will be a center of learning, collaboration, and innovation across all disciplines.</p>", "people": ["dsmall@media.mit.edu"], "title": "E14 1/2", "modified": "2016-12-05T00:16:22.380Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["design-ecology"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "e14-12"}, {"website": "", "description": "", "people": [], "title": "Algorithmic Detection of Vaccine-denialist sentimen Clusters in Social Networks", "modified": "2018-05-01T18:46:53.597Z", "visibility": "PUBLIC", "start_on": "2018-05-01", "location": "", "groups": ["scalable-cooperation"], "published": false, "active": false, "end_on": "2018-12-03", "slug": "algorithmic-d"}, {"website": "", "description": "<p>When a die is picked up, it attempts to teach patterns, progressions of patterns, numbers, counting, arithmetic, and probability depending on user successes. It gives instructions: \"turn dice to find four\" or \"turn dice to add two.\" The tasks increase and decrease in difficulty based upon the user's success. The dice are adaptive educational toys for a range of ages and skill levels (from infant to third grade). This adaptive toy demonstrates a novel capacitive 3-D accelerometer. The water-filled vessel accelerometer is an inexpensively manufactured, low-technology way to measure orientation. Water within a cavity sloshes to be close to capacitive plates on various sides of the accelerometer. A micro-computer compares capacitances of the sides of the tetrahedron or cube cavity to determine its orientation. The dice use this novel capacitive, tetrahedral, position sensor, as well as voice feedback and adaptive user modeling to teach mathematical relations in a progressive manner.</p>", "people": [], "title": "Educational Dice", "modified": "2016-12-05T00:16:22.655Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2001-09-01", "slug": "educational-dice"}, {"website": "", "description": "<p>Conventional holograms require illuminators to be mounted on walls or ceilings near the hologram. Edge-lit holograms are a new type of white-light hologram that allow the light source to be included within the mount itself, ensuring a compact and carefully aligned illumination. This project explores the fundamental diffraction and imaging properties of these holograms, with a view toward making their images deeper, brighter, and clearer.</p>", "people": [], "title": "Edge-Lit Holograms", "modified": "2016-12-05T00:16:22.568Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-441", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "edge-lit-holograms"}, {"website": "", "description": "<p>Cooperative media distribution provides a solution to the scalability and efficiency problem of real-time media distribution and information dissemination in general. The flip-side of this is that collaborative media distribution is a destabilizing issue for entities that rely on a centralized architecture for revenue. In order to reconcile the economic realities of media distribution with the technical necessity of network scalability, a suitable economic model is required. We are developing licensing and payment schemes to enable and encourage cooperative distribution, while providing viable revenue schemes for intellectual property holders.</p>", "people": ["lip@media.mit.edu"], "title": "Economic Models for Cooperative Media Distribution", "modified": "2016-12-05T00:16:22.592Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "economic-models-for-cooperative-media-distribution"}, {"website": "", "description": "<p>During the Ejewels workshops, participants used a combination of basic electronics materials and basic craft materials to create jewelry with lights that glow, flash, and change color. These workshops enabled participants to become simultaneously engaged in a diverse set of investigations into: the nature of materials (for example, how light reflects against and refracts through different objects); the workings of basic electronics (gaining a practical, hands-on experience with serial and parallel circuits, short-circuiting, and the concepts underlying Ohm's law); and issues of personal identity (engaging in discussions of how  jewelry reveals aspects of the wearer).</p>", "people": ["sylvan@media.mit.edu", "mres@media.mit.edu"], "title": "Ejewels Workshops", "modified": "2016-12-05T00:16:22.810Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-020A", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "ejewels-workshops"}, {"website": "", "description": "<p>The Electronic Necklace demonstrates that sensors can be responsive and interactive devices, even without a computer. The necklace is activated mechanically, without batteries or electronics, simply by the user playing with it. For example, turning one of the beads turns all the beads. If the user turns on an LED, the necklace will light up. It can be illuminated in various ways depending on how the beads are turned. The necklace can also be augmented to include sound, so that the beads squeak when turned. This project sets a baseline for other work in context-aware computing, where sensors and computers with memory are used to add explicit task, system, and user models to objects in our everyday environment.</p>", "people": [], "title": "Electronic Necklace", "modified": "2016-12-05T00:16:22.913Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "electronic-necklace"}, {"website": "", "description": "<p>Electric Price Tags are a realization of a mobile system that is linked to technology in physical space. The underlying theme is that being mobile can mean far more than focusing on a portable device\ufffdit can be the use of that device to unlock data and technology embedded in the environment. In its current version, users can reconfigure the price tags on a store shelf to display a desired metric (e.g., price, unit price, or calories). While this information is present on the boxes of the items for sale, comparisons would require individual analysis of each box. The visualization provided by Electric Price Tags allows users to view and filter information in physical space in ways that were previously possible only online.</p>", "people": ["borovoy@media.mit.edu", "lip@media.mit.edu"], "title": "Electric Price Tags", "modified": "2016-12-05T00:16:22.895Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "electric-price-tags"}, {"website": "", "description": "<p>This project is aimed at building a machine learning pipeline that will discover and predict links between the visible structure of villages and cities (using satellite and aerial imaging) and their inhabiting social networks. The goal is to estimate digitally invisible villages in India and Sub-Saharan Africa. By estimating the social structure of these communities, our goal is to enable targeted intervention and optimized distribution of information, education technologies, goods, and medical aid. Currently, this pipeline is implemented using a GPU-powered Deep Learning system. It is able to detect buildings and roads and provide detailed information about the organization of the villages. The output will be used to construct probabilistic models of the underlying social network of the village. Moreover, it will provide information on the population, distribution of wealth, rate and direction of development (when longitudinal imaging data is available), and disaster profile of the village.</p>", "people": ["dkroy@media.mit.edu", "mmv@media.mit.edu"], "title": "AINA: Aerial Imaging and Network Analysis", "modified": "2017-02-16T16:35:50.415Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2016-12-31", "slug": "aina-aerial-imaging-and-network-analysis"}, {"website": "", "description": "<p>High-quality sound in professional audio production has required large ASICs or power-hungry DSPs to get the work done. Even the MPEG-4 audio standard has unrealistic demands in its core technology to be ported to low-power handhelds and consumer products. This project has brought the same high audio standards to low-power processors embedded in hand-held formats. This is a redesign of the Structured Audio core that Media Lab contributed to the MPEG-4 standard. SA's floating-point processing has now been redesigned to suit a low-power 16-bit processor, and the first port to an Analog Devices 16-bit Blackfin processor is now running and demonstrable. While ring tones and MIDI-encoded melodies are trivial, the power of this system is revealed not only in its 64-voice synthetic orchestras with audio-post effects (reverb, chorusing) but also in simultaneous voice processing with automatic harmonization and tempo tracking. Try doing that on your mobile!</p>", "people": ["bv@media.mit.edu"], "title": "Embedded Audio Systems", "modified": "2016-12-05T00:16:23.042Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "embedded-audio-systems"}, {"website": "", "description": "<p>The quantity and availability of video content is soaring due to the combination of television networks and the Internet. The aim of this project is to develop more effective means to manage, search, and translate video content. We are developing algorithms that interpret language in video (speech and closed caption text) by exploiting aspects of the non-linguistic context, or situation, conveyed by the accompanying video. We model situations by automatically finding patterns within low-level audio/video features that represent events. Event patterns are then mapped to words spoken in the video in order to create a \ufffdgrounded\ufffd dictionary of word meanings. Our research focuses on sports video, in particular, on Major League Baseball games. We are exploring applications in multimedia search and video-based machine translation.</p>", "people": ["dkroy@media.mit.edu", "mbf@media.mit.edu"], "title": "Sports Video Search Using Situated Natural Language Processing", "modified": "2016-12-05T00:16:22.930Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "sports-video-search-using-situated-natural-language-processing"}, {"website": "", "description": "<p>Embedded Networks are digital networks designed to form links among everyday objects. By design, embedded network links are wireless, self-organizing, and low-power. Control of the network is decentralized. There is no base station or centralized point of command, and distributed, each node shares in the overall maintenance of the network and routing of data packets. The goal of this research is to create networks that are exceptionally easy to set up and maintain. Providing such \"instant infrastructure\" requires a major rethinking of network architectures. Research in Embedded Networks focuses on novel techniques for control, routing and power management to realize low-cost, packet-switched digital networks.</p>", "people": ["mike@media.mit.edu"], "title": "Embedded Networks", "modified": "2016-12-05T00:16:22.973Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-468", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "embedded-networks"}, {"website": "", "description": "<p>The Emonique project is a collaboration between the Interactive Cinema group and the Story Networks group (Media Lab Europe) to develop an improvisational installation performance using the Emonic Environment. The goal of this event is to show how, using the Emonic Environment, the mobile phone can be transformed into an instrument for audiovisual improvisation. The installation aims to entice the members of the audience to use their phones as recorders and controllers to create structures of sonic samples in a collective manner, thus making the audience creative, improvisational artists and actors within a surrounding informational landscape.</p>", "people": ["gid@media.mit.edu"], "title": "Emonique", "modified": "2016-12-05T00:16:23.103Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-368", "groups": ["interactive-cinema"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "emonique"}, {"website": "", "description": "<p>EmBuddy is a digital creature that senses its user's stress levels via an EEG headset. When EmBuddy's user is stressed, EmBuddy will wiggle in empathy. EmBuddy's user can calm this wiggling either by petting EmBuddy's stroke sensors and/or by calming his mind. This prototype was built by Sen Ando, Aydin Arpa, Sam Leuscher, and me for Hiroshii Ishii's Tangible Interfaces class. EmBuddy's motorized body was constructed by Sam, its skin and stroke sensors were constructed by Sen and me, and its software was written by Aydin, based on my MindRider sketch.</p>", "people": ["raskar@media.mit.edu", "ishii@media.mit.edu", "holtzman@media.mit.edu"], "title": "EmBuddy", "modified": "2016-12-05T00:16:23.124Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["camera-culture", "information-ecology", "tangible-media"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "embuddy"}, {"website": "", "description": "<p>The Emonator is a new musical instrument for amateurs and professionals alike. It gives users a three-dimensional tactile interface to control music using their hands, and can be used as either a stand-alone gestural input device or in conjunction with a traditional musical keyboard or microphone. The Emonator generates audio output by mapping the perfomer's expressive gestures to different musical parameters. The sculptable surface of the Emonator can control a variety of sonic parameters, ranging from the audible harmonics of an additive synthesis engine to the activity level of algorithmically generated music.</p>", "people": ["tod@media.mit.edu"], "title": "Emonator", "modified": "2016-12-05T00:16:23.146Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-492", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "emonator-2"}, {"website": "", "description": "<p>The Emonator project is aimed at affective information browsing and exchange. We are using an innovative gesture controller as our input interface, and music/video as our media. The applications include real-time sound synthesis, MIDI control, live video browsing, and more. The multipurpose gesture interface allows for interactive control of music and video, and is controlled by hand gestures. The device, which consists of 144 rods arranged in a square pattern, derives its position from 12 optical boards, and is controlled by a field programmable gate array. The Emonator is also the first input interface for the Emonic Environment project. The hardware for the Emonator project was collaboratively designed by Dan Overholt, a recent master's candidate in the Lab's Opera of the Future group, and Andrew Yip, who constructed the interface.</p>", "people": ["gid@media.mit.edu"], "title": "Emonator", "modified": "2016-12-05T00:16:23.167Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-368", "groups": ["interactive-cinema"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "emonator"}, {"website": "", "description": "<p>EmoteMail is an email client that is augmented to convey aspects of the state of the writer during the composition of email to the recipient. The client captures facial expressions and typing speed and introduces them as design elements. These contextual cues provide extra information that can help the recipient decode the tone of the email. Moreover, the contextual information is gathered and automatically embedded as the sender composes the email, allowing an additional channel of expression.</p>", "people": ["picard@media.mit.edu"], "title": "EmoteMail", "modified": "2016-12-05T00:16:23.189Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "emotemail"}, {"website": "", "description": "<p>Media Perspective brings a data visualization into 3D space. This data sculpture represents mainstream media coverage of Net Neutrality over 15 months, during the debate over the FCC's classification of broadband services. Each transparent pane shows a slice in time, allowing users to physically move and look through the timeline. The topics cutting through the panes show how attention shifted between aspects of the debate over time. </p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "elplatt@media.mit.edu"], "title": "Media Perspective", "modified": "2016-12-05T00:16:23.229Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2016-01-01", "slug": "media-perspective"}, {"website": "", "description": "<p>Social-emotional communication difficulties lie at the core of autism spectrum disorders, making interpersonal interactions overwhelming, frustrating, and stressful. We are developing the world's first wearable affective technologies to help the growing number of individuals diagnosed with autism\ufffdapproximately 1 in 150 children in the United States\ufffdlearn about nonverbal communication in a natural, social context. We are also developing technologies that build on the nonverbal communication that individuals are already using to express themselves, to help families, educators, and other persons who deal with autism spectrum disorders to better understand these alternative means of nonverbal communication. </p>", "people": ["micahrye@media.mit.edu", "picard@media.mit.edu", "mehoque@media.mit.edu"], "title": "Emotional-Social Intelligence Toolkit", "modified": "2016-12-05T00:16:23.293Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "emotional-social-intelligence-toolkit"}, {"website": "", "description": "<p>People who have difficulty communicating verbally (such as many people with autism) sometimes send nonverbal messages that do not match what is happening inside them. For example, a child might appear calm and receptive to learning\ufffdbut have a heart rate over 120 bpm and be about to meltdown or shutdown. This mismatch can lead to misunderstandings such as \"he became aggressive for no reason.\" We are creating new technologies to address this fundamental communication problem and enable the first long-term, ultra-dense longitudinal data analysis of emotion-related physiological signals. We hope to equip individuals with personalized tools to understand the influences of their physiological state on their own behavior (e.g., \"which state helps me best maintain my attention and focus for learning?\"). Data from daily life will also advance basic scientific understanding of the role of autonomic nervous system regulation in autism.</p>", "people": ["rmorris@media.mit.edu", "mgoodwin@media.mit.edu", "picard@media.mit.edu"], "title": "Emotion Communication in Autism", "modified": "2016-12-05T00:16:23.256Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "East Lab", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "emotion-communication-in-autism"}, {"website": "", "description": "<p>The technology in this project changes facial expressions in videos without the system knowing anything in particular about the person's face ahead of time. There are a few reasons to create something like this: first, it provides an artistic tool with which to alter photos or videos; second, it could be set up to let people open-endedly explore their facial communication and expressiveness by playing with a real-time video of their own current face; finally, E-DJ demonstrates an unexpected way in which we can't always trust the video information we love to consume. </p>", "people": ["picard@media.mit.edu", "silver@media.mit.edu"], "title": "Emotional DJ", "modified": "2016-12-05T00:16:23.393Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "Cube", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "emotional-dj"}, {"website": "", "description": "<p>EMotoPhone adds a new dimension to cell-phone interaction, augmenting verbal communication by allowing users to send personalized emoticons over the phone to show how they are feeling. The EMotoPhone icons can also be sent independently, outside of the context of a conversation. For example, if you are too busy to engage in a conversation, but you want to let someone know that you are thinking of them, you can send them an EMotoPhone wink.  Using real faces for the EMotoPhone icons makes cell-phone interaction richer, more personalized, and more fun.</p>", "people": ["geek@media.mit.edu"], "title": "EMotoPhone", "modified": "2016-12-05T00:16:23.414Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-384C", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "emotophone"}, {"website": "", "description": "<p>Empowering Minds proposes a constructionist, networked model for teacher learning. The focus of the work will be to explore a new constructionist model for professional development of teachers. The design of effective and meaningful learning environments will be informed by a technologically rich support structure that facilitates reflection, technological fluency, and collaboration. (Funded by the Higher Education Authority of Ireland.)</p>", "people": ["papert@media.mit.edu", "gid@media.mit.edu"], "title": "Empowering Minds", "modified": "2016-12-05T00:16:23.446Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-441", "groups": ["future-of-learning"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "empowering-minds"}, {"website": "", "description": "<p>Cities around the world are striving to improve livability by way of reducing dependency on fossil-fuels cars. How might we leverage the autonomous technology to help fulfill this vision, while ensuring the flow of people and goods across the city? The Persuasive Electric Vehicle (PEV) is a small, on-demand, shared, agile, autonomous, and functionally hybrid tricycle. We believe it will become a critical platform in the constellation of emerging mobility systems. The PEV will be a shared bike platform for people's inner-urban and last-mile travel needs, and for delivering goods on-demand around the clock. To deploy it in the real world, it is necessary to match the fleet supply with its demand. This simulator enables cities around the world to forecast the fleet size based on proxy demands from taxis, shared bikes, shared car services, and Call Detail Records (CDR).</p>", "people": ["mcllin@media.mit.edu", "kll@media.mit.edu", "ptinn@media.mit.edu"], "title": "Hybrid Autonomous Shared Bike Fleet Deployment Simulator", "modified": "2016-12-08T17:15:49.102Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": "2016-01-01", "slug": "hybrid-autonomous-shared-bike-fleet-deployment-simulator"}, {"website": "", "description": "<p>\"Encephalodome\" (working title) is an art+science game under development for the dome projection (planetarium) setting of the Lower Eastside Girls Club. Players will wear inexpensive Electroencephalography (EEG) devices to both control and contribute to the game. They can expressively explore science through activities like concentrating, meditating, closing their eyes, and moving their bodies. By fusing many kinds of science data sets into a vast spatial experience, \ufffdEncephalodome\ufffd will engage players in natural beauty beyond the scales of human perception. \"Encephalodome\" gameplay focuses on ocean acidification: increased pollution is changing the pH of the oceans, thus affecting the growth of sea vertebrates and shellfish. \"Encephalodome\" will invite its users to interactively role-play prototypical sea organisms like coral, plankton, jellyfish, and lobster through decades of increased carbon emissions. </p>", "people": ["holtzman@media.mit.edu"], "title": "Encephalodome", "modified": "2016-12-05T00:16:23.549Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "encephalodome"}, {"website": "", "description": "<p>Improving adult learning, especially for adults who are unemployed or unable to financially support their families, is a challenge that affects the future wellbeing of millions of individuals in the US. We are working with the Joyce Foundation, employers, learning researchers, and the Media Lab community to prototype three to five new models for adult learning that involve technology innovation and behavioral insights. </p>", "people": ["ps1@media.mit.edu", "jnazare@media.mit.edu", "kamcco@media.mit.edu"], "title": "Making Learning Work", "modified": "2016-12-05T00:16:23.585Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["ml-learning"], "published": true, "active": false, "end_on": "2016-06-01", "slug": "making-learning-work"}, {"website": "", "description": "<p>As the population ages, acuity in one or more sensory channels often diminishes or may be totally lost. Augmenting or compensating for loss in the perceptual system by taking advantage of sensory data outside the normal human range and mapping it to meaningful perceptual information has the potential of giving an ordinary person enhanced sensory perception (ESP).\nSensory deficiency is not restricted to any particular segment of the population, however.  For example, we tend to be myopic about ourselves, and thus can benefit from psychological mirrors in the form of trainers or therapists who can assess and guide our physical and/or mental development. In this spirit, \"Reflective Biometrics\" is a novel approach to analyzing and interpreting biometric sensory information for self monitoring and examination.  It is self-examination via technology as a mirror. Biometric technologies in service of the individual can serve as reflectors that enhance our self-awareness, self-understanding, and health, and they can facilitate our interaction with computers and with each other by augmenting our perceptual system.</p>", "people": ["picard@media.mit.edu"], "title": "Enhanced Sensory Perception", "modified": "2016-12-05T00:16:23.620Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-443D", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "enhanced-sensory-perception"}, {"website": "", "description": "<p>It's common for us to attach different emotions to each moment in our lives, but when we capture and interact with them, we are often constrained to flat, 2D encapsulations of video, audio, or photographic recordings. In this digital age we often look at memories through a screen with hundreds of digital files, further removing the event's emotion. What if we could experience this emotional energy again, and dynamically interact with it in infinitely complex ways? We present a concept and prototype that explores a novel physical-visual language of dynamic, emotionally expressive waveforms, designed to transform how we perceive different forms of energy in our daily lives. With the power of computation hidden within the physical materials of the interface, we create an interactive form that takes one form of energy and transmutes it into a waveform as its output, or Wave Alchemy.</p>", "people": ["sujoy@media.mit.edu", "lip@media.mit.edu", "geek@media.mit.edu", "pattie@media.mit.edu"], "title": "Wave Alchemy: Perception and Reminiscence of Expressive Moments through Waves", "modified": "2018-12-04T20:33:12.363Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["viral-communications", "fluid-interfaces", "living-mobile"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "wave-alchemy-perception-and-reminiscence-of-expressive-moments-through-waves"}, {"website": "", "description": "<p>We are exploring techniques to integrate digital codes into physical objects. Spanning both the hard and the soft, this work entails incorporating texture patterns into the surfaces of objects in a coded manner. Leveraging advancements in rapid prototyping and manufacturing capabilities, techniques for creating deterministic encoded surface textures are explored. The goal of such work is to take steps towards a self-descriptive universe in which all objects contain within their physical structure hooks to information about how they can be used, how they can be fixed, what they're used for, who uses them, etc. Our motivation is to transform opaque technologies into things that teach and expose information about themselves through the sensing technologies we already, or foreseeably could, carry on us.</p>", "people": ["lip@media.mit.edu", "trich@media.mit.edu"], "title": "Encoded Reality", "modified": "2016-12-05T00:16:23.690Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "encoded-reality"}, {"website": "", "description": "<p>Endangered Senses is a wearable that allows people to experience a sense that endangered animals have but is not possessed by humans. The project is an elephant-inspired costume which investigates their ability to detect infrasonic and seismic vibrations. The wearable has long telescoping sleeves which conceal the arms and hands and connect to the floor. Thus the human is asked to sacrifice defining human characteristics (bipedal, with opposable thumbs) in order to experience a supplemental sense. The audio signals are broadcast to create a shared sensory experience.</p>", "people": ["csik@media.mit.edu"], "title": "Endangered Senses", "modified": "2016-12-05T00:16:23.707Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "endangered-senses"}, {"website": "", "description": "<p>Erase the Border is a web campaign and voice petition platform. It tells the story of the Tohono O'odham people, whose community has been divided along 75 miles of the US-Mexico border by a fence. The border fence divides the community, prevents tribe members from receiving critical health services, and subjects O'odham to racism and discrimination. This platform is a pilot that we are using to research the potential of voice and media petitions for civic discourse.</p>", "people": ["dignazio@media.mit.edu", "ethanz@media.mit.edu"], "title": "Erase the Border", "modified": "2016-12-05T00:16:23.811Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "erase-the-border"}, {"website": "", "description": "<p>The ESP team is designing and building embedded sensor packs for expeditionary projects. Earlier efforts pioneered human vital-sign monitoring (with marathon runners, cyclists, Army rangers, and members of Everest expeditions). This is a new DARPA-funded effort aimed at developing a breakthrough in new sensor system architectures<ETH>sensor packs that are ultrasmall, robust, and capable of recording and transmitting a wide variety of bio-, geo-, and environmental data. We are currently planning an ambitious series of expeditions to begin this spring.</p>", "people": ["mike@media.mit.edu"], "title": "ESP: Embedded Sensor Packs", "modified": "2016-12-05T00:16:23.833Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-468", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "esp-embedded-sensor-packs"}, {"website": "", "description": "<p>EventNet is the common-sense approach to goal planning. By extracting cause-effect and action-goal links from Open Mind Common Sense, we build an associative network of tens of thousands of nodes about human actions and goals. For example, EventNet is able to infer that in order to go from home to the movies you should drive your car. It is a toolkit for the easy use of interactive applications.</p>", "people": ["lieber@media.mit.edu"], "title": "EventNet", "modified": "2016-12-05T00:16:23.958Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "eventnet"}, {"website": "", "description": "<p>Success in a networked society will require not just new skills and new knowledge, but new ways of thinking. Rather than seeing the world as a clockwork mechanism, people will need to think in more ecological terms, recognizing the importance of adaptation and improvisation, and understanding how patterns can arise from many simple, local interactions. We are developing new technologies and activities to help people develop as \"eThinkers.\"</p>", "people": ["mres@media.mit.edu"], "title": "eThinking", "modified": "2016-12-05T00:16:23.886Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "ethinking"}, {"website": "", "description": "<p>Using exhaled water vapor as the display medium, we explore projecting into \"thin air,\" eliminating the need to carry a bulky display around.</p>", "people": ["mlj@media.mit.edu"], "title": "Exhale-a-Display", "modified": "2016-12-05T00:16:24.012Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "exhale-a-display"}, {"website": "", "description": "<p>Measuring the distance from the skin to the bone and soft tissue mechanical proper- ties is important to custom designing prosthetic sockets for amputee patients using a computer aided method. The current state-of-the-art method to obtain such informa- tion is via MRI scans. However, MRI scans are expensive, not widely accessible, and may not be as accurate if there is a time gap between when the MRI scan is taken and when the design process takes place. In this project, I designed and implemented a hand-held apparatus which measures both the skin-to-bone depth and soft tissue mechanical properties. With a PC interface, this method involves gathering and pro- cessing data from an ultrasound transducer, a force sensor, and an accelerometer. The procedure of use involves rotating the apparatus around the limb while main- taining a light contact to acquire skin-to-bone depth, and indenting the apparatus into the limb to acquire soft tissue mechanical properties. Here I show that a minia- turized apparatus as such can measure tissue boundaries and tissue indentation with sub-millimeter precision and out performs a commercial ultrasound imaging system in my case study, which makes custom computer prosthetic socket design easier, more affordable, and more accessible.\n                    \n                </p>", "people": ["kmoerman@media.mit.edu", "srkeyes@media.mit.edu", "zixiliu@media.mit.edu", "danask@media.mit.edu"], "title": "An Ultrasonic Sensing and Indentation Apparatus for Assessment of Tissue Geometry and Mechanical Properties", "modified": "2017-08-16T19:47:11.126Z", "visibility": "LAB", "start_on": "2016-09-01", "location": "", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": "2017-08-25", "slug": "an-ultrasonic-sensing-and-indentation-apparatus-for-assessment-of-tissue-geometry-and-mechanical-properties"}, {"website": "", "description": "<p>Conversion rates (ratio of visitors to buyers) on the Internet are amazingly low. In our current work, we suggest that one of the most noted advantages of electronic commerce\ufffdlow search costs\ufffdalso turns out to be a major reason for low conversion rates. We claim that the low search costs associated with finding products and information about them can cause indecision and procrastination. Moreover, the possibility of finding additional information after a decision has been made has a high potential for regret, which also increases indecision to buy. We are interested in exploring how to overcome these indecision problems. We suggest that imposing deadlines can act as a psychological device that will promote more decisive actions. The mechanism upon which we focus is one where discounts, but not product offers, have a limited lifetime (in the form of expiring discounts).</p>", "people": [], "title": "Expiring Coupons for e-Markets", "modified": "2016-12-05T00:16:24.055Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-314", "groups": ["erationality"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "expiring-coupons-for-e-markets"}, {"website": "", "description": "<p>We are exploring the methods by which traditional artisans construct new electronic technologies using contextually novel materials and processes, incorporating wood, textiles, reclaimed and recycled products, as well as conventional circuitry. Such artisanal technologies often address different needs, and are radically different in form and function than conventionally designed and produced products.</p>", "people": ["mellis@media.mit.edu", "leah@media.mit.edu"], "title": "Exploring Artisanal Technology", "modified": "2016-12-05T00:16:24.075Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "exploring-artisanal-technology"}, {"website": "", "description": "<p>A smile is a multi-purpose expression. We smile to express rapport, polite disagreement, delight, sarcasm, and often, even frustration. Is it possible to develop computational models to distinguish among smiling instances when delighted, frustrated, or just being polite? In our ongoing work, we demonstrate that it is useful to explore how the patterns of smile evolve through time, and that while a smile may occur in positive and in negative situations, its dynamics may help to disambiguate the underlying state. </p>", "people": ["picard@media.mit.edu", "mehoque@media.mit.edu"], "title": "Exploring Temporal Patterns of Smile", "modified": "2016-12-05T00:16:24.096Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "exploring-temporal-patterns-of-smile"}, {"website": "", "description": "<p>ExtrAct, a set of Internet-based databasing, mapping, and communications technologies for communities impacted by natural gas development, is a novel platform for community education and civic action. Its objective is to create and distribute open-source, Web-based tools for mapping, analyzing, and intervening in this industry based on supplementing data obtained from state and federal agencies with user-generated reports, complaints, and experiences. All of these tools, though accessible individually, will share information through a unified database. Because these tools will serve both urban and rural populations, we are also developing innovative paper and phone interfaces to the Web services. To develop these tools we are working with a network of lawyers, citizen\ufffds alliances, national activist organizations, and environmental health experts in Colorado, New Mexico, Ohio, New York, Pennsylvania, West Virginia, and Texas.</p>", "people": ["ethanz@media.mit.edu"], "title": "ExtrACT", "modified": "2016-12-05T00:16:24.163Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-001", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "extract"}, {"website": "", "description": "<p>A \"society\" of intelligent mobile cameras collaboratively solves calibration and scene-modeling problems through exchange of data. The small prototypes are built starting with StrongARM-based PDAs running Linux. Each has a camera with motorized pan/tilt head, IEEE 802.11 wireless networking, and a motor for traveling along an overhead monorail-style track.</p>", "people": ["vmb@media.mit.edu"], "title": "Eye Society", "modified": "2016-12-05T00:16:24.185Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-368", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "eye-society"}, {"website": "", "description": "<p>Expressive musical re-performance is about enabling a person to experience the creative aspects of a playing a favorite song regardless of technical expertise. This is done by providing users with computer-linked electronic instruments that distills the instruments' interface but still allows them to provide expressive gesture. The next note in an audio source is triggered on the instrument, with the computer providing correctly pitched audio and mapping the expressive content onto it. Thus, the physicality of the instrument remains, but requires far less technique. We are implementing an expressive re-performance system using commercially available, expressive electronic musical instruments and an actual recording as the basis for deriving audio. Performers will be able to select a voice within the recording and re-perform the song with the targeted line subject to their own creative and expressive impulse.</p>", "people": ["joep@media.mit.edu"], "title": "Expressive Re-Performance", "modified": "2016-12-05T00:16:24.206Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "expressive-re-performance"}, {"website": "", "description": "<p>We propose a set of customizable, easy-to-understand, and low-cost physiological toolkits in order to enable people to visualize and utilize autonomic arousal information. In particular, we aim for the toolkits to be usable in one of the most challenging usability conditions: helping individuals diagnosed with autism. This toolkit includes: wearable, wireless, heart-rate and skin-conductance sensors; pendant-like and hand-held physiological indicators hidden or embedded into certain toys or tools; and a customized software interface that allows caregivers and parents to establish a general understanding of an individual's arousal profile from daily life and to set up physiological alarms for events of interest. We are evaluating the ability of this externalization toolkit to help individuals on the autism spectrum to better communicate their internal states to trusted teachers and family members.</p>", "people": ["mgoodwin@media.mit.edu", "picard@media.mit.edu"], "title": "Externalization Toolkit", "modified": "2016-12-05T00:16:24.144Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-443D", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "externalization-toolkit"}, {"website": "", "description": "<p>Exquisite Corpus is a collaborative writing application that aims to explore new and more \"indirect\" forms of collaboration similar to those in open-source software or in Wikipedia. EC allows collaborators to contribute short vignettes in traditional \"scene\" (i.e., screenplay or script) form and then, using heuristics and AI techniques, helps to integrate these into larger aggregate works suitable for use as a movie or play script. These aggregate works aim to provide consistent characters and themes through the work. The system provides a platform for investigating and improving computer support of indirect collaboration.</p>", "people": ["walter@media.mit.edu"], "title": "Exquisite Corpus", "modified": "2016-12-05T00:16:24.232Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "exquisite-corpus"}, {"website": "", "description": "<p>Eye-aRe is a system designed to detect and communicate the intentional information conveyed in eye movement. This glasses-mounted, wireless device stores and transfers information based on users' eye motion and external IR devices, thus promoting an enriched experience with their environment. This project describes how the system measures eye motion and utilizes this as an implicit input channel to a sensor system and computer. In the primary scenario, eye motion detection is used to recognize a user's gaze. When the user\ufffds eyes are fixed, the system infers that he is paying attention to something in his environment and then tries to facilitate an exchange of information in either direction on his behalf.</p>", "people": [], "title": "Eye-aRe", "modified": "2016-12-05T00:16:24.272Z", "visibility": "PUBLIC", "start_on": "2000-09-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2002-01-01", "slug": "eye-are"}, {"website": "", "description": "<p>Millions of people worldwide need glasses or contact lenses to see or read properly. We introduce a computational display technology that predistorts the presented content for an observer, so that the target image is perceived without the need for eyewear. We demonstrate a low-cost prototype that can correct myopia, hyperopia, astigmatism, and even higher-order aberrations that are difficult to correct with glasses.</p>", "people": ["raskar@media.mit.edu", "gordonw@media.mit.edu", "naik@media.mit.edu"], "title": "Eyeglasses-Free Displays", "modified": "2016-12-05T00:16:24.496Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2016-09-01", "slug": "eyeglasses-free-displays"}, {"website": "", "description": "<p>While modern communication technologies mean that we can connect to more people, these connections lack the affective subtleties inherent in situated interactions. EyeJacking is an application for the sharing of experiences in which one or more persons \ufffdeyejack\ufffd a person\ufffds visual field to share what he or she sees. Using a wearable camera/micorphone system, remote interaction partners can share an experience first-hand and play an active role in shaping the experience. We explore the application of EyeJacking as a tool for situated learning for individuals on the autism spectrum, where parents, caregivers, or peers could \ufffdeyejack\ufffd and tag the world remotely. We also explore the application of EyeJacking to leverage the power of the masses to bootstrap people-sense abilities in robots. </p>", "people": ["picard@media.mit.edu"], "title": "EyeJacking: See What I See", "modified": "2016-12-05T00:16:24.376Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-450", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "eyejacking-see-what-i-see"}, {"website": "", "description": "<p>The goal of the project is to build a computational tool that will allow students to explore the realm of image processing, signal processing, matrices and filters.\nVery complex mathematical and physical concepts are embedded in these fields of knowledge. We normally believe that only specialists have the technical background to think about them. However, the use of computation offers an intuitive way of exploring these ideas these ideas, through digital image, sound and signal processing.\nWhy are these fields of knowledge important? We don\ufffdt believe in creating tool to teach one specific subject \ufffd the idea is not to better explain a derivative or a Fourier transform.\nTurtle geometry was not a tool to teach angles and polygons, but rather offered an invaluable and innovative paradigm of mathematical exploration and construction. We believe that extending the Logo aesthetics to the field of matrices, filters, variations and non-conventional spaces of mathematical representation could be a powerful construction tool.\nImage processing software is usually opaque: the end user has no idea of the algorithms or the operations that it is performing. EyeLogo would be both a tool to explore and demystify the realm of image and a powerful construction tool that could enhance current Logo-based environments.</p>", "people": ["cavallo@media.mit.edu"], "title": "EyeLogo", "modified": "2016-12-05T00:16:24.414Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-489", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "eyelogo"}, {"website": "", "description": "<p>The Seung Lab at MIT's Brain + Cognitive Sciences Department has developed EyeWire, a game to map the brain. To date, it has attracted an online community of over 50,000 \"citizen neuroscientists\" who are mapping the 3D structure of neurons and discovering neural connections. Playful Systems is collaborating with the Seung Lab to reconsider EyeWire as a large scale mass-appeal mobile game to attract 1MM players or more. We are currently developing mobile, collaborative game mechanics, and shifting the focus to short-burst gameplay.</p>", "people": ["tjlevy@media.mit.edu", "gregab@media.mit.edu", "slavin@media.mit.edu", "cwwang@media.mit.edu"], "title": "EyeWire", "modified": "2016-12-05T00:16:24.523Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "eyewire"}, {"website": "", "description": "<p>Fab FM explores the possibilities for personal fabrication of consumer electronic devices.  It is a wood- and fabric-cased FM radio that can be manufactured in small volumes by an individual with access to a laser cutter.  Each radio can be customized with materials (e.g., wood or fabric) provided by the customer.  Because the radio can be produced from its digital design files using minimal infrastructure, it offers a diverse set of possible business models and distribution schemes.  For example, radios could be sold as kits to be assembled by the customer, or produced by individuals in many different cities.  </p>", "people": ["mellis@media.mit.edu", "leah@media.mit.edu"], "title": "Fab FM", "modified": "2016-12-05T00:16:24.557Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "fab-fm"}, {"website": "", "description": "<p>The Face Interface system allows users to select facial gestures to control the Windows interface without a keyboard or mouse. The system recognizes head shaking, nodding, eyes and mouth open/closed, and head position through an avatar, giving users the ability to easily view possibilities and change their approaches. Attention Meter demonstrated the ability to rapidly program interfaces for using these features in a variety of environments, including C++, Flash, and Max-MSP. This previous work taught us the usefulness of the face as an input in human-computer interaction, and using this knowledge, Face Interface can inform and critique the user's design choices. This is an exemplar for a new input approach that allows users to define their interactions without programming and to experience and learn the design parameters of a new and unfamiliar system.</p>", "people": [], "title": "Face Interface", "modified": "2016-12-05T00:16:24.637Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "face-interface"}, {"website": "", "description": "<p>This project focuses on new methods for analyzing, visualizing, and interacting with large text and speech collections. We are exploring transparent and interactive knowledge representations, graphical representations that provide users with the gist of complex underlying data, and methods for displaying data at multiple scales. This work has a wide range of applications in data mining and knowledge management.</p>", "people": ["dkroy@media.mit.edu", "decamp@media.mit.edu"], "title": "Seeing Meaning", "modified": "2016-12-05T00:16:24.814Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "seeing-meaning"}, {"website": "", "description": "<p>Segu\ufffd is a browsing agent that can discern the user's changes of interest by examining browsing history. The agent represents the pages in your browsing history, not via a hierarchical list or graph, but by using a series of \"skeins\" which represent changes in interest over time. \"Your history is only the beginning...\"</p>", "people": ["lieber@media.mit.edu"], "title": "Segu\ufffd", "modified": "2016-12-05T00:16:24.758Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-301", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2003-12-30", "slug": "segu"}, {"website": "", "description": "<p>Distributed power meters monitor electricity consumption at each outlet and broadcast measurements over the power line for other nodes to read. The collected information is stored and graphed by a computer with a serial port connection to any of the nodes in the home. The computer can remotely shut down outlets to save power from appliances in standby mode.</p>", "people": ["mike@media.mit.edu"], "title": "SeeGreen", "modified": "2016-12-05T00:16:24.785Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "seegreen"}, {"website": "", "description": "<p>Now that mobile phones are starting to have 3D display and capture capabilities, there are opportunities to enable new applications that enhance person-person communication or person-object interaction. This project explores one such application: acquiring 3D models of objects using cellphones with stereo cameras. Such models could serve as shared objects that ground communication in virtual environments and mirrored worlds, or in mobile augmented reality applications.</p>", "people": ["geek@media.mit.edu"], "title": "SeeIt-ShareIt", "modified": "2016-12-05T00:16:24.862Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "seeit-shareit"}, {"website": "", "description": "<p>The Self-Cam is a wearable camera apparatus that consists of a chest-mounted camera aimed at the wearer\ufffds face. Self-Cam was designed to be used in conjunction with a belt-mounted computer and real-time mental-state inference software that can be used with visual, auditory, or tactile output as personal feedback for the wearer. As the camera faces inward, many privacy issues are avoided\ufffdonly those who choose to wear the Self-cam appear in the recorded video. Head movement can be seen and analyzed alongside facial expressions because the system rests on the chest and the light, simple nature of the structure allows it to be worn without any physical discomfort. By wearing the Self-Cam, you can explore who you appear to be from the outside. The Self-Cam acts as an objective point of view that might help you to understand yourself in a different light.</p>", "people": ["picard@media.mit.edu"], "title": "Self-Cam", "modified": "2016-12-05T00:16:24.911Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "self-cam"}, {"website": "", "description": "<p>A first-step toward origami robotics, I/O paper is a pair of origami papers in which the red (controller) paper senses how it is being folded and the white (output) paper follows. When the white paper is flipped over, blintz folding allows the paper to get up, wobble around, and even flip itself over. The microcontroller and circuitry is on the body of the red paper and the white paper is actuated by shape memory alloy.</p>", "people": ["leah@media.mit.edu", "jieqi@media.mit.edu"], "title": "Self-Folding Origami Paper", "modified": "2016-12-05T00:16:24.946Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "self-folding-origami-paper"}, {"website": "", "description": "<p>We are interested in investigating designs for a self-revealing audio ballot. Traditional audio ballots are best described as awkward afterthoughts heavily tied to visual representation. We are exploring a multivocal, multichannel system that will allow a faster and more easily accessible interpretation of the same information contained in visual displays.</p>", "people": [], "title": "Self-Revealing Interface for Audio Ballots", "modified": "2016-12-05T00:16:25.075Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "self-revealing-interface-for-audio-ballots"}, {"website": "", "description": "<p>We have built a compact piezoelectric push button and associated minimal circuitry that is able to transmit a digital RFID code to the immediate region (50-100 foot radius) upon a single button push, without the need of batteries or other energy sources. Such devices have the potential of enabling the introduction of controls and interfaces into interactive environments without requiring any wiring, optical/acoustic lines of sight, or batteries.</p>", "people": ["geppetto@media.mit.edu", "joep@media.mit.edu"], "title": "Self-Powered / Wireless Push Button Controller", "modified": "2016-12-05T00:16:25.016Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-441", "groups": ["responsive-environments", "cc"], "published": true, "active": false, "end_on": "2003-12-30", "slug": "self-powered-wireless-push-button-controller"}, {"website": "", "description": "<p>We are developing a multimodal processing system called Fuse to explore the effects of visual context on the performance of speech recognition. We propose that a speech recognizer with access to visual input may \"second guess\" what a person says based on the visual context of the utterance, thereby increasing speech recognition accuracy. To implement this idea, several problems of grounding language in vision (and vice versa) must be addressed. The current version of the system consists of a medium vocabulary speech-recognition system, a machine-vision system that perceives objects on a tabletop; a language acquisition component that learns mappings from words to objects and spatial relations; and a linguistically driven focus of visual attention. A corpus of naturally spoken, fluent speech was used to evaluate system performance; speech ranged from simple constructions such as \"the vertical red block\" to more complex utterances such as \"the large green block beneath the red block.\" We found that integrating visual context reduces the error rate of the speech recognizer by over 30 percent. We are currently investigating implications of this improved recognition rate on the overall speech understanding accuracy of the system. This work has applications in contextual natural language understanding for intelligent user interfaces. For example, in wearable computing applications, awareness of the user's physical context may be leveraged to make better predictions of the user\ufffds speech to support robust verbal command and control.</p>", "people": ["dkroy@media.mit.edu"], "title": "Semantic Priming of Speech Recognition Using Visual Context", "modified": "2016-12-05T00:16:25.048Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "semantic-priming-of-speech-recognition-using-visual-context"}, {"website": "", "description": "<p>As computers move off the desktop they must be able to detect interaction with a user in their immediate environment. We are developing the instrumentation and algorithms to measure such gestures through the influence of the user's body on weak electric fields. This provides interfaces that are completely unobtrusive (the devices could be built into ordinary furniture), and that can inexpensively and reliably match the limits of human performance in unconstrained environments.</p>", "people": ["neilg@media.mit.edu", "rehmi@media.mit.edu"], "title": "Field Imaging", "modified": "2016-12-05T00:16:25.129Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "field-imaging"}, {"website": "", "description": "<p>Microfabricated biosensors based on silicon field-effect are being developed for applications where traditional biological methods are not suitable. This is a non-optical tool aimed at label-free detection of DNA hybridization, enzyme activity, and metabolic changes in single cells.</p>", "people": ["scottm@media.mit.edu"], "title": "Field-Effect Biosensors", "modified": "2016-12-05T00:16:25.154Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-420", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "field-effect-biosensors"}, {"website": "", "description": "<p>This project proposes both to investigate how a more open and flexible construction toolkit enables rich learning in mechanical and structural engineering as well as to investigate how introducing engineering concepts through an open-ended art/storytelling project provides for a welcoming entry to the hard sciences. Through using raw materials instead of pre-designed parts, we will investigate the exploration by the learner of design from raw material to part, as well as from part to whole. Through construction, we will examine how builders develop a proficiency in externalizing their ideas about motion, structure, and form, and how reflection upon, discussion about, and re-internalization of this ability to externalize ideas changes the way they think about, talk about, and approach future designs. The constructions are centered around the theme of artistic interactive 3-D storytelling. Through this, we will explore how builders express and incorporate their own interests and stories into their designs, and as a result, become engaged in learning new concepts.</p>", "people": ["cavallo@media.mit.edu"], "title": "Flexible and Appropriable Materials for Constructing Knowledge", "modified": "2016-12-05T00:16:25.206Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-368", "groups": [], "published": true, "active": false, "end_on": "2007-09-01", "slug": "flexible-and-appropriable-materials-for-constructing-knowledge"}, {"website": "", "description": "<p>This research focuses on developing a smart material capable of sensing, distilling, and interpreting environmental stimuli while offering mechanical flexibility.  The device itself is composed of a series of small, interconnectable nodes.  Each node has its own embedded processing and a host of multi-modal sensors.  Physically, the system's nodular design allows for scalability as well as customize-ability.  Computationally, the design allows researchers to experiment with resource allocation, information exchange, power management, and differing scopes of processing in sensor networks.\n</p>", "people": ["joep@media.mit.edu"], "title": "Flexible High-Density Grid Sensor Network", "modified": "2016-12-05T00:16:25.233Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "flexible-high-density-grid-sensor-network"}, {"website": "", "description": "<p>Flexor is an arm sleeve with a bend sensor, an annunciator, and electro-luminescent lights. It keeps track of arm movement and uses models of arm motion to create entertaining and useful feedback for physical therapy or exercise. If you move your arm in natural gestures, the electro-luminescent lights sparkle like sequins with every movement. When you bend your arm for exercise, as in a weight-lifting bicep curl, the sleeve counts repetitions, encouraging body awareness. A variable resistor in the sleeve senses body position, while an on-board microprocessor facilitates interaction through electro-luminescence and sound. Therapy can also be coupled with expression, communication, performance, play, jewelry, and tools. This playful exploration could also be used to teach people how to protect their ligaments and muscles from strain and injury while playing tennis, squash, or golf, or even picking up their small children.</p>", "people": [], "title": "Flexor Exercise Sleeve", "modified": "2016-12-05T00:16:25.269Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "flexor-exercise-sleeve"}, {"website": "", "description": "<p>Inspired by the fact that people are communicating more and more through technology, Flickr This explores ways for people to have emotion-rich conversations through all kinds of media provided by people and technology. By grounding them in shared media, the technology allows remote people to have conversations that are more like face-to-face experiences. Flickr This lets viewable content provide structure for a conversation; conversation can move between synchronous and asynchronous, and evolve into a richer collaborative conversation/media.</p>", "people": ["geek@media.mit.edu"], "title": "Flickr This", "modified": "2016-12-05T00:16:25.339Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "E15-383", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "flickr-this"}, {"website": "", "description": "<p>Flossing is the phenomenon of drivers travelling back and forth on a designated route to show off their vehicles. The Flosser is a software package designed to be integrated into your car to help pre-determined flossing groups arrange meetings and communicate in real time. The system runs off of GPS data and GPRS networks on a Linux platform. The software provides advanced social rules capable of helping cars and people organize better, as well as connecting the car and city. The project hopes to answer a large number of questions raised by location-based tools and the way the car interacts with the city.</p>", "people": ["rchin@media.mit.edu"], "title": "Flosser", "modified": "2016-12-05T00:16:25.404Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "flosser"}, {"website": "", "description": "<p>This project aims to create an electronic controller with many different sensor affordances that adapts to the gestural preferences of the user during the course of the interaction.  Our research includes an electronic music interface; departing from existing music controllers and most current multimodal interfaces, this system allows users to train the system to recognize their own personalized gestures and to establish the mappings from those gestures to sound. This approach turns the paradigm for musical-instrument design on its head, giving the device the ability to adapt to the player. Viewed as a data-collection platform that will be used by many subjects, the controller will be a powerful vantage point from which to study universal patterns in the way people associate gesture and sound. Beyond breaking ground in electronic musical instrument design, this work is investigating important and topical issues in learning systems for multimodal and adaptive user interfaces.</p>", "people": ["joep@media.mit.edu"], "title": "FlexiGesture", "modified": "2016-12-05T00:16:25.312Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "flexigesture"}, {"website": "", "description": "<p>Flights of Fantasy is an interactive research and installation project that focuses on the acts of receiving and sending video messages. Inspired by an analogy between the carrier pigeon and electronic networks, the installation is designed to occupy two rooms. As visitors move between the spaces, they either signal their wish to receive a fragment by opening a door in a forest of birdcages, or they co-construct story fragments using a construction interface that resembles a child's pocket puzzle. As the visitors move between the two spaces, the installation invites them to reflect on an essential attribute of communication: that one cannot be at once the prime creator/sender and a prime receiver of a message or story. Creation changes the opportunity for the message or story to surprise. A similar construction and receiving interface can be found on the WWW.\n                                 \nThis installation contradicts the passive-receptive reverie traditionally associated with the cinematic experience.  Moving through the space in the DeCordova Museum, or playing on the virtual site, the visitor is confronted with a subtle call to action. Without action, there is no story created and no story received.</p>", "people": ["barbara@media.mit.edu", "gid@media.mit.edu"], "title": "Flights of Fantasy", "modified": "2016-12-05T00:16:25.385Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "", "groups": ["interactive-cinema"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "flights-of-fantasy"}, {"website": "", "description": "<p>Fluid Voice is a group communication platform that allows both real-time and asynchronous group activity participation. Users can form local audio broadcast networks for group discussion, similar to a conference call. Disconnected group members can be included in these discussions asynchronously: the audio is recorded and sent to them opportunistically. In this way, membership of a group persists in the face of disconnection caused by communication failures and mobility. Our platform supports other asynchronous applications: polling and voting, sharing of wish lists, and text messaging.  We anticipate extending Fluid Voice to integrate messaging with environmental displays.</p>", "people": ["lip@media.mit.edu"], "title": "Fluid Voice", "modified": "2016-12-05T00:16:25.497Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-495", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "fluid-voice"}, {"website": "", "description": "<p>ReflectOns are objects that help people think about their actions and change their behavior based on subtle, ambient nudges delivered at the moment of action. Certain tasks, such as figuring out the number of calories consumed, or amount of money spent eating out, are generally difficult for the human mind to grapple with. By using in-place sensing combined with gentle feedback and understanding of users' goals, we can recognize behaviors and trends, and provide a reflection of their own actions tailored to enable both better understanding of the repercussions of those actions, and changes to their behaviors to help them better match their own goals.</p>", "people": ["sajid@media.mit.edu", "pattie@media.mit.edu"], "title": "ReflectOns: Mental Prostheses for Self-Reflection", "modified": "2018-12-04T20:41:09.428Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "reflectons-mental-prostheses-for-self-reflection"}, {"website": "", "description": "<p>FocalSpace is a system for focused collaboration utilizing spatial depth and directional audio. We present a space where participants, tools, and other physical objects within the space are treated as interactive objects that can be detected, selected, and augmented with metadata. Further, we demonstrate several scenarios of interaction as concrete examples. By utilizing diminishing reality to remove unwanted background surroundings through synthetic blur, the system aims to attract participant attention to foreground activity.</p>", "people": ["liningy@media.mit.edu", "ishii@media.mit.edu"], "title": "FocalSpace", "modified": "2016-12-05T00:16:25.453Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "focalspace"}, {"website": "", "description": "<p>Flow is an augmented interaction project that bridges the divide between our non-digital objects and items and our ecosystem of connected devices. By using computer vision, Flow enables our traditional interactions to be augmented with digital meaning, thus allowing an event in one environment to flow into the next. Through this, physical actions such as tearing a document can have a mirrored effect and meaning in our digital environment, leading to actions such as the deletion of the associated digital file. This project is part of an initial exploration that focuses on creating an augmented interaction overlay for our environment, enabling users to redefine their physical actions.</p>", "people": ["holtzman@media.mit.edu"], "title": "Flow", "modified": "2016-12-05T00:16:25.480Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "flow"}, {"website": "", "description": "<p>NGO2.0 is a project grown out of the work of MIT's New Media Action Lab. The goal of NGO2.0 is to strengthen the digital and social media literacy of Chinese grassroots NGOs. &nbsp;Learn more at&nbsp;<a href=\"http://www.ngo20.org/\">www.ngo20.org</a>.</p><p><i>Project Staff:&nbsp;Jing Wang, Yu Wang, Su Han, and Huan Sun</i></p><p>Since 2009, the project has established collaborative relationships with IT corporations, universities, and city-based software developers' communities to advocate the development of a new brand of public interest sector that utilizes new media and nonprofit technology to build a better society. NGO2.0 addresses three major need categories of grassroots NGOs: communication, resources, and technology. Within each category, NGO2.0 developed and implemented online and offline projects. These include: Web 2.0 training workshops, Web 2.0 toolbox, a crowdsourced philanthropy map, news stories and videos for NGOs, crowd funding project design, NGO-CSR Partnership Forum, database of Chinese NGOs, and online survey of Chinese NGOs' Internet usage.</p><p>See <a href=\"http://www.ngo20map.com\">the crowdsourced philanthropy map</a>.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "NGO2.0", "modified": "2018-08-27T20:27:58.872Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "ngo20"}, {"website": "", "description": "<p>Frame It is an interactive, blended, tangible-digital puzzle game intended as a play-centered teaching and therapeutic tool. Current work is focused on the development of a social-signals puzzle game for children with autism that will help them recognize social-emotional cues from information surrounding the eyes. In addition, we are investigating if this play-centered therapy results in the children becoming less averse to direct eye contact with others. The study uses eye-tracking technology to measure gaze behavior while participants are exposed to images and videos of social settings and expressions. Results indicate that significant changes in expression recognition and social gaze are possible after repeated uses of the Frame It game platform.</p>", "people": ["micahrye@media.mit.edu", "picard@media.mit.edu"], "title": "Frame It", "modified": "2016-12-05T00:16:25.626Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "frame-it"}, {"website": "", "description": "<p>The Free City project promotes awareness and facilitates access to free and low-cost events, services and  opportunities that are locally available.  By using the technologies in our What's Up toolkit, we aim to reduce the information gap, foster social  connectivity and unleash the learning potential of urban centers,  contributing to the development of cities that more educated, sustainable, inclusive and democratic.  </p>", "people": ["ethanz@media.mit.edu", "leob@media.mit.edu"], "title": "Free City", "modified": "2016-12-05T00:16:25.643Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "free-city"}, {"website": "", "description": "<p>The FreeD is a hand-held, digitally controlled milling device that is guided and monitored by a computer while still preserving the craftsperson's freedom to sculpt and carve. The computer will intervene only when the milling bit approaches the planned model. Its interaction is either by slowing down the spindle speed or by drawing back the shaft; the rest of the time it allows complete freedom, letting the user to manipulate and shape the work in any creative way.</p>", "people": ["joep@media.mit.edu"], "title": "FreeD", "modified": "2016-12-05T00:16:25.664Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "freed"}, {"website": "", "description": "<p>With the arrival of mobile phones, the concept of calling has moved from calling a place to calling a person, which has quite distinctive affordances. Frontdesk proposes a place-based communication tool that is accessed primarily through any mobile device and features voice calls and text chat. The application uses \ufffdplace\ufffd loosely to define a physical space created by a group of people that have a shared context of that place. Examples of places could be different parts of a workspace in a physical building, such as the machine shop, caf\ufffd, or Speech + Mobility group area at the Media Lab. When a user calls any of these places, frontdesk routes their call to all people that are \ufffdchecked-in\ufffd to that place.</p>", "people": ["geek@media.mit.edu"], "title": "frontdesk", "modified": "2016-12-05T00:16:25.682Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "frontdesk"}, {"website": "", "description": "<p>Full-Contact Poetry is a digital play space for children's poetic expression. It is a software environment in which children can express their poetic thoughts, create their interpretations of writing by others and also share these expressions. The environment combines ideas from literary theory and analysis with constructionism to extend tools for poetic expression. Children can experience poetry by playing with words as objects, experimenting with typographic effects, moving words through space and navigating into and through the text, while also being able to incorporate and reconfigure sound and image.</p>", "people": ["cavallo@media.mit.edu"], "title": "Full-Contact Poetry", "modified": "2016-12-05T00:16:25.707Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-320", "groups": ["future-of-learning-2"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "full-contact-poetry"}, {"website": "", "description": "<p>Computer-generated holographic images typically sacrifice image variations with up-and-down viewer motion (vertical parallax). However, there are several applications that require both horizontal and vertical parallax; this considerably complicates the design of hologram recording technology. This project studies one- and two-optical-step methods for producing full-parallax, hard-copy holograms of digital data.</p>", "people": [], "title": "Full-Parallax Synthetic Holograms", "modified": "2016-12-05T00:16:25.733Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-420", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "full-parallax-synthetic-holograms"}, {"website": "", "description": "<p>We are exploring how to navigate large information spaces naturally, quickly, and seamlessly between graphical and physical space using body and gesture. The goal is to create an interface where users can manipulate digital space as if they were using telekinesis.</p>", "people": ["daniell@media.mit.edu", "ishii@media.mit.edu"], "title": "g-stalt", "modified": "2016-12-05T00:16:25.755Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-368", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "g-stalt"}, {"website": "", "description": "<p>We study augmentation of the creative writing process through a new text editor. This text editor presents short-length snippets from books and Internet sources such as blogs and Twitter in the moment of the writing process. The snippets are selected in such a way as to be relevant to the content that the user is trying to produce. This text editor explores a number of ideas including: Is it possible to augment creativity by introducing just-in-time information? Is it more efficient to consume content in the moment of creation? Is it possible to develop a notion of \"books\" that do not have to be consumed in a serial process?</p>", "people": ["pattie@media.mit.edu"], "title": "Text Editor for Augmented Writing", "modified": "2018-12-04T20:49:33.011Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "text-editor-for-augmented-writing"}, {"website": "", "description": "<p>The Furp (\"Future of Urban Planning\") project exists as a first step toward disseminating the Luminous Room project into the world at large. We are collaborating with MIT Professor Eran Ben-Joseph in using a customized version of the Urp system (see Luminous Room) in his Site and Urban Systems Planning class. The system has recently made its in-class debut, and is now undergoing intensive week-by-week modifications and refinements in response to what's being learned about its real-world usability. Our short-term goal is to transform Urp from a \"proof-of-concept\" system into a tool sufficiently convenient and facile to permit ongoing and casual use in a classroom. Longer-term goals  include extrapolation from these class-centered experiments to a better understanding of which modalities and contexts are appropriate for broader  applications for luminous-tangible interaction techniques.</p>", "people": ["ishii@media.mit.edu", "jh@media.mit.edu"], "title": "Furp", "modified": "2016-12-05T00:16:25.842Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "furp"}, {"website": "", "description": "<p>We are exploring new methods of call setup, allowing the originating party to participate in the decision as to whether to alert the called party. A small community of trusted callers will have the ability to \"listen in\" to the called party's office or location (see Nomadic Radio), with this audio modified to convey context and speaker identity, but not content.  The caller can then decide whether to interrupt or leave a message.</p>", "people": ["geek@media.mit.edu", "stefanm@media.mit.edu"], "title": "Garblephone", "modified": "2016-12-05T00:16:25.940Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-344", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2000-09-01", "slug": "garblephone"}, {"website": "", "description": "<p>We are building a prototype infrastructure for real-time collection and distribution of comprehensive information about road traffic and conditions on a network of streets. Small (2 cm diameter) cylinders placed just under the road surface count passing vehicles by detecting magnetic-field disturbances. Periodically, these sensors transmit an updated count by radio to a regional repeater, which relays them to a central processing station. Each package, which costs about $40, is powered by a single lithium battery, and can be installed in a few minutes without tearing up the road. The central station uses the sensor data to compute queue-length and departure-rate statistics for each road segment, and uses Little's Formula to assign a time in seconds to each road link. This database is published once a minute on the Internet. Vehicles will use CDPD or a similar wireless network standard to download the relevant sections of the database, and plan optimum (shortest-time or least-fuel) routes through the road network, informing the driver of suggested route changes in real time. We will also explore adding other minimally expensive sensors to this package, such as piezo pickups for detecting road vibration, and perhaps accident signatures and temperature and humidity sensors to determine local road conditions.</p>", "people": ["joep@media.mit.edu"], "title": "Sensate Roadbeds", "modified": "2016-12-05T00:16:25.964Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-357", "groups": ["responsive-environments", "cc"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "sensate-roadbeds"}, {"website": "", "description": "<p>This research aims to classify gene expression data sets into different categories, such as normal vs. cancer. The main challenge is that thousands of genes are measured in the micro-array data, while only a small subset of genes are believed to be relevant for disease classification. We have developed a novel approach called \"predictive automatic relevance determination;\" this method brings Bayesian tools to bear on the problem of selecting which genes are relevant, and extends our earlier work on the development of the \"expectation propagation\" algorithm. In our simulations, the new method outperforms several state-of-the-art methods, including support-vector machines with feature selection and relevance-vector machines.</p>", "people": ["picard@media.mit.edu"], "title": "Gene Expression Data Analysis", "modified": "2016-12-05T00:16:26.009Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "gene-expression-data-analysis"}, {"website": "", "description": "<p>The genieBottles system presents a story that is told by three genies that live in glass bottles. When a bottle is opened, the genie contained inside is released and begins to talk to the user. If several genies are released at once, they converse with each other. The physical bottles are  graspable \"containers\" and \"controls\" for the digital story information. The genieBottles use a simple state transition model for interactive storytelling, in which the system plays back the appropriate segment of audio depending on the state it is in, as well as the appropriate segment(s) of audio to transition from one state to another.\n                                 \nThe overall form of the genieBottles story is quite abstract, and does not have a highly structured narrative progression or plot. When users interact with the system, they capture the genies at a particular moment in time, during which they talk about their state of being in bottles, about their pasts, and about their expectations or desires for the future. The physical interface is tightly linked to, and limits, the form of the story: the play-out (i.e., the time of the interaction) corresponds to a brief segment in the genies' lives in which they chat and allude to a larger and more complex story. The interaction time thus constrains the plot time and hence the narrative.</p>", "people": ["ishii@media.mit.edu"], "title": "genieBottles", "modified": "2016-12-05T00:16:26.044Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "geniebottles"}, {"website": "", "description": "<p>GeoSCAPE is a reconstructive tool for capturing measurement data in field archaeology and for facilitating a 3-D visualization of an excavation rendered in computer graphics. This project is carried out by extending a recently developed orientation-aware digital measuring tape, HandSCAPE, which has been examined to address the efficiency of bridging measuring and modeling for on-site application areas. GeoSCAPE allows the user to navigate the archaeological excavation site with virtually immediate access to excavation information on-site. The timely and accurate result would verify clear interpretations while the excavator obtains increments of information on-site. This will improve collaboration between on-site and laboratory archaeological research.</p>", "people": ["ishii@media.mit.edu"], "title": "GeoScape", "modified": "2016-12-05T00:16:26.068Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "geoscape"}, {"website": "", "description": "<p>We are all equipped with two extremely expressive instruments for performance: the body and the voice. By using computer systems to sense and analyze human movement and voices, artists can take advantage of technology to augment the body's communicative powers. However, the sophistication, emotional content, and variety of expression possible through original physical channels is often not captured by the technologies used for analyzing them, and thus cannot be intuitively transferred from body to digital media. To address these issues, we are developing systems that use machine learning to map continuous input data, whether of gesture or voice, to a space of expressive, qualitative parameters. We are also developing a new framework for expressive performance augmentation, allowing users to create clear, intuitive, and comprehensible mappings by using high-level qualitative movement descriptions, rather than low-level descriptions of sensor data streams.</p>", "people": ["ejessop@media.mit.edu", "tod@media.mit.edu"], "title": "Gestural Media Framework", "modified": "2016-12-05T00:16:26.104Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "gestural-media-framework"}, {"website": "", "description": "<p>Have you ever been in the middle of a conversation and needed to share your location with the other party? Geo.gy is a location shortener service. It allows you to easily share your location with your peers by encoding it in a short URL which we call a \"geolink.\" It is platform-independent, and based on HTML5, so you can use any device with a modern browser to generate a geolink, simply by visiting the project's page. There are no user accounts, so geolinks remain anonymous. You can use Geo.gy to add location context to a post, SMS, anything you want decorated with location context.</p>", "people": ["ypod@media.mit.edu", "lip@media.mit.edu"], "title": "Geo.gy: Location Shortener", "modified": "2016-12-05T00:16:26.130Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "geogy-location-shortener"}, {"website": "", "description": "<p>Using the Preschool of the Future environment, children can create stories that come to life in the real world. We are developing interfaces that enable children to author stories in the physical environment\u2013stories where robots are the characters and children are not only the observers, but also the choreographers and actors in the stories. To do this, children author stories and robot behaviors using a simple digital painting interface. By combining the physical affordances of painting with digital media and robotic characters, stories can come to life in the real world. Programming in this environment becomes a group activity when multiple children use these tangible interfaces to program advanced robot behaviors.</p>", "people": ["cynthiab@media.mit.edu"], "title": "Storytelling in the Preschool of the Future", "modified": "2018-03-29T17:09:20.173Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "storytelling-in-the-preschool-of-the-future"}, {"website": "", "description": "<p>An open publishing platform for visualization, social sharing, and data analysis of geospatial data.</p>", "people": ["ishii@media.mit.edu"], "title": "GeoSense", "modified": "2016-12-05T00:16:26.218Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "geosense"}, {"website": "", "description": "<p>The Gesture Ball explores the area of human-computer interaction by implementing a system wherein the human user will be able to use a gesture-based interface to play music. By creating and selecting motions, the user can manipulate the Gesture Ball's music. The Gesture Ball will learn the user's mental model of how such an interface should work, and modify itself to meet those expectations.</p>", "people": [], "title": "Gesture Ball", "modified": "2016-12-05T00:16:26.252Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "gesture-ball"}, {"website": "", "description": "<p>Gestures Everywhere is a multimodal framework for supporting ubiquitous computing. Our framework aggregates the real-time data from a wide range of heterogeneous sensors, and provides an abstraction layer through which other ubiquitous applications can request information about an environment or a specific individual. The Gestures Everywhere framework supports both low-level spatio-temporal properties, such as presence, count, orientation, location, and identity; in addition to higher-level descriptors, including movement classification, social clustering, and gesture recognition.</p>", "people": ["joep@media.mit.edu"], "title": "Gestures Everywhere", "modified": "2016-12-05T00:16:26.302Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "gestures-everywhere"}, {"website": "", "description": "<p>Getting Hands-On with Soft Circuits is a set of instructional materials which seeks to expose middle and high school students to the creative, expressive, and computationally engaging domain of e-textiles. Engaging in hands-on activities, such as creating soft, electronic textile (e-textile) circuits, is one promising path to building self-efficacy and scientific understanding \ufffd both of which can have a dramatic impact on diversity in the field of computing. The instructional materials include a workshop activity guide and an accompanying kit of low-cost craft and electronic components. </p>", "people": ["leah@media.mit.edu"], "title": "Getting Hands-On with Soft Circuits", "modified": "2016-12-05T00:16:26.336Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "getting-hands-on-with-soft-circuits"}, {"website": "", "description": "<p>What if you could see what the past looked like from where you are standing? What if you could relive any event that happened at your current location? Rather than just reading about an event, we want to be immersed in it and experience it ourselves. Ghosts of the Past allows you to create, save, and geotag panoramic canopies. Anyone who subsequently visits that space can see what you have seen, joining with you to create time-lapsed socialization. Since each canopy is time-stamped and geotagged, it gives the user an anchor in space while they explore history. Any event, special or mundane, can be captured for anyone in the same location to view. QR codes are posted in building locations with an active canopy.</p>", "people": ["holtzman@media.mit.edu", "lip@media.mit.edu"], "title": "Ghosts of the Past", "modified": "2016-12-05T00:16:26.419Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["viral-communications", "information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "ghosts-of-the-past"}, {"website": "", "description": "<p>This work will present case studies of how people can build understanding of some ideas in motion control by mediating their experiential knowledge about body motion using interactive sensing and control computational environments. The computational tools are designed to attempt to draw upon how people express their understanding of the physical experience and create bridges to more formal understanding and representation of the phenomena. The goal of this case study is to demonstrate that although the essential importance of formal representation cannot be doubted, it can be shown that expert practices, in fact, involve other non-formal forms of knowledge as well. This awareness, in a learning context, implies that a deep understanding of formal knowledge is developed not only by learning the formal descriptions in isolation, but by incorporating them with other forms of understanding as well.</p>", "people": ["cavallo@media.mit.edu"], "title": "Giving the Head a Hand", "modified": "2016-12-05T00:16:26.506Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-368", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "giving-the-head-a-hand"}, {"website": "", "description": "<p>We address two critical elements of news: that it informs, and that it is trustworthy. Glance creates dynamic, real-time, semantic control over news presentation that reveals the inherent slant that underlies coverage of an event. The goal is to empower readers to understand their news intake through a visualization of metadata that empowers readers to choose their news source based on computed metrics rather than sensationalized headlines. Relevant additional information, such as sentiment of text and public reaction, is gathered on each topic to further give readers a richer news-scape.</p>", "people": ["vdiep@media.mit.edu", "lip@media.mit.edu"], "title": "Glance", "modified": "2016-12-05T00:16:26.560Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "glance"}, {"website": "", "description": "<p>Globetoddler is a new project that focuses on remote awareness and shared experiences from children's point of view. The project aims to develop a technology and software solution that makes it easier, as well as more fun and rewarding, for young children (3-6 years) to interact with their remote, traveling parents. Newly conducted interviews indicate a need for a system with very different interfaces and interaction styles on the the two sides: on the parent's side the system consists of a fairly simple mobile application, whereas the child's side consists of a combination of a physically interactive toy interface and a guiding virtual avatar. </p>", "people": ["geek@media.mit.edu"], "title": "Globetoddler", "modified": "2016-12-05T00:16:26.632Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-384C", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "globetoddler"}, {"website": "", "description": "<p>Every country has a brand, negative or positive, and that brand is mediated in part by its global press coverage. We are measuring and ranking the perceptions of the 20 most populous countries by crowdsourcing those perceptions through a \"World News Quiz.\" Quiz-takers match geographically vague news stories to the countries they think they occurred in, revealing how they positively or negatively perceive them. By illustrating the way these biases manifest among English and Chinese speakers, we hope to help news consumers and producers be more aware of the incomplete portrayals they have internalized and propagated.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "erhardt@media.mit.edu"], "title": "Global Brands", "modified": "2016-12-05T00:16:26.667Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2015-09-15", "slug": "global-brands"}, {"website": "", "description": "<p>GlobalMind is a network of multilingual/multicultural common-sense databases. Now we present how GlobalMind can enhance the communication between different cultures and different languages. Intercultural Assistant is a mobile application of GlobalMind.</p>", "people": [], "title": "GlobalMind: Intercultural Assistant", "modified": "2016-12-05T00:16:26.690Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-309", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "globalmind-intercultural-assistant"}, {"website": "", "description": "<p>Globuddy is a common-sense-based application that helps English-speakers  to communicate in a foreign language. In addition to translating text, Globuddy provides relevant words and concepts for the traveler's specific situation. Common-sense reasoning is done with OMCSNet. Because users need a translator when they are on the street and away from their computers, we are using handheld devices like cell phones and PDAs.</p>", "people": ["lieber@media.mit.edu"], "title": "Globuddy", "modified": "2016-12-05T00:16:26.710Z", "visibility": "PUBLIC", "start_on": "2002-09-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "globuddy"}, {"website": "", "description": "<p>Glume is a computationally enhanced translucent modeling medium which offers a generalized, modular, scalable platform with the physical immediacy of a soft and malleable tangible material. The Glume system consists of soft and translucent augmented interlocking modules (each embedded with a full-spectrum LED) which communicate capacitively to their neighbors to determine a network topology and are responsive to human touch. We envision Glume as a viable tool for modeling, visualization, and simulation of three-dimensional data sets in which users construct and manipulate models whose morphology is determined through the distributed system. The Glume system provides a novel means for expressing and investigating organic forms and processes not possible with existing materials by relaxing the rigidity of structure in previous solid building-block approaches.</p>", "people": ["ishii@media.mit.edu"], "title": "Glume", "modified": "2016-12-05T00:16:26.785Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "glume"}, {"website": "", "description": "<p>Wearable devices and ambient displays need to atomize video to evocative excerpts as glanceable as a still image. Glyph is a web-based tool for generating expressive GIFs from video. The tool integrates scene detection, video stabilization, video manipulation, and loop detection into a simple, web-based authoring interface. Glyph allows for creating GIFs from video with more editorial control than just choosing a clip's start and end time\ufffddiminishing some regions of movement in the clip, and highlighting others; erasing a jarring jump between the start and end of the GIF; imbuing a still image with just enough dynamism to hold our eyes and pique our interest. The result is a subtle, dynamic moving image that's lightweight, transmissible, and immediately engaging.</p>", "people": ["lip@media.mit.edu"], "title": "Glyph", "modified": "2016-12-05T00:16:26.820Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "glyph"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: normal;\">Vast regions of the world are unmapped by com\u00admercial services, and communities living there are digitally invisible. Visible Communities is a system that combines what local people using smartphones see on the ground with what computers can detect from satellite images, to create an interactive map at a fine resolution that continuously improves. The map captures both spatial and social data: houses and the paths connecting them, and the households living there and their relationships.</span><br></p><p>Enabling communities to put themselves on the map is a powerful way to increase their own visibility, and in turn serves institutional needs to improve infrastructure planning and humanitarian aid delivery. Existing approaches to do community-driven mapping either require outside experts to facilitate, or the results are lower-tech and not easy to keep up to date. In collaboration with Partners in Health (PIH), and supported by the MIT Tata Center, we are piloting this social machine in a sparsely populated, hilly region with a Community Health Worker (CHW) network in Burera, Rwanda.</p><p>The smartphone app enables CHWs to self-map their communities. We are intentionally designing an intuitive pre-literacy touch interface, enabling a wide range of users to participate without training. By removing barriers for people at the base of the socio-economic pyramid and designing with social dynamics in mind, we hope to unlock existing, self-motivated human potential.</p>", "people": ["dkroy@media.mit.edu", "koehrsen@media.mit.edu", "schaad@media.mit.edu"], "title": "Visible Communities", "modified": "2017-10-16T15:35:47.765Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2017-05-31", "slug": "visible-communities"}, {"website": "", "description": "<p>The Smart Cities and Cognitive Machines groups have teamed up with the Center for Future Banking to design a concept banking store in the Boston/Cambridge area. This will be a fully functional banking center that simultaneously serves as a living laboratory\u2013a place where new technologies and interior configurations can quickly be installed, electronically monitored (unobtrusively, and with due concern for privacy) to evaluate their effectiveness in use under demanding real-world conditions, and iteratively redesigned in response to this feedback. Utilizing the Media Lab's extensive expertise in sensing, data collection, management and analysis of large-scale datasets, and data visualization, we will be able to create an adaptive environment that embodies a robotic cognitive architecture capable of intelligently responding to the occupants and visitors to the building. Architecturally, the flagship should vividly represent commitments to effective engagement with the community that it serves, sustainability, and forward-looking innovation.</p>", "people": ["dkroy@media.mit.edu", "rchin@media.mit.edu"], "title": "Beacon Concept Store with the Center for Future Banking", "modified": "2018-03-29T17:09:43.769Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-001", "groups": ["smart-cities", "social-machines"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "beacon-concept-store-with-the-center-for-future-banking"}, {"website": "", "description": "<p>GrabIt allows users to collaboratively explore the principles of power generation, transfer, and consumption via  tangible wooden sticks. Pushing on one stick generates power that is transfered to other sticks in real time allowing users to see, and feel, the effects of losses and electrical loading. All energy in the GrabIt system is derived from its users and therefore makes the mapping between input, output, and losses transparent. Variable-color light bulbs can also be connected to the system to serve as simplified voltage indicators during use.</p>", "people": ["ishii@media.mit.edu"], "title": "GrabIt", "modified": "2016-12-05T00:16:26.848Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "grabit"}, {"website": "", "description": "<p>When friends give directions, they often don't describe the whole route, but instead provide landmarks along the way that they think will be familiar. Friends can assume we have certain knowledge because they know our likes and dislikes. Going My Way attempts to mimic a friend by learning about where you travel, identifying the areas that are close to the desired destination from your frequent path, and picking a set of landmarks to allow you to choose a familiar one. When you select one of the provided landmarks, Going My Way will provide directions from it to the destination.</p>", "people": ["geek@media.mit.edu"], "title": "Going My Way", "modified": "2016-12-05T00:16:26.894Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-384C", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "going-my-way"}, {"website": "", "description": "<p>Got Sleep? is an Android application to help people to be aware of their sleep-related behavioral patterns and tips about how they should change their behaviors to improve their sleep. The application evaluates people's sleep habits before they start using the app, tracks day and night behaviors, and provides feedback about what kinds of behavior changes they should make and whether the improvement is achieved or not. </p>", "people": ["picard@media.mit.edu", "akanes@media.mit.edu"], "title": "Got Sleep?", "modified": "2016-12-05T00:16:26.921Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["affective-computing", "advancing-wellbeing"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "got-sleep"}, {"website": "", "description": "<p>Graffiti Codes transform the space around you into a mobile-readable environment.   Anyone can draw a simple shape on anything, like graffiti, and the mobile device reads it by simply tracing the outline.  It's a human-created VR code. This work diverges from the camera-scanning model and uses accelerometer-based paths to unlock data. Where a QR code cannot be easily generated in the field, Graffiti Codes only require a marker and a surface.</p>", "people": ["jlrubin@media.mit.edu", "lip@media.mit.edu"], "title": "Graffiti Codes", "modified": "2016-12-05T00:16:26.946Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "graffiti-codes"}, {"website": "", "description": "<p>With the Marketing and the Placebo Effect project, we are investigating how various economic, cultural, and social factors such as pricing, branding, or stigma could affect how people will experience their medications.</p>", "people": [], "title": "grassroots mapping", "modified": "2016-12-05T00:16:27.036Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-485A", "groups": [], "published": true, "active": false, "end_on": "2009-09-01", "slug": "grassroots-mapping"}, {"website": "", "description": "<p>GroundTruth is a dynamic online map of changing topographic structures (roads, checkpoints, barriers, and settlements) and human narratives in the Occupied Palestinian Territory. This community-driven map is populated with multiple data layers collected by human-rights organizations and a network of informed inhabitants. This community and crisis mapping effort aims to unite disparate datasets and provide a more comprehensive picture of changing restrictions and events. We believe this \ufffdground truth,\ufffd correlating spatial mapping with daily human experience, is crucial to understanding the changing nature of the occupation and enabling cooperative action by organizations and individuals in the region.</p>", "people": ["csik@media.mit.edu"], "title": "GroundTruth", "modified": "2016-12-05T00:16:27.125Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-001", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "groundtruth"}, {"website": "", "description": "<p>TellTale is a story construction kit whose goal is to help young children create and experiment with the structure and content of personal narratives. The design consists of a number of modular body components and one head piece used to record and play audio segments created by a child, or by several children. The body parts can contain stories or story fragments that can be combined in different orders to create new narrative configurations, letting children experiment with plot, transitions, endings, and beginnings\ufffdbasically anything they can imagine. In essence, it is a \"tangible audio processor\" for children who have stories to tell but who might not yet have the skills necessary to communicate their ideas in writing. TellTale's design will evolve as we explore with children how storytelling and personal narrative expression technologies of this sort can encourage the development of literacy skills.</p>", "people": [], "title": "TellTale", "modified": "2016-12-05T00:16:27.072Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-320", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "telltale-2"}, {"website": "", "description": "<p>GuideShoes is a wearable system that explores how aesthetic forms of expression can deliver both an aesthetic experience and background information simultaneously while walking around a city. Incorporating GPS, wireless spread-spectrum technology, and a MIDI synthesizer, the system tracks the user, and selects and assembles small musical patterns dynamically dependent on the user's progress. Statistically significant results of user testing have shown that users that listened to the musical cues provided by the GuideShoes system were able to extract navigational signals, such as \"continue\" or \"you made a wrong turn\" from the musical experience.</p>", "people": ["gid@media.mit.edu"], "title": "GuideShoes", "modified": "2016-12-05T00:16:27.176Z", "visibility": "PUBLIC", "start_on": "1997-01-01", "location": "E15-368", "groups": ["interactive-cinema"], "published": true, "active": false, "end_on": "1999-09-01", "slug": "guideshoes"}, {"website": "", "description": "<p>Both professional and hobbyist musicians rely heavily on their computers to make music, and usually find themselves with hard drives full of samples. The majority are individual drum samples, often called \"hits\" or \"one shots.\" Arranging these samples in folders is usually done manually, by listening to each sample and moving it into a desired folder. While making music, retrieval of these samples is done, once more, by tedious auditions of each and every one. This project is a first step towards making the life of the computer-based musician a little bit easier by automatically classifying these samples and allowing better methods of retrieval.</p>", "people": ["tod@media.mit.edu"], "title": "Automatic Drum Sample Classification", "modified": "2016-12-05T00:16:27.276Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "automatic-drum-sample-classification"}, {"website": "", "description": "<p>Guiding Light is a navigation-based application that provides directions by projecting them onto physical spaces both indoors and outdoors. It enables a user to get relevant spatial information by using a mini projector in a cell phone. The core metaphor involved in this design is that of a flashlight which reveals objects in and information about the space it illuminates. For indoor navigation, Guiding Light uses a combination of e-compass, accelerometer, proximity sensors, and tags to place information appropriately. In contrast to existing heads-up displays that push information into the user's field of view, Guiding Light works on a pull principle, relying entirely on users' requests and control of information.</p>", "people": ["geek@media.mit.edu"], "title": "Guiding Light", "modified": "2016-12-05T00:16:27.328Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "guiding-light"}, {"website": "", "description": "<p>With CogNotes, new music composition tools become platforms for cognitive assessment. CogNotes users engage their memory as they emerge as composers and participants in their own health process. Together with partners the Lincoln Park Performing Arts School and the Yamaha Corporation, a group of seniors are undertaking an extensive multi-month music composition workshop built around Tod Machover's Hyperscore program. The program is outfitted with cognitive measures sensitive to the earliest transition to Alzheimer's Disease, validated as part of post-doc Adam Boulanger's research with Harvard Medical School and the Alzheimer's Association. Disease assessment can be part of your everyday, creative, and rewarding life. You can be the manager of your own health information as part of the activities you love.</p>", "people": ["tod@media.mit.edu"], "title": "CogNotes: Cognitive Assessment in Social Media-Enabled Creativity Tools", "modified": "2016-12-05T00:16:27.456Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "cognotes-cognitive-assessment-in-social-media-enabled-creativity-tools"}, {"website": "", "description": "<p>Senseboard is one facet of our using tangible media for manipulating abstract information. It allows the user to arrange small magnetic pucks on a grid, where each puck represents a piece of information to be organized, such as a message, file, bookmark, citation, presentation slide, movie scene, or newspaper story. As the user manipulates the physical puck, the corresponding digital information is projected onto the board. Special pucks may be placed\non the board to execute commands or request additional information. We seek to combine the benefits of physical manipulation (natural, fluid, rapid, two-handed, multi-person interaction) with the benefits we can get from computer augmentation (interactive commands, functions, queries, operations, importing and exporting data, and remote collaboration). We believe this type of interface is thus more effective for tasks involving organizing, grouping, and manipulating types of information that have no inherent physical representation, and it provides an example of a tangible interface for a typical \"knowledge worker\" task.</p>", "people": ["ishii@media.mit.edu", "jpatten@media.mit.edu"], "title": "Senseboard", "modified": "2016-12-05T00:16:27.366Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "senseboard"}, {"website": "", "description": "<p>The goal of this project is to produce a guilt detector. We have created an experiment that is designed to produce feelings of guilt of varying levels in different groups while we record EKG and skin conductivity. By examining the differences in physiology across the conditions, we have\nexlored how one might build a classifier to determine which condition, and thus which level of guilt, an individual is experiencing.</p>", "people": ["picard@media.mit.edu"], "title": "Guilt Detection", "modified": "2016-12-05T00:16:27.574Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "Cube", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "guilt-detection"}, {"website": "", "description": "<p>Discourse on hackathons tends to emphasize projects and project creators rather than the events as a social practice within existing communities. Hackathons have a history as a community-building method for education and creation. More recently, institutions have used hackathons to invite conversation and design with groups affected by those institutions. This step towards broader participation is obfuscated by stories that focus on the creation of products and the lucky geniuses whose work is appropriated by institutions. Critiques of hackathons often accept the same assumptions, focusing on high-profile events, critiquing the small number of sustained projects, and questioning hackathons as a form of entrepreneurial free labor.</p>", "people": ["ethanz@media.mit.edu", "jnmatias@media.mit.edu", "bl00@media.mit.edu"], "title": "HackathonFAQ", "modified": "2016-12-05T00:16:27.595Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "hackathonfaq"}, {"website": "", "description": "<p>ARIA (Annotation and Retrieval Integration Agent) is a software agent that acts as an assistant to a user writing email or Web pages. As the user types a story, it does continuous retrieval and ranking on a photo database. It can use descriptions in the story text to semi-automatically annotate pictures based on how they are used. This project is improving ARIA's automatic annotation capabilities through world-aware semantic understanding of the text; making photo retrieval more robust by using Open Mind Common Sense to make semantic connections between the story text and annotations; and learning personal common sense through the text that can then be used to improve photo retrieval by enabling personalized semantic connections.</p>", "people": ["lieber@media.mit.edu"], "title": "Common-Sense and Semantic Understanding for ARIA", "modified": "2016-12-05T00:16:27.796Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "common-sense-and-semantic-understanding-for-aria"}, {"website": "", "description": "<p>Providing computers and Internet access to rural communities can help overcome barriers of geographic isolation, lack of services, and poor infrastructure. However, the technologies themselves can create an entirely new set of barriers around language, literacy, and usability. Community Knowledge Sharing aims to provide a single multi-literate environment for communities to share experiences and learn from each other.</p>", "people": [], "title": "Community Knowledge Sharing", "modified": "2016-12-05T00:16:27.822Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-391", "groups": ["edevelopment", "e-markets"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "community-knowledge-sharing"}, {"website": "", "description": "<p>Computer workstations can provide a much-needed user interface to advanced telephony functions, provided a path exists between the workstation and switch. Controlling call set-up from a user\ufffds workstation allows a greater degree of personalization and dynamic call-handling, both for outgoing and incoming calls. This project is being implemented in the ISDN environment of MIT\ufffds campus telephone network, using PhoneServer, a computer network interface to Basic Rate ISDN switching.</p>", "people": ["geek@media.mit.edu"], "title": "Computers and Telephony", "modified": "2016-12-05T00:16:27.645Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "computers-and-telephony"}, {"website": "", "description": "<p>How best to govern society and promote cooperation is a centuries-old debate: is cooperation best maintained by a central authority, or is it better handled by more decentralized forms of governance? Using mathematical models, we show that when some actors can bribe a powerful centralized authority, they can completely undermine cooperation in society. Counterintuitively, a weaker centralized authority is more effective because it allows peer punishment to restore cooperation in the presence of corruption. Our results help explain why citizen participation is a fundamental necessity for policing the commons.</p><p><b>Scientific writings:<br></b></p><p>S. Abdallah, R. Sayed, I. Rahwan, B. LeVeck, M. Cebrian, A. Rutherford, J. Fowler (2014). <a href=\"http://rsif.royalsocietypublishing.org/content/11/93/20131044\">Corruption Drives the Emergence of Civil Society</a>. Journal of the Royal Society Interface. 11(93).</p><p><b>Selected Media: </b><a href=\"http://corruptionresearchnetwork.org/acrn-news/corruption-drives-the-emergence-of-civil-society\">Anti-Corruption Research Network (part of Transparency International)</a></p>", "people": [], "title": "Corruption-Resistant Cooperation: Institutions vs. Crowds", "modified": "2017-03-23T01:07:23.288Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "corruption-resistant-cooperation-institutions-vs-crowds"}, {"website": "", "description": "<p>HandSCAPE is an orientation-aware digital tape measure used as an input device for digitizing field measurements, and visualizing the volume of the resulting vectors with computer graphics. Using embedded orientation-sensing hardware, HandSCAPE captures relevant vectors on each linear measurement and transmits this data wirelessly to a remote computer in real-time. Combining physical measuring and computer modeling as a seamless step enhances the efficiency of on-site measuring tasks with the addition of digital functionality. The potential utility of HandSCAPE is for application areas such as archeological surveys, interior design, and storage-space allocation. Thus, HandSCAPE is a tangible interface that provides seamless relationships between digital and physical space, and preserves human senses and skills.</p>", "people": ["ishii@media.mit.edu"], "title": "HandSCAPE", "modified": "2016-12-05T00:16:27.913Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "handscape"}, {"website": "", "description": "<p>Competitive chess is an exciting spectator sport. It is fast-paced, dynamic, and deeply psychological. Unfortunately, most of the game's drama is only visible to spectators who are themselves expert chess players. DeepView seeks to use computational tools to make the drama of high-level chess accessible to novice viewers. There is a long tradition of software trying to beat human players at chess; DeepView takes advantage of algorithmic tools created in the development of advanced chess engines such as Deep Blue, but instead uses them to understand and explain the styles of individual players and the dynamics of a given match. It puts into the hands of chess commentators powerful data science tools that can calculate player position preferences and likely game outcomes, helping commentators to better explain the exciting human story inside every match.</p>", "people": ["gregab@media.mit.edu", "slavin@media.mit.edu"], "title": "DeepView: Computational Tools for Chess Spectatorship", "modified": "2016-12-05T00:16:27.884Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": "2015-07-01", "slug": "deepview-computational-tools-for-chess-spectatorship"}, {"website": "", "description": "<p>Truth Goggles attempts to decrease the polarizing effect of perceived media bias by forcing people to question all sources equally by invoking fact-checking services at the point of media consumption. Readers will approach even their most trusted sources with a more critical mentality by viewing content through various \"lenses\" of truth.</p>", "people": ["schultzd@media.mit.edu", "holtzman@media.mit.edu"], "title": "Truth Goggles", "modified": "2018-05-14T18:06:26.935Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "truth-goggles"}, {"website": "", "description": "<p>Individuals diagnosed with autism spectrum disorder (ASD) often have intense, focused interests. These interests, when harnessed properly, can help motivate an individual to persist in a task that might otherwise be too challenging or bothersome. For example, past research has shown that embedding focused interests into educational curricula can increase task adherence and task performance in individuals with ASD. However, providing this degree of customization is often time-consuming and costly and, in the case of computer-mediated interventions, high-level computer-programming skills are often required. We have recently designed new software to solve this problem. Specifically, we have built an algorithm that will: (1) retrieve user-specified images from the Google database; (2) strip them of their background; and (3) embed them seamlessly into Flash-based computer programs.</p>", "people": ["rmorris@media.mit.edu", "picard@media.mit.edu"], "title": "Customized Computer-Mediated Interventions", "modified": "2016-12-05T00:16:28.100Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "customized-computer-mediated-interventions"}, {"website": "", "description": "<p>A picture may be worth a thousand words, but are they the words we're looking for? The dataCam introduces the notion of associating non-visual data with  every image we capture. For every click of the shutter, the dataCam records relevant, non-visual information, such as where and when the picture was  taken, who is in it, the environmental conditions, and any other details the photographer decides are important. After processing (or downloading), the  dataCam format makes it possible to build a large database of images, searchable in non-traditional ways, such as by altitude or by the blood pressure of  the people pictured.</p>", "people": ["mike@media.mit.edu"], "title": "dataCam: Taking Pictures in a Whole New Way", "modified": "2016-12-05T00:16:28.130Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-468", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "datacam-taking-pictures-in-a-whole-new-way"}, {"website": "", "description": "<p>Driving is an ideal test bed for detecting stress in natural situations. Four types of physiological signals (electrocardiogram, electromyogram, respiration, and skin conductivity related to autonomic nervous system activation) were collected in a natural driving situation under various driving conditions. The occurrence of natural stressors was designed into the driving task and validated using driver self-report, real-time, third-party observations, and independently coded video records of road conditions and facial expression. Features reflecting heart-rate variability derived from the adaptive Bayesian spectrum estimation, the rate of skin-conductivity orienting responses, and the spectral characteristics of respiration were extracted from the data. Initial pattern-recognition results show separation for the three types of driving states: rest, city, and highway, and some discrimination within states for cases in which the state precedes or follows a difficult turn-around or toll situation. These results yielded from 89-96 percent accuracy in recognizing stress level. We are currently investigating new, advanced means of modeling the driver data.</p>", "people": ["picard@media.mit.edu"], "title": "Detection and Analysis of Driver Stress", "modified": "2016-12-05T00:16:28.156Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "detection-and-analysis-of-driver-stress"}, {"website": "", "description": "<p>The Display for Indirect Collaboration Environments (DICE) system collected dynamic electronic artwork and displayed it in shared spaces while reacting to people in the surrounding environment. DICE was designed especially for use at Computer Clubhouses, where there is a tradition of displaying static artwork (such as Photoshop projects) on the walls, but few ways for members to publicly display their animations, videos, and other dynamic creations. DICE helped facilitate idea sharing, feedback, and collaboration processes in a community by persistently displaying members' saved work. DICE audience members, who previously may have had limited interactions with others, became aware of the capabilities and interests of their peers. DICE also leveraged sensor-driven awareness of the environment which indicated when the author of a displayed project was in the room and welcomed potential collaborators.</p>", "people": ["millner@media.mit.edu", "mres@media.mit.edu"], "title": "DICE: Display for Indirect Collaboration Environments", "modified": "2016-12-05T00:16:28.039Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-020F", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "dice-display-for-indirect-collaboration-environments"}, {"website": "", "description": "<p>We developed a music control surface which enables integration between any musical instruments via a versatile, customizable, and inexpensive user interface. This sensate surface allows capacitive sensor electrodes and connections between electronics components to be printed onto a large roll of flexible substrate unrestricted in length. The high-dynamic, range-capacitive sensing electrodes can not only infer touch, but near-range, non-contact gestural nuance in a music performance. With this sensate surface, users can \"cut\" out their desired shapes, \"paste\" the number of inputs, and customize their controller interfaces, which can then send signals wirelessly to effects or software synthesizers. We seek to find a solution for integrating the form factor of traditional music controllers seamlessly on top of one's instrument, while adding expressiveness to performance by sensing and incorporating movements and gestures to manipulate the musical output.</p>", "people": ["nanwei@media.mit.edu", "pragun@media.mit.edu", "joep@media.mit.edu", "nanzhao@media.mit.edu"], "title": "Customizable Sensate Surface for Music Control", "modified": "2016-12-05T00:16:28.069Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "customizable-sensate-surface-for-music-control"}, {"website": "", "description": "<p>HandWave is a small, wireless, networked skin conductance sensor that can be worn or used in many different form factors. Skin conductance is the best known measure of arousal (whether emotional, cognitive, or physical) and this device makes it easy to gather this information from mobile users. Many existing affective computing systems make use of sensors that are inflexible and often physically attached to supporting computers. In contrast, HandWave allows an additional degree of flexibility by providing ad hoc wireless networking capabilities to a wide variety of Bluetooth devices as well as adaptive biosignal amplification. As a consequence, HandWave is useful in games, tutoring systems, experimental data collection, and augmented journaling, among other applications. The Handwave builds on the earlier Galvactivator project.</p>", "people": ["picard@media.mit.edu"], "title": "HandWave", "modified": "2016-12-05T00:16:28.203Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "Cube", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "handwave"}, {"website": "", "description": "<p>Happenstance is flexible storytelling testbed that expands the traditional literary and theatrical notions of place and situation to accommodate interactive, on-the-fly story construction. Important aspects of story content and context are made visible, tangible, and manipulable by systematically couching them within the metaphors of ecology, geology, and weather. Information-rich environments become conceptual landscapes that grow, change, and evolve over time and through use. Current information follows a natural cycle modeled on the Earth's water cycle. Older information, history, and complex conceptual constructs\ufffdbuilt up by the flow of data over time\ufffdare manifested in the rock and soil cycles. Directed inquiries, explorations of theory, and activities associated with the audience's personal interests are captured and reflected by plant growth. In a process which mimics realistic physical modeling, metaphor is implemented as metaphysical systems operating on metadata. As a result, information itself is imbued with sets of systemic, semi-autonomous behaviors which allow it to move and act intelligently within the story world or other navigable information spaces. Interestingly, the operation of this trope-forming micro-world also resembles the internal psychology of characters.</p>", "people": ["gid@media.mit.edu", "beb@media.mit.edu"], "title": "Happenstance", "modified": "2016-12-05T00:16:28.231Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-427", "groups": ["interactive-cinema"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "happenstance"}, {"website": "", "description": "<p>Working with Juan Alberto Belloch, the mayor of Zaragoza, Spain, MIT developed a programmable water wall composed of digitally controlled solenoids that release water droplets to create a dynamic, urban-scale intervention for Expo 2008. This project focuses on the potential of advanced communications and media technology in the public realm of the Milla Digital in the center of the old city of Zaragoza, rather than in its buildings and private development. The result is an urban design and digital framework, and specific proposals for digitally enhanced environments that will serve the learning, skill development, and social interests of Zaragoza's citizens, as well as making the city more attractive for business growth and tourism.</p>", "people": ["susanne@media.mit.edu", "rchin@media.mit.edu"], "title": "Digital Water Wall: Zaragoza, Spain", "modified": "2016-12-05T00:16:28.255Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "digital-water-wall-zaragoza-spain"}, {"website": "", "description": "<p>This project explores different applications for tapered-aperture audio transducers, where incoming acoustic wavefronts correlate with spatial patterns that we deposit onto large-area distributed acoustic transducers.  Our initial efforts focused on developing very simple sensors that determine the bearing of incoming sound waves, giving the computer easy access to directional audio information. The initial devices were broadband, passive monopulse acoustic receivers made from PVDF foil. By properly shaping the electrode patterns on the PVDF and mounting the foil appropriately, a simple ratio of two signals determines acoustic bearing over a wide frequency range without dispersive effects. The transducers that we have developed have responded well into the audio band, and provided directional cues for nearby acoustic transients resulting from different kinds of activity. We are now exploring other interesting audio applications of distributed-aperture transducers, and developing different ways to fabricate them.</p>", "people": ["joep@media.mit.edu"], "title": "Distributed Aperture Acoustic Transducers", "modified": "2016-12-05T00:16:28.277Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["responsive-environments", "cc", "personal-fabrication"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "distributed-aperture-acoustic-transducers"}, {"website": "", "description": "<p>Garments that embody unique aesthetic properties\ufffdfor instance, light, sound, or motion rather than color, pattern, or cut\ufffdprovide the possibility of enhancing and enriching our daily interactions with people, places, schedules, and self. Saturnpants and Iris are two such garments, creating interactive relationships with their owners and the people around them through the tight integration of electrical components into the aesthetic framework of the garments themselves.</p>", "people": [], "title": "Dynamic Clothing: Saturnpants and Iris", "modified": "2016-12-05T00:16:28.306Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-301", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "dynamic-clothing-saturnpants-and-iris"}, {"website": "", "description": "<p>Computing systems and electronic devices often frustrate users by seeming to have a will of their own. Designers and engineers thus generally strive for the most unobtrusive, invisible systems possible; nevertheless, these systems are often inflexible when confronted with the complexities of the real world, and this inflexibility seems willful. The urge to make systems invisible means that opportunities for generative conflict\ufffdthe kind of give and take that makes dealing with people so stimulating\ufffdare often ignored in device design. The Haptic Opposition (HO) project explores what happens when apparatuses claim their physical space, and looks for positive interactions that may happen with uncontrollable devices. Mechanical parts potentially moveable both by human beings and machine control are examined as a nexus of human-machine conflict. In a series of experimental installations, the HO project will consist of technical systems built with increasing free will and the power to enforce it; subtle distortions of a user's manipulation will grow into an autonomous behavior.</p>", "people": ["csik@media.mit.edu"], "title": "Haptic Opposition", "modified": "2016-12-05T00:16:28.357Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-489", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "haptic-opposition"}, {"website": "", "description": "<p>We are in the third year of a multi-year project to see how close we can come to building a synthetic canine whose everyday common sense, ability to learn, and ability to empathize approaches that of a real canine. The past year has been largely focused on dog learning and training. In particular, Duncan can be trained to do many of the same things that real dogs can do using a technique borrowed from real dog training, called \"clicker-training.\" Technically, this requires him to do state-space discovery, behavioral adaptation, and motor learning in an integrated, goal-driven fashion. While we are continuing this work, we are also broadening our focus to examine issues of social behavior and learning as well as spatial learning. Two installations currently feature Duncan: sheep/dog: Trial By Eire, and Clicker By Eire.</p>", "people": [], "title": "Duncan the Highland Terrier", "modified": "2016-12-05T00:16:28.477Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-320", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "duncan-the-highland-terrier"}, {"website": "", "description": "<p>The ability to learn is a potentially compelling and important quality for interactive synthetic characters; this project investigates a practical approach to real-time learning for them. The implementation is grounded in techniques of reinforcement learning, and informed by insights from animal training. It simplifies the learning task for characters because it enables them to take advantage of predictable regularities in their world; allows them to make maximal use of any supervisory signals; and makes them easy to train. We have built an autonomous, animated dog that can be trained by a technique used with real dogs called \"clicker training.\" Capabilities demonstrated include training to recognize and use acoustic patterns as cues for actions, as well as synthesizing new actions from novel paths through its motion space. This work demonstrates that by addressing the problems of state, action, and state-action space discovery at the same time, the solution for each becomes easier. Finally, we articulate heuristics and design principles that make learning practical for synthetic characters.</p>", "people": [], "title": "Dobie: Integrated Learning for Interactive Synthetic Characters", "modified": "2016-12-05T00:16:28.566Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-441", "groups": [], "published": true, "active": false, "end_on": "2002-01-01", "slug": "dobie-integrated-learning-for-interactive-synthetic-characters"}, {"website": "", "description": "<p>Most e-commerce sites today are little more than electronic catalogs of product offerings. Consumer input is limited to requirements questionnaires, search engines, and accepting or rejecting particular offerings. But in complex purchases, such as real estate, cars, or computers, it is often difficult to specify exactly what you want, and priorities and preferences often change in the process of exploration. We are investigating software agents that use machine learning, context sensitivity, and predictive interfaces. We would like these agents to act as advisors to consumers much as a real-estate agent or travel agent would, implicitly inferring general preferences from a history of relatively unconstrained reactions to specific examples.</p>", "people": ["lieber@media.mit.edu"], "title": "E-Commerce When You Don't Know What You Want", "modified": "2016-12-05T00:16:28.625Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "e-commerce-when-you-dont-know-what-you-want"}, {"website": "", "description": "<p>The Dual Reality Lab is an experiment aimed at an interface between distributed sensor networks (which collect information about the real world), and online worlds (which allow people to interact and create in a virtual environment). In this case, we are connecting multimodal sensor data taken throughout the Media Lab to events happening in a virtual version of the Media Lab in the online world of Second Life. We hope to discover how data from sensor networks can be used as a creative medium to enhance virtual worlds, and how virtual worlds can act as an intuitive means for browsing sensor data.</p>", "people": ["joep@media.mit.edu"], "title": "Dual Reality Lab: Using Sensor Networks to Merge Real and Virtual", "modified": "2016-12-05T00:16:28.449Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "dual-reality-lab-using-sensor-networks-to-merge-real-and-virtual"}, {"website": "", "description": "<p>Imagine the full power of a theater lighting system, complete with an expert human lighting controller, at your disposal to light your home. Such a system could put light where you need it, when you need it. You could talk to the controller, who would then translate your wishes into lighting configurations, taking into account numerous contextual factors such as who is in the room, where they are, what other sources of light are effecting the room, and so forth. Elvis is a conversational, robotic chandelier designed with these goals in mind. By combining elements of computer vision, robotic control, and grounded language understanding, Elvis demonstrates a new kind of device that is aware of its surroundings and able to communicate with people using natural communication.</p>", "people": ["dkroy@media.mit.edu"], "title": "Elvis: A Situation-Aware Conversational Chandelier", "modified": "2016-12-05T00:16:28.501Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "elvis-a-situation-aware-conversational-chandelier"}, {"website": "", "description": "<p>Hearplay provides a tool for sharing speech-annotated media on networked PocketPCs. It uses asynchronous audio to help us stay aware of what others in our community are doing, what they are interested in, and how available they are for real-time or offline interaction. The system provides a spontaneous and relevant audio \"broadcast\" to an affinity group of mobile users.</p>", "people": ["swheeler@media.mit.edu", "geek@media.mit.edu"], "title": "Hearplay", "modified": "2016-12-05T00:16:28.540Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "hearplay"}, {"website": "", "description": "<p>It has been theoretically proven that a wireless diversity scheme that utilizes a relay node to assist in conveying information is more efficient in throughput and transmission power. This project aims to construct and demonstrate such a system and its feasibility in a viral architecture. Optimal transmitters, relays, and receivers are built and their collaboration is measured and compared to theoretical results. Methods to exploit the benefits of such a scheme are explored.</p>", "people": ["lip@media.mit.edu"], "title": "Efficient Wireless Antenna Sharing", "modified": "2016-12-05T00:16:28.650Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "efficient-wireless-antenna-sharing"}, {"website": "", "description": "<p>HealthMap is a multilingual, real-time disease outbreak tracking and visualization system. Launched in fall 2006, the Web site collects over 300 reports per day in seven languages, from both general news media and public health sources around the world. Updated hourly, the system filters these reports to determine relevance, disease, location, and duplication clustering by means of a series of custom-designed, automated, text-processing algorithms. Relevant reports are then aggregated and displayed on a freely available dashboard where users can tailor the view according to date, disease, location, and source. HealthMap provides an overview of real-time information on emerging infectious diseases, and has particular interest for public health officials and international travelers.</p>", "people": ["fmoss@media.mit.edu"], "title": "HealthMap", "modified": "2016-12-05T00:16:28.767Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-320", "groups": ["new-media-medicine"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "healthmap"}, {"website": "", "description": "<p>Have you ever wondered what makes an ad memorable? We have performed a comprehensive review of literature concerning advertising, memory, and emotion. A summary of results are available.</p>", "people": ["picard@media.mit.edu"], "title": "Emotion and Memory", "modified": "2016-12-05T00:16:28.819Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "emotion-and-memory"}, {"website": "", "description": "<p>With HealthMap's Outbreaks Near Me application, you have all of HealthMap's latest real-time disease outbreak information at your fingertips. Open the app and see all current outbreaks in your neighborhood, including news about H1N1 influenza (\"swine flu\"). Search and browse outbreak reports on the interactive map, and set up the app to alert you with a notice automatically whenever an outbreak is occurring in your area. If you know of an outbreak not yet on the map, be the first to report it using the app's unique outbreak reporting feature. You will be credited and your report will be featured on the Website. With this iPhone app, we are launching an exploration of crowd-sourced, user-generated, people-powered disease outbreak tracking and collaboration.</p>", "people": ["fmoss@media.mit.edu"], "title": "HealthMap iPhone App: Outbreaks Near Me", "modified": "2016-12-05T00:16:28.880Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-320", "groups": ["new-media-medicine"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "healthmap-iphone-app-outbreaks-near-me"}, {"website": "", "description": "<p>We have created a system that allows a person's heart rate to be displayed on an ambient orb color display. This system uses a heart-rate sensor developed at the Media Lab. This system has many applications, including baby monitors, home health-care monitoring, company meeting tools, and novel user interfaces or games.</p>", "people": ["fletcher@media.mit.edu", "picard@media.mit.edu", "ishii@media.mit.edu"], "title": "Heart-Rate Ambient Display", "modified": "2016-12-05T00:16:28.904Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "heart-rate-ambient-display"}, {"website": "", "description": "<p>We are developing wearable sensors that measure cardiovascular parameters such as heart rate and heart rate variability (HRV) in real time. HRV provides a sensitive index of autonomic nervous system activity. These sensors will be capable of communication with mobile devices such as the iPhone and iPod Touch.</p>", "people": ["picard@media.mit.edu", "zher@media.mit.edu"], "title": "Heartphones", "modified": "2016-12-05T00:16:28.929Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-448", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "heartphones"}, {"website": "", "description": "<p>Helios provides an automatic way of socializing one's video interactions. It is a Chrome browser plug-in that records user's encounters with embedded videos on the web. This data is contributed to a group collection so that one can readily see what is trending among friends and where the outliers are. In addition the data is processed by Glue for metadata tagging. </p>", "people": ["lip@media.mit.edu", "dahlseng@media.mit.edu"], "title": "Helios", "modified": "2016-12-05T00:16:28.961Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "helios"}, {"website": "", "description": "<p>Spheres and Splinters is a new work composed by Tod Machover for hypercello, electronics, and responsive visuals commissioned fo Faster than Sound at Aldeburgh Music. The work premiered with celllist Peter Gregson in the UK in 2010 and had its US premiere as part of FAST Festival of Art, Science, and Technology in celebration of MIT's 150th anniversary. Utilizing audio analysis and a multitude of wireless sensors on the cello and the bow that capture how the instrument is being played, the performer has control over transformations and extensions of the sound produced. This control extends into the ambisonic spatialization of sound in the performance space. The performance data is also used to produce realtime visual accompaniment on an array of LED strips surrounding the cellist.</p>", "people": ["patorpey@media.mit.edu", "tod@media.mit.edu", "benb@media.mit.edu"], "title": "Spheres and Splinters", "modified": "2016-12-05T00:16:29.010Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "spheres-and-splinters"}, {"website": "", "description": "<p>subTextile is a toolkit for behavioral textiles: the intersection of on-body computation and electronic textiles focusing on the interactive capability of electronic textiles. It provides a powerful visual programming language and hardware platform specifically designed to create complete behavioral textile systems. Using a rich, goal-oriented interface, subTextile makes it possible for technical novices to explore electronic textiles, while providing open-ended expandability to experts. As e-textiles mature, better tools and techniques are needed by artists and designers experimenting with these new materials. The subTextile project was created to support cross-pollination between technical and design disciplines in the hopes of fostering greater creativity and depth within the field.</p>", "people": ["sajid@media.mit.edu", "pattie@media.mit.edu"], "title": "subTextile: A Construction Kit for Computationally Enabled Textiles", "modified": "2016-12-05T00:16:29.182Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "subtextile-a-construction-kit-for-computationally-enabled-textiles"}, {"website": "", "description": "<p>Our smartphones take active attention while we use them to navigate streets, find restaurants, meet friends, and remind us of tasks. SuperShoes allows us to access this information in a physical, ambient form through a foot interface. SuperShoes takes us to our destination; senses interesting people, places, and events in our proximity; and notifies us about tasks, all while we immerse ourselves in the environment. We explore a physical language of interaction afforded by the foot through various tactile senses. By weaving digital bits into the shoes, SuperShoes liberates information from the confines of screens and onto the body.</p>", "people": ["holtzman@media.mit.edu"], "title": "SuperShoes", "modified": "2016-12-05T00:16:29.225Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "supershoes"}, {"website": "", "description": "<p>Holosuite is an application that simulates holographic rendering on advanced 3D displays, connecting remote users together where they can collaborate, visualize and share 3D information across the internet with full motion parallax and stereoscopic rendering.</p>", "people": [], "title": "holosuite", "modified": "2016-12-05T00:16:29.322Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2016-09-01", "slug": "holosuite"}, {"website": "", "description": "<p>Using your personal fabratory, explore a world in which 3D printers cost as little as today's inkjets and are found in every home. We've developed several sub-$100 machines that demonstrate the practicality of this future, and greatly expand the range of items that can be created on your desktop. These new capabilities have far-reaching implications for personalization of products, direct-to-consumer production, and the creation of \"information objects.\"</p>", "people": ["holtzman@media.mit.edu"], "title": "Home Fabratory", "modified": "2016-12-05T00:16:29.358Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "home-fabratory"}, {"website": "", "description": "<p>By working in the same public spot over a long period of time, street vendors and panhandlers often amass a large amount of social capital.  We are building tools for street vendors to reify their social networks online. These tools will help them leverage this social capital for upward mobility, and enhanced community building.</p>", "people": ["ethanz@media.mit.edu", "borovoy@media.mit.edu"], "title": "Homeless Neighbors", "modified": "2016-12-05T00:16:29.140Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "homeless-neighbors"}, {"website": "", "description": "<p>The Hook-Ups system is a set of technologies and activities that enables young people to create interactive experiences by programming connections between physical and digital media. With the Hook-Ups system, young people integrate sensors with a myriad of materials to create their own tangible interfaces. These interfaces control digital images and sounds in computer programs (such as games or responsive art pieces) the young people write. For example, a 10-year-old created a paper-plate-based flying saucer, added a sensor, then wrote a program to control an animation of a flying saucer on her computer screen.</p>", "people": ["millner@media.mit.edu", "mres@media.mit.edu"], "title": "Hook-Ups", "modified": "2016-12-05T00:16:29.472Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "hook-ups"}, {"website": "", "description": "<p>We are working on Systems Thinking Blocks, a new modeling language for dynamic interconnected systems that can be simulated using digital computation. The language is implemented as a a Digital Manipulative, a playful set of wooden blocks with embedded digital computation, enabling children to experiment with different configurations of light patterns and in the process explore the generic structures of dynamic interconnected systems. </p>", "people": ["mres@media.mit.edu"], "title": "Systems Thinking Blocks", "modified": "2016-12-05T00:16:29.497Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-120B", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "systems-thinking-blocks"}, {"website": "", "description": "<p>Through human actions, conversations, and the creation of memories, physical spaces are transformed into social constructs and can become the arena for story construction and narrative development. This project explores how such narrative spaces can be created in a tangible form, allowing audiences to collectively reflect upon and navigate through complex, spatially structured, and multi-viewpoint stories. In Tangible Spatial Narratives, visual landscapes are projected onto the TViews interaction surface, providing a spatial framework for the overall story and the many perspectives and narrative threads that emerge as it unfolds. Users interact with the system by moving graspable pawns and other tangible objects around on the interaction surface. The interaction can often be a collaborative activity, in which several users work together to navigate through the story.</p>", "people": ["gid@media.mit.edu"], "title": "Tangible Spatial Narratives", "modified": "2016-12-05T00:16:29.576Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "tangible-spatial-narratives"}, {"website": "", "description": "<p>StoryNet is a large database of story-scripts and inference tools that can be used for case-based common-sense reasoning. Since people are by nature experts at using story structures to organize and manage the complexity of life, we have turned to the general public to gather over a million story-scripts using various Web-based, story-building interfaces. As our acquisition systems are released and tested, we anticipate that our representations will be revised and enhanced as we learn from the human storytelling impulse and the success and limitations of the inference mechanisms we develop. StoryNet is one of a suite of common-sense tools at the Media Lab, including ConceptNet and LifeNet.</p>", "people": ["barbara@media.mit.edu", "minsky@media.mit.edu", "gid@media.mit.edu"], "title": "StoryNet", "modified": "2016-12-05T00:16:29.597Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "storynet"}, {"website": "", "description": "<p>This project is developing new tools for querying databases through the manipulation of physical objects. Our approach should allow multiple scientists, managers, or other groups of people to navigate, view, and discuss database contents collaboratively. We embody parameterized SQL queries as physical tokens. These tokens are manipulated, interpreted, and graphically augmented on a series of sliding racks. We map physical token sliders to parameter range selections; token adjacencies to Boolean AND/OR operations; and multiple racks to parenthetical groupings. The interface should support querying to a wide range of relational databases; initial target domains include product databases, media databases, network management, and bioinformatics.</p>", "people": ["ishii@media.mit.edu"], "title": "Tangible Query Interfaces", "modified": "2016-12-05T00:16:29.793Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "tangible-query-interfaces"}, {"website": "", "description": "<p>TellTale is a story construction kit whose goal is to help young children create and experiment with the structure and content of personal narratives. The design consists of a number of modular body components and one head piece used to record and play audio segments created by a child, or by several children. The body parts can contain stories or story fragments that can be combined in different orders to create new narrative configurations, letting children experiment with plot, transitions, endings, and beginnings\ufffdbasically anything they can imagine. In essence, it's a \"tangible audio processor\" for children who have stories to tell but who might not yet have the skills necessary to communicate their ideas in writing. TellTale's design will evolve as we explore with children how storytelling and personal narrative expression technologies of this sort can encourage the development of literacy skills.</p>", "people": ["ishii@media.mit.edu"], "title": "TellTale", "modified": "2016-12-05T00:16:29.847Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-441", "groups": ["tangible-media", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "telltale"}, {"website": "", "description": "<p>A fundamental question has arisen out of our current effort to create systems for digital media content authoring over the Web: \"How do we manage all of this content?\"  More importantly, how do we manage it efficiently over the Web in a cross-platform manner? A suite of core technologies is being developed that includes a universal secure login architecture, Web-based storage lockers, seamless back-end database connectivity and transfigurability of all digital media types, and an engine for visible common sense to enable a new kind of markup paradigm. Impact areas for this project include handheld devices, Web-hosting services, and digital video/still image management.</p>", "people": [], "title": "Treehouse Studio", "modified": "2016-12-05T00:16:29.892Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "treehouse-studio"}, {"website": "", "description": "<p>The ubiquitous sensor network and media explorer integrates sound, images, and diverse data sampled throughout the Media Lab into a visual, aural and haptic experience. Real time data drives the system, allowing the observer to sense the overall activity of the Media Lab. Sound and images are streamed into our system from a selection of 45 sensor nodes distributed throughout our Lab. Observers can interact with the piece, both observing the generic activity of the Media Lab and drilling down to browse media streamed from particular locations. The installation reflects the essence of the Media Lab as a holistic entity.</p>", "people": ["nanwei@media.mit.edu", "joep@media.mit.edu"], "title": "Ubiquitous Sensor Network Navigator and Media Explorer", "modified": "2016-12-05T00:16:29.911Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "ubiquitous-sensor-network-navigator-and-media-explorer"}, {"website": "", "description": "<p>The ability to author new kinds of digital media content rests upon two constraints: first, are there people that can imagine what is coming next? and second, are there people that can write the computer programs to realize that content? In the last five years, we have released two systems for fostering a new creative culture around graphical programming. The first, Design By Numbers (http://dbn.media.mit.edu), is an introduction to programming for the mathematically uninclined; the second, Processing (http://www.processing.org), is an advanced graphics-programming system suitable for production-class experimentation. The impact areas for this project include graphic design education, new media education, and computer art.</p>", "people": [], "title": "How to Program Visual Systems", "modified": "2016-12-05T00:16:29.931Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "how-to-program-visual-systems"}, {"website": "", "description": "<p>For 3D displays to be successful, they must be bright enough to compete with 2D displays and not diminish display resolution. To date, stacked-LCD displays have employed parallax barriers, which use pinhole or stripe patterns to provide view-dependent imagery. We show a prototype that adapts the imagery on both layers to multi-view 3D content, increasing brightness while maintaining display resolution. This promises a future of devices with sharp 2D screens and 3D displays with full horizontal and vertical parallax.</p>", "people": ["raskar@media.mit.edu", "holtzman@media.mit.edu"], "title": "HR3D: Glasses-Free 3DTV", "modified": "2016-12-05T00:16:29.951Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["camera-culture", "information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "hr3d-glasses-free-3dtv"}, {"website": "", "description": "<p>How can we encourage people to express themselves in new ways?  And how do people deal with objects that possess (presumed) agency? These two seemingly unrelated questions are brought together in the design and study of syngva, a robotic creature that moves in response to human non-speech vocal sounds. On the one hand, syngva is a creature that, through its movement, encourages the production of sounds that are otherwise silenced, bringing out what was unspeakable. On the other hand, syngva learns about these sounds over time, changing its motions and therefore prodding people to try different things. Thus, syngva serves as an anthropological probe, enabling us to better understand people's interactions with robotic creatures, while at the same time providing a potential space for unconventional experiences.</p>", "people": ["tod@media.mit.edu"], "title": "syngva: Encouraging the Expression of the Unspeakable", "modified": "2016-12-05T00:16:29.720Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "syngva-encouraging-the-expression-of-the-unspeakable"}, {"website": "", "description": "<p>Given motion capture samples of Charlie Chaplin's walk, is it possible to synthesize other motions\ufffdsay, ascending or descending stairs\ufffdin his distinctive style? More generally, in analogy with handwritten signatures, do people have characteristic motion signatures that individualize their movements? If so, can these signatures be extracted from example motions? Furthermore, can extracted signatures be used to recognize, say, a particular individual's walk subsequent to observing examples of other movements produced by this individual?  We are developing an algorithm that extracts motion signatures and uses them in the animation of graphical characters. For example, given a corpus of walking, stair ascending, and stair descending motion data collected over a group of subjects, plus a sample walking-motion for a new subject, our algorithm can synthesize never-before-seen ascending and descending motions in the distinctive style of this new individual. </p>", "people": ["picard@media.mit.edu"], "title": "Human Motion Signatures", "modified": "2016-12-05T00:16:30.275Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-443D", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "human-motion-signatures"}, {"website": "", "description": "<p>The Human Speechome Project is an effort to observe and computationally model the longitudinal language development of a single child at an unprecedented scale. To achieve this, we are recording, storing, visualizing, and analyzing communication and behavior patterns in over 200,000 hours of home video and speech recordings. The tools that are being developed for mining and learning from hundreds of terabytes of multimedia data offer the potential for breaking open new business opportunities for a broad range of industries\ufffdfrom security to Internet commerce.</p>", "people": ["dkroy@media.mit.edu", "soroush@media.mit.edu", "mbf@media.mit.edu", "decamp@media.mit.edu"], "title": "Human Speechome Project", "modified": "2016-12-05T00:16:30.317Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-441", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "human-speechome-project"}, {"website": "", "description": "<p>Car seats, and other seating devices (such as wheel chairs) are the product of a steady evolution that began with regular chairs. Starting from zero, considering the physical, medical, and cognitive demands of driving, can we make a \"seat\" to make driving safer, better, and more fun? That makes us go faster on a race track and safer in a city or when driving on snow? Can we make a \"seat\" that excites the parts of our brain activated by bicycling, skiing, or playing a musical instrument? Based on previous work with the \"Athlete Seat,\" we are rethinking vehicle control in a way that allows for improved performance and driver well-being.</p>", "people": [], "title": "Humanseat", "modified": "2016-12-05T00:16:30.386Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "Cube", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "humanseat"}, {"website": "", "description": "<p>This project is a self-sustaining sculptural installation that depicts the changes in outside weather conditions. By means of a simple mechanical system and smart materials, the sculpture responds to precipitation, light, and temperature to move colored glass panels into a wall of ever-changing designs, additionally creating awareness of the passage of time.</p>", "people": [], "title": "Hydrocycle", "modified": "2016-12-05T00:16:30.406Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "Upper Atrium", "groups": [], "published": true, "active": false, "end_on": "2009-01-01", "slug": "hydrocycle"}, {"website": "", "description": "<p>HouseFly combines audio-video recordings from multiple cameras and microphones to generate an interactive, 3D reconstruction of recorded events.  Developed for use with the longitudinal recordings collected by the Human Speechome Project, this software enables the user to move freely throughout a virtual model of a home and to play back events at any time or speed. In addition to audio and video, the project explores how different kinds of data may be visualized in a virtual space, including speech transcripts, person tracking data, and retail transactions.</p>", "people": ["dkroy@media.mit.edu", "decamp@media.mit.edu"], "title": "HouseFly: Immersive Video Browsing and Data Visualization", "modified": "2016-12-05T00:16:30.496Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-441", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "housefly-immersive-video-browsing-and-data-visualization"}, {"website": "", "description": "<p>A web technology-based update of Smarter Objects and the reality editor project.</p>", "people": ["pattie@media.mit.edu", "heun@media.mit.edu"], "title": "Hybrid Objects", "modified": "2016-12-05T00:16:30.529Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "hybrid-objects"}, {"website": "", "description": "<p>A new violin bow has been developed that can measure intricate aspects of violin technique: the subtle elements of physical gesture that immediately and directly impact the sound of the instrument while playing. A sensing system was implemented to measure changes in position, acceleration, and the downward and lateral strains in the bow stick; these sensors were fashioned using electromagnetic field sensing, MEMS accelerometers, and foil strain gauges. The resulting system is wireless and may be used to play an electric or acoustic violin. It can be used to exploit aspects of traditional violin bowing technique to allow sophisticated control of audio effects on the amplified sound of an electric violin, as well as synthesized sounds. New research focuses on calibrated measurement and analysis of bowing parameters in common bowing techniques and styles and the corresponding changes in the audio produced. </p>", "people": ["tod@media.mit.edu"], "title": "Hyperviolin Bow", "modified": "2016-12-05T00:16:30.588Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-443D", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "hyperviolin-bow"}, {"website": "", "description": "<p>Facing issues of food crisis by overpopulation, this project explores a possible future where a small community of activists arises to design an edible cockroach that can survive in harsh environments. These genetically modified roaches are designed to pass their genes to the next generations; thus the awful black and brown roaches will be pushed to extinction by the newly designed, cute, colorful, tasty, and highly nutritional \"pop roach.\" The color of these \"pop roaches\" corresponds to a different flavor, nutrition, and function, while the original ones remain black or brown, and not recommended to be eaten. How will genetic engineering shift our perception of food and eating habits? Pop Roach explores how we can expand our perception of cuisine to solve some of the world's most pressing problems.</p>", "people": ["sputniko@media.mit.edu", "aih@media.mit.edu"], "title": "Pop Roach", "modified": "2017-10-11T20:31:47.421Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["design-fiction"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "pop-roach"}, {"website": "", "description": "<p>iCom is a communication portal intended to become part of the built environment of a set of collaborating research groups. It currently links our building with sponsor sites. It serves several purposes, including acting as a sort of ambient porthole, a live interpersonal communication system, and a community messaging center. Recent updates include voice control of the system.</p>", "people": ["vmb@media.mit.edu"], "title": "iCom", "modified": "2016-12-05T00:16:30.692Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-368", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "icom"}, {"website": "", "description": "<p>Hyperscore in the Hospital was an opportunity to use the Hyperscore composition system with a wide array of patients from the Tewksbury State Hospital.  The patients exhibited diseases such as schizophrenia, cerebral palsy, bipolar disorder, spina bifida, and various dementias. Through the use of Hyperscore, the researchers addressed major treatment goal areas for a number of patients.  On the mental health unit, staff reported that patients were exhibiting a marked decrease in self-damaging behaviors, while maintaining attention to a task, communicating sentiments of self-worth, and exhibiting developments in social behavior.  On the physical health unit, patients displayed fine-motor control over the interface, where for several patients, there was little prior evidence to indicate that such physical potential existed.  These measures, and many more, promise a wealth of benefits to be achieved in the domain of music and health-care services.  In the future we hope to design interfaces that are specifically tailored to investigate the exciting field of music, mind and health.</p>", "people": ["tod@media.mit.edu"], "title": "Hyperscore in the Hospital", "modified": "2016-12-05T00:16:30.719Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "hyperscore-in-the-hospital"}, {"website": "", "description": "<p>Everyday objects are beginning to find expression in digital networks, creating a new family of objects that display personalized behavior and memory. But how can you weave a thousand objects in one room into the web? How can you interconnect them without a tangle of wires, the burden of battery packs, or a PhD in network administration? The goal of this work is to develop Hyphos, a wireless network for interconnecting thousands of everyday objects. By using very short-range transceivers, and relaying messages among the nodes, a new class of network emerges: a Hyphos network. This network is self-organizing with a low cost per node; transmissions are tightly localized, resulting in high bandwidth and low power consumption; fully distributed routing and control ensures robust connections even as nodes roam. \n</p>", "people": ["mike@media.mit.edu"], "title": "Hyphos", "modified": "2016-12-05T00:16:30.744Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "hyphos"}, {"website": "", "description": "<p>The \ufffdI/O Brush\ufffd is a drawing tool that not only releases ink, but also captures ink from the artist's immediate environment. Like an old pen or a paintbrush, it functions as both a tool to pick up ink and a tool to draw. But instead of picking up ink or paint, the I/O brush picks up color, texture, and movement from any surface or material. It looks and feels like a real paintbrush, but embedded inside is a tiny video camera with a special light, and a touch-sensitive Wacom pen. The artist may brush any surface outside of the drawing canvas (cloth, skin, her friend\ufffds long hair waving with the wind), and the fiber optics on the brush light up to indicate that it's picked up that surface, and the artist can then draw on the canvas with the special ink.</p>", "people": ["ishii@media.mit.edu", "stefanm@media.mit.edu"], "title": "I/O Brush", "modified": "2016-12-05T00:16:30.804Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "io-brush"}, {"website": "", "description": "<p>I/O Stickers is an electronics construction kit made up of adhesive sensors and actuators. Users can place these special electronic stickers onto contact points in pre-wired and pre-programmed pages, and the pages will transmit the state of the input (sensor) sticker to a corresponding output (actuator) sticker. Building the electronics is a simple matter of matching the sticker to the correct footprint. Users can design the interaction by choosing the sensor and actuator stickers, and then personalize the interface by decorating over the flat, electronic stickers with their choice of craft materials. I/O Stickers is designed to empower users to create electronics while also leveraging existing skills in craft, resulting in works that are creatively expressive as well as technically sophisticated.</p>", "people": ["cynthiab@media.mit.edu", "leah@media.mit.edu", "jieqi@media.mit.edu"], "title": "I/O Stickers", "modified": "2016-12-05T00:16:30.839Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["high-low-tech", "personal-robots"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "io-stickers"}, {"website": "", "description": "<p>In the Renaissance, kings rarely traveled to find a bride rather an artist was sent to paint her likeness. These artisans were often not the official court painter but lesser known ones who also illuminated manuscripts as their highly refined trade. One particular Netherlands family established a workshop serving the British monarch, Henry the VIII. They produced miniature portraits called Limnings, setting a trend that would continue in England and on the Continent for centuries. For today\ufffds celebrated personages, Digital Limnings are also miniature memories  where the past is stored in the present.</p>", "people": ["judith@media.mit.edu", "monster@media.mit.edu"], "title": "ID/entity", "modified": "2016-12-05T00:16:30.860Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-468", "groups": [], "published": true, "active": false, "end_on": "2002-01-01", "slug": "identity"}, {"website": "", "description": "<p>(Im)possible Baby is a speculative design project that aims to stimulate discussions about the social, cultural, and ethical implications of emerging biotechnologies that could enable same-sex couples to have their own, genetically related children. Delivering a baby from same-sex parents is not a sci-fi dream anymore, due to recent developments in genetics and stem cell research. In this project, the DNA data of a lesbian couple was analyzed using 23andme to simulate and visualize their potential children, and then we created a set of fictional, \"what if\" future family photos using this information to produce a hardcover album which was presented to the couple as a gift. To achieve more public outreach, we worked with the Japanese national television service, NHK, to create a 30-minute documentary film following the whole process, which aired in October 2015.</p>", "people": ["sputniko@media.mit.edu", "aih@media.mit.edu"], "title": "(Im)possible Baby", "modified": "2017-10-11T20:32:40.918Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["design-fiction", "ethics"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "impossible-baby"}, {"website": "", "description": "<p>Who are you now? This project aims to model the answer to that question, in all its relevant dimensions and constructs, within the context of an online and physical multi-user environment. Using federated identity management systems as a starting point, we are exploring interoperable expressions of names, rights, duties, preferences, self-expression, and pointers to other information. The prototypes are being designed against various concept car approaches so that driver and passengers can be authenticated to enter the car, enjoy personalized physical and data environments in it, and enable transactions and other interactions with people and systems in other cars or places. Modules and extensions will include support for self-motivational ticklers, health and wellness regimes, group dynamics intervention and religious or spiritual framing and thematic reminders. The expression dialect is currently XML, with LDAP compliant directory interoperability, and integration hooks for PIMs, phones, car systems, and desktop applications. </p>", "people": ["dang@media.mit.edu"], "title": "Identity Convergence: Physical, Digital, and Beyond", "modified": "2016-12-05T00:16:31.002Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "identity-convergence-physical-digital-and-beyond"}, {"website": "", "description": "<p>What if a building could sense what was happening inside of it, and tell the world?  This project explores the concept of automated micro blogging\ufffdautomatically generating and triggering short messages about what is going on in a particular context.  The current platform is built around a modular sensor network which combines proximity, temperature, light, and sound values to make guesses about an environment and put those guesses to words.</p>", "people": ["holtzman@media.mit.edu"], "title": "If These Walls Could Tweet", "modified": "2016-12-05T00:16:31.046Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "if-these-walls-could-tweet"}, {"website": "", "description": "<p>Illuminating Clay is system for the real-time computational analysis of physical landscape models. Users manipulate a malleable landscape model while the topography of the model is captured in real time by a laser-scanning device. The surface geometry of the model serves as an input to a library of landscape-analysis functions. The results of this analysis are projected back into the workspace and registered with the surfaces of the model. The system allows users to interact simultaneously with both physical and computational representations of the landscape.</p>", "people": ["ishii@media.mit.edu"], "title": "Illuminating Clay", "modified": "2016-12-05T00:16:31.177Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "illuminating-clay"}, {"website": "", "description": "<p>iLOS provides information to the driver of a passenger car. This system will make use of selective reflective coatings coupled with unique holographic optics, allowing the overlay of pertinent information in the driver's view zone.</p>", "people": ["rchin@media.mit.edu"], "title": "iLOS, In Line of Sight Display", "modified": "2016-12-05T00:16:31.198Z", "visibility": "LAB", "start_on": "2002-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2004-09-01", "slug": "ilos-in-line-of-sight-display"}, {"website": "", "description": "<p>This research aims to understand the types of technological and conceptual tools that communities need to develop rich understandings of local history and the process of investigating it. We are building tools that allow users to geo-reference images from their photo albums and Web pages. There are libraries of images in every home, and we want to provide people with tools to upload their historical records.</p>", "people": ["walter@media.mit.edu"], "title": "Image Maps", "modified": "2016-12-05T00:16:31.263Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-320G", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "image-maps"}, {"website": "", "description": "<p>Most computational systems take for granted the servitude of computers, hoping to make them invisible entities that intuit our needs, and seamlessly respond to them. Linked with this idea is the one that we can formally analyze any situation, and program an overarching structure for the computer\ufffds task. Much of this assumption stems from the military origins of computers, and the perceived need for total control; the experiences we have with computers reinforces this paradigm of direct manipulation. Most human activity, however, does not follow such formal structures. Our computer monitors routinely sport half a dozen post-it notes, evidence of the weakness of completely formal systems. Humans routinely improvise their activities, from casual social encounters to their first experience with a machine. For Improvitronic Islands, a small group at the Media Lab worked with two masters of Jazz improvisation, George Lewis and Toshinori Kondo, to develop computer music systems that help children learn principles of improvisation. The system enabled children to quickly use two of their most basic faculties for music making\ufffdbody rhythm and voice\ufffdand to use them in collaborative, improvisational performances.</p>", "people": ["csik@media.mit.edu"], "title": "Improvitronic Islands", "modified": "2016-12-05T00:16:31.354Z", "visibility": "PUBLIC", "start_on": "2001-12-31", "location": "", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "improvitronic-islands"}, {"website": "", "description": "<p>We present an indoor positioning system that measures location using disturbances of the Earth's magnetic field by structural steel elements in a building. The presence of these large steel members warps the geomagnetic field such that lines of magnetic force are locally not parallel. We measure the divergence of the lines of the magnetic force field using e-compass parts with slight physical offsets; these measurements are used to create local position signatures for later comparison with values in the same sensors at a location to be measured. We demonstrate accuracy within one meter 88 percent of the time in experiments in two buildings and across multiple floors within the buildings.</p>", "people": ["nanwei@media.mit.edu", "joep@media.mit.edu", "geek@media.mit.edu"], "title": "Indoor Location Sensing Using Geo-Magnetism", "modified": "2016-12-05T00:16:31.462Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "indoor-location-sensing-using-geo-magnetism"}, {"website": "", "description": "<p>InfoField aims to seamlessly connect physical space with the ubiquitous network, helping users select information in a dense RFID-tagged environment. The system uses a wearable RFID reader that monitors every tag within range (~1.5m), and sensor-embedded RFID tags which provide orientation, light level, and proximity data along with their identification codes. InfoField derives the user's interactions with the corresponding host object from the sensor data and enables browsing, logging, and selecting of the physical object's data. This system, fully compatible with EPC protocol, expands the RFID application area, including location tracking, inventory monitoring, activity logging, physical manipulation of data, and recommender systems.</p>", "people": ["holtzman@media.mit.edu"], "title": "InfoField", "modified": "2016-12-05T00:16:31.577Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "infofield"}, {"website": "", "description": "<p>The advent of PDAs, digital cameras, audio recorders, and wearables enables the collection of useful information in a variety of media. But how should it be organized? Information Pipette aims to use software agents to organize automatically the information to \"make sense\" for a particular task or user's context. For example, we are working on a scenario of assembling directions by taking pictures along a route or by recording audio advice at each step. Audio is run through a speech recognizer and pictures through an Optical Character Recognition (OCR) program (to read the street signs). While each of these methods alone is ineffectual, their information taken together and compiled with knowledge contextualized for the task can produce conherent descriptions.</p>", "people": ["lieber@media.mit.edu"], "title": "Information Pipette", "modified": "2016-12-05T00:16:31.647Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "information-pipette"}, {"website": "", "description": "<p>\n                    CremateBot is an apparatus that takes in human-body samples, such as fingernails, hair, or dead skin, and turns them into ashes through the cremation process. The process of converting human remains to ashes becomes a critical experience for observers, causing witnesses to question their sense of existence and physical self through the conversion process. CremateBot transforms our physical self and celebrates our rebirth through self-regeneration. The transformation and rebirth open our imagination to go beyond our physical self and cross the span of time. Similar to Theseus' paradox, the dead human cells\u2013which at one point were considered part of our physical selves and helped to define our sense of existence\u2013are continually replaced with newly generated cells. With recent advancements in implants, biomechatronics, and bioengineered organs, how we define ourselves is increasingly blurred.\n                </p>", "people": ["sputniko@media.mit.edu", "dkc@media.mit.edu"], "title": "CremateBot: Transform, Reborn, Free", "modified": "2017-10-11T20:33:19.419Z", "visibility": "LAB", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["design-fiction"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "crematebot-transform-reborn-free"}, {"website": "", "description": "<p>This research demonstrates how a person's visual pattern on a computer screen can identify his or her task or question. This is based on research and literature that correlates a person's vision pattern with interests.</p>", "people": [], "title": "InVision", "modified": "2016-12-05T00:16:31.847Z", "visibility": "PUBLIC", "start_on": "1999-09-01", "location": "E15-320", "groups": ["context-aware-computing", "counter-intelligence"], "published": true, "active": false, "end_on": "2002-01-01", "slug": "invision"}, {"website": "", "description": "<p>The past several years have shown a proliferation of sensor and wireless communication technologies across the urban landscape, enabling large-scale, real-time data collection and information sharing. We are increasingly living in an \ufffdinstrumented city,\ufffd in which all aspects of the environment can be quantified, analyzed, and monitored. Now, low-cost sensing technologies embedded in and connected to cell phones and other ubiquitous communications devices enable grassroots data collection and analysis. What does this mean for the future of urban experience?</p>", "people": [], "title": "Instrumented Cities", "modified": "2016-12-05T00:16:31.980Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "instrumented-cities"}, {"website": "", "description": "<p>inStink explores what happens when we can manipulate smell by computer as easily as we currently manipulate sound and video. We have built prototype systems for smell production that use smell as an ambient medium, to communicate presence, activity, and abstract information. We are also examining the use of aromas to aid learning, in conjunction with other media, and as a communication device.</p>", "people": ["mike@media.mit.edu"], "title": "inStink", "modified": "2016-12-05T00:16:31.950Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-068", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "instink"}, {"website": "", "description": "<p>Interactive C was a programming environment used by robotic controllers such as the rev. 2.21 6.270 controller and the Handy Board.  It gave users the ability to control a robot by using C commands and additional functions tailored specifically for robotics (i.e.: actuator control, sensor inputs).  Interactive C allowed users to either enter commands interactively or load the program in as a file. The heart of Interactive C is a compiler that converts C commands into pseudocode for a custom stack machine implemented for the controller board.  The current version of Interactive C was written in C and has not been updated for several years.  This project focused on the re-implementation of this compiler to port it to Java and to enhance its current features\ufffdopening the door for future improvements in the hardware Interactive C can support.</p>", "people": ["mres@media.mit.edu"], "title": "Interactive C", "modified": "2016-12-05T00:16:32.091Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2000-09-01", "slug": "interactive-c"}, {"website": "", "description": "<p>We are currently designing and fabricating a nano-liter label-less integrated DNA detection platform capable of performing on-chip amplification and detection. </p>", "people": ["scottm@media.mit.edu"], "title": "Integrated DNA Sensor", "modified": "2016-12-05T00:16:32.030Z", "visibility": "LAB", "start_on": "2004-01-01", "location": "E15-420", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "integrated-dna-sensor"}, {"website": "", "description": "<p>Agent software can perform tasks automatically on behalf of a user, but how does the agent come to learn what the user wants? Sometimes the agent can learn just by observing user behavior, but there may also be instances where the user must instruct the agent more explicitly. This \"instructibility\" aspect is the focus of this project. The user may present examples of behavior that the agent should follow and give advice to the agent as to how the examples should be interpreted. The agent must give feedback to the user so that the user understands what the agent knows and is capable of doing. Multimodal interaction is important in both the instruction and feedback.</p>", "people": ["lieber@media.mit.edu"], "title": "Instructible Agents", "modified": "2016-12-05T00:16:32.057Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "instructible-agents"}, {"website": "", "description": "<p>Originating from the \"Wearable Seat\" concept, we are designing a seat that has the sensual qualities of clothing while allowing the driver to make natural movements, as, for example, in skiing. These movements are mapped and used to control a vehicle, resulting in a new, athletic, and sensual way of driving a vehicle.</p>", "people": [], "title": "Interactive Seat", "modified": "2016-12-05T00:16:32.184Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "interactive-seat"}, {"website": "", "description": "<p>The Interactive Fountain is a volumetric water display in which laminar water colums are perturbed to create illuminated pixels. As the perturbations scan downward, illuminating LEDs are pulse-width modulated to scan out a 3-D image. The fountain will employ the use of new UV LEDs and phosphorescent dye to create as bright an image as possible.  A show-control software system is also being developed that would enable the images in the fountain to be linked with audio and other content.</p>", "people": ["tod@media.mit.edu"], "title": "Interactive Fountain", "modified": "2016-12-05T00:16:32.141Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-483", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "interactive-fountain"}, {"website": "", "description": "<p>We present the world's first interactive \"scratch and sniff.\"  We don't consider this a serious application, but it starts to explore possible uses for computer-controlled scent, and to open up new potentials for alternate interaction modalities.</p>", "people": ["mike@media.mit.edu"], "title": "Interactive Scratch & Sniff", "modified": "2016-12-05T00:16:32.294Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-068", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "interactive-scratch-sniff"}, {"website": "", "description": "<p>The Interface Tailor is an agent that attempts to adapt a system in response to affective feedback. Frustration is being used as a fitness function to select between a wide variety of different system behaviors. Currently, the Microsoft Office Assistant (or Paperclip) is one example interface that is being made more adaptive. Ultimately the project seeks to provide a generalized framework for making all software more tailor-able.</p>", "people": ["picard@media.mit.edu"], "title": "Interface Tailor", "modified": "2016-12-05T00:16:32.318Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-001", "groups": ["affective-computing", "e-markets"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "interface-tailor"}, {"website": "", "description": "<p>The Internet constitutes an enormous electronic architecture that defines spaces without regard to physical structure. We navigate these spaces with browsers, moving from place to place with a click on a link. Internaut proposes that a physical architecture may be derived from the shape of the network and navigated with a first-person 3-D game engine. We are currently working simultaneously on processing Internet structures into maps and modifying an open-source version of the Quake2 engine from ID Software to be usable with this project. These spatial realms are inhabitable communally: as you wander the halls of the 'Net you may run into people you know and people you don't\ufffdyou can talk to them or arrange a meeting with a friend at a Web site.</p>", "people": [], "title": "Internaut", "modified": "2016-12-05T00:16:32.399Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-301", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "internaut"}, {"website": "", "description": "<p>The Interspace Theater project explores the concept of an online theater space for performing visual narratives. Elements such as stage and actors are visually represented using both abstract graphics and photo/video imagery. Performances are created in real-time by human controllers who interact with each other within the space through the manipulation of actor elements. The live performances can be viewed on the Web by the online audience.</p>", "people": [], "title": "Interspace Theater", "modified": "2016-12-05T00:16:32.372Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-301", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "interspace-theater"}, {"website": "", "description": "<p>We design games for humans by observing naturally occurring behaviors and recreating them in a synthetic environment. We are now extending this procedure to animals based on our understanding of their behavior. Thus what we create are not only pet \"toys,\" but also tools for gaining insight into animal minds and motivation. We are in the early phase of this research. We have constructed a \"smart perch,\" complete with a controller, 170 LCD screen, speakers and a Webcam. This perch will serve as a hardware platform upon which we will add different forms of software. We are not trying to teach the bird how to \"surf the net.\"  Rather, we are trying to give these birds a set of tools for interacting with their environment.</p>", "people": ["impepper@media.mit.edu"], "title": "InterPet Explorer", "modified": "2016-12-05T00:16:32.427Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-318", "groups": ["pet-projects"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "interpet-explorer"}, {"website": "", "description": "<p>inTouch explores new forms of interpersonal communication through touch. Force-feedback technology is employed to create the illusion that people, separated by distance, are interacting with a shared physical object. The \"shared\" object provides a haptic link between geographically distributed users, opening up a channel for physical expression over distance.</p>", "people": ["ishii@media.mit.edu"], "title": "inTouch", "modified": "2016-12-05T00:16:32.468Z", "visibility": "PUBLIC", "start_on": "1995-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "intouch"}, {"website": "", "description": "<p>We are developing the next generation of invention kits for kids, expanding the range of what kids can design, create, and invent. This project builds on our previous work on programmable-brick technologies, which led to the LEGO MindStorms and PicoCricket products.</p>", "people": ["millner@media.mit.edu", "mres@media.mit.edu", "ericr@media.mit.edu", "bss@media.mit.edu", "nrusk@media.mit.edu"], "title": "Invention Kits for Kids", "modified": "2016-12-05T00:16:32.571Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "invention-kits-for-kids"}, {"website": "", "description": "<p>Bringing deliberative process and consensus decision-making to the 21st century! A practical set of tools for assisting in meeting structure, deliberative process, brainstorming, and negotiation. Helping groups to democratically engage with each other, across geographies and time zones.</p>", "people": ["geek@media.mit.edu"], "title": "InterTwinkles", "modified": "2016-12-05T00:16:32.502Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "intertwinkles"}, {"website": "", "description": "<p>Jabberstamp is the first tool that allows children to synthesize drawings and voices. To use Jabberstamp, children create drawings, collages, or paintings on normal paper, and then press a special rubber stamp onto the page to record sounds into their drawings. When they touch the marks of the stamp with a small trumpet, the sounds play back, retelling the stories they have created. Jabberstamp can be used to embed names, narratives, characters' voices, and environmental sound effects into drawings, helping children to communicate their stories with peers and adults, and allowing them to record and situate stories in personally meaningful contexts to share with others\ufffdbefore they have mastered writing.</p>", "people": ["ishii@media.mit.edu"], "title": "Jabberstamp", "modified": "2016-12-05T00:16:32.720Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "jabberstamp"}, {"website": "", "description": "<p>The Digital Aura project deals with ways to give end users better control over their personal information, leveraging principles of social and physical proximity. People are quite good at establishing a social style and using it in different communications contexts, but they do less well when the communication is mediated by computer networks. It is hard to control what information is revealed and how one's digital persona will be presented or interpreted. In this project, we ameliorate this problem by creating a \"Virtual Private Milieu,\" a \"VPM,\" that allows networked devices to act on our behalf and project a \"digital aura\" to other people and devices around us in a manner analogous to the way humans naturally interact with one another. The dynamic aggregation of the different auras and facets that the devices expose to one another creates social spheres of interaction between sets of active devices, and consequently between people. We demonstrate our prototype of the \"Social Dashboard,\" a readily usable control for one's aura. Finally, we present \"Comm.unity,\" a software package that allows developers and researchers easy implementation and deployment of local and distant social applications, and present several applications developed over this platform.</p>", "people": ["lip@media.mit.edu", "nadav@media.mit.edu"], "title": "Digital Aura", "modified": "2016-12-05T00:16:32.623Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "digital-aura"}, {"website": "", "description": "", "people": [], "title": "SNaSI: Wearable device to help the blind with social Navigation", "modified": "2017-10-12T20:57:36.285Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": [], "published": false, "active": false, "end_on": "2017-02-01", "slug": "snasi-wearable-device-to-help-the-blind-with-social-navigation"}, {"website": "", "description": "<p>Traditional writing instruments have the potential to enable new forms of interactions and collaboration though digital enhancement. <a href=\"http://dl.acm.org/citation.cfm?id=2380309&amp;CFID=661831763&amp;CFTOKEN=43505533\">This work specifically enables the user to utilize pen and paper as input mechanisms for content to be displayed on a shared interactive whiteboard.</a> We designed a pen cap with an infrared led, an actuator and a switch. Pointing the pen cap at the whiteboard allows users to select and position a \"canvas\" on the whiteboard to display handwritten text while the actuator enables resizing the canvas and the text. It is conceivable that anything one can write on paper anywhere, could be displayed on an interactive whiteboard.\n                    \n                </p>", "people": [], "title": "Point & Share", "modified": "2016-12-05T17:41:40.122Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2011-12-01", "slug": "point-share"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: normal;\">The MIT Media Lab collaborated with London-based theater group </span><a style=\"font-size: 18px; font-weight: normal;\" href=\"https://www.punchdrunk.com/\">Punchdrunk</a><span style=\"font-size: 18px; font-weight: normal;\"> to create an online platform connected to their </span><a style=\"font-size: 18px; font-weight: normal;\" href=\"http://www.sleepnomorenyc.com/#share\">New York City production of <i>Sleep No More</i></a><span style=\"font-size: 18px; font-weight: normal;\">. In the live show, masked audience members explore and interact with a rich environment, discovering their own narrative pathways. We developed an online companion world to this real-life experience, through which online participants partner with live audience members to explore the interactive, immersive show together. Pushing the current capabilities of web standards and wireless communications technologies, the system delivered personalized multimedia content allowing each online participant to have a unique experience co-created in real time by his own actions and those of his onsite partner.  This project explored original ways of fostering meaningful relationships between online and onsite audience members, enhancing the experiences of both through the affordances that exist only at the intersection of the real and the virtual worlds.</span><br></p><p>The first version of the extended <i>Sleep No More</i> was offered to a limited public in May 2012. Further elaborations and next steps are currently being evaluated.</p>", "people": ["ejessop@media.mit.edu", "patorpey@media.mit.edu", "gershon@media.mit.edu", "bmayton@media.mit.edu", "tod@media.mit.edu", "akito@media.mit.edu", "jieqi@media.mit.edu", "benb@media.mit.edu", "jhaas@media.mit.edu"], "title": "Remote Theatrical Immersion: Extending \"Sleep No More\"", "modified": "2017-04-03T19:51:26.732Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2012-05-01", "slug": "remote-theatrical-immersion-extending-sleep-no-more"}, {"website": "", "description": "<p>Sleep is perhaps the most critical human activity for well-being, but seldom receives the priority it deserves. We get less rest than we need, and the respite we do achieve is often not in&nbsp;proper or comfortable postures. We present a <b style=\"font-size: 18px;\">smart pillow \u201cRePose\u201d</b><span style=\"font-size: 18px; font-weight: 400;\">&nbsp;to explore a new genre of soft, shape-changing auto-adaptive furniture.&nbsp;This new class of interactive \"Smart Furniture\" device themed around pillows, couches, and beds are designed to transform for, conform to, and inform the user as they rest. Using inflatable pouches and pressure-sensors, RePose detects uncomfortable or unergonomic usage and changes its shape and stiffness accordingly to promote good posture. The result is a pillow that is comfortable for everyone in every situation, and is capable of a variety of additional functionalities for sensing and actuation.</span><br></p>", "people": ["djfitz@media.mit.edu", "ishii@media.mit.edu"], "title": "RePose", "modified": "2017-12-07T03:48:02.073Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["tangible-media", "advancing-wellbeing"], "published": true, "active": false, "end_on": "2016-09-01", "slug": "repose-smart-pillow"}, {"website": "", "description": "<p>There is a great opportunity to provide kids and adults with a collection of programmable, computationally enhanced, inexpensive, and user-friendly fabrication tools so that they can make a variety of things for meeting their own personal needs as well as the social challenges of the communities in which they live. But current computer-aided design tools are the main obstacle in increasing creativity and productivity. LaserLogo has been designed to address this issue. LaserLogo is a vector-based, graphic version of the programming language Logo, where the graphical turtle serves as a concrete metaphor for understanding the representations used in describing project files for common fabrication applications. In addition to a programming approach to designing objects, traditional direct manipulation tools are also available. The current designs from LaserLogo can be made with laser and waterjet cutters.</p>", "people": [], "title": "JET", "modified": "2016-12-05T00:16:32.740Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["personal-fabrication"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "jet"}, {"website": "", "description": "", "people": [], "title": "PerForm", "modified": "2018-04-18T20:56:36.870Z", "visibility": "LAB-INSIDERS", "start_on": "2017-11-01", "location": "", "groups": [], "published": false, "active": false, "end_on": "2019-04-18", "slug": "perform"}, {"website": "", "description": "<p>Jeux Deux is an interactive concerto for Hyperpiano, Orchestra, and Live Computer Images.  Utilizing novel software, Jeux Deux allows a virtuosic pianist to interact with a real acoustic piano (Yamaha Disklavier) in real-time, creating sounds and textures that are unplayable by a human player alone.  Live computer graphics are generated in real-time from the piano's data and projected above the performer.</p>", "people": ["tod@media.mit.edu"], "title": "Jeux Deux", "modified": "2016-12-05T00:16:32.771Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-443C", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "jeux-deux"}, {"website": "", "description": "<p>We classify characters in Tamil, a south Indian language,&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">using convolutional neural networks (ConvNets) into 35 different&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">classes. ConvNets are biologically inspired neural&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">networks. Unlike other vision learning approaches where&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">features are hand designed, ConvNets can automatically&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">learn a unique set of features in a hierarchical manner. We&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">augment the ConvNetJS library for learning features by using&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">stochastic pooling, probabilistic weighted pooling, and&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">local contrast normalization to establish a new state-of-the art&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">of 94.4% accuracy on the IWFHR-10 dataset.&nbsp;</span></p>", "people": [], "title": "Tamil Handwriting Recognition", "modified": "2016-12-05T18:10:30.101Z", "visibility": "PUBLIC", "start_on": "2014-11-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2014-12-15", "slug": "tamil-handwriting-recognition"}, {"website": "", "description": "<p>BTNz! is a lightweight, viral interface consisting of a button and a screen strategically positioned around the Media Lab complex to foster social interactions within the community. Users will be able to upload messages to be displayed on the screen when the button is pushed. The goal is to see if the action of pressing a tangible button makes people more aware of what is going on throughout the community. In some ways, BTNz! is a \"twitter of billboards.\"  The idea is to get people together with almost no overhead, and in a fun way, with a single-dimension interface. The work includes building an application environment and collecting and analyzing data on the emergent social activities. Later work may involve tying identity to button-pushers and providing more context-aware messages to the users.</p>", "people": ["kll@media.mit.edu", "lip@media.mit.edu"], "title": "BTNz!", "modified": "2017-10-18T01:12:00.161Z", "visibility": "GROUP", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["city-science", "viral-communications"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "btnz"}, {"website": "", "description": "<p><b><i>The Intertidal Experimentation Workshop will take place September 29 and 30 (9am to 2pm) at the MIT Media Lab, open to students ages 8-14. To register, please visit <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScK6OlnRFis3YNc2ijTHh6QHu_KtDUIGGdQbG3jsVusJrUdEQ/viewform\">this link</a>.&nbsp;</i></b></p><p><i>Field Exploration in Boston's Intertidal Zone </i>is a two-day, hands-on educational workshop for neurodiverse youth in the Greater Boston area, in which participants will use the city of Boston as a classroom, laboratory, and creative playground. Together, scientists, engineers, and artists will take to the field as explorers in order to answer questions related to ecology, biology, chemistry, art, and more.&nbsp;</p><p>Workshop sessions will take place from 9am to 2pm on September 29 and 30, 2018. We will gather at the MIT Media Lab (75 Amherst Street, Cambridge) prior to traveling to field sites located within Greater Boston.&nbsp;</p><p><b>Day I. Introduction to Intertidal Ecology and Experimental Design. <br></b>Day I of this workshop will take place at the MIT Media Lab, where students will learn about the ecology of rocky intertidal zones as well as experimental design. Students will additionally work collaboratively in small groups to develop hypotheses about the phenomena occurring in Boston's urban intertidal zones which they will subsequently test on-site during Day II.&nbsp;</p><p><b>Day II. Data Collection, Interpretation, and Presentation</b>. <br>On Day II, following the field investigation, students will regroup at the Media Lab to get a crash course on data interpretation and visualization. Thereafter, they will present their work to classmates as well as parents and community members through text, graphics, and more.&nbsp;</p><p><i>Participation is free of charge, and all materials and meals are provided.&nbsp;</i><i>For questions, or opportunities for involvement, please contact Avery Normandin (ave@media.mit.edu).</i>&nbsp;</p><p><b>Why Urban Oceans?<br></b>Presently, over 40% of the world\u2019s population lives within 100 kilometers of the coastline, often in seaside megalopolises. While it is known that urban-adjacent marine ecosystems are subjected to unique stressors\u2014namely unparalleled amounts of pollution stemming from urban runoff\u2014efforts related to ocean conservation, as well as marine ecological investigation, most frequently concern the open sea, beyond the immediate reaches of urban ecosystems.</p><p>To better inform regulatory actions related to urban ocean protection, we must understand the unique qualities of these ecological bodies\u2014the seas of cities\u2014particularly as global changes (climate change, rapid urbanization) increase strain on these fragile systems. </p><p>In parallel, given technological-driven paradigmatic shifts in our ability to characterize the unknown world, we are driven to generate innovative and novel platforms for education in the environmental sciences: experiential, instructional excursions which will empower and inspire urban populations to spearhead efforts to sculpt the future of their territories. </p><p>Ideally, these sorts of experiences will cater to all individuals, regardless of gender, race, or cognitive differences.</p><p><b>A Workshop for Neuroinclusivity&nbsp;<br></b>\"Citizen science\" (or Open Science) movements have&nbsp;generated robust momentum for allowing communities to delineate the natural world\u2014or speculate on its future\u2014in hands-on and creative ways. As part of a larger effort to cultivate a future generation of environmentally engaged and justice-focused citizen scientists\u2014and in line with the&nbsp;outreach efforts of the Media Lab's&nbsp;<a href=\"https://www.media.mit.edu/groups/open-ocean/overview/\">Open Ocean Initiative</a>\u2014we have developed&nbsp;<i>Field Experimentation in Boston's Intertidal Zone:</i>&nbsp;a two-day pilot workshop for Boston-area&nbsp;<a href=\"https://medium.com/mit-media-lab/hands-on-ecology-fostering-neuroinclusivity-in-stem-education-c4d8cab61b7\">neurodivergent</a>&nbsp;(e.g., autistic, dyslexic, dyspraxic, ADD, ADHD) youth, in which participants will learn about the ecology of rocky and intertidal systems, develop a hypothesis surrounding these bodies, and subsequently execute a field investigation to test this hypothesis.&nbsp;Students will have the option of approaching fieldwork as a scientist (or engineer), an artist, or a writer (poet, journalist).</p><p>We envision that use of easy-to-access, public sites for the pilot workshop will further democratize the potential to recapitulate similar endeavors in ecological exploration and immersive learning.</p>", "people": ["devora@media.mit.edu", "ave@media.mit.edu"], "title": "Field Experimentation in Boston's Intertidal Zone", "modified": "2019-04-22T17:58:25.679Z", "visibility": "PUBLIC", "start_on": "2018-04-02", "location": "", "groups": ["open-ocean", "sculpting-evolution"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "field-experimentation-in-boston-s-intertidal-zone"}, {"website": "", "description": "<p>By 2030, nine billion people will populate the globe and six out of every 10 will live in cities. The future of global food production will mandate a paradigm shift to resource-leveraged and environmentally sound urban food-growing solutions. The CityFARM project explores building-integrated agriculture and environmentally optimized growing. We are exploring what it means\u2013technologically, environmentally, and socially\u2013to design industrially scalable agricultural systems in the heart of urban areas. Through innovative research, and through development of hydroponic and aeroponic systems, diagnostic and networked sensing, building integration, and reductive energy design, CityFARM methodology reduces water consumption by 90 percent, eliminates chemical pesticides, and reduces embodied energy in produce by a factor of four. By fundamentally rethinking \"grow it THERE and eat it HERE,\" we can eliminate environmental contaminants and increase access to nutrient-dense produce in our future cities.</p>", "people": ["calebh@media.mit.edu", "kll@media.mit.edu", "elplatt@media.mit.edu"], "title": "CityFARM", "modified": "2017-10-18T01:14:51.880Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "cityfarm"}, {"website": "", "description": "<p>How can we help people reflect on their own learning process? The goal of this project is to develop new technological tools and pedagogical strategies to cultivate reflection. Jots are brief updates that people write as they use our Scratch programming environment, to describe their thoughts, frustrations, and excitement.  Users' Jots are displayed on their Scratch user pages, so they can explore their own processes and share them with others.</p>", "people": ["mres@media.mit.edu", "ericr@media.mit.edu"], "title": "Jots", "modified": "2016-12-05T00:16:32.836Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "jots"}, {"website": "", "description": "<p>Our eyes are our our mind's window to the external world. Vision is the primary way we sense our environment, and is a reliable indicator of our attention. As sensors, eyes evolved over millions of years to be fast, precise, and accurate, especially for tracking visual elements of interest (targets), like predators, prey, or baseballs.&nbsp;However, from an HCI perspective, these amazing abilities are also desirable actuators as well as sensors. What if the eyes can not only be a passive window of focus, but an active spatial cursor to indicate our target of focus? Also, what if we add <i>lasers</i>?</p>", "people": ["djfitz@media.mit.edu", "novysan@media.mit.edu"], "title": "LaserVision", "modified": "2017-12-07T04:43:36.579Z", "visibility": "PUBLIC", "start_on": "2015-10-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2015-12-16", "slug": "laser-face"}, {"website": "", "description": "<p>The Junkyard Jumbotron (JJ) lets one take a collection of random screens and instantly stitch them into one large virtual display simply by taking a picture of their arrangement. The software works with laptops, smartphones, tablets\ufffdanything that runs a web browser. It shows a new way of using mobile devices to create a feeling of community: ganging mobile devices together to create a shared experience. And the JJ is designed from the ground up to make the process of connecting heterogeneous user devices together \"in the wild\" easy and fun, with no anti-social wireless configuration, app installation, or device compatibility anxiety.</p>", "people": ["csik@media.mit.edu", "borovoy@media.mit.edu", "rahulb@media.mit.edu", "lip@media.mit.edu"], "title": "Junkyard Jumbotron", "modified": "2016-12-05T00:16:32.886Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "junkyard-jumbotron"}, {"website": "", "description": "<p>The PowerSuit is a micro-energy harnessing material that functions based on temperature differentials between a person's skin and the outside environment. The skin becomes an activated landscape that can be used for micro-power generation. The idea is to consider small increments of energy as useful toward a specific purpose such as lighting safety LEDs while running at night time on cold days. This project is the beginning of an exploration in materials structures that yield micro-power through temperature differentials.</p>", "people": ["kll@media.mit.edu"], "title": "PowerSuit: Micro-Energy Harvesting", "modified": "2017-10-18T01:19:05.316Z", "visibility": "LAB-INSIDERS", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "powersuit-micro-energy-harvesting"}, {"website": "", "description": "<p>If everyone says time is relative, why is it still so rigidly defined? There have been many attempts to address the issue of coordinating schedules, but each of these attempts runs into an issue of rigidity: in order to negotiate an event, a specific time must be designated in advance. This model is inherently poor at accommodating life's unpredictability. Kairoscope looks at time from a human perspective: allowing people to coordinate events socially and on the fly, without worrying about precision. This project evaluates the potential implications of a shared, malleable schedule, as well as the data inputs and user interactions necessary to create such a system.</p>", "people": ["holtzman@media.mit.edu"], "title": "Kairoscope: Social Time", "modified": "2016-12-05T00:16:32.939Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "kairoscope-social-time"}, {"website": "", "description": "<p>The Kaoss network allows an interdependent group of musicians and novices to influence and control their own, as well as each other's, musical output using a mixture of digital and acoustic musical instruments. The project, which draws on theories in system dynamics, decentralized systems, expression, and aesthetics, investigates how coherent and meaningful music can emerge from an interconnected group of professionals and novices. Such a constellation bears the promise of providing new sorts of rich and immersive musical collaborations where the audience takes an active part in the musical experiance.</p>", "people": ["tod@media.mit.edu"], "title": "Kaoss", "modified": "2016-12-05T00:16:33.003Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-491", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "kaoss"}, {"website": "", "description": "<p>KidCAD is a digital clay interface for children to remix toys. KidCAD allows children to imprint 2.5D shapes from physical objects into their digital models by deforming a malleable gel input device, deForm. Users can mashup existing objects, edit and sculpt or draw new designs on a 2.5D canvas using physical objects, hands and tools as well as 2D touch gestures. Finally designs can be 3D printed so that children can play with the toys they designed using KidCAD.</p>", "people": ["ishii@media.mit.edu"], "title": "KidCAD", "modified": "2016-12-05T00:16:33.106Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "kidcad"}, {"website": "", "description": "<p>It takes quite a few people to keep the visuals flowing for live broadcast or display: cameramen filming, technicians cutting and fading clips, and still others controlling the segue animations and copy. Lacking access to that kind of manpower or equipment, we have built a digital video camera with onboard controls that allows for live, real-time editing and special effects for immediate redisplay or broadcast. A wireless version is currently in development.</p>", "people": [], "title": "Key Grip", "modified": "2016-12-05T00:16:33.085Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-301", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "key-grip"}, {"website": "", "description": "<p>kiNET is an active wall that performs, changes, shifts, and adapts. The wall is no longer an opaque plane that encloses our bodies in a space; rather, it is an artistic intervention in a space, a window, a painting, a canvas. Distributed electromagnetic nodes actuate on a surface, creating propagating ripples. The canvas ceases to be a static surface on which the artist lays his or her work, and becomes a body in movement.</p>", "people": ["csik@media.mit.edu"], "title": "kiNET", "modified": "2016-12-05T00:16:33.144Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "kinet"}, {"website": "", "description": "<p>We present a stiffness-changing interface based on a magneto-rheological (MR) fluid. The device consists of a material&nbsp;surface with electromagnetically&nbsp;induced visco-elasticity,&nbsp;which acts as a proxy for stiffness during tangible interaction with the material. We present several advantages of&nbsp;this enabling technology and outline potential applications&nbsp;and routes for future development.</p>", "people": ["udayan@media.mit.edu", "djfitz@media.mit.edu", "sareen@media.mit.edu", "ishii@media.mit.edu", "andresc@media.mit.edu"], "title": "inFlux", "modified": "2017-12-07T04:43:01.290Z", "visibility": "PUBLIC", "start_on": "2015-11-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2015-12-15", "slug": "influx"}, {"website": "", "description": "<p>Kinetic Sketch-Up is a system for prototyping motion and kinetic transformability into interfaces. A series of actuated modules allow users to embed programmable actuation into tangible interfaces, dynamically coupling digital information with self-reconfigurable structures. Modules in the systems combine varied materials and technologies for actuation  creating different forms of motion. This provides a method to prototype perceptual qualities which motion can elicit in the design of an interface: drawing our attention, providing physical feedback, and conveying information through physical change. As interaction design begins to incorporate many of the interaction principles of robotics, the system provides a new design tool and vocabulary for motion construction in the emerging areas of transformable products, environments, and architecture. </p>", "people": ["ishii@media.mit.edu"], "title": "Kinetic Sketch-Up", "modified": "2016-12-05T00:16:33.257Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "kinetic-sketch-up"}, {"website": "", "description": "<p>We use text-capable mobile devices, such as cellphones with SMS and two-way pagers, as mobile interfaces to our desktop computers, combining PDA functionality, communication, and Web access into a single device. Rather than put intelligence into the portable device, we rely on the wireless network to connect to services that enable access to multiple desktop databases, such as your calendar or Rolodex, and external sources, such as news, weather, and traffic.</p>", "people": ["geek@media.mit.edu", "stefanm@media.mit.edu"], "title": "Knothole", "modified": "2016-12-05T00:16:33.289Z", "visibility": "PUBLIC", "start_on": "1997-01-01", "location": "E15-344", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "knothole"}, {"website": "", "description": "<p>This is a suite of devices and protocols to support applications in wearable human/social sensing linked to a distributed camera and vision system. The current system includes a sensate wristwatch with biological and gestural sensors, and a lapel-pin device with motion and audio-affect sensing. These all communicate with wall-mounted devices (Portals), each of which has a high-resolution camera, environmental sensors, and a localization system for all devices in the network. All devices record data and audio in sync with the recorded video. A full-spec Zigbee network supports device synchronization and mesh networking. All devices have enough on-board power to extract features from the data.</p>", "people": ["joep@media.mit.edu"], "title": "Lab-Wide and Wearable Sensor and Video Network", "modified": "2016-12-05T00:16:33.391Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-351", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "lab-wide-and-wearable-sensor-and-video-network"}, {"website": "", "description": "<p>With growing dependence on digital voice and text messaging, it is crucial to be able to access messages in the most intuitive fashion. The LampMail project aims to construct a basic prototype of an interactive lamp, which is also used as an interface for voicemail retrieval and control. The lamp is built in the shape of a cat to enhance the idea of interactivitiy. The fiberoptic \"whiskers\" of the cat light up when a new voicemail message is received. When the lamp is turned on, the \"eyes\" (bulbs) light up and the lamp starts playing the new messages. The \"nose\" is a rotary switch-control used to skip to the next message. </p>", "people": ["geek@media.mit.edu"], "title": "LampMail", "modified": "2016-12-05T00:16:33.422Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "lampmail"}, {"website": "", "description": "<p>Social media are becoming increasingly heterogeneous in nature and usage as online communication evolves. Understanding how communities use and repurpose media is important to our understanding of technology and society. We present Landscape of Words, a visualization that uses topic modeling to statistically summarize the semantics of arbitrarily large groups of individual contributors. Common topics, linguistic styles, and memes are projected onto an interactive 3-D landscape, allowing users to explore the usage patterns of millions of individuals. Our first prototype examines Twitter, illustrating how trends in topics evolve over time.</p>", "people": ["judith@media.mit.edu"], "title": "Landscape of Words", "modified": "2016-12-05T00:16:33.447Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-390", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "landscape-of-words"}, {"website": "", "description": "<p>More and more computational activities revolve around collecting, accessing, and manipulating large sets of data, but introductory approaches for learning programming typically are centered around algorithmic concepts and flow of control, not around data. Computational exploration of data, especially data-sets, has been usually restricted to predefined operations in spreadsheet software like Microsoft Excel. This project builds on the Scratch programming language and environment to allow children to explore data and datasets. With the extensions provided by this project, children can build Scratch programs to not only manipulate and analyze data from online sources, but also to collect data through various means such as surveys and crowd-sourcing. This toolkit will support many different types of projects like online polls, turn-based multiplayer games, crowd-sourced stories, visualizations, information widgets, and quiz-type games.</p>", "people": ["sdg1@media.mit.edu", "mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "Learning with Data", "modified": "2016-12-11T15:29:24.776Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "learning-with-data"}, {"website": "", "description": "<p>Designed specifically as a tool to help young elementary school children explore the fundamentals of mechanical motion, this project combined LEGO-based motion modules with a java-based search tool. Each motion module isolated and highlighted a specific type of mechanical motion (i.e. eccentric motion, up-and-down movement) which could then be built upon or integrated into a child's existing creations. The software component could then be used to lookup the mechanics underlying each module or as a reference to help the child choose and build the type of motion they were trying to replicate.     \n</p>", "people": ["mres@media.mit.edu"], "title": "Learning About Motion", "modified": "2016-12-05T00:16:33.493Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "1999-09-01", "slug": "learning-about-motion"}, {"website": "", "description": "<p>Landman Report Card is the first in a suite of applications designed to help communities affected by extractive industries to recognize, report, and act on their interests. LRC allows landowners to document, discuss, and rate their experiences with landmen, the professional negotiators who work for oil and gas companies. We are currently deploying the application in communities in several states in the US.</p>", "people": ["ethanz@media.mit.edu"], "title": "Landman Report Card", "modified": "2016-12-05T00:16:33.531Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "Cube", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "landman-report-card"}, {"website": "", "description": "<p>A very simple and inexpensive scanning laser rangefinder system has been developed for use as a precise gestural interface in front of a \"smart\" interactive surface. This device works by detecting the phase shift between the emitted laser and the detected reflection off of a bare hand. A simple microprocessor locates peaks in the intensity data corresponding to different objects, and outputs the angle and range information for each one over a serial connection. Unlike computer vision systems, our 2-D scanner measurement is unambiguous; it requires essentially no processing, and is unaffected by background light.  We have recently re-engineered our prototype and are making more systems to explore new applications in collaboration with sponsors and research partners.  We are also exploring integrating this system into small, portable barcode scanners.</p>", "people": ["joep@media.mit.edu"], "title": "LaserWall", "modified": "2016-12-05T00:16:33.553Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "laserwall"}, {"website": "", "description": "<p>This project is developing new ways of working with local communities to create long-term self-sufficiency through the introduction of new methodologies and technologies. Computational technologies (i.e., low/alternative power and low-cost technologies) can open both new possibilities for development and new ways of learning and working. These opportunities can result in a deeper understanding of the environment and of applicable innovations leading to self-sufficiency. The methods used should take a systemic view and actively encourage and support sustainable development, active participation, ownership by and empowerment of local communities, and integrated participative evaluation processes. One of the primary areas upon which we will focus is learning: not learning in the abstract or in the typical school sense, but instead learning in the context of the local needs of the community, such as food production, environmental issues, and infrastructure.</p>", "people": ["cavallo@media.mit.edu", "papert@media.mit.edu", "calla@media.mit.edu"], "title": "Learning and Community Development", "modified": "2016-12-05T00:16:33.586Z", "visibility": "LAB", "start_on": "2001-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2001-01-01", "slug": "learning-and-community-development"}, {"website": "", "description": "<p>Have you ever been forwarded an email that you just can't believe? Our inboxes are rife with misinformation. The truth is out there, just not when we actually need it. LazyTruth is a Gmail gadget that surfaces verified truths when you receive common chain emails. It all happens right in your inbox, without requiring you to search anywhere. The result is that it becomes much more convenient for citizens to combat misinformation, rather than acquiesce to its volume. Whether it's political rumors, gift card scams, or phishing attempts, fact is now as convenient as fiction. </p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "stempeck@media.mit.edu"], "title": "LazyTruth", "modified": "2016-12-05T00:16:33.622Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "lazytruth"}, {"website": "", "description": "<p>We are creating a network of entities called \"Learning Hubs.\" The two primary goals are to create at least one new pilot for an innovative learning environment, and to form a local group to develop, guide, research, and help others appropriate successful models. Participants believe that changes in the learning environment are possible, desirable, and urgently needed as digital technology spreads; that the introduction of computers into schools is inadequate; and that larger changes will not be automatic consequences of the presence of technology in schools. Local Learning Hubs will serve as public-access technology and learning centers, schools, centers for community development, incubators for small technology-based businesses, sites for professional development of educators, and centers for intellectual and political discussion. Local Learning Hubs differ by country; this international network is essential, creating a critical mass of concrete examples of innovative learning environments.</p>", "people": ["cavallo@media.mit.edu", "papert@media.mit.edu", "calla@media.mit.edu"], "title": "Learning Hubs", "modified": "2016-12-05T00:16:33.641Z", "visibility": "LAB", "start_on": "2000-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2000-01-01", "slug": "learning-hubs"}, {"website": "", "description": "<p>MusicBox focuses on the problem of navigating a large body of music.  It aims to help you find music you like, both inside your own collection (to match a particular mood, for example), or from a body of entirely new music.  MusicBox visualizes your music collection in space, giving each track a location based on how similar it sounds to other tracks.  This new manner of navigation stands in stark contrast to traditional, text-dependent media players like iTunes and Windows Media Player.</p>", "people": ["tod@media.mit.edu"], "title": "MusicBox: Navigating the Space of Your Music", "modified": "2016-12-05T00:16:33.826Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "musicbox-navigating-the-space-of-your-music"}, {"website": "", "description": "<p>\"I can't do this\" and \"I'm not good at this\" are common statements made by kids while trying to learn. Usually triggered by affective states of confusion, frustration, and hopelessness, these statements represent some of the greatest problems left unaddressed by educational reform. Education has emphasized conveying a great deal of information and facts, and has not modeled the learning process. When teachers present material to the class, it is usually in a polished form that omits the natural steps of making mistakes (feeling confused), recovering from them (overcoming frustration), deconstructing what went wrong (not becoming dispirited), and finally starting over again (with hope and maybe even enthusiasm). Learning naturally involves failure and a host of associated affective responses. This project aims to build a computerized learning companion that facilitates the child's own efforts at learning. The goal of the companion is to help keep the child's exploration going, by occasionally prompting with questions or feedback, and by watching and responding to the affective state of the child\ufffdwatching especially for signs of frustration and boredom that may precede quitting, for signs of curiosity or interest that tend to indicate active exploration, and for signs of enjoyment and mastery, which might indicate a successful learning experience. The companion is not a tutor that knows all the answers but rather a player on the side of the student, there to help him or her learn, and in so doing, learn how to learn better.</p>", "people": ["bickmore@media.mit.edu", "picard@media.mit.edu", "reilly@media.mit.edu", "atenea@media.mit.edu"], "title": "Learning Companion", "modified": "2016-12-05T00:16:33.851Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "learning-companion"}, {"website": "", "description": "<p>We are investigating a novel pixellated vibrotactile device based on the hardware and software infrastructure developed in previous MLE-UL projects. This project, however, introduces the innovation of being both input and output devices for human-computer interaction, simultaneously. The resulting device will be modular, act as both an input and output device, and be self-configuring. Plug-in software will be developed for application authoring in existing commercial and open source packages, and a number of use scenarios will be developed and evaluated. (Funded by the Higher Education Authority of Ireland.)</p>", "people": ["joep@media.mit.edu"], "title": "Self-Organized Pixellated Vibrotactile Input-Output Device", "modified": "2016-12-05T00:16:33.708Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "self-organized-pixellated-vibrotactile-input-output-device"}, {"website": "", "description": "<p>Many younger students find science and engineering uninteresting and difficult, while university students often learn numerical methods but don't get practical design experience. We have found robot building to be a highly engaging and pedagogically rich activity for students of all ages, facilitating explorations of sensing, control, and mechanism. This research included the development of technologies to facilitate robot-building workshops, and the evaluation of students' learning processes.</p>", "people": ["mres@media.mit.edu"], "title": "Learning Engineering by Designing Robots", "modified": "2016-12-05T00:16:33.879Z", "visibility": "PUBLIC", "start_on": "1992-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "1999-09-01", "slug": "learning-engineering-by-designing-robots"}, {"website": "", "description": "<p>We aim to computationally model the meaning of music by taking advantage of community usage and description\ufffdusing the self-selected and natural similarity clusters, opinions ,and usage patterns as labels and ground truth to inform on-line and unsupervised \"music acquisition\" systems that learn about music by listening and reading. We present a framework for capturing community metadata from free-text sources, audio representations robust enough to handle event and meaning relationships yet general enough to work across domains of music, and a machine-learning framework for learning the relationship between meaning and music automatically and iteratively from a cold start. These unbiased and organic machine-learning approaches show superior accuracy in music and multimedia intelligence tasks such as similarity, artist classification, and recommendation.</p>", "people": ["bv@media.mit.edu"], "title": "Learning the Meaning of Music", "modified": "2016-12-05T00:16:33.961Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "learning-the-meaning-of-music"}, {"website": "", "description": "<p>We approach the problem of how machines, and humans, can learn words that describe actions. We put forth that such words are grounded not in the sensory-motor aspect of an action, but rather in the intentions of the person performing the action. We therefore pose the problem of action-word learning in two stages: intention recognition and linguistic mapping. The first of these stages is cast as a plan-recognition problem in which state-action sequences are parsed using a probabilistic online chart parser. The second stage casts mapping in a Bayesian framework, employing algorithms used in speech recognition and machine translation.</p>", "people": ["dkroy@media.mit.edu", "mbf@media.mit.edu"], "title": "Learning Words for Actions", "modified": "2016-12-05T00:16:34.031Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-487", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "learning-words-for-actions"}, {"website": "", "description": "<p>This project addresses the need for friendlier solutions to the issue of medication compliance. Sensepad is a portable platform which can automatically identify, sense, and track the medications which are placed on it.  Information is provided to the user via a graphical display embedded into the sensing surface.  By monitoring how often the medications  are used, this interface can provide the patient and/or doctor with a usage  history and warn the patient about possible adverse drug interactions. Future versions of this device will also be able to count pills and connect to the Internet via a modem or two-way pager. \n</p>", "people": ["fletcher@media.mit.edu", "neilg@media.mit.edu"], "title": "Sensepad Medication Monitor", "modified": "2016-12-05T00:16:33.986Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "sensepad-medication-monitor"}, {"website": "", "description": "<p>Procedural representations, enabled through programming, are a powerful tool for digital illustration, but writing code conflicts with the intuitiveness and immediacy of direct manipulation. Para is a digital illustration tool that uses direct manipulation to define and edit procedural artwork. Through creating and altering vector paths, artists can define iterative distributions and parametric constraints. Para makes it easier for people to create generative artwork, and creates an intuitive workflow between manual and procedural drawing methods.</p>", "people": ["jacobsj@media.mit.edu", "mres@media.mit.edu"], "title": "Para", "modified": "2016-12-11T15:31:18.256Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2016-09-01", "slug": "para"}, {"website": "", "description": "<p>LifeNet is a probabilistic spatial and temporal model allowing common-sense physical simulation. Mixtures of gaussians are used to map from the physical world's real numbers to human language's symbolic conceptual world. Humans are very good at moving around and physically manipulating the world in which they live, quickly considering many collections of physical events and objects in order to choose a set of actions. The current state of the art in computer physics simulations does not take advantage of common-sense knowledge such as \"things usually fall if not supported.\" Building a common-sense physics simulation allows us to take a paragraph of text and quickly reconstruct the physical situation in more or less detail as required for inference, planning, and further reflective algorithms. Immediately possible applications include interpreting sensor data, comparing physical descriptions of events in text, and efficiently planning in multiscale, robotic physical manipulation environments.</p>", "people": ["joep@media.mit.edu", "minsky@media.mit.edu"], "title": "LifeNet: Common-Sense Physics", "modified": "2016-12-05T00:16:34.163Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-309", "groups": ["society-of-mind"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "lifenet-common-sense-physics"}, {"website": "", "description": "<p>In Linear Mandala, a single participant wears a custom headset outfitted with an electroencephalography (EEG) sensor. The participant walks alongside a row of monitors and speakers. Video, sound, and the physical headset are designed to support contemplation. In real time, a shadow-like avatar pushes a ball of objects representing each participant's brain activity. The participant must maintain a consistent brain state for the avatar and ball to move forward in tandem with his physical movement.</p>", "people": ["holtzman@media.mit.edu"], "title": "Linear Mandala", "modified": "2016-12-05T00:16:34.363Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "linear-mandala"}, {"website": "", "description": "<p>LilyPond is a budding e-textile Web community that fosters creative collaboration through the sharing of personal projects. Home to a growing repository of skill- and project-based tutorials, LilyPond provides support for young adults who want to design and create soft, interactive circuits with the LilyPad Arduino toolkit.</p>", "people": ["leah@media.mit.edu"], "title": "LilyPond", "modified": "2016-12-05T00:16:34.329Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "lilypond"}, {"website": "", "description": "<p>The goal of this work is to create a communication link between disjoint public spaces for sociable casual conversation by combining the ease of conversation in public online forums with the physicality and the affordances of a public space.  In particular, we are interested in extending, modifying and reinventing the \"video-wall\" interface.</p>", "people": ["judith@media.mit.edu"], "title": "Linked Spaces", "modified": "2016-12-05T00:16:34.422Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "linked-spaces"}, {"website": "", "description": "<p>ListenIn uses audio to link two environments; it is motivated by the desire by caregivers to monitor domestic environments with elderly or infant occupants. Wireless microphones in the home collect sound, classify it, and then transmits either iconic representations of the sound, a few seconds of the sound itself, or, if the sound contains speech, a \"garbled\" version of the sound. The remote client plays these snippets of sound as background audio, allowing one to listen in while away from home as an occasional, ambient auditory experience.</p>", "people": ["geek@media.mit.edu"], "title": "ListenIn", "modified": "2016-12-05T00:16:34.443Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "listenin"}, {"website": "", "description": "<p>LipSync is an interactive broadcast of famous monologues available in many different languages to multiple users at the same time. Users direct their smartphones to \"tune in\" to their desired language, either audio or closed captioning, by simply pointing them at the relevant part of the screen. This allows intuitive selection of the speaker and language preference. The precise synchronization between the video and the audio streams creates a seamless experience, where the user's natural motions give voice to moving lips. Multiple people can listen to different audio streams associated with the video toward which the mobile phone camera is pointed. This demonstrates with two technologies developed in the Viral Spaces group: CoSync and VRCodes.</p>", "people": ["lip@media.mit.edu"], "title": "LipSync", "modified": "2016-12-05T00:16:34.467Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "lipsync"}, {"website": "", "description": "<p>In this interactive experience we are interested in enabling quick input actions to Google Glass. The application allows users to trace an object or region of interest in their live view. We use the trace as the foundation for allowing the user to indicate interest in a visual region. Once selected, the user can choose to apply filters to the region, annotate the selection through speech input, or capture text through optical character recognition. These selection and processing tools could naturally integrate with quick note-taking applications where limited touchpad input precludes such input. The Live Trace app demonstrates the effectiveness of gestural control for head-mounted displays.</p>", "people": ["geek@media.mit.edu"], "title": "Live Trace", "modified": "2016-12-05T00:16:34.536Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "live-trace"}, {"website": "", "description": "<p>Currently, Web users have little knowledge about the activities of fellow users. They cannot see the flow of on-line crowds or identify centers of on-line activity. LiveWeb visualizes user activities at a Web site. It graphically lays out the structure of the site and then overlays real-time user accesses.  This system allows a user to perceive both the broad trends and the individual behaviors of other Web visitors.\n</p>", "people": ["judith@media.mit.edu"], "title": "LiveWeb", "modified": "2016-12-05T00:16:34.552Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-449", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "liveweb"}, {"website": "", "description": "<p>Plateau is a Java toolkit that aides developers in building scaleable collaborative applications. Collaborative applications rely on the transmission of a large amount of updates between hosts to operate correctly. As the number of client hosts grows, the amount of updates that each host needs to process increases asymptotically faster, choking slow hosts. Plateau will allow developers to use update prioritization at the client end to evenly distribute load over time, preventing slow hosts from choking. This will increase the number of users that can use a collaborative application before it breaks. \n</p>", "people": ["judith@media.mit.edu"], "title": "Load Plateau", "modified": "2016-12-05T00:16:34.612Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "load-plateau"}, {"website": "", "description": "<p>Run your hand across this wallpaper to turn on a lamp, play music, or control your toaster. This project experiments with interactive wallpaper that can be programmed to monitor its environment, control lighting and sound, and generally serve as a beautiful and unobtrusive way to enrich environments with computational capabilities. The wallpaper itself is flat, constructed entirely from paper and paint. The paper is paired with magnetic electronic modules that serve as sensors, lamps, network interfaces, and interactive decorations.</p>", "people": ["mellis@media.mit.edu", "leah@media.mit.edu"], "title": "Living Wall", "modified": "2016-12-05T00:16:34.586Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-368", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "living-wall"}, {"website": "", "description": "<p>Location Linked Information (LLI) introduces a scalable infrastructure to support access to and creation of virtual information nuggets that are tied to a physical space. Imagined as initially being embodied by an interactive, dynamic map viewed on a handheld computer, the system provides two functions for its urban users: 1) the retrieval of information about their surroundings, and 2) the optional annotation of location for communal benefit. LLI is implemented as an extension of the Jabber instant messaging protocol.</p>", "people": [], "title": "Location Linked Information", "modified": "2016-12-05T00:16:34.635Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "NE18-4FL", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "location-linked-information"}, {"website": "", "description": "<p>LocoRadio is a mobile, augmented-reality, audio browsing system that immerses you within a soundscape as you move. To enhance the browsing experience in high-density spatialized audio environments, we introduce a UI feature, \"auditory spatial scaling,\" which enables users to continuously adjust the spatial density of perceived sounds. The audio will come from a custom, geo-tagged audio database. The current demo uses iconic music to represent restaurants. As users move in the city, they encounter a series of pieces of music and the perception enhances their awareness of the numbers, styles, and locations of nearby restaurants. </p>", "people": ["geek@media.mit.edu"], "title": "LocoRadio", "modified": "2016-12-05T00:16:34.707Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "locoradio"}, {"website": "", "description": "<p>Can the voting machine tell when the stress on a voter might compromise their vote? A voter might be agitated because they aren\ufffdt sure of whom to vote for, because someone is manipulating them or because something else is wrong. It is our hypothesis that an agitated voter will have this agitation reflected in their mouse movements, and that by recording and reasoning about mouse movement, we will be able to determine if a voter is somehow agitated when casting their vote.</p>", "people": [], "title": "Long Voting", "modified": "2016-12-05T00:16:34.674Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "long-voting"}, {"website": "", "description": "<p>Loom creates visualizations of the participants and interactions in a  threaded newsgroup.  The renderings reveal patterns indicative of people's  role in the community and of the type of discussion prevalent in a  particular group.  The goal is to develop a visual vocabulary capable of  intuitively conveying this information.</p>", "people": ["judith@media.mit.edu"], "title": "Loom 1", "modified": "2016-12-05T00:16:34.847Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "loom-1"}, {"website": "", "description": "<p>Sensetable is a system that wirelessly, quickly, and accurately tracks the positions of multiple objects on a flat display surface. The tracked objects have a digital state, which can be controlled by physically modifying them using dials or tokens. We have developed several new interaction techniques and applications on top of this platform. Our current work focuses on business supply-chain visualization using system-dynamics simulation.</p>", "people": ["ishii@media.mit.edu", "jpatten@media.mit.edu"], "title": "Sensetable", "modified": "2016-12-05T00:16:34.760Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "sensetable"}, {"website": "", "description": "<p>Based on Karrie Karahalios' past work on visualizing Usenet newsgroups, Loom2 is a more extensive version, trying to explore visually the identity of both the space and the people within. Loom2 attempts to develop a visual language that intuitively presents the socially salient features of online communication. Loom2's visualization is multiscale, showing a \"landscape\" view (an overview of many groups); a medium view (showing patterns within a single group\ufffdthis is what the original Loom project attempted); a closeup view (portraying a single conversation); and a personal view (showing a person over time and group). Loom2 is intuitive and aims to develop a new visualization aesthetic, one that legibly depicts the meaning and feel of the content. Recent work includes developing database software for storing and analyzing Usenet messages, a series of design sketches, and software to test various ideas, in addition to constructing prototype software for a solid visualization derived from the previous sketches. We are developing this primary prototype and analyzing it as an effective tool for gaining an intuitive sense of social landscapes. We are also exploring different ways to extract more detailed social information from the text of various messages.</p>", "people": ["judith@media.mit.edu"], "title": "Loom2", "modified": "2016-12-05T00:16:34.873Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-449", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "loom2"}, {"website": "", "description": "<p>\"Loops\" combines the work of the Lab's Synthetic Characters group, Merce Cunningham, and digital artists Paul Kaiser and Shelley Eshkar. A visualization of Cunningham's dance for hands of the same name, this research was built upon, and extended from, the work of the Synthetic Characters group in a number of ways, motivating the creation of new motor system representations, new graphics-rendering techniques, and new creature-signaling systems.</p>", "people": ["tod@media.mit.edu"], "title": "Loops", "modified": "2016-12-05T00:16:34.905Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-450", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "loops"}, {"website": "", "description": "<p>LostInBoston.org is about helping Bostonians work together to make neighborhoods more visitor-friendly. Community groups are partnering with local businesses and institutions to design signs that call out the key spots in their areas. Signs are placed on private land in public places.</p>", "people": ["ethanz@media.mit.edu", "csik@media.mit.edu", "borovoy@media.mit.edu", "rahulb@media.mit.edu"], "title": "LostInBoston.org", "modified": "2016-12-05T00:16:34.928Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "lostinbostonorg"}, {"website": "http://erhardtgraeff.com", "description": "<p>Everyone in the city is an expert on their own experience of that city. So how might we integrate new forms of citizen input into the planning and transformation of public spaces around Mexico City, using both digital and non-digital strategies? EncuestaCDMX is a civic technology platform developed with the Laboratorio para la Ciudad that combines in-person surveys and responses from a version of the Action Path location-based survey app to inform city planning decisions. The survey responses power a real-time public dashboard of the feedback available to both city planners and residents for accountability.</p>", "people": ["emreiser@media.mit.edu", "ethanz@media.mit.edu", "rahulb@media.mit.edu", "erhardt@media.mit.edu"], "title": "EncuestaCDMX", "modified": "2016-12-05T00:16:34.976Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2015-10-01", "slug": "encuestacdmx"}, {"website": "", "description": "<p>Based upon prior work on telepresence and tangible interfaces, LumiTouch explores emotional communication in tangible form. The LumiTouch system consists of a pair of interactive picture frames. When one user touches her picture frame, the other picture frame lights up. This touch is translated to light over an Internet connection.  We introduce a semi-ambient display that can transition seamlessly from periphery to foreground in addition to communicating emotional content. In addition to enhancing the communication between loved ones, people can use LumiTouch to develop a personal emotional language.</p>", "people": ["ishii@media.mit.edu"], "title": "LumiTouch", "modified": "2016-12-05T00:16:35.016Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "lumitouch"}, {"website": "", "description": "<p>Jazz music is difficult to analyze harmonically because it purposefully uses ambiguous harmonies during improvisation. This project aims to understand its harmony from the vantage of three levels: time-frequency analysis beyond the estimation of individual notes, understanding of chord sequences, and knowledge stemming from the broader context of jazz music. One measure of this project's success will be when the harmonic analysis can automatically generate a walking bass accompaniment.</p>", "people": [], "title": "Machine Harmonic Transcription of Jazz", "modified": "2016-12-05T00:16:35.114Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2009-09-01", "slug": "machine-harmonic-transcription-of-jazz"}, {"website": "", "description": "<p>How can we pervasively transform all of architectural space, so that every surface is rendered capable of displaying and collecting visual information? We start with the I/O Bulb, a conceptual evolution of the ordinary lightbulb: one which not only projects high-resolution information, but which also simultaneously collects live video of the region onto which it is projected. The Luminous Room results when we seed an enclosed space with a multiplicity of coordinated I/O Bulbs: enough, specifically, so that every location is treated by at least one I/O Bulb. \"Urp\" is a new I/O-Bulb-mediated urban planning workbench; here, simple architectural models cast accurate shadows, pedestrian-level wind patterns can be observed for different arrangements of buildings, reflections off the surfaces of glass buildings onto surrounding terrain are made visible.</p>", "people": ["ishii@media.mit.edu", "jh@media.mit.edu"], "title": "Luminous Room/Urp", "modified": "2016-12-05T00:16:35.036Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "luminous-roomurp"}, {"website": "", "description": "<p>M-Views is a system for the development and deployment of context-aware mobile cinematic narratives. Designed around a generic messaging framework, the project hosts a map agent that allows for location discovery on an 802.11b network. An advanced story-scripting system and authoring tool allows authors to develop space-time story relationships with visual and simulation feedback. Cinematic narrative examples propose alternate methods of engaging with audience participants. A critical aspect of the work is the exploration of techniques to augment intercreativity, narrative play, and collective co-constructed production among participants who may or may not know each other.  </p>", "people": ["gid@media.mit.edu"], "title": "M-Views", "modified": "2016-12-05T00:16:35.054Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-351", "groups": ["gray-matters"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "m-views"}, {"website": "", "description": "<p>Machine Therapy takes two forms: a series of wearable body organs (comforting armors) that are incorporated and used by participants, and the facilitation of live-action psychoanalytic sessions (therapy sessions) with motorized machines. By accessing and vitalizing the interplay of people and machines through psychotherapeutic techniques, a social awareness is presented and individuals are invited to question and reinvent their own existence and their relationships with the machines sharing their space. The Wearable Body Organs are a series of very visible, spectacular or carnival-like play-use objects, devices, or equipments. They offer context-sensitive functionality for their wearers, while simultaneously announcing their own need for existence through public use (without being hidden and small, as is the trend with consumer gadgets and self-helping devices). The Machine Sessions are sometimes private and sometimes public engagements/experiments/performances.</p>", "people": ["monster@media.mit.edu", "csik@media.mit.edu"], "title": "Machine Therapy", "modified": "2016-12-05T00:16:35.166Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "machine-therapy"}, {"website": "", "description": "<p>We are exploring the impact of mobile phones on the future of credit card usage and payments. We are attempting to reinvent the payment process through mobile devices by providing users with tools to help reflect on their finances and eating habits.  We have deployed the Meal Time system on the MIT campus, interlinking it with MIT's TechCASH payment system.  Students have been actively using it to share their experiences and choose locations to eat.</p>", "people": ["lip@media.mit.edu"], "title": "Meal Time", "modified": "2016-12-05T00:16:35.191Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "meal-time"}, {"website": "", "description": "<p>Can a robot and magician collaborate on stage to create a believable, evocative performance? Close human-robot proximity and coordination on a performance stage is a recent development (rapid passing of objects between human hands and robot grippers). Our tools allow us to compose a human-robot performance that blends pre-rendered choreography with key moments of dynamic interactivity to enhance the realism of the character. For example, as the robot is playing back a series of poses, it might also track the face of the performer to maintain eye contact. We are studying how perceived agency and blended static/dynamic interactivity might affect an audience's perception of the performance and how changes in computational robot choreography might also influence a viewer's emotional state. We have built trajectory timeline composition software, a sympathetic interface to an industrial robot, and custom hardware to achieve magic effects.</p>", "people": ["cynthiab@media.mit.edu", "lukulele@media.mit.edu", "dnunez@media.mit.edu"], "title": "Magician-Robot Interaction", "modified": "2016-12-05T00:16:35.243Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "magician-robot-interaction"}, {"website": "", "description": "<p>Books are challenged and banned in public schools and libraries across the country. But which books, where, by whom, and for what reasons? The Mapping Banned Books project is a partnership between the Center for Civic Media, the American Library Association, and the National Coalition Against Censorship to a) visualize existing data on book challenges, b) detect what the existing data doesn't capture, and c) devise new methods to surface suppressed speech. </p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "Mapping Banned Books", "modified": "2016-12-05T00:16:35.289Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "mapping-banned-books"}, {"website": "", "description": "<p>Working with the New World Symphony, we measured participant skin conductance as they attended a classical concert for the first time. With the sensor technology, we noted times when the audience reacted or engaged with the music and other times when they became bored and drifted away. Our overall findings suggest that transitions, familiarity, and visual supplements can make concerts accessible and exciting for new concert goers. We hope this work can help entertainment industries better connect with their customers and refine the presentation of their work so that it can best be received by a more diverse audience.</p>", "people": ["picard@media.mit.edu", "hedman@media.mit.edu"], "title": "Making Engaging Concerts", "modified": "2016-12-05T00:16:35.306Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "making-engaging-concerts"}, {"website": "", "description": "<p>We are developing a set of participatory \"maker\" activities to engage youth in creating tangible projects that depict stories about themselves and their worlds. These activities introduce electronics and computational tools as a medium to create, connect, express, and derive meaning from personal narratives. For example, we are offering workshops where participants design sewable circuits and bring them together to create a collaborative Story Quilt. Through the Making with Stories project we are exploring how story-based pedagogy can inspire youth participation in arts and engineering within formal and informal learning environments.</p>", "people": ["mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "Making with Stories", "modified": "2016-12-05T00:16:35.326Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "making-with-stories"}, {"website": "", "description": "<p>Map Scratch is an extension of Scratch that enables kids to program with maps within their Scratch projects. With Map Scratch, kids can create interactive tours, games, and data visualizations with real-world geographical data and maps.</p>", "people": ["sdg1@media.mit.edu", "mres@media.mit.edu", "bss@media.mit.edu"], "title": "Map Scratch", "modified": "2016-12-05T00:16:35.343Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "map-scratch"}, {"website": "", "description": "<p>MatchMaker is an automated collaborative filtering system that recommends friends to people on Facebook by analyzing and matching people's online profiles with the profiles of TV characters. The goal of MatchMaker is to produce friend recommendations with rich contextual information through collaborative filtering in the existing social network. Using relationships in TV programs as a parallel comparison matrix, MatchMaker projects these relationships into reality to help people find friends whose personality and characteristics have been voted to suit them well by their social network. MatchMaker also encourages more TV content viewing by using the social network context and connections to provoke people's curiosity of TV characters whom they have been matched with in their social network.</p>", "people": ["holtzman@media.mit.edu"], "title": "MatchMaker", "modified": "2016-12-05T00:16:35.439Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "matchmaker"}, {"website": "", "description": "<p>MCam is an Internet-enabled camera disguised as a milk container. It allows users with cell phones or PDAs to remotely snap a picture of their refrigerator's interior. People don't upgrade their fridges frequently, making it difficult for new Internet-enabled refrigerators to gain popularity. This type of technology is a direct alternative to Internet-enabled refrigerators that are almost prohibitive to acquire.</p>", "people": [], "title": "Mcam", "modified": "2016-12-05T00:16:35.464Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-320", "groups": ["context-aware-computing", "counter-intelligence"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "mcam"}, {"website": "", "description": "<p>How can we better understand people\ufffds emotional experiences with a product or service? Traditional interview methods require people to remember their emotional state, which is difficult. We use psychophysiological measurements such as heart rate and skin conductance to map people\ufffds emotional changes across time. We then interview people about times when their emotions changed, in order to gain insight into the experiences that corresponded with the emotional changes. This method has been used to generate hundreds of insights with a variety of products including games, interfaces, therapeutic activities, and self-driving cars.</p>", "people": ["picard@media.mit.edu", "hedman@media.mit.edu"], "title": "Measuring Customer Experiences with Arousal", "modified": "2016-12-05T00:16:35.545Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "measuring-customer-experiences-with-arousal"}, {"website": "", "description": "<p>There have been few studies of bowing data from real players, in part due to the difficulty in capturing this information. We have designed an  interface to measure bowing parameters produced by real players, while maintaining the portability and playability of a traditional violin bow. This interface, the Hyperbow, consists of a carbon-fiber violin bow with a custom wireless sensing system (with accelerometers, gyroscopes, electric field position and force sensors). The Hyperbow is partnered with a Yamaha Silent Violin SV-200 also augmented with gesture sensors. This playable measurement system is the core component of an experimental setup used to investigate the bowing parameters produced by real violinists. In these investigations, the parameters are recorded with the sound produced during performances of different bowing techniques (d\ufffdtach\ufffd, martel\ufffd, spiccato). These data can then be analyzed to help understand the various strategies employed by violinists to achieve similar goals in sound production.</p>", "people": ["tod@media.mit.edu"], "title": "Measurement of Violin Bowing Technique", "modified": "2016-12-05T00:16:35.576Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "measurement-of-violin-bowing-technique"}, {"website": "", "description": "<p>Digital media is increasingly becoming part of our everyday world, inviting us to think, communicate, and express ourselves more heterogeneously than ever. With the realization of an ubiquitous mobile channel for exchange, our interface to media must become more of a partner or an accomplice. The concept of media fabrics suggests that media can become synergistic with our selves, intention-aware of its potential for meaning-making. Such fabrics can invite us to spontaneously co-construct and reflect on the media content and meaning in collaborative and individual improvisational frameworks.</p>", "people": ["barbara@media.mit.edu", "gid@media.mit.edu"], "title": "Media Fabrics", "modified": "2016-12-05T00:16:35.694Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "media-fabrics"}, {"website": "", "description": "<p>The Media Matrix project draws on ideas from distributed, embedded, and parallel computing in order to address the creation and management of databases composed of large collections of physical objects (e.g. mini DVs in a rack or books in a library). The project's primary goal, however, is to serve as both embodiment and test bed for ideas in distributed, embedded, and ubiquitous computing. The Media Matrix concept drew inspiration from concepts in the Tangible Media Group (TMG) and the Object-Based Media Group's Paintable Computing project. Collaborators Josh Lifton and TMG's Jay Lee initiated the Media Matrix project as a final project for Ted Selker's Industrial Design Intelligence class offered Fall 2000. It has since been carried on by Josh in the Responsive Environments Group (ResEnv). Related work includes Pushpin Computing in ResEnv and TouchCounters in TMG. The Media Matrix was presented as an interactive poster at CHI 2001 in Seattle, WA. The next stage in this project involves converting over to battery power, eliminating the need of the powered shelf system. Each unit will consume under a microamp in sleep mode. A special coded \"flashlight\" can be beamed at any unit, causing it to wake up and read the ID sent in the optical beam. This ID can be compared to the local ID on the object and/or transmitted to neighboring objects in the matrix, allowing the requested object to be rapidly located.</p>", "people": ["joep@media.mit.edu"], "title": "Media Matrix", "modified": "2016-12-05T00:16:35.765Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-344", "groups": ["responsive-environments", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-09-01", "slug": "media-matrix-2"}, {"website": "", "description": "<p>The Media Windshield is a demonstration that changes the use of an automobile windshield to serve multiple purposes. When no one is in the car, it can present information to the outside world: advertising, augmentation of the road signs, even personalized information for a party that is walking by. When the driver gets into the car, it adjusts the audio to be appropriate for their seating arrangement and puts up an Internet interface. When the driver lies back, the windshield can be used as a movie screen for DVD videos. Finally, when the driver is using the car, the windshield can shade the sun and lights, put up virtual signs, and annotate difficult-to-see objects.</p>", "people": [], "title": "Media Windshield", "modified": "2016-12-05T00:16:35.913Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "media-windshield"}, {"website": "", "description": "<p>The mediaBlocks prototype is a tangible user interface based upon small,  digitally tagged wooden blocks. The blocks serve as physical icons (\"phicons\") for the containment, transport, and manipulation of online  media. MediaBlocks interface with media input and output devices such as  video cameras and projectors, allowing digital media to be rapidly \"copied\"  from a media source and \"pasted\" into a media display.  MediaBlocks are  also compatible with traditional GUIs, providing seamless gateways between  tangible and graphical interfaces. Finally, mediaBlocks act as physical  \"controls\" in tangible interfaces for tasks such as sequencing collections of media elements.</p>", "people": ["ishii@media.mit.edu"], "title": "mediaBlocks", "modified": "2016-12-05T00:16:36.027Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-441", "groups": ["tangible-media", "personal-fabrication"], "published": true, "active": false, "end_on": "1998-12-30", "slug": "mediablocks"}, {"website": "", "description": "<p>We are developing a lower-cost portable system which allows passive RFID tags to be tracked on a flat surface. By creating a system that tracks the 2-D position of multiple tags, it is possible to create a variety of applications ranging from multi-user interactive tabletops as well as RFID smart shelves, using simple low-cost IC-tags.</p>", "people": ["fletcher@media.mit.edu", "ishii@media.mit.edu"], "title": "SenseTable II, Passive Tag Version", "modified": "2016-12-05T00:16:35.857Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "sensetable-ii-passive-tag-version"}, {"website": "", "description": "<p>Involving communities in the design process results in products that are more responsive to a community's needs, more suited to accessibility and usability concerns, and easier to adopt. Civic media tools, platforms, and research work best when practitioners involve target communities at all stages of the process: iterative ideation, prototyping, testing, and evaluation. In the codesign process, communities act as codesigners and participants, rather than mere consumers, end-users, test subjects, or objects of study. In the Codesign Studio, students practice these methods in a service learning project-based studio, focusing on collaborative design of civic media with local partners. The Toolkit will enable more designers and researchers to utilize the co-design process in their work by presenting current theory and practices in a comprehensive, accessible manner.</p>", "people": ["msauter@media.mit.edu", "ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "Codesign Toolkit", "modified": "2016-12-05T00:16:36.084Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "codesign-toolkit"}, {"website": "", "description": "<p>Medley deals with personal, physically proximate communications. We look at different aspects of data sharing between members of a community where there is better connectivity among group members than to a core network at large such as may be found with the $100 laptop. We diffuse the information among the population, sharing memory and localizing traffic. We face research questions of how information is distributed, what conflicts there are between memory, energy, and communication costs, how one integrates local storage, and how we can use social parameters (e.g., friendships) to determine routing. Additional research interests relate to the human interface aspect, such as what parts of the platform should be exposed to the user and/or give him control over and what parts should be made transparent.</p>", "people": ["ypod@media.mit.edu", "lip@media.mit.edu", "nadav@media.mit.edu"], "title": "Medley", "modified": "2016-12-05T00:16:36.156Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-495", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "medley"}, {"website": "", "description": "<p>One dimension of our group\ufffds unifying goals is research in learning itself, but postulating new ideas for learning is not enough. What can we do to help bring about real impact and tap the latent global learning potential to provide opportunities for every child (and adult) to have the learning environments they deserve? We make a conceptual distinction along three integrated lines: content, tools and environments, and real-world initiatives. Each of our projects emphasizes at least one of these. We work on extremely low-cost technologies to achieve a high ratio of computational material to learner and provide an immersive environment that opens new possibilities for learning otherwise not achievable. We develop new content and support materials that take advantage of access. We create demonstration projects to explicate new ideas, show possibilities, and provide a means for reflection for learning and to facilitate the change process.</p>", "people": ["cavallo@media.mit.edu", "papert@media.mit.edu"], "title": "Mega-Change in Learning", "modified": "2016-12-05T00:16:36.219Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-368", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "mega-change-in-learning"}, {"website": "", "description": "<p>The emergence of digital music on the Internet requires new information-retrieval methods adapted to specific characteristics and needs. A query-by-humming system, which can find a piece of music in the digital music repository based on a few hummed notes, can help people search for music on the Internet, even if they cannot memorize the title or any other text information about the music. New melody representation and matching methods are employed in this project. Combining Internet, audio signal processing, and database techniques, we are attempting to provide a friendlier interface for Internet music searches.</p>", "people": ["bv@media.mit.edu"], "title": "Melody Retrieval on the Web", "modified": "2016-12-05T00:16:36.244Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "melody-retrieval-on-the-web"}, {"website": "", "description": "<p>MelodyMorph is an interface for constructing melodies and making improvised music. It removes a constraint of traditional musical instruments: a fixed mapping between space and pitch. What if you blew up the piano so you could put the keys anywhere you want? With MelodyMorph you can create a customized musical instrument, unique to the piece of music, the player, or the moment.</p>", "people": ["mres@media.mit.edu", "ericr@media.mit.edu"], "title": "MelodyMorph", "modified": "2016-12-05T00:16:36.329Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "melodymorph"}, {"website": "", "description": "<p>To help people improve their reading of faces during natural conversations, we developed a video tool to evaluate this skill. We collected over 100 videos of conversations between pairs of both autistic and neurotypical people, each wearing a Self-Cam. The videos were manually segmented into chunks of 7-20 seconds according to expressive content, labeled, and sorted by difficulty\u2014all tasks we plan to automate using technologies under development. Next, we built a rating interface including videos of self, peers, familiar adults, strangers, and unknown actors, allowing for performance comparisons across conditions of familiarity and expression. We obtained reliable identification (by coders) of categories of smiling, happy, interested, thinking, and unsure in the segmented videos. The tool was finally used to assess recognition of these five categories for eight neurotypical and five autistic people. Results show some autistics approaching the abilities of neurotypicals while several score just above random.</p>", "people": ["picard@media.mit.edu"], "title": "Evaluation Tool for Recognition of Social-Emotional Expressions from Facial-Head Movements", "modified": "2017-06-02T12:52:36.575Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "evaluation-tool-for-recognition-of-social-emotional-expressions-from-facial-head-movements"}, {"website": "", "description": "<p>Meteorite is a permanent, underground, interactive museum that opened in Essen, Germany in June 1998.  Our group designed all of the interaction for the museum, and Tod Machover composed a special  \"walk-through\" opera that varies as visitors explore the building.  A TransFlow Room was designed to allow up to fifty visitors to simultaneously control a coherent environment of interactive sound and image, playing the one hundred sensors lining the walls to create, shape, modify, synchronize, and save-or-eliminate interlocking fragments.\n</p>", "people": ["tod@media.mit.edu"], "title": "Meteorite Museum", "modified": "2016-12-05T00:16:36.487Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "1998-12-30", "slug": "meteorite-museum"}, {"website": "", "description": "<p>The Meta Meta Project is an open-source movement aimed at accomplishing a simple, powerful goal: take in media, spit back metadata. The system can power any project that needs to be smart with its content. For instance, identifying important themes, building smart associations between chunks, and augmenting content with additional information layers. Using a single standardized API, programmers can submit multiple forms of media (text, image, video, audio), and extract information from that media (keywords, OCR, entities). For more information about the project and the contributors visit http://www.metametaproject.org.</p>", "people": ["holtzman@media.mit.edu"], "title": "Meta Meta Project", "modified": "2016-12-05T00:16:36.529Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "meta-meta-project"}, {"website": "", "description": "<p>Programming is the art of constructing a story about objects and what they do in various situations, expressed in programming languages which are easy for the computer to accurately convert into executable code, but difficult for people to write and understand. This project explores using descriptions in a natural language like English as a representation for programs. We cannot convert arbitrary English descriptions to fully specified code, but we can use an expressive subset of English as a visualization tool. Simple descriptions of program objects and their behavior are converted to scaffolding (underspecified) code fragments that can be used as feedback for the designer and then elaborated. Our parser can infer a surprising amount of information about program structure from relations implicit in the linguistic structure; we call this \"programmatic semantics.\" Our program editor, Metafor, dynamically converts a user's stories into program code. Users found it useful as a brainstorming tool.</p>", "people": ["lieber@media.mit.edu", "pattie@media.mit.edu"], "title": "Metafor: Programming by Storytelling", "modified": "2016-12-05T00:16:36.559Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "metafor-programming-by-storytelling"}, {"website": "", "description": "<p>The goal of the Metazine is to provide a jumping-off point for research into the magazine of the future. We have developed an application for Internet tablets to supplement and enhance the content of paper magazines with digital content in a way that preserves the flexibility and robustness of the tried-and-true paper format. The Metazine follows coded links placed near articles and advertisements in a magazine to seamlessly deliver digital video or images on demand.</p>", "people": ["holtzman@media.mit.edu"], "title": "Metazine", "modified": "2016-12-05T00:16:36.582Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "metazine"}, {"website": "", "description": "<p>We are looking at the emotional experience created when children learn games. Why do we start games with the most boring part, reading directions? How can we create a product that does not create an abundance of work for parents? Key insights generated from field work, interviews, and measurement of electrodermal activity are: kids become bored listening to directions, \"it's like going to school\"; parents feel rushed reading directions as they sense their children's boredom; children and parents struggle for power in interpreting and enforcing rules; children learn games by mimicking their parents, and; children enjoy the challenge of learning new games.</p>", "people": ["picard@media.mit.edu", "hedman@media.mit.edu"], "title": "The Frustration of Learning Monopoly", "modified": "2016-12-05T00:16:36.607Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "the-frustration-of-learning-monopoly"}, {"website": "", "description": "<p>Mental health therapies are complex and, to be computer-deliverable, must be customizable and adaptive.  We are applying software engineering principles to automate, and make customizable and adapatable, such therapies via a Web-based application.  The technology also provides a new platform for studying the cognitive process and neural circuitry of therapy to further non-pharmacological methods of health interventions and management.  </p>", "people": ["esb@media.mit.edu", "barbara@media.mit.edu"], "title": "Metatherapy: Customized, Adaptive Therapy Systems", "modified": "2016-12-05T00:16:36.692Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-435", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "metatherapy-customized-adaptive-therapy-systems"}, {"website": "", "description": "<p>Metropath(ologies) is an installation that immersively explores the experience of living in our increasingly information filled\ufffdand information extracting\ufffdurban world, a world that is both vibrantly connected and sinisterly surveillant.</p>", "people": ["judith@media.mit.edu"], "title": "Metropath(ologies)", "modified": "2016-12-05T00:16:36.714Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-348", "groups": [], "published": true, "active": false, "end_on": "2009-09-01", "slug": "metropathologies"}, {"website": "", "description": "<p>Metarchivist is a virtual environment for organizing and browsing documents and other content. It seeks to provide a seamless space for both browsing content and visualizing high-level relationships among resources.</p>", "people": ["vmb@media.mit.edu"], "title": "Metarchivist", "modified": "2016-12-05T00:16:36.628Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "Garden Conference Room", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "metarchivist"}, {"website": "", "description": "<p>A successfully engineered physical system will reach a state of complexity such that it needs its own vocabulary and semantics for proper comprehension. Among many other things, I have designed a three-tier control system for an autonomous submarine, a language for a tabletop quantum computer, and two proposed experiments in neuroengineering: a neural prosthesis that would program a fly's natural avionics, and a 'microbrain' comprising 1,000 cultured neurons that could perform a sophisticated behavioral task. \n</p>", "people": ["esb@media.mit.edu", "neilg@media.mit.edu"], "title": "Metaoperating Systems", "modified": "2016-12-05T00:16:36.671Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-023", "groups": ["physics-and-media", "personal-fabrication"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "metaoperating-systems"}, {"website": "", "description": "<p>This project aims to demonstrate rapid-prototyping of logic gates in the field. We are working on a  candidate mechanism of microfluidic logic at low Reynolds numbers. Cascades of appropriate, simple, nonlinear elements allow synthesis of any logic arbitrary nonlinear function. We are using the nonlinear medium of air because it is so readily available. We are studying Newtonian fluid flow at low Reynolds numbers in specially designed micro-geometries ranging from 100 microns to a few microns. Fluidic Logic was an active field of research in the 60s and 70s, but died prematurely because it could not keep up with the shrinking sizes and improved performance of electronic circuits. Current manufacturing processes can easily create extremely small feature sizes, made possible by increasing the resolution of precision machines and improved metrology techniques. </p>", "people": ["neilg@media.mit.edu"], "title": "Microfluidic Logic", "modified": "2016-12-05T00:16:36.784Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "microfluidic-logic"}, {"website": "", "description": "<p>Midas is a touch-based personalization system that bridges the gap between personal area networks (PANs) and explicit authentication. It is designed to interact with ubiquitous interfaces in the objects and environment surrounding us in order to achieve seamless access to information via public and pseudo-public surfaces and environments that modify themselves to the needs to the users on the fly and on contact. By confining the communication to touched object while avoiding the need to use an explicit authentication token such as a RFID, we enable new means of customizing our surroundings to our needs.</p>", "people": ["sajid@media.mit.edu", "pattie@media.mit.edu"], "title": "Midas", "modified": "2016-12-05T00:16:36.807Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "midas"}, {"website": "", "description": "<p>Collection and analysis of longitudinal observational data of child behavior in natural, ecologically valid, non-laboratory settings holds significant benefits for advancing the understanding of autism and other developmental disorders. We developed the Speechome Recorder\ufffda portable version of the embedded recording technology originally developed for the Human Speechome Project\ufffdto facilitate cost-effective deployment in special-needs clinics and homes. Recording child behavior daily in these settings will enable us to study developmental trajectories of autistic children from infancy through early childhood, as well as atypical dynamics of social interaction as they evolve on a day-to-day basis. Its portability makes possible potentially large-scale comparative studies of developmental milestones in both neurotypical and autistic children. Data-analysis tools developed in this research aim to reveal new insights toward early detection, provide more accurate assessments of context-specific behaviors for individualized treatment, and shed light on autism.</p>", "people": ["dkroy@media.mit.edu", "mgoodwin@media.mit.edu", "soroush@media.mit.edu", "decamp@media.mit.edu"], "title": "Speechome Recorder for the Study of Child Development Disorders", "modified": "2016-12-05T00:16:36.855Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-441", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "speechome-recorder-for-the-study-of-child-development-disorders"}, {"website": "", "description": "<p>Story Mat stores children's storytelling play by recording their voices and the movements of the toys they play with. These stories revive on the mat as other children play and tell their stories.</p>", "people": [], "title": "Story Mat", "modified": "2016-12-05T00:16:36.906Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "story-mat"}, {"website": "", "description": "<p>SpaceBox explores the addressing of location-specific messages (text, voice, images) to an intended recipient or group of recipients. Its key features include tagging places of interest when a sender is physically present at that location and projecting messages to a specific location. The recipient would receive such messages only when in the vicinity of the tagged location.</p>", "people": ["geek@media.mit.edu"], "title": "SpaceBox: Location-Based Messaging", "modified": "2016-12-05T00:16:36.738Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-383", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "spacebox-location-based-messaging"}, {"website": "", "description": "<p>This is a fuzzy logic programming environment for embeddable computing that targets non-technical adults or inexperienced hardware developers, in particular fashion and graphic designers who are interested in using computational elements in their work. Control of the system builds upon fuzzy logic reasoning of small microcontrollers, and programming is done through a server-side Web interface.</p>", "people": [], "title": "Fuzz", "modified": "2016-12-05T00:16:37.040Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-301", "groups": [], "published": true, "active": false, "end_on": "2002-01-01", "slug": "fuzz"}, {"website": "", "description": "<p>The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, c++ machine-learning library that has been specifically designed for real-time gesture recognition. The GRT has been created as a general-purpose tool for allowing programmers with little or no machine-learning experience to develop their own machine-learning based recognition systems, through just a few lines of code. Further, the GRT is designed to enable machine-learning experts to precisely customize their own recognition systems, and easily incorporate their own algorithms within the GRT framework.  In addition to facilitating developers to quickly create their own gesture-recognition systems, the machine-learning algorithms at the core of the GRT have been designed to be rapidly trained with just a few examples examples for each gesture. The GRT therefore allows a more diverse group of users to easily integrate gesture recognition into their own projects.</p>", "people": ["joep@media.mit.edu"], "title": "Gesture Recognition Toolkit", "modified": "2016-12-05T00:16:37.060Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "gesture-recognition-toolkit"}, {"website": "", "description": "<p>We\ufffdve developed a set of tools and techniques that make it easy to use microcontrollers as an art or craft material, embedding them directly into drawings or other artifacts. We use the ATtiny45 from Atmel, a small and cheap (~$1) microcontroller that can be glued directly to paper or other objects. We then construct circuits using conductive silver ink, dispensed from squeeze bottles with needle tips. This makes it possible to draw a circuit, adding lights, speakers, and other electronic components.</p>", "people": ["mellis@media.mit.edu", "leah@media.mit.edu", "jieqi@media.mit.edu"], "title": "Microcontrollers As Material", "modified": "2016-12-05T00:16:37.081Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "microcontrollers-as-material"}, {"website": "", "description": "<p>Mime is a compact, low-power 3D sensor for short-range gestural control of small display devices. The sensor's performance is based on a novel signal processing pipeline that combines low-power time-of-flight (TOF) sensing for 3D hand-motion tracking with RGB image-based computer vision algorithms for finer gestural control. Mime is an addition to a growing number of input devices developed around the engineering design philosophy of sacrificing generality for battery-friendly and accurate performance to retain the portability advantages of our smart devices. We demonstrate the utility of Mime for head-mounted display control and smartphones with a variety of application scenarios, including 3D spatial input using close range gestures, gaming, on-the-move interaction, and operation in cluttered environments and in broad daylight conditions.</p>", "people": ["geek@media.mit.edu"], "title": "Mime", "modified": "2016-12-05T00:16:37.270Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "mime"}, {"website": "", "description": "<p>Hear&There is an augmented reality system using audio as the primary interface. Using the authoring component of this system, individuals can leave \"audio imprints,\" consisting of several layers of music, sound effects, or recorded voice, at a location outdoors. Using the navigation component, individuals can hear imprints by walking into the area that the imprint occupies. Furthermore, imprints can be linked together, whereby an individual is directed from one imprint to related imprints in the area.</p>", "people": ["judith@media.mit.edu"], "title": "Hear&There", "modified": "2016-12-05T00:16:37.408Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-449", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "hearthere"}, {"website": "", "description": "<p>We are developing knowledge representations and natural language processing algorithms that connect spatial language to physical environments. Applications include spatial language understanding for robots, computer games, and natural language interfaces for video retrieval.</p>", "people": ["dkroy@media.mit.edu"], "title": "Grounding Spatial Language: Robots and Maps", "modified": "2016-12-05T00:16:37.240Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "grounding-spatial-language-robots-and-maps"}, {"website": "", "description": "<p>Minecraft is a video game focused on creativity and building. Players build constructions out of textured cubes in a 3D world\ufffdeverything from a hut, to a train station, to a fully functional computer. Why can't we take those virtual creations, and bring them into the real world? Minecraft.Print() is our attempt to do so by creating a bridge between Minecraft and the real world, via 3D printers. A Minecraft player defines a 3D space to be printed, after which the software extracts the object, structure, or other creation from the virtual space and creates 3D-printable version. Minecraft.Print() takes advantage of the basic CAD functions of the game, thus allowing 14,000,000 (and counting) players to experience 3D modeling and printing\ufffdan area previously limited to those with more specific technical backgrounds. </p>", "people": ["sandy@media.mit.edu"], "title": "Minecraft.Print()", "modified": "2016-12-05T00:16:37.296Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "minecraftprint"}, {"website": "", "description": "<p>This project pushes our work on the miniaturization of low-power radios and sensors for embedded intelligence into very small form factors that will enable unobtrusive wireless sensor packages to be leveraged in many (e.g., wearable, medical) applications. In addition, the project addresses applications that use a wireless agent platform running on embedded Java processors connected to the same low-power radios; this continues work done by the National Microelectronics Research Centre (NMRC) at University College Cork. The research capitalizes upon the Media Lab, Media Lab Europe, and NMRC's shared interest in emergent behavior arising from the interaction of software agents. (Funded by the Higher Education Authority of Ireland.)</p>", "people": ["joep@media.mit.edu"], "title": "Functional Integration for Embedded Intelligence", "modified": "2016-12-05T00:16:37.319Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "MLE", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "functional-integration-for-embedded-intelligence"}, {"website": "", "description": "<p>Wireless sensor systems are currently being deployed in a wide variety of lightweight mobile applications such as detecting degenerative diseases, monitoring remote habitats, and tracking the safety of housebound elders. However, current implementations suffer from short lifespans due to high energy use and limited battery size. To enter the consumer mainstream, these applications must be far more energy efficient. This project explores adaptable sensor-driven power management for wireless sensor systems as a means of increasing efficiency. Rather than fully activate nodes on a schedule or in response to very simple stimuli, this project explores an automated framework that we term \"groggy wakeup,\" where the system is activated at increasing energy levels in response to evolving stimuli. This way, the system only becomes fullly awake when an interesting phenomenon is encountered, and resources are appropriately conserved.</p>", "people": ["joep@media.mit.edu"], "title": "Groggy Wakeup: An Automated Framework for Power-Efficient Detection in Smart Sensor Systems", "modified": "2016-12-05T00:16:37.343Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "groggy-wakeup-an-automated-framework-for-power-efficient-detection-in-smart-sensor-systems"}, {"website": "", "description": "<p>MindRider is a helmet that translates electroencephalogram (EEG) feedback into an embedded LED display. For the wearer, green lights indicate a focused, active mental state, while red lights indicate drowsiness, anxiety, and other states not conducive to operating a bike or vehicle. Flashing red lights indicate extreme anxiety (panic). As many people return to cycling as a primary means of transportation, MindRider can support safety by adding visibility and increased awareness to the cyclist/motorist interaction process. In future versions, MindRider may be outfitted with an expanded set of EEG contacts, GPS radio, non-helmet wearable visualization, and other features to increase the cyclist's awareness of self and environment. These features may also allow for hands-free control of cycle function. A networked set of MindRiders may be useful for urban planning and emergency response situations.</p>", "people": ["holtzman@media.mit.edu"], "title": "MindRider", "modified": "2016-12-05T00:16:37.473Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "mindrider"}, {"website": "", "description": "<p>We introduce SociaBowl, a dynamic table centerpiece to promote positive social dynamics in 2-way cooperative conversations. A centerpiece such as a bowl of food, a decorative flower arrangement, or a container of writing tools, is commonly placed on a table around which people have conversations. We explore the design space for an augmented table and centerpiece to influence how people may interact with one another. We present an initial functional prototype to explore different choices in materiality of feedback, interaction styles, and animation and motion patterns. These aspects are discussed with respect to how it may impact people\u2019s awareness of their turn taking dynamics as well as provide an additional channel for expression. Potential enhancements for future iterations in its design are then outlined based on these findings.</p>", "people": ["ishii@media.mit.edu", "joaleong@media.mit.edu"], "title": "SociaBowl: A Dynamic Table Centerpiece to Mediate Group Conversations", "modified": "2019-05-04T23:42:45.225Z", "visibility": "PUBLIC", "start_on": "2018-11-12", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2019-01-07", "slug": "sociabowl"}, {"website": "", "description": "<p>Motivated by the question, \ufffdWhy do people behave strangely while driving cars with GPS navigation systems?\ufffd, we reviewed previous work in the field and paid attention to the psychological (short- and long-term) effects\ufffdmindlessness\ufffdin human minds while interacting with these machines. This psychological phenomenon is analyzed in the framework of mindlessness/mindfulness theory (Ellen Langer, 1989), and we suggest a new approach to the user-interface (UI) design based on the analysis. In this project, we explore problems found in GPS-based car-navigation systems, and suggest new design ideas\ufffdsuch as illusion of control or perceived control\ufffdin UI, via the preliminary experiment of \ufffdMindful Walking.\ufffd In addition, we present some other design possibilities based on Langer's work. Lastly, we suggest a new role for an intelligent agent.</p>", "people": ["geek@media.mit.edu"], "title": "Mindless Machine", "modified": "2016-12-05T00:16:37.550Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-384C", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "mindless-machine"}, {"website": "", "description": "<p>We are developing a spoken-language interface for interacting with mobile robots. In our approach, language understanding is treated as a process of homeostatic control. The interpretation of words is treated as an extension of non-linguistic processes of perceptual interpretation. If successful, this research may result in robust natural-language interfaces for robots which may also be applied to a variety of other application domains including entertainment and assistive technologies.</p>", "people": ["dkroy@media.mit.edu"], "title": "Hermes: Homeostatic Control for a Conversational Mobile Robot", "modified": "2016-12-05T00:16:37.505Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-441", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "hermes-homeostatic-control-for-a-conversational-mobile-robot"}, {"website": "", "description": "<p>We are developing a new type of microfabricated accelerometer based on the optical interferometer. The interferometer consists of surface-micromachined interdigital fingers that are alternately attached to a proof mass and support substrate. Illuminating the fingers with coherent light generates a series of diffracted optical beams. Sub-angstrom displacements between the proof mass and frame are detected by measuring the intensity of a diffracted beam. The structure is fabricated with a two-mask silicon process and detected with a standard laser diode and photodetector. We estimate that the minimum detectable acceleration is six orders of magnitude below the acceleration of gravity (i.e., 2 ug/rootHz in a 1 Hz bandwidth centered at 650 Hz). Current emphasis is on packaging this device within a volume less than 10 cubic centimeters.</p>", "people": ["scottm@media.mit.edu"], "title": "High-Resolution Interferometric Accelerometer", "modified": "2016-12-05T00:16:37.646Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-420", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "high-resolution-interferometric-accelerometer"}, {"website": "", "description": "<p>The thrust of this research is the development of picture-coding techniques that are an order of magnitude more compact than existing codes for certain types of materials, and are also more easily manipulated. For example, we are attempting to store an entire year of a serial television program on a two-hour digital videodisc (DVD). In addition, since interactive video has suffered from a lack of \"re-usability\" (images seen once become boring on round two), this coding technique can make a video game where the same footage never reappears. The work is beginning with some long-term characterizations of television programs.</p>", "people": ["lip@media.mit.edu"], "title": "Infinite Digital Videodisc", "modified": "2016-12-05T00:16:37.671Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-344", "groups": ["media-and-networks"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "infinite-digital-videodisc"}, {"website": "", "description": "<p>MACK is a guide that helps visitors to the Media Lab learn about what we do, and how to find us. A life-sized, on-screen, animated robot explains the Lab's people, projects, and groups, and gives directions about how to find them within the Lab. The agent shares a model of the Lab with the visitor, around which the two participants can center their discussion. Based on our previous research on multimodal dialogue systems, shared collaborative spaces, and direction-giving interaction, this system leverages people's natural language abilities to provide a richly interactive and easy-to-use kiosk.</p>", "people": ["sylvan@media.mit.edu"], "title": "MACK: Media Lab Autonomous Conversational Kiosk", "modified": "2016-12-05T00:16:37.727Z", "visibility": "LAB", "start_on": "2000-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "mack-media-lab-autonomous-conversational-kiosk"}, {"website": "", "description": "<p>MakeBelieve is a story-generation agent that uses common-sense knowledge to interactively compose short fictional texts with a user. A user starts a story, and MakeBelieve attempts to continue it by freely imagining possible sequences of events for the character the user has chosen. The agent uses \"common sense\" about causality and how the world works, mined from the Open Mind Common Sense corpus, and combines this with very simple lingustic techniques for story generation to produce pithy but interesting stories. MakeBelieve also uses common sense to evaluate a story it has written to catch logically inconsistent and incoherent events and actions.</p>", "people": ["lieber@media.mit.edu"], "title": "MakeBelieve: Interactive Computer Story Generation Using Common Sense", "modified": "2016-12-05T00:16:37.693Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "makebelieve-interactive-computer-story-generation-using-common-sense"}, {"website": "", "description": "<p>The Mixer-Subverter is an online system that allows children to integrate the activites of play (from giving to stealing; from sharing to being forced to receive) and the activities of video editing (creating, juxtaposing, controlling) into a never-ending process of mix and subversion. It invites the storyteller within each of us to compose and visualize movies, images, and sound environments while writing a story. In addition, the Mixer-Subverter encourages playful collaboration in an exchange network of unique media artifacts.</p>", "people": ["gid@media.mit.edu"], "title": "Mixer-Subverter", "modified": "2016-12-05T00:16:37.838Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "mixer-subverter"}, {"website": "", "description": "<p>MobiCollab explores the non-intrusive use of mobile phones in meetings. Currently, mobile phones are only non-intrusive during meetings if used for text messaging, and do not provide a specific set of tools for meetings. MobiCollab allows users to record key moments in a meeting and then collaboratively generate a transcript of these events. We have shown that MobiCollab can be used in a meeting without disturbing the meeting while providing a record of events better than hand-written notes.</p>", "people": [], "title": "MobiCollab", "modified": "2016-12-05T00:16:37.866Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "mobicollab"}, {"website": "", "description": "<p>As more powerful and spatially aware Augmented Reality devices become available, we can leverage the user\u2019s context to embed reality with audio-visual content that enables learning in the wild. Second-language learners can explore their environment to acquire new vocabulary relevant to their current location. Items are identified, \"labeled\" and spoken out loud, allowing users to make meaningful connections between objects and words. As time goes on, word groups and sentences can be customized to the user's current level of competence. When desired, a remote expert can join in real-time for a more interactive \"tag-along\" learning experience.<br></p>", "people": ["cdvm@media.mit.edu", "pattie@media.mit.edu"], "title": "WordSense", "modified": "2018-08-20T16:21:23.964Z", "visibility": "PUBLIC", "start_on": "2016-09-15", "location": "", "groups": ["ml-learning", "fluid-interfaces"], "published": true, "active": false, "end_on": "2018-05-31", "slug": "wordsense-learning-language-in-the-wild"}, {"website": "", "description": "<p>Documentary videography is a practice of observing the real world, recording video of desired events, and organizing the collection for presentation to an audience. Observations and decisions made during capture highly impact the process of documentary creation. The Mindful Camera is designed to support the story-construction goals of the documentary videographer during capture and organization of content collections. The video camera is imbued with rudimentary story-understanding capabilities incited by annotation by the videographer in the moment of capture. The camera uses common-sense knowledge and reasoning to expand metadata of captured video clips, suggest shots to the videographer, and track story patterns in the information track of a video collection. The capture process leverages the videographer\ufffds observations during capture to increase the narrative possibility of his or her video collections.</p>", "people": ["barbara@media.mit.edu", "gid@media.mit.edu"], "title": "Mindful Camera", "modified": "2016-12-05T00:16:37.924Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "mindful-camera"}, {"website": "", "description": "<p>MirrorFugue is an interface for the piano that bridges the gap of location in music playing by connecting pianists in a virtual shared space reflected on the piano. Built on a previous design that only showed the hands, our new prototype displays both the hands and upper body of the pianist. MirrorFugue II may be used for watching a remote or recorded performance, taking a remote lesson, and remote duet playing.</p>", "people": ["x_x@media.mit.edu", "ishii@media.mit.edu"], "title": "MirrorFugue II", "modified": "2016-12-05T00:16:37.955Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "mirrorfugue-ii"}, {"website": "", "description": "<p>We are developing a special version of our Scratch programming language to enable people to create, play, and share interactive media on mobile devices. Mobile Scratch is designed especially for interacting with the outside world, taking inputs from microphone, camera, and external sensors, and communicating with other mobile devices. We are running an initial pilot project at an innovative school serving children in low-income communities in India.</p>", "people": ["jmaloney@media.mit.edu", "kbrennan@media.mit.edu", "silver@media.mit.edu", "mres@media.mit.edu"], "title": "Mobile Scratch", "modified": "2016-12-05T00:16:38.085Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "mobile-scratch"}, {"website": "", "description": "<p>Current sports-medicine practices for understanding the motion of athletes while engaged in their sport of choice are limited to camera-based marker tracking systems that generally lack the fidelity and sampling rates necessary to make medically usable measurements; they also typically require a structured, stable \"studio\" environment, and need considerable time to set up and calibrate. The data from our system provides the ability to understand the forces and torques that an athlete's joints and body segments undergo during activity. It also allows for precise biomechanical modeling of an athlete's motion. The application of sensor fusion techniques is essential for optimal extraction of kinetic and kinematic information. Also, it provides an alternative measurement method that can be used in out-of-lab scenarios. </p>", "people": ["mtl@media.mit.edu", "joep@media.mit.edu"], "title": "Sensor Fusion for Gesture Analyses of Baseball Pitching", "modified": "2016-12-05T00:16:38.261Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "sensor-fusion-for-gesture-analyses-of-baseball-pitching"}, {"website": "", "description": "<p>Using the built-in camera of a mobile phone and inexpensive printed tags, users can leave and retrieve video messages in specific locations or associated to specific objects. At the heart of the project is d-touch: an open-source system for computer vision recognition of markers. D-touch's peculiarity, compared to other marker systems, is that its recognition mechanism is based on topology rather than geometry. As a consequence, the system allows considerable freedom in the visual design of the markers, so that they can be \"hidden\" or designed to fit the specific aesthetic requirements of different applications.</p>", "people": ["pattie@media.mit.edu"], "title": "Mobile Video D-Touch", "modified": "2016-12-05T00:16:38.328Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "mobile-video-d-touch"}, {"website": "", "description": "<p>MobileEssence explores the non-intrusive use of mobile phones in meetings. Currently mobile phones are only non-intrusive during meetings if used for text messaging, they do not currently provide a specific set of tools for meetings. MobileEssence allows users to record key moments in a meeting and then collaboratively generate a transcript of these events.</p>", "people": [], "title": "MobileEssence", "modified": "2016-12-05T00:16:38.350Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "--Choose Location", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "mobileessence"}, {"website": "", "description": "<p>Five billion of the world's six some billion people live in developing countries, where many have no access to basic medical treatments and preventive care. Moca, or \"Mobile Care,\" is cell-phone-based telemedicine system specifically for use in developing countries. The system allows semi-skilled community health workers to go into the field carrying a smartphone, collect diagnostic information, transmit it to a central medical records system for expert review, and receive real-time decision support. The project is build on open platforms: the Android smartphone platform and the OpenMRS medical record system. OpenMRS is designed for use in developing countries and is deployed in a dozen countries. Members of the Moca team traveled to the Philippines to explore a pilot deployment in Capiz; they are also developing partnerships with other organizations around the world.</p>", "people": ["fmoss@media.mit.edu"], "title": "Moca", "modified": "2016-12-05T00:16:38.164Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-320", "groups": ["new-media-medicine"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "moca"}, {"website": "", "description": "<p>Can your information on social networking sites be used to create emotional experiences? The Nostalgia Room ranks your Facebook photos based on tags, likes, comments, and albums in order to create a 3D visual memoir. Swinging on a motion-tracked swing with emotive music creates a multi-sensory immersive experience that evokes your memories.</p>", "people": ["holtzman@media.mit.edu", "pattie@media.mit.edu"], "title": "Nostalgia Room", "modified": "2016-12-05T00:16:38.196Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces", "information-ecology"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "nostalgia-room"}, {"website": "", "description": "<p>This project investigates the nature of mental representations of musical structure through cognitive study and machine modeling. These models create a core for systems that can organize sounds in a manner consistent with human aesthetic reasoning, without relying on preset rules of any given \"artistic style.\" The emphasis is on identifying musical structures through perceptual processes and recognition, in contrast to modeling reasoning based on musical training.</p>", "people": ["bv@media.mit.edu"], "title": "Modeling Musical Structure", "modified": "2016-12-05T00:16:38.468Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "--Choose Location", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "modeling-musical-structure"}, {"website": "", "description": "<p>This project aims to build a modular lighting system where users can customize the design and lighting patterns.</p>", "people": ["picard@media.mit.edu", "akanes@media.mit.edu"], "title": "Modular Light for Better Sleep", "modified": "2016-12-05T00:16:38.487Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "modular-light-for-better-sleep"}, {"website": "", "description": "<p>The Open Government Information Awareness program is an effort to increase transparency in the United States government. As the US government increases its supervision of civilian lives, it is crucial to ensure accountability. Since democracy requires an informed public, every effort must be made to give citizens access to government information. The Open Government Information Awareness program builds a framework for US citizens to construct and analyze the world's most comprehensive database on our government. Citizens will explore data, track events, find patterns, and build risk profiles, all in an effort to encourage action.</p>", "people": ["csik@media.mit.edu"], "title": "Open Government Information Awareness", "modified": "2016-12-05T00:16:38.508Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-020D", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "open-government-information-awareness"}, {"website": "", "description": "<p>We offer case studies in the ways that digital fabrication allows us to treat the designs of products as a kind of source code: files that can be freely shared, modified, and produced. In particular, the case studies combine traditional electronic circuit boards and components (a mature digital fabrication process) with laser-cut or 3D printed materials. They demonstrate numerous possibilities for individual customizations both pre- and post-fabrication, as well as a variety of potential production and distribution processes and scales.</p>", "people": ["mellis@media.mit.edu", "leah@media.mit.edu", "mres@media.mit.edu"], "title": "Open Source Consumer Electronics", "modified": "2016-12-05T00:16:38.528Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "open-source-consumer-electronics"}, {"website": "", "description": "<p>The Museum of Modern Art in New York City presents a compelling environment in which to explore technnologies for\nsmart environments. We are working with a team led by Terry Riley, the Chief Curator for Architecture and Design, initially on an exhibit exploring changing notions of public and private spaces. The goal of this installation is to replace conventional kiosks with embedded sensing and ambient displays so that the supporting electronic information is accessible in its natural, rich context.\n</p>", "people": ["neilg@media.mit.edu", "rehmi@media.mit.edu"], "title": "MOMA", "modified": "2016-12-05T00:16:38.575Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "moma"}, {"website": "", "description": "<p>How can emotions be conveyed, expressed, and felt? Drift Bottle is a project exploring interfaces that allow users to \"feel\" others\ufffd emotions to promote their communication. We have developed a voice message-exchange web service. Based on that, we design and develop several terminals with different interfaces which convey emotions via media such as light, smell, and motion. One solution is to convey the emotions in voice messages via different colors of light. Our latest effort is conveying emotions via smells, with the intention of arousing the same emotions in the receivers.</p>", "people": ["vmb@media.mit.edu", "slysun@media.mit.edu"], "title": "Drift Bottle", "modified": "2016-12-05T00:16:38.605Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "drift-bottle"}, {"website": "", "description": "<p>Light that is imprisoned within the CRT can only be interpreted as being of and pertaining to everyday information processing. By recontextualizing the synthetic light into other vehicles, such as wearable displays or light sculpture, we can examine the potential of organic graphics in everyday spaces and situations.</p>", "people": [], "title": "Embodied Light", "modified": "2016-12-05T00:16:38.637Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-448", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "embodied-light"}, {"website": "", "description": "<p>TagMe is an end-user toolkit for easy creation of responsive objects and environments. It consists of a wearable device that recognizes the object or surface the user is touching. The user can make everyday objects come to life through the use of RFID tag stickers, which are read by an RFID bracelet whenever the user touches the object. We present a novel approach to create simple and customizable rules based on emotional attachment to objects and social interactions of people. Using this simple technology, the user can extend their application interfaces to include physical objects and surfaces into their personal environment, allowing people to communicate through everyday objects in very low-effort ways.</p>", "people": ["rboldu@media.mit.edu", "amores@media.mit.edu", "pattie@media.mit.edu", "xavib@media.mit.edu"], "title": "TagMe", "modified": "2018-10-11T18:48:35.308Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-01-01", "slug": "tagme"}, {"website": "", "description": "<p>This project involves the development of nanofabrication techniques that achieve a spatial resolution of 10 nm and below. In one system, single-wall carbon nanotubes are combined with a scanning-probe microscope to oxidize anodically the surface of atomically flat titanium. In another, micromechanical devices with interferometric sensors are being designed and fabricated to control the separation of an electrode gap with angstrom precision. The goal is to use such techniques to study biomolecular activity in-situ.</p>", "people": ["scottm@media.mit.edu"], "title": "Molecular-Scale Device Fabrication", "modified": "2016-12-05T00:16:38.752Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-420", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "molecular-scale-device-fabrication"}, {"website": "", "description": "<p>Moodtrack grew from a desire to create soundtracks based only on natural language input. By breaking down music into essential forms, music can be reconstructed based on affective cues. Applications include sample management, film music supervision, Web publishing, and personal sound devices.</p>", "people": ["walter@media.mit.edu"], "title": "Moodtrack", "modified": "2016-12-05T00:16:38.796Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "Pond", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "moodtrack"}, {"website": "", "description": "<p>Remember when we made a connection by handing someone a photo? Now we fiddle with too many cables, menus, and communication channels, and those individual connections get drowned out. Can we return to physical experiences while retaining the collective intelligence of the network? Tableau is a side table that stores and retrieves memories. It may put friends' photo postcards in the drawer, or post mementos to your online scrapbook. This is an example of task-centric computing, where the interface is distributed across connected physical objects. Apps that run in the cloud can weave available objects into environmental I/O, giving users computing experiences that fit into the flow of life.</p>", "people": ["jkestner@media.mit.edu", "holtzman@media.mit.edu"], "title": "Tableau", "modified": "2016-12-05T00:16:38.828Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "tableau"}, {"website": "", "description": "<p>In online environments, we often rely on some combination of pictures and pseudonyms to show others what kind of person we are. In more anonymous contexts, we tend to rely on non-representational images while on services like Facebook, identifiable pictures are the norm. In this project, we seek to turn the iconic profile picture into something more active and dynamic. With tailor, you can easily apply dynamic graphical effects and data-driven information to an existing profile picture on Twitter. Tailor will automatically update your profile picture based on these overlays. These overlays might include your current location, number of followers, or visualizations of your favorite team's recent record. Tailor will also support scenarios like automatically updating your picture when you upload pictures to Flickr, visually aging pictures the longer you have them, morphing between different pictures, or programmatically generating interesting abstract profile pictures.</p>", "people": ["geek@media.mit.edu"], "title": "tailor", "modified": "2016-12-05T00:16:38.848Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "tailor"}, {"website": "", "description": "<p>Knowledge exists not only in our minds but in the world around us.  The field of tangible user interfaces aims to release knowledge inside computers into the physical world, and to extend our everyday world with graspable representations of knowledge as easily manipulated as everyday objects. Tangible programming provides a mechanism for directly manipulating procedures, configurations, and cybernetic behavior with our hands.    We have created a set of electronic, stackable LEGO bricks which reveal their order and identity.  We are using these bricks as a programming interface to a number of computational environments, including the behavior of children's toy cars and trains, microwave ovens (to specify the execution of a recipe), and music synthesizers.  \n</p>", "people": ["mres@media.mit.edu"], "title": "Tangible Programming with LEGO Bricks", "modified": "2016-12-05T00:16:38.895Z", "visibility": "PUBLIC", "start_on": "1997-01-01", "location": "E15-068", "groups": ["lifelong-kindergarten", "counter-intelligence"], "published": true, "active": false, "end_on": "1999-09-01", "slug": "tangible-programming-with-lego-bricks"}, {"website": "", "description": "<p>The Village Visualizer is visualization of a learner\ufffds social network. It highlights people with similar interests and projects, so that users can find others with common interests, communicate about projects, collaborate, and find resources. The system is a demonstration of how visualizations can address the particular needs of communities of learners, principally children and adolescents. </p>", "people": ["sylvan@media.mit.edu", "mres@media.mit.edu"], "title": "Village Visualizer", "modified": "2016-12-05T00:16:38.926Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "Cube", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "village-visualizer"}, {"website": "", "description": "<p>Carbon nanotubes consist of a graphite sheet of carbon atoms formed into tubes 1-10 nm in diameter. We are developing methods for synthesizing and manipulating the tubes and working to create novel electronic and electromechanical devices with them.</p>", "people": ["scottm@media.mit.edu"], "title": "Synthesis and Manipulation of Carbon Nanotubes", "modified": "2016-12-05T00:16:38.776Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-015", "groups": ["nanoscale-sensing", "personal-fabrication"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "synthesis-and-manipulation-of-carbon-nanotubes"}, {"website": "", "description": "<p>Monkey Business is a system to keep distributed members of a group aware of each other\ufffds presence and activities in a light-hearted manner, while striving to remain non-intrusive. The system also aims to facilitate unplanned and informal communication among distributed colleagues. It consists of a network of animatronic agents, specifically monkeys, which are situated in the offices or rooms of each member of a group. Through subtle movements, gestures, and sounds, the monkeys indicate the current activities of the other members of the group. The monkeys are meant to be ambient, at the periphery of one\ufffds attention; but they can also be used more proactively as communication mechanisms, and promote informal exchanges among members of a distributed team.</p>", "people": ["geek@media.mit.edu", "stefanm@media.mit.edu"], "title": "Monkey Business", "modified": "2016-12-05T00:16:38.974Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-384C", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "monkey-business"}, {"website": "", "description": "<p>MonkeyToGo is a mobile extension of Monkey Business, our animatronic, remote- awareness network. Cell phones running our MonkeyToGo application can easily be added to the network in order to interact with and receive information from the agents in real time. If active, the connected cell phone vibrates and plays short animations and sound snippets in order to inform its user of ongoing activities at the remote locations. The MonkeyToGo user can also use the cell phone to \"tickle\" (call for attention) or to set up a voice connection with other monkey agents. Several cell phones can be added at the same time.</p>", "people": ["geek@media.mit.edu"], "title": "MonkeyToGo", "modified": "2016-12-05T00:16:39.007Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "monkeytogo"}, {"website": "", "description": "<p>The computer's emerging capacity to communicate an individual's affect raises critical ethical concerns. Additionally, designers of perceptual computer systems face moral decisions about how the information gathered by computers with sensors can be used. As humans, we have ethical considerations that come into play when we observe and report each other's behavior. Computers, as they are currently designed, do not employ such ethical considerations. This project assess the ethical acceptability of affect sensing in three different adversarial contexts, where within each context there are also different kinds of motivations (self-oriented and charity-oriented) for the individuals to perform as best as they can.    </p>", "people": ["picard@media.mit.edu"], "title": "Moral Sensors", "modified": "2016-12-05T00:16:39.096Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "moral-sensors"}, {"website": "", "description": "<p>Mountain is a visualization of email that focuses on people instead of the messages they have exchanged. The project explores the ever-growing lists of contacts that people keep over the years via email.</p>", "people": ["judith@media.mit.edu"], "title": "Mountain", "modified": "2016-12-05T00:16:39.244Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "mountain"}, {"website": "", "description": "<p>MOVITS aim to make the exchanging, sharing, and publishing of phone-captured media more integrated, evolving, and fun. Modifiable presentation templates allow the user to put together multi-media messages containing images, movies, audio, and text. We imagine these messages as a form of movie \"Post-It note\" that can be quickly created, exchanged, modified, or destroyed. These MOVITS can also become part of an evolving chain of MOVITS, as media is shared between phones and also published and downloaded from the Web. This project is being developed in conjunction with Nokia.</p>", "people": ["gid@media.mit.edu"], "title": "MOVITS", "modified": "2016-12-05T00:16:39.304Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-320B", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "movits"}, {"website": "", "description": "<p>Mr. Java is the Media Lab's wired coffee machine, which keeps track of usage patterns and user preferences. The focus of this project is to give Mr. Java a tangible customer-feedback system that collects data on user complaints or compliments.  \"Thumbs-up\" and \"thumbs-down\"  pressure sensors were built and their signals integrated with the state of the machine to gather data from customers regarding their ongoing experiences with the machine.  Potentially, the data gathered can be used to learn how to improve the system. The system also portrays an affective, social interface to the user: helpful, polite, and attempting to be responsive to any problems reported.\n</p>", "people": ["picard@media.mit.edu"], "title": "Mr. Java:  Customer Support", "modified": "2016-12-05T00:16:39.336Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "3rd Floor Kitchen", "groups": ["affective-computing", "counter-intelligence"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "mr-java-customer-support"}, {"website": "", "description": "<p>MugShots enables visual communication though everyday objects. We embed a small display into a coffee mug, an object with frequent daily use. Targeted for the workplace, the mug transitions between different communication modes in public and private spaces. In the private office space, the mug is an object for intimate communication between remote friends; users receive emoticon stickers via the display. When brought to a public area, the mug switches to a pre-selected image of the user's choice, serving as a social catalyst to trigger conversations in public spaces.</p>", "people": ["cindykao@media.mit.edu", "geek@media.mit.edu"], "title": "MugShots", "modified": "2016-12-05T00:16:39.445Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "mugshots"}, {"website": "", "description": "<p>MTM \"Little John\" is a multi-purpose, mid-size, rapid prototyping machine with the goal of being a personal fabricator capable of performing a variety of tasks (3D printing, milling, scanning, vinyl cutting) at a price point in the hundreds rather than thousands of dollars. The machine was designed and built in collaboration with the MTM (Machines that Make) Project at MIT Center for Bits and Atoms.</p>", "people": ["pattie@media.mit.edu", "linder@media.mit.edu"], "title": "MTM \"Little John\"", "modified": "2016-12-05T00:16:39.413Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "mtm-little-john"}, {"website": "", "description": "<p>This project explores how a group Web space can become more dynamic and useful, rather than a static display of finished projects. We have given our Web server its own identity, MrWeb. MrWeb is designed to act as another member of the group, whose job is to display, coordinate, organize, and facilitate the way the group interacts. Our hypothesis is that by making interactions with the Web server more transparent, group members will be more likely both to keep information up-to-date and to document experiences for the group.</p>", "people": [], "title": "MrWeb", "modified": "2016-12-05T00:16:39.483Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "mrweb"}, {"website": "", "description": "<p>Remot-IO is a system for mobile collaboration and remote assistance around Internet-connected devices. It uses two head-mounted displays, cameras, and depth sensors to enable a remote expert to be immersed in a local user's point of view, and to control devices in that user's environment. The remote expert can provide guidance through hand gestures that appear in real time in the local user's field of view as superimposed 3D hands. In addition, the remote expert can operate devices in the novice's environment and bring about physical changes by using the same hand gestures the novice would use. We describe a smart radio where the knobs of the radio can be controlled by local and remote users. Moreover, the user can visualize, interact, and modify properties of sound waves in real time by using intuitive hand gestures.</p>", "people": ["amores@media.mit.edu", "pattie@media.mit.edu"], "title": "Remot-IO: A System for Reaching into the Environment of a Remote Collaborator", "modified": "2018-08-20T16:23:49.991Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces", "terrestrial-sensing"], "published": true, "active": false, "end_on": "2016-12-31", "slug": "remot-io-a-system-for-reaching-into-the-environment-of-a-remote-collaborator"}, {"website": "", "description": "<p>This project involves providing a traditional bed with multimedia computing capabilities, eye-tracking, and coordination of the electronics with a projection screen mounted above the bed. The system provides the user with an alarm clock that projects a sunrise on the ceiling at the time the user wishes to wake. It allows the user to go to sleep with a star-lit sky projection, or with a constellation game. And if you play the game well past your bedtime, the system may ask you about resetting your alarm to a later time the following morning. It makes reading in bed easier by projecting your book onto the screen, eliminating the need to prop yourself up using your elbows or a pillow.</p>", "people": ["win@media.mit.edu"], "title": "Multimedia Bed", "modified": "2016-12-05T00:16:39.686Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "multimedia-bed"}, {"website": "", "description": "<p>The entire hyperinstruments project was founded on the idea of giving individuals powerful control over many layers and levels of music, in a way that has previously only been possible in complex multitrack digital recording studios\ufffdand then only in non-real-time, as sound engineers, producers, and musicians manipulate and arrange every last sound of the CDs we hear, often achieving \"perfection\" but sacrificing the intuition, gestuality, and communication of real-time music performance. Using the many conceptual and technical advances of over ten years of hyperinstrument development, we are returning to our original inspiration, to develop a multi-interface environment designed for the precise, subtle, and expressive manipulation of multiple layers of sound. We are construcing an environment\ufffdand a special hand-manipulated sensor interface\ufffdthat can be tuned either as a high-end system for the most discerning musicians, or as a \"sonic playground\" for the youngest children. The first application of the MMM will be as a complement to the Sensor Chair in the Future Music Blender.</p>", "people": ["tod@media.mit.edu"], "title": "Multi-Modal-Mixer (MMM)", "modified": "2016-12-05T00:16:39.600Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-483", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "multi-modal-mixer-mmm"}, {"website": "", "description": "<p>We are developing a new interface for text chat, to assist conversational partners who don't necessarily speak each others' languages perfectly. It will be especially useful to connect students trying to learn foreign languages with native speakers of that language who are trying to learn the first student's language. The idea is to provide \"bad translations\" together with tools that let the user quickly analyze the original and translated versions to correct translation mistakes, understand other possibilities for the meaning of a word or phase, provide subject context that helps disambiguate words, provide additional vocabularly in the other language for continuing the conversation, enlist the conversation partner in helping a user learn how to say something, and understand where there may be cultural differences between the speakers. It relies on natural language processing technology together with common-sense knowledge bases in both languages and cultures.</p>", "people": ["lieber@media.mit.edu"], "title": "Multilingual Educational Chat Assistant", "modified": "2016-12-05T00:16:39.639Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-385", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "multilingual-educational-chat-assistant"}, {"website": "", "description": "<p>We are extending our Scratch programming language to interact across networks. This extension will enable kids to bring information and artifacts from the Web and from other users' projects into their own Scratch projects. In designing interactive networked projects, kids are able to think about how their individual ideas and creations can connect to the Internet, their friends, their communities, and the world.</p>", "people": ["jmaloney@media.mit.edu", "mres@media.mit.edu"], "title": "NetScratch", "modified": "2016-12-05T00:16:39.862Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "Cube", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "netscratch"}, {"website": "", "description": "<p>In the online world, many of the social cues that are fundamental to coherent communication and the establishment of a vibrant community are missing. The goal of social visualization research is to make legible the patterns and interactions that occur in online social environments. This project extends that research into the audio domain. We are developing techniques for audibly depicting online crowds. Key to this work is both the development of a 3-D audio environment and the creation of a cognitively grounded and aesthetically interesting sound representation.</p>", "people": ["judith@media.mit.edu"], "title": "Murmurized", "modified": "2016-12-05T00:16:39.790Z", "visibility": "PUBLIC", "start_on": "2001-12-31", "location": "", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "murmurized"}, {"website": "", "description": "<p>The Hyperinstruments group is exploring new ways to visualize music both at the song-level and the collection-level. At song-level: SoundSieve is a music visualizer that takes the intrinsic qualities of a musical piece\ufffdpitch, time, and timbre\ufffdand makes their patterns readily apparent in a visual manner. For example, you can quickly pick out repeating themes, chords, and complexity from the pictures and video. At collection-level: MusicBox focuses on the problem of navigating a large body of music. It aims to help you find music you like, both inside your own collection (to match a particular mood, for example), or from a body of entirely new music. MusicBox visualizes your music collection in space, giving each track a location based on how similar it sounds to other tracks. This new manner of navigation stands in stark contrast to traditional, text-dependent media players like iTunes and Windows Media Player.</p>", "people": ["tod@media.mit.edu"], "title": "Music Visualization", "modified": "2016-12-05T00:16:39.838Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "music-visualization"}, {"website": "", "description": "<p>For the 42nd annual Bachman's/Marshall Field's flower show, we worked with landscape designer Julie Messervy and the production team from Marshall Field's to develop a set of interactive music and sound experiences that could integrate into their large indoor garden. This included four Yamaha Disklavier pianos playing different pieces that visitors could transform by using a touchpad station in front of each piano, a forest with actuated illuminated windchimes that would respond to visitors blowing on pinwheels, a pretend underwater garden with \"Music Shapers\" that made environmental and musical sounds when squeezed, and music that tied those sounds together, along with new material that was presented at the end of the experience.</p>", "people": ["tod@media.mit.edu"], "title": "Music in the Garden", "modified": "2016-12-05T00:16:39.923Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "music-in-the-garden"}, {"website": "", "description": "<p>Almost all kids love to sing, even if they sadly forget this later in life. Music is never work: you can only \"play\" it. Today's incredible music synthesizers are a stepping stone to tomorrow's possibilities, and kids will lead the way. Imagine a toy piano that sounds like a Steinway, a stuffed toy that sings lullabies, or an opera that is performed with a cast of toys while children play with them. We are devoting considerable effort to designing a new world of musical play for children, starting with the very young. A coordinated collection of Music Toys is being designed, covering three basic categories: Simple Things, Music Shapers, and Big Thing.</p>", "people": ["tod@media.mit.edu"], "title": "Music Toys", "modified": "2016-12-05T00:16:39.892Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-495", "groups": ["opera-of-the-future", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "music-toys"}, {"website": "", "description": "<p>The Music Shapers are soft, squeezable instruments that allow players to shape and explore musical compositions. Using capacitive sensing and conductive embroidery to measure the squeezing gesture, Shapers allow children access to high-level musical concepts such as contour, timbre, and orchestration, rarely accessible by traditional musical instruments.</p>", "people": ["tod@media.mit.edu"], "title": "Music Shapers", "modified": "2016-12-05T00:16:39.947Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "music-shapers"}, {"website": "", "description": "<p>Stage performances present many challenges and opportunities in the field of robotics. Here we create a family of furniture robots that look and act like organic entities for the production of Death and the Powers. Mei-Mei is a six-legged walking robot that is being developed in the lab as a moving workbench. It can move forward, backward, and even turn around with differential steering control. Di-Di is a modular robot that can transform itself into a sofa onstage and walk with a rolling, lurching, and gliding motion. These two robots will also be endowed with interactive behavior structured around scenes, beats, and actions. The design of these robots not only incorporates technological, conceptual, and aesthetic innovations, but also coordinates with narrative and musical materials in the opera.</p>", "people": [], "title": "Musical Furniture", "modified": "2016-12-05T00:16:40.067Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "musical-furniture"}, {"website": "", "description": "<p>The Musical Jacket, first developed in fall 1997, has recently undergone hardware and software improvements, as well as functional updating. A special version was prepared for permanent exhibition at the reconfigured Innoventions (Walt Disney World, Orlando), and a new application is being designed for Internet MP3 soundfile selection and play.</p>", "people": ["tod@media.mit.edu"], "title": "Musical Jacket", "modified": "2016-12-05T00:16:40.093Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-483", "groups": ["opera-of-the-future", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "1998-12-30", "slug": "musical-jacket"}, {"website": "", "description": "<p>Robots and performers make beautiful music together. The opera \"Death and the Powers\" features a chorus of seven-foot tall, autonomous, polymorphic Operarobots and three large fifteen-foot tall robotic walls. At various times, these function as characters, set pieces, and lighting elements. Using state-of-the-art control electronics, and a novel real-time performance control system, a total of 9 individually addressable Operabots reflect on, participate in, and illuminate the action onstage.</p>", "people": [], "title": "Musical Robotics", "modified": "2016-12-05T00:16:39.998Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "musical-robotics"}, {"website": "", "description": "<p>The musicbox is a tangible interface supporting remote awareness through the medium of music and light. The musicbox is linked over the Internet to the music and light levels surrounding a remote piano. The system communicates live music and a sense of physical movement through a lighted wooden \"music box,\" while also providing browsable access to music previously played by the device.</p>", "people": ["ishii@media.mit.edu"], "title": "musicbox", "modified": "2016-12-05T00:16:40.024Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "1998-12-30", "slug": "musicbox"}, {"website": "", "description": "<p>Different people respond to and query for music in vastly different ways. Someone's favorite song is only barely recognizable by another: this leads us to question why and how we attach preference and memory to musical information. We are working on an architecture to model the individual and singular representation of a piece of music by a human listener. This model can then be applied to common music retrieval tasks such as recommendation and search, which in turn could leverage the power of the immediate delivery of music over networks while allowing users to discover new and varied music.</p>", "people": ["bv@media.mit.edu"], "title": "Musical Preference Understanding", "modified": "2016-12-05T00:16:40.153Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "musical-preference-understanding"}, {"website": "", "description": "<p>Musicscape is a two-dimensional, spatial music navigation interface designed for browsing large-scale sound archives. It simulates a 2D sound field by applying Head-Related Transfer Function (HRTF), and enables users to virtually walk around the sound space with a computer mouse.</p>", "people": ["geek@media.mit.edu"], "title": "Musicscape", "modified": "2016-12-05T00:16:40.184Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-310", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "musicscape"}, {"website": "", "description": "<p>Musicpainter is a networked, graphical composing environment that encourages sharing and collaboration within the composing process. It provides a social environment where users can gather and learn from each other. The approach is based on sharing and managing music creation in small and large scales. At the small scale, users are encouraged to begin composing by conceiving small musical ideas, such as melodic or rhythmic fragments, all of which are collected and made available to all users as a shared composing resource. The collection provides a dynamic source of composing material that is inspiring and reusable. At the large scale, users can access full compositions that are shared as open projects. Users can listen to and change any piece. The system generates an attribution list on the edited piece, allowing users to trace how it evolves in the environment. </p>", "people": ["geek@media.mit.edu"], "title": "Musicpainter", "modified": "2016-12-05T00:16:40.246Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-310", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "musicpainter"}, {"website": "", "description": "<p>Novel biosensor based on nanomechanical bending.</p>", "people": ["scottm@media.mit.edu"], "title": "Nanomechanical Biosensing", "modified": "2016-12-05T00:16:40.295Z", "visibility": "LAB", "start_on": "2001-01-01", "location": "E15-441", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "nanomechanical-biosensing"}, {"website": "", "description": "<p>Singing on-pitch typically requires the ability to hear oneself and make fine motor corrections to the vocal cords. This hearing-adjusting feedback  loop is fine when a person can hear small pitch variations. But what about those who have trouble hearing when they are \"off\"? We are investigating how the sense of touch can be used to improve a person's ability to sing on-pitch or follow a rhythm. Our system does active pitch-tracking of a singer, and gives immediate vibrational feedback indicating when he or she is off, and by how much. We hope that this system can be used as a training device, to improve singing performance, or to train new singing behaviors like harmonizing. Future work will also investigate how vibrotactile feedback can help timing-related musical performance, such as singing in rounds and drumming.</p>", "people": [], "title": "Tactile Feedback for Singing and Music Performance", "modified": "2016-12-05T00:16:40.338Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "tactile-feedback-for-singing-and-music-performance"}, {"website": "", "description": "<p>Novel nanomechanical biosensor designed and fabricated in Spring '01 MEMS class. The sensor consists of two adjacent flexible cantilevers connected to the substrate through stiff honeycomb-shaped support structure. The flexible cantilevers bend in response to a stress induced on their surface by chemical or biological interactions. The geometry of the device enables simple delivery of bio-molecules to the surface of each cantilever, using commercially available pipettes. Deflection of the cantilevers is interferometrically detected using the interdigitated (ID) fingers. The ID finger set between the support structure and the cantilever is used to detect the absolute deflection of that cantilever. The ID set between the two cantilevers enables direct detection of relative bending, which eliminates disturbances caused by temperature changes in the environment.</p>", "people": ["scottm@media.mit.edu"], "title": "Nanomechanical Biosensor", "modified": "2016-12-05T00:16:40.456Z", "visibility": "LAB", "start_on": "2001-01-01", "location": "E15-420", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "nanomechanical-biosensor"}, {"website": "", "description": "<p>In this project we are attempting to create a large set of simple, mobile, wirelessly connected computing units and to treat them as a physical test-bed to explore and demonstrate ad hoc routing and self-organization, active networking, distributed multicasting, and distributed storage of information. These networks are characterized by ad hoc organizations and mobility, and they feature cooperative computing and transmission techniques operating in real-time, and performing distributed processing functions of media delivery and sensing.</p>", "people": ["lip@media.mit.edu"], "title": "Net-Hockey", "modified": "2016-12-05T00:16:40.653Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-483", "groups": ["media-and-networks"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "net-hockey"}, {"website": "", "description": "<p>What is the point of interaction for bio-data collected on a daily basis  from black boxes on your person? NetWeight is a scale which weighs the  person, ID's the person through a toe-scan, and then becomes a network  device for interacting with data collected previously from activities such  as running a marathon or climbing Mount Everest if you happened to do  that...\n</p>", "people": ["mike@media.mit.edu"], "title": "NetWeight", "modified": "2016-12-05T00:16:40.678Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "1996-12-30", "slug": "netweight"}, {"website": "", "description": "<p>You can Telnet directly to my ear\ufffdbut no, I won't tell you the IP address! The mobile user wears a wirelessly networked computer with a headset. A daemon listens to incoming connections on the telnet port and reads messages out loud, employing a text-to-speech engine.</p>", "people": [], "title": "Networked Ear", "modified": "2016-12-05T00:16:40.696Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "--Choose Location", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "networked-ear"}, {"website": "", "description": "<p>A live music event where musicians are located remotely from each other is possible through the Internet but highly constrained by network latency. This is especially true with rhythmic music that requires tight synchrony, or in situations where musicians are separated by long distances. To overcome time delays, we propose an intelligent system that listens to the audio input at one end and synthesizes a predicted audio output at the other. In this context, we study how our musical exposure, or enculturation, gives rise to musical anticipation. Moreover, as we admit that such prediction cannot be error-free, we aim to model the musical intentions of the performers and the expectations of the listeners.</p>", "people": ["bv@media.mit.edu"], "title": "Network Music Performance", "modified": "2016-12-05T00:16:40.748Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-310", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "network-music-performance"}, {"website": "", "description": "<p>New Day New Standard is an interactive hotline that informs nannies, housekeepers, eldercare-givers, and their employers about the landmark Domestic Workers' Bill of Rights, passed in New York State in November 2010. Operating in English and Spanish, it's a hybrid application that combines regular touchtone phones and Internet-based telephony within an open source framework. The Center for Civic Media and REV- (http://www.rev-it.org) are currently developing Call to Action, a generalized version of the platform and associated GUI to allow other groups to create interactive hotlines for a wide range of use cases. NDNS was presented to the White House's Open Government Initative.</p>", "people": ["rahulb@media.mit.edu", "leob@media.mit.edu"], "title": "New Day New Standard: (646) 699-3989", "modified": "2016-12-05T00:16:40.766Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "new-day-new-standard-646-699-3989"}, {"website": "", "description": "<p>MACH, My Automated Conversation coacH, is a system for people to practice social interactions in face-to-face scenarios. MACH consists of a 3D character that can \"see,\" \"hear,\" and make its own \"decisions\" in real time. The system was validated in the context of job interviews with 90 MIT undergraduate students. Students who interacted with MACH demonstrated significant performance improvement compared to the students in the control group. We are currently expanding this technology to open up new possibilities in behavioral health (e.g., treating people with Asperger syndrome, social phobia, PTSD) as well as designing new interaction paradigms in human-computer interaction and robotics.</p>", "people": ["picard@media.mit.edu", "mehoque@media.mit.edu"], "title": "MACH: My Automated Conversation coacH", "modified": "2019-05-17T14:16:05.131Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "mach-my-automated-conversation-coach"}, {"website": "", "description": "<p>This project explores the creation and use of a physical/digital  construction kit. Triangles is a physical computer interface in the form of a construction kit of identical, flat, plastic triangles. The triangles connect together both physically and digitally with magnetic, conducting connectors. When the pieces contact one another, specific connections can trigger specific digital events, allowing a simple but powerful means of physically interacting with digital information. Users can create both two- and three- dimensional patterns whose exact configuration is known to the computer. Triangles is a flexible system in that it can be reconfigured at hardware and software levels for a variety of uses. It is versatile, with applications from storytelling and education to rapid prototyping of human-computer interfaces.</p>", "people": ["ishii@media.mit.edu"], "title": "Triangles", "modified": "2016-12-05T00:16:41.029Z", "visibility": "PUBLIC", "start_on": "1995-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "triangles"}, {"website": "", "description": "<p>NPS is a way to archive and share your news geographically. Through NPS you can create a shared archive of news, as well as represent that archive on a searchable map. You can choose to share your news only with your group, or with the public in general. Location matters when sharing news, and NPS can be used to find out about news in your area.</p>", "people": ["ethanz@media.mit.edu", "csik@media.mit.edu"], "title": "News Positioning System", "modified": "2016-12-05T00:16:41.142Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-001", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "news-positioning-system"}, {"website": "", "description": "<p>Within the next three years, another billion people will make regular use of cell phones, continuing the fastest adoption of a new technology in history. This will unleash a wave of entrepreneurship, collaboration, and wealth creation, thereby transforming this Next Billion into a powerful force in the global economy. By innovating applications and business models, and by testing them locally in the field, this initiative explores how mobile technologies can reduce friction in bottom of the pyramid markets across the developing world.</p>", "people": [], "title": "Next Billion Network", "modified": "2016-12-05T00:16:41.108Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-384", "groups": [], "published": true, "active": false, "end_on": "2007-09-01", "slug": "next-billion-network"}, {"website": "", "description": "<p>Newsflow is a dynamic, real-time map of news reporting that displays both the latest top stories as well as the news organizations which covered them. All articles are from the last few minutes. Viewing news in this way lets us see how the choice of 'top stories' by news bureaus is geographically unequal, or rather, what areas of the world are neglected by various national news sources. Built with HTML5 on the dynamic mapping framework Cartagen, Newsflow draws on real-time data from over 200 news organizations as well as Google, Yahoo, and other sources. The ability to view such data in real time offers viewers a chance to see how news editors shape national attention as stories unfold.</p>", "people": ["dsmall@media.mit.edu"], "title": "Newsflow: Where News Happens", "modified": "2016-12-05T00:16:41.176Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["design-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "newsflow-where-news-happens"}, {"website": "", "description": "<p>NGDB is designed to be a fast, flexible database of Usenet newsgroup messages that supports a variety of applications, including visualization of the conversations, designing novel interfaces for news reading, and conducting basic sociological inquiry into the activity in the groups. It combines a relational database with INN, the open-source Usenet server, to work transparently within the existing news infrastructure.</p>", "people": ["judith@media.mit.edu"], "title": "Newsgroup Database (NGDB)", "modified": "2016-12-05T00:16:41.210Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "newsgroup-database-ngdb"}, {"website": "", "description": "<p>NewsPad is a collaborative article editor that empowers small communities to write articles collaboratively through community sourcing, structured stories, and distributed syndication.</p>", "people": ["ethanz@media.mit.edu", "jnmatias@media.mit.edu"], "title": "NewsPad", "modified": "2016-12-05T00:16:41.269Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "newspad"}, {"website": "", "description": "<p>NewsJack is a media remixing tool built from Mozilla's Hackasaurus.  It allows users to modify the front pages of news sites, changing language and headlines to change the news into what they wish it could be.</p>", "people": ["ethanz@media.mit.edu", "holtzman@media.mit.edu", "rahulb@media.mit.edu"], "title": "NewsJack", "modified": "2016-12-05T00:16:41.442Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["civic-media", "information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "newsjack"}, {"website": "", "description": "<p>Functionally, television content delivery has remained largely unchanged since the introduction of television networks.  NeXtream explores an experience where the role of the corporate network is replaced by a social network. User interests, communities, and peers are leveraged to determine television content, combining sequences of short videos to create a set of channels customized to each user. This project creates an interface to explore television socially, connecting a user with a community through content, with varying levels of interactivity: from passively consuming a series, to actively crafting one's own television and social experience.</p>", "people": ["holtzman@media.mit.edu"], "title": "NeXtream: Social Television", "modified": "2016-12-05T00:16:41.348Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "nextream-social-television"}, {"website": "", "description": "<p>No Park is a Web site for people interested in the hidden politics of the urban environment; the way public space is appropriated for art making; and how people re-interpret the urban landscape for recreation and pleasure. Here you will find guides for parkour, buildering, weird architecture, public art, hidden green space, and the politics of our cities.</p>", "people": ["csik@media.mit.edu"], "title": "No Park", "modified": "2016-12-05T00:16:41.462Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-001", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "no-park"}, {"website": "", "description": "<p>We are building a nonlinear dynamic system that constitutes a new kind of phase-locked loop. This Noise Locked Loop (NLL) will entrain with an incoming pseudo-random bit stream generated by a matched LFSR, instead of simple sinusoidal signals. The hope is that this will be a cheap and very effective front-end for spread-spectrum receivers, allowing spread-spectrum to be deployed for new applications. For example, one could potentially use spread-spectrum clock distribution on large chips. In the process, we have begun to understand how to compute using nonlinear dynamic systems.</p>", "people": ["neilg@media.mit.edu"], "title": "Noise Locked Loop", "modified": "2016-12-05T00:16:41.489Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-023", "groups": ["physics-and-media", "silicon-biology"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "noise-locked-loop"}, {"website": "", "description": "<p>Nomadic Radio explores speech interfaces for communication purposes in a wearable computer. Incoming messages and phone calls are filtered, with graceful alerting based on spatially presented audio.  Nomadic Radio is adaptive to user situation and previous reactions, to avoid, for example, annoying message announcements when busy.</p>", "people": ["nitin@media.mit.edu", "geek@media.mit.edu"], "title": "Nomadic Radio", "modified": "2016-12-05T00:16:41.520Z", "visibility": "PUBLIC", "start_on": "1995-12-31", "location": "E15-344", "groups": ["living-mobile", "e-markets"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "nomadic-radio"}, {"website": "", "description": "<p>O-Link is a communication tool developed by NTT Comware in collaboration with the Tangible Media group. O-Link allows users to link video clips to an object intuitively, as if sealing wonderful memories of a trip in a souvenir. It also has an object-recognition feature that loads the video clips tagged with the object and plays them back automatically, just by putting the object in front of the monitor. NTT Comware aims to provide novel services based on O-Link that anyone can use\ufffdregardless of his or her computer literacy\ufffdsuch as intergenerational communication tools, video manual services directly equipped with products, or educational infrastructures for the exchange of student work and opinion.</p>", "people": ["ishii@media.mit.edu"], "title": "O-Link", "modified": "2016-12-05T00:16:41.665Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "o-link"}, {"website": "", "description": "<p>Tim Berners-Lee envisions information on the Web being given well-defined meaning, creating the Semantic Web, which would then enable user-interface agents to assist with complex, multi-step, context-dependent tasks, such as arranging travel, appointments, and online purchases. Such agents are assumed to achieve their tasks by composing Web services that perform the functions of individual steps. However, to date, much of the work on the Semantic Web has concerned how to describe and represent the individual services and resources, and not what the agents do to compose them. To avoid hard-coding each task the user wishes to perform, we must give the agent the ability to dynamically compose Web Services according to what will satisfy the user's goals. We are using a large common-sense knowledge base to allow user interface agents to anticipate user goals and flexibly connect them with Web services that can accomplish those goals.</p>", "people": ["lieber@media.mit.edu"], "title": "Using Common-Sense Reasoning to Enable the Semantic Web", "modified": "2016-12-05T00:16:41.689Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "using-common-sense-reasoning-to-enable-the-semantic-web"}, {"website": "", "description": "<p>Biosensors are being developed for applications where traditional methods from the field of biology are not suitable, or are too complicated or time consuming. Emphasis is placed on the direct and parallel detection of unlabeled biomolecules using microfabricated structures. Current experiments are aimed at detecting multilayer of polyelectrolytes, DNA hybridization, enzyme activity (phosphorylation), and change in the metabolism of single cells.</p>", "people": ["scottm@media.mit.edu"], "title": "Novel Biosensors", "modified": "2016-12-05T00:16:41.733Z", "visibility": "LAB", "start_on": "2000-01-01", "location": "E15-420", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "novel-biosensors"}, {"website": "", "description": "<p>The most common functions of mobile devices involve alerting the user; whether notifying of incoming calls and messages or reminding of calendar events, the system has to catch the user's attention. This causes undesired interruptions to those surrounding the mobile device user, and in some circumstances the notification can be disruptive even for the user himself. A wearable peripheral display, embedded in a pair of eyeglasses, was built to deliver notification cues in a private, subtle, and non-obtrusive way. The display is composed of arrays of small LEDs of different colors, each placed at the end of the arms near the lens. The LEDs are lit (at very dim intensity) to display moving patterns in the wearers' peripheral vision. The display has a Bluetooth interface, so it can be controlled by standard mobile devices.</p>", "people": ["pattie@media.mit.edu"], "title": "Notifying Glasses", "modified": "2016-12-05T00:16:41.595Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "notifying-glasses"}, {"website": "", "description": "<p>This project is an experiment in material and scale: a life-sized pop-up book that you can open up and step into, made using only cardboard, an X-acto knife, tape, and glue. Inside the book is a kinetic mural of breathing pleated flowers.  As you tug on a string of beads leading from one flower, the rest come to life, moving like puppets using a series of strings attached to motors. The mural itself is drawn using conductive fabric and copper tape, which serve as both expressive and functioning traces within the circuit. Electronic components are also openly displayed and emphasized to explain the electronic workings behind the mural.</p>", "people": ["leah@media.mit.edu", "jieqi@media.mit.edu"], "title": "Novel Architecture", "modified": "2016-12-05T00:16:41.823Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "novel-architecture"}, {"website": "", "description": "<p>The goal of this project is to allow a user with a Bluetooth-enabled cell phone or PDA to gather personally relevant information about, and interact with, inanimate objects such as books, tabletops, or works of art in the user's vicinity. The system allows users to passively gather information about their environment and get just-in-time reminders or suggestions from useful objects in the environment. For example, walking into a room full of augmented books, the books of interest to the user \"blink\" their LEDs, thereby making it easier for the user to find the books that may be of interest.</p>", "people": ["pattie@media.mit.edu"], "title": "Object Awareness", "modified": "2016-12-05T00:16:41.900Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "object-awareness"}, {"website": "", "description": "<p>Analogous to our work in representing video as a set of objects and a scripting language, we are exploring methods for recording audio such that the result is a set of localized sound sources and an acoustical model for the space in which they are placed. Techniques involved include source separation and blind deconvolution.</p>", "people": ["vmb@media.mit.edu"], "title": "Object-Based Audio Capture", "modified": "2016-12-05T00:16:41.930Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "object-based-audio-capture"}, {"website": "", "description": "<p>How can technology help us understand ourselves better? In order to measure the physiological arousal of children with sensory challenges such as ASD and ADHD, tools were developed to help children understand and control what makes them overexcited. Using iCalm hardware, children in therapy sessions measured their arousal while eating, throwing tantrums, playing in ball pits, and making challenging choices. Beyond progressive findings in the field of occupational therapy, this research is a basis for bio-information technology: tools to help children, their parents, and their teachers better understand what is going on in their bodies in a comfortable, affordable, and adaptable way. With future work, technology will be developed to help children understand and control their own internal states. In addition, this project will go beyond children\ufffds therapy\ufffdhelping adults in various settings including business and home life.</p>", "people": ["mgoodwin@media.mit.edu", "picard@media.mit.edu", "hedman@media.mit.edu"], "title": "Objective Self: Understanding Internal Responses", "modified": "2016-12-05T00:16:41.968Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-450", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "objective-self-understanding-internal-responses"}, {"website": "", "description": "<p>Invisibilia seeks to explore the use of Augmented Reality (AR), head-mounted displays (HMD), and depth cameras to create a system that makes invisible data from our environment visible, combining widely accessible hardware to visualize layers of information on top of the physical world. Using our implemented prototype, the user can visualize, interact with, and modify properties of sound waves in real time by using intuitive hand gestures. Thus, the system supports experiential learning about certain physics phenomena through observation and hands-on experimentation.</p>", "people": ["amores@media.mit.edu", "pattie@media.mit.edu"], "title": "Invisibilia: Revealing Invisible Data as a Tool for Experiential Learning", "modified": "2018-08-20T16:39:45.640Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces", "terrestrial-sensing"], "published": true, "active": false, "end_on": "2016-08-31", "slug": "invisibilia-revealing-invisible-data-as-a-tool-for-experiential-learning"}, {"website": "", "description": "<p>Omo is an alternative relational object. While similar to \"carebots\" and companion robots, Omo draws on ongoing Machine Therapy work revealing the psychological, social, and political dynamics between people and machines. As a result, Omo's role is empathic and sometimes unexpected rather than normative. Omo breathes and senses the breathing of anyone interacting closely with it, matching--or seeking to lead--patterns of breathing. Omo does not always privilege soothing.test</p>", "people": ["monster@media.mit.edu", "csik@media.mit.edu"], "title": "Omo", "modified": "2016-12-05T00:16:42.031Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-020D", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "omo"}, {"website": "", "description": "<p>While research has given us high-fidelity technology, such as 24-bit digital audio, the lush jungle of one-bit sound beckons to be explored. This research leverages the last 25 years of DSP/microcontroller technology and music culture to design elegant electronic instruments, making sounds both nostalgic and cutting-edge to reach people emotionally.</p>", "people": ["csik@media.mit.edu"], "title": "One-Bit Synthesis", "modified": "2016-12-05T00:16:42.082Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "one-bit-synthesis"}, {"website": "", "description": "<p>OnObject transforms any object or surface into a gesture-triggered interactive toy. By applying an RFID tag to an object, grabbing it wearing a sensing device, and making hand gestures like shake, swing and tilt, you can trigger interactive audio output in stuffed animals, a mug, your desk, or your body.</p>", "people": ["ishii@media.mit.edu"], "title": "OnObject", "modified": "2016-12-05T00:16:42.210Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "onobject"}, {"website": "", "description": "<p>OnTheRun is a location-based exercise game designed for the iPhone. The player assumes the role of a fugitive trying to gather clues to clear his name. The game is played outdoors while running, creating missions that are tailored to the player's neighborhood and running ability. The game is primarily an audio experience, and gameplay involves following turn-by-turn directions, outrunning virtual enemies, and reaching destinations.</p>", "people": ["geek@media.mit.edu"], "title": "OnTheRun", "modified": "2016-12-05T00:16:42.233Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "ontherun"}, {"website": "", "description": "<p>The Open Mind Common Sense project has collected hundreds of thousands of statements of common-sense knowledge from volunteers on the Internet, using a variety of online activities in several different languages. Open Mind Commons aims to use analogical reasoning to make connections between similar ideas while highlighting the relevant differences as well. These analogies can give a computer a better understanding of the relationships between objects, situations, and cultures. It is often difficult to search through and coordinate lexical information across data sources, each of which has its own separate interface and viewing software. We have approached this problem by creating a unified, flexible interface for various natural-language processing resources.</p>", "people": ["lieber@media.mit.edu", "minsky@media.mit.edu"], "title": "Open Mind Commons", "modified": "2016-12-05T00:16:42.348Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "open-mind-commons"}, {"website": "", "description": "<p>Open Spaces combines the use of public displays with private mobile devices.  This allows group collaboration and shared use of public screens.  We demonstrate this via a public/private Scrabble game and a shared calendar.  The underlying architecture extends to use social predicates to determine who sees what.</p>", "people": ["lip@media.mit.edu"], "title": "Open Spaces", "modified": "2016-12-05T00:16:42.396Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "open-spaces"}, {"website": "", "description": "<p>OPENCODE is a programming tool, perhaps the most elegantly simple programming environment ever. It offers \"two click\" programming for the inexperienced, along with sharing and tagging features for more advanced users. In essence, OPENCODE is a programming tool designed to leverage programming communities to enhance creative potential.</p>", "people": ["holtzman@media.mit.edu"], "title": "OPENCODE", "modified": "2016-12-05T00:16:42.429Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "opencode"}, {"website": "", "description": "<p>When crowd maps track an eco-disaster, social information isn't enough to determine the severity of the disaster. This Ushahidi plugin introduces infrared environmental maps so that social and satellite data can be analyzed together. It's been deployed in Jakarta and NYC.</p>", "people": ["holtzman@media.mit.edu"], "title": "OpenIR: Crowd Map", "modified": "2016-12-05T00:16:42.465Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "openir-crowd-map"}, {"website": "", "description": "<p>OpenLocker is a Simple Registration compatible OpenID authentication server. Users have lockers that act as their identity pages as well as an RSS aggregator. The identity URI corresponds to unique locker locations, and passwords are replaced with combination locks, creating anonymity in the URI and preventing the use of the same password the users may use throughout multiple sites.</p>", "people": ["holtzman@media.mit.edu"], "title": "OPENLOCKER", "modified": "2016-12-05T00:16:42.485Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "openlocker"}, {"website": "", "description": "<p>OPENTAG is an RFID development platform for advancing ubiquitous electronic tagging of items and people. With OPENTAG, we break through the barrier of application-specific and proprietary RFID product engineering by developing our own tag and associated firmware. An OPENTAG can adapt to varying demands as a tagged item moves through its life cycle from manufacturing to supply-chain to retail to consumer to disposal. OPENTAG is security conscious and privacy friendly.</p>", "people": ["holtzman@media.mit.edu"], "title": "OPENTAG", "modified": "2016-12-05T00:16:42.514Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "opentag"}, {"website": "", "description": "<p>Information processing in ultrafast laser dynamics by constrained cavity optimization</p>", "people": ["neilg@media.mit.edu"], "title": "Optical Programming", "modified": "2016-12-05T00:16:42.579Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "optical-programming"}, {"website": "", "description": "<p>NMR spectrometers are in almost every lab and hospital for their unique ability to do chemical spectroscopy and imaging. They could be incredibly interesting in homes and offices as new types of diagnostic instruments. Part of what holds back this development is that a human user is required to supervise the magnetic field of the spectrometer to ensure it is as homogeneous as possible. An algorithm has been developed to make this process dramatically easier and can be done solely with a computer.</p>", "people": ["neilg@media.mit.edu"], "title": "Optimal Shimming", "modified": "2016-12-05T00:16:42.600Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-489", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "optimal-shimming"}, {"website": "", "description": "<p>As virtual environments become more widely adopted for different tasks, it is critical that we develop techniques for designing spaces and interfaces that clearly suggest certain kinds of behavior for users, both in virtual environments and for people who interact with a virtual environment through a mixed-reality boundary. To explore these issues, we have designed and deployed a mixed-reality table soccer game in which teams of players\ufffdreal and virtual\ufffduse two different interfaces to play a soccer-like game together.</p>", "people": ["judith@media.mit.edu"], "title": "Stiff People's League", "modified": "2016-12-05T00:16:42.644Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "stiff-peoples-league"}, {"website": "", "description": "<p>Our Story Tree is a collaborative, online storytelling environment that explores the idea of a people\ufffds history. It focuses on people of Armenian descent, offering them a virtual venue where they can meet, make, and share rich media stories about their everyday lives, discover connections between their family and the wider community, and\ufffdthrough reflection and interpersonal communication\ufffddevelop a shared inter-generational understanding of their complex history. The project combines network connectivity, Confectionary, a state-of-the-art story making toolkit, and a series of participatory ateliers within the Armenian community that introduce people of all ages to rich story-making and story-reflecting.</p>", "people": ["gid@media.mit.edu"], "title": "Our Story Tree", "modified": "2016-12-05T00:16:42.841Z", "visibility": "LAB", "start_on": "2007-01-01", "location": "E15-368", "groups": [], "published": true, "active": false, "end_on": "2008-09-01", "slug": "our-story-tree"}, {"website": "", "description": "<p>This project has developed a software agent that finds people who have never met, but share similar interests, and introduces them to each other. Such introductions can automatically form interest groups and coalitions, and can be used to locate someone knowledgeable in a particular area. Each participant runs a copy of the agent, and these individual copies find each other, as appropriate, on the network and begin the introduction process. The project is an experiment in creating a decentralized, fault-tolerant application that handles potentially sensitive information (such as people's mail, their personal files, or lists of their particular interests) in a responsible and privacy-protecting fashion, using cryptographic and other techniques. The goal is ubiquitous deployment across the Internet.</p>", "people": ["pattie@media.mit.edu"], "title": "Yenta: Matchmaking Agents", "modified": "2016-12-05T00:16:42.865Z", "visibility": "PUBLIC", "start_on": "1994-12-31", "location": "", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "yenta-matchmaking-agents"}, {"website": "", "description": "<p>Origami Desk is an interactive installation where users learn to fold paper into beautiful shapes. Origami Desk improves on the inscrutable origami diagrams we all know and love by showing videos that demonstrate what the hands should do, projecting lines onto the paper indicating where the folds should be, and monitoring the paper folding to give the budding origami artist feedback should his folding go awry.</p>", "people": ["fletcher@media.mit.edu", "mike@media.mit.edu", "rehmi@media.mit.edu"], "title": "Origami Desk", "modified": "2016-12-05T00:16:42.891Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-468", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "origami-desk"}, {"website": "", "description": "<p>The digital revolution has fundamentally changed our lives. Multimedia content-creation tools allow us to instantiate and share ideas easily, but most outcomes only exist on-screen and online--the physical world and everyday objects are largely excluded from a parallel explosion of mechatronic object creation. Services like Ponoko and Shapeways allow professionals and non-professionals to access computer-aided manufacturing (CAM) tools like 3D-printing and laser-cutting, but there are few (if any) design tools for creating complex mechanical assemblies that take full advantage of CAM systems. Creating unique mechatronic artifacts\ufffdOriginal Machines\ufffdthus requires more specific and sophisticated design tools than exist today. Object-oriented mechatronics is a parametric design approach that connects knowledge about mechanical assemblies and electronics with the requirements of digital manufacturing processes. The approach addresses the missing link between accessible rapid-manufacturing services and currently available design tools, creating new opportunities for self-expression through mechatronic objects and machines.</p>", "people": ["cynthiab@media.mit.edu"], "title": "originalMachines", "modified": "2016-12-05T00:16:42.920Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "originalmachines"}, {"website": "", "description": "<p>A palimpsest is a manuscript consisting of a later writing superimposed upon an original writing. This name has been borrowed for the title of this project that aims to superimpose layers of recorded social interaction and present them as a single image. In contrast to conferencing tools and portals that enable chance encounters between distant locations, the palimpsest facilitates chance encounters between different points in time.</p>", "people": ["stefan@media.mit.edu"], "title": "Palimpsest", "modified": "2016-12-05T00:16:43.126Z", "visibility": "PUBLIC", "start_on": "2001-12-31", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "palimpsest"}, {"website": "", "description": "<p>People naturally express frustration through the use of their motor skills. The purpose of the Affective Tangibles project is to develop physical objects that can be grasped, squeezed, thrown, or otherwise manipulated via a natural display of affect. Constructed tangibles include a PressureMouse, affective pinwheels that are mapped to skin conductance, and a voodoo doll that can be shaken to express frustration. We have found that people often increase the intensity of muscle movements when experiencing frustrating interactions.</p>", "people": ["picard@media.mit.edu"], "title": "Affective Tangibles", "modified": "2016-12-05T00:16:43.232Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "affective-tangibles"}, {"website": "", "description": "<p>Papert's World invites colleagues, collaborators, friends, and family of Seymour Papert to contribute rich-media stories about his life, his research, and research that he inspired. Participants use the rich-media story authoring and publishing tool, Confectionary, to collaborate on a \"media scrapbook\" collection. The site is continually updated with useful features such as a shared media library. </p>", "people": ["gid@media.mit.edu"], "title": "Papert's World", "modified": "2016-12-05T00:16:43.255Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-368", "groups": [], "published": true, "active": false, "end_on": "2008-09-01", "slug": "paperts-world"}, {"website": "", "description": "<p>Part.Preview adds three-dimensional input and output devices to traditional 3D printers and fabrication machines. These enhanced capabilities enable users to \"print preview\" 3D objects before actually creating them.</p>", "people": ["holtzman@media.mit.edu"], "title": "Part.Preview", "modified": "2016-12-05T00:16:43.205Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "partpreview"}, {"website": "", "description": "<p>We used tiny, wearable computers to create \"participatory simulations\" for a new generation of educational activities in which students could learn about dynamical systems by actually participating in simulations of the systems. For example, students could simulate the spread of an epidemic in which a \"virus\" jumps from one wearable computer to another. Our preliminary analysis indicated that the combination of physical and computer interaction lead to a richer learning experience than was possible with traditional computer-simulation activities, or with traditional group activities without computer support.</p>", "people": ["mres@media.mit.edu"], "title": "Participatory Simulations", "modified": "2016-12-05T00:16:43.283Z", "visibility": "PUBLIC", "start_on": "1995-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2000-09-01", "slug": "participatory-simulations"}, {"website": "", "description": "<p>The Participatory Networked Camera (PNC) project supports ad hoc communication and image-sharing between independent camerapeople. The ongoing development of the PNC project includes several prototypes of still and motion cameras that are augmented with temp-tie software, allowing camerapeople to network in decentralized ways without a central director or producer. We have structured experimental productions with a micro-goal approach to enable a rotating set of makers to participate in a production.</p>", "people": ["gid@media.mit.edu"], "title": "Participatory Networked Camera", "modified": "2016-12-05T00:16:43.307Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-368", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "participatory-networked-camera"}, {"website": "", "description": "<p>The automated processing of natural language by computer has become a paramount concern in nearly every field of human endeavor. The amount of information available in current literature and even the mass of information being generated on a daily basis is well beyond the scope of human understanding in any detail. Because of the sheer breadth of information, human analysis and curation of this information is rarely comprehensive and often error-prone. Machine-aided curation of this information could increase its utility by allowing more comprehensive analyses, more complete summarization, and extensive comparisons to current knowledge. Such curation requires a method for arriving at a high-level semantic description of the text at hand. Patent Semantics is an attempt to attack this problem in the domain of biochemical synthesis, by building structured representations of the procedures involved in different synthesis descriptions, grounding the components of those models, and developing algorithms by which detailed, human-readable comparisons of the descriptions may be produced. </p>", "people": ["dkroy@media.mit.edu"], "title": "Patent Semantics", "modified": "2016-12-05T00:16:43.352Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "patent-semantics"}, {"website": "", "description": "<p>Pearls of Wisdom (PoW) is a suite of computational tools and activities designed to engage learners in intentional reflection on their learning experiences. PoW supports the construction and sharing of Pearls, intentional-reflection artifacts containing project how-to information and refection on design and learning experiences. Pearls give learners an \"objects-to-reflect-with\" to make deeper connections between those meaningful and important ideas the learner has chosen to share. The collection of Pearls embodies the ideas and activities culturally resonant within the community. This diversity of perspectives may then be appropriated by other learners. The project goal is to evolve a community of practice around reflection and other deep-learning activities. That included reflection practices that emerged at both the learner and mentor levels.  This work has culminated in the Cooperative Constructionism Framework, a guide for leveraging the learning potential of constructionist environments and expanding the constructionist-learning space. The study took place at the Flagship Computer Clubhouse, part of an international network of over one hundred after-school technology centers for youth from underserved communities.</p>", "people": ["mres@media.mit.edu"], "title": "Pearls of Wisdom", "modified": "2016-12-05T00:16:43.424Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "pearls-of-wisdom"}, {"website": "", "description": "<p>Pavlov is a virtual pet that encourages you to be physically active. He has ambient presence in the screens with which you interact. Pavlov is happy and healthy when you have walked a certain number of steps every day. When you are sedentary for a while, Pavlov nags you to take him out for a walk. He also craves to be the leader of all Pavlovs in your area. He can only be so when you, as his owner, become the most physically active person amongst your friends. Pavlov pings you every day at a certain time, telling you that he is going to have a dog-fight with other Pavlovs. You have the option to watch the dog-fight. Otherwise Pavlov simply tells you if he has won the fight, which may indicate that today you were more physically active than your friends.</p>", "people": ["sujoy@media.mit.edu", "geek@media.mit.edu"], "title": "Pavlov", "modified": "2016-12-05T00:16:43.374Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "pavlov"}, {"website": "", "description": "<p>Peddl creates a localized, perfect market. All offers are broadcasts, allowing users to spot trends, bargains, and opportunities. With GPS- and Internet-enabled mobile devices in almost every pocket, we see an opportunity for a new type of marketplace which takes into account your physical location, availability, and open negotiation. Like other real-time activities, we are exploring transactions as an organizing principle among people that, like Barter, may be strong, rich, and long-lived.</p>", "people": ["dlakatos@media.mit.edu", "ishii@media.mit.edu", "lip@media.mit.edu"], "title": "Peddl", "modified": "2016-12-05T00:16:43.452Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["viral-communications", "tangible-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "peddl"}, {"website": "", "description": "<p>The Pengachu Pocket Linux Server is our effort to bring affordable open-source and open-hardware computing to individuals and communities around the world. We've designed a first generation prototype in a PDA form factor which includes the Linux core, a small LCD display, a button/knob interface, an MPEG audio DSP, a wireless LAN, and a docking station which includes keyboard, wired LAN, and battery charger functionality.</p>", "people": ["neilg@media.mit.edu", "rehmi@media.mit.edu"], "title": "Pengachu Pocket Linux Server", "modified": "2016-12-05T00:16:43.476Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-023", "groups": ["physics-and-media", "health", "personal-fabrication"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "pengachu-pocket-linux-server"}, {"website": "", "description": "<p>The need for low-cost sensors and ID tags is ongoing. Our work explores how ID and sensor information can be encoded in the electromagnetic spectrum of inexpensive material structures without the need for electronic chips. Through the use of novel electromagnetic structures and active materials, it is possible to manipulate and to control the electromagnetic response of structures in order to create a \"representation language\" for conveying ID and sensor information. Applications include smart packaging, automatic inventory systems, and low-cost sensors for home, industrial, and environmental uses.</p>", "people": ["fletcher@media.mit.edu", "neilg@media.mit.edu"], "title": "Penny Tags: General", "modified": "2016-12-05T00:16:43.495Z", "visibility": "PUBLIC", "start_on": "1993-12-31", "location": "E15-023", "groups": ["physics-and-media", "silicon-biology", "health", "cc", "toys-of-tomorrow", "personal-fabrication", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "penny-tags-general"}, {"website": "", "description": "<p>PegBlocks are networked tactile transducers. As users manipulate the array of pegs, sliding them back and forth, motion is converted to electricity and converted back into motion throughout the rest of the network. The resulting movements of the pegs are determined not by any one individual input, but instead by the networked group as a whole.</p>", "people": ["ishii@media.mit.edu"], "title": "PegBlocks", "modified": "2016-12-05T00:16:43.570Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "pegblocks"}, {"website": "", "description": "<p>Designed with attention to the creation and expression of an historical perspective, Storied Objects present us with an object-centric view of their existence over time.  By acknowledging that an object's life can be reflected through storied output of its genealogy, history of social and cultural interactions, utilitarian features of the object, and many circumstances of which we are not aware, we explore methods for appropriate technological augmentation of an object, enabling the capture, reflection, sharing, and revealing of its stories. Through this approach, designers can enrich form and function of an object with narratives based on the historicity of everyday things. We have devised a case study, the Audio Bench, that considers how the capability for recording and expressing history can add aesthetic and cultural value\ufffda \ufffdstoriedness\u201d\u2014to the object. </p>", "people": ["gid@media.mit.edu"], "title": "Storied Objects", "modified": "2016-12-05T00:16:43.520Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "storied-objects"}, {"website": "", "description": "<p>Over the last decade, tracking people has attracted considerable research interest. Cameras enable efficient tracking of people but it is a nontrivial task, especially when the sensors have a non-overlapping range. The goal is to track\ufffdthat is, both locate and identify\ufffdpeople around a multi-story building with a sparse network formed by non-overlapping cameras and RFID sensors.</p>", "people": ["joep@media.mit.edu"], "title": "People Tracking System", "modified": "2016-12-05T00:16:43.620Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "people-tracking-system"}, {"website": "", "description": "<p>PICO is a new type of human-computer interface combining the usability advantages of mechanical systems with the abstract computational power of computers. Before modern computers, mechanical devices used for computation were limited to relatively simple tasks, but easy to use and understand because operation details are observable. Computers can handle more abstract and complex tasks, but the processes are hidden and difficult to understand. We are merging software-based computation with dynamic physical processes that are exposed to and modified by the user. By designing interfaces employing the dynamic behavior of objects in the physical world as an interface vocabulary, we aim to create interfaces that allow people and computers to collaborate in novel ways. Projects include an interface for planning cellular telephone networks on an actuated tabletop sensing surface; objects on this surface are moved under software control using electromagnets, but also by users standing at the table, who can physically intervene in the computational optimization. We will investigate whether the communication bandwidth afforded by an interface that heavily engages the sense of touch can change the approach that experts take to solving some types of optimization problems.</p>", "people": ["ishii@media.mit.edu", "jpatten@media.mit.edu"], "title": "PICO", "modified": "2016-12-05T00:16:43.663Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "pico"}, {"website": "", "description": "<p>Digital Synesthesia looks to evolve the idea of human-computer interfacing and give way for human-world interacting. It aims to find a way for users to experience the world by perceiving information outside of their sensory capabilities. Modern technology already offers the ability to detect information from the world that is beyond our natural sensory spectrum; however, there is still no real way for our brains and body to incorporate this new information as a part of our sensory toolkit, so that we can understand our surrounding world in new and undiscovered ways. The long-term vision of this work is to give users the ability to turn senses on and off depending on the desired experience. This project is part of the Ultimate Media initiative and will be applied to the navigation and discovery of media content.</p>", "people": ["talfaro@media.mit.edu", "vmb@media.mit.edu"], "title": "Digital Synesthesia", "modified": "2019-05-17T14:21:46.728Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["object-based-media", "ultimate-media"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "digital-synesthesia"}, {"website": "", "description": "<p>Most modern electronic musical instruments are constructed of artifacts from the military-industrial complex. As their components become obsolete, musicians must invent new technologies for synthesizing electronic music.  Using naturally occurring materials, this design will survive on Earth for at least 3 million years\ufffdbecause it does not require expensive, intricate manufacturing processes.  </p>", "people": ["csik@media.mit.edu"], "title": "Permanent Synthesis", "modified": "2016-12-05T00:16:43.696Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-001", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "permanent-synthesis"}, {"website": "", "description": "<p>Personal Opera is a radically innovative creative environment that enables anyone to create musical masterpieces sharing personal thoughts, feelings, and memories. Based on our design of, and experience with, such projects as Hyperscore and the Brain Opera, we are developing a totally new environment to allow the incorporation of personal stories, images, and both original and well-loved music and sounds. Personal Opera builds on our guiding principle that active music creation yields far more powerful benefits than passive listening. Using music as the through-line for assembling and conveying our own individual legacies, Personal Opera represents a new form of expressive archiving: easy to use and powerful to experience. In partnership with the Royal Opera House in London, we have begun conducting Personal Opera workshops specifically targeting seniors to help them tell their own meaningful stories through music, text, visuals, and acting.</p>", "people": ["patorpey@media.mit.edu", "tod@media.mit.edu"], "title": "Personal Opera", "modified": "2016-12-05T00:16:43.803Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "Swatch Lab", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "personal-opera"}, {"website": "", "description": "<p>Personas is a component of the Metropath(ologies) exhibit, recently on display at the MIT Museum by the Sociable Media Group. It uses sophisticated natural language processing and the Internet to create a data portrait of one's aggregated online identity. In short, Personas shows you how the Internet sees you.</p>", "people": ["judith@media.mit.edu"], "title": "Personas", "modified": "2016-12-05T00:16:43.825Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-318", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "personas"}, {"website": "", "description": "<p>Every 98 seconds, a person in the United States is sexually abused. Every 16 hours,&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">a woman in the United States is murdered by her romantic partner or ex-partner.&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">Sexual abuse, assault, and harassment are regarded as some of the most common&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">human rights violations in the world by the United Nations. Our work examines&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">methods to prevent sexual assault, from pre-historic times to latest technologies,&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">to inform contemporary designs. Intrepid investigates multiple methods to&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">detect initial signs of assault and develop methods for communication and prevention&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">of assault. We also explore olfactory stimuli as a potential means to prevent sexual&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">assault in real-time.</span></p>", "people": ["manisham@media.mit.edu", "ethanz@media.mit.edu", "geek@media.mit.edu"], "title": "Intrepid", "modified": "2018-04-20T17:21:38.623Z", "visibility": "PUBLIC", "start_on": "2016-01-04", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2017-06-30", "slug": "intrepid"}, {"website": "", "description": "<p>With the tremendous explosion of community photos, how can we present a coherent visual model for a large-scale community photo collection? In this project, we aim to build a photo space explorer for effective image browsing, retrieval, and annotation on top of our existing, distributed, community storage mechanism. The key is to identify object and feature points relations among community photos in an effective and scalable way, and to distribute the storage through an ensemble of mobile devices. The photo space explorer also enables users to navigate community photo collections using a 3-D browser.</p>", "people": ["lip@media.mit.edu"], "title": "Photo Space", "modified": "2016-12-05T00:16:43.897Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "photo-space"}, {"website": "", "description": "<p>Professional ballroom dancer Adrianne Haslet-Davis lost her natural ability to dance when her left leg was amputated below the knee following the Boston Marathon bombings in April 2013. Hugh Herr was introduced to Adrianne while meeting with bombing survivors at Boston's Spaulding Rehabilitation Hospital. For Professor Herr, this meeting generated a research challenge: build Adrianne a bionic ankle prosthesis, and restore her ability to dance. The research team for this project spent some 200 days studying the biomechanics of dancing and designing the bionic technology based on their investigations. The control system for Adrianne was implemented on a customized BiOM bionic ankle prosthesis.  </p>", "people": ["bevinlin@media.mit.edu", "hherr@media.mit.edu"], "title": "Dancing Control System for Bionic Ankle Prosthesis", "modified": "2016-12-05T00:16:43.994Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": "2013-03-31", "slug": "dancing-control-system-for-bionic-ankle-prosthesis"}, {"website": "", "description": "<p>This project introduces the concept of physical one-way functions. Modern public key cryptography relies extensively on one-way functions and trapdoor one-way functions. Our use of physical mechanisms for one-way functions is motivated by emerging cryptographic applications which require a means for authenticating digital information that itself cannot be copied, and adds little cost to the system. The asymmetry that we exploit is in the ease of constructing and characterizing large physical structures with fine feature sizes, and the difficulty in reproducing them.</p>", "people": ["neilg@media.mit.edu"], "title": "Physical One-Way Functions", "modified": "2016-12-05T00:16:43.939Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "physical-one-way-functions"}, {"website": "", "description": "<p>We propose a new approach to physical telepresence, based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In the research paper, we describe the concept of shape transmission, and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of users' body parts can be altered to amplify their capabilities for teleoperation. A preliminary evaluation found that users were able to manipulate simple objects remotely, and found many different techniques for manipulation that highlight the expressive nature of our system.</p>", "people": ["daniell@media.mit.edu", "olwal@media.mit.edu", "ishii@media.mit.edu"], "title": "Physical Telepresence", "modified": "2016-12-05T00:16:44.043Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["tangible-media", "terrestrial-sensing"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "physical-telepresence"}, {"website": "", "description": "<p>Imagine taking a digital picture with your new camera-phone and the picture is able to annotate itself. For example, if you are at Fenway Park and take a picture, the picture will automatically be annotated with keywords such as \"Fenway Park,\" \"Baseball,\" or \"Red Sox.\" This project takes advantage of a GPS receiver and a geographical search engine called MetaCarta to annotate pictures in real time with relevant keywords. These keywords help users to make sense of their surroundings when they take the picture, as well as to organize and retrieve pictures afterwards.</p>", "people": ["pattie@media.mit.edu"], "title": "Photowhere", "modified": "2016-12-05T00:16:44.110Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "photowhere"}, {"website": "", "description": "<p>Picture This! is a video editing tool progressing toward children's natural expression of play while telling stories with their toys. We use technology to offer visual feedback regarding how the scene looks from the point of view of an imaginary audience. If the toy has an immediately accessible visual perspective, a new world is opened to the child. The toy helps her explore visual and narrative perspectives of character props, expanding the discovery of her environment. The child storyteller enters the world of the movie maker. Cameras become part of a toy system showing how things look from a toy's point of view. They can be integrated in LEGO people, car drivers, and even coffee mugs! The video process, supported by gesture-induced editing, benefits children in practicing social interrelationships and visual perspective taking.</p>", "people": ["ishii@media.mit.edu"], "title": "Picture This!", "modified": "2016-12-05T00:16:44.135Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-350", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "picture-this"}, {"website": "", "description": "<p>Most uses of digital technologies pull young people away from hands-on experimentation with the physical world. The Playful Invention and Exploration (PIE) Network has bridged that divide, integrating digital technologies and hands-on experimentation in a new generation of science and engineering education activities that have proven very engaging for young people and their families. PIE activities have been developed by a network of six museums around the country, using Cricket technology developed at the Media Lab (with research support from NSF). Science museums around the country are now incorporating Crickets and PIE activities into their public programs.</p>", "people": ["sylvan@media.mit.edu", "mres@media.mit.edu", "bss@media.mit.edu", "nrusk@media.mit.edu"], "title": "PIE Network", "modified": "2016-12-05T00:16:44.166Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "pie-network"}, {"website": "", "description": "<p>Piezing is an outfit which generates power using the natural gestures of the human body in motion. Around the joints of the elbows and hips, the garment is embedded with piezoelectric material elements which generate an electric potential in response to applied mechanical stress. The electric potential is then stored as voltage in a centralized small battery and later can be discharged into a device.</p>", "people": ["ishii@media.mit.edu"], "title": "Piezing", "modified": "2016-12-05T00:16:44.263Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "piezing"}, {"website": "", "description": "<p>An electric tambourine that is completely powered by the playing of the instrument.  The jingles of the tambourine are lined with piezoelectric elements, which generate voltage when impacted. This voltage is then harvested to turn on LED lights on the tambourine. The harder the tambourine rattles, the greater the voltage generated by the piezoelectric elements and thus the brighter the light. Yellow LED lights on the jingles light up when the corresponding piezo is rattled. If the tambourine is played with enough force, blue and red LED lights on the band also light up. Thus, the player can both hear and see the music generated by this instrument.</p>", "people": ["leah@media.mit.edu", "jieqi@media.mit.edu"], "title": "Piezo Powered Tambourine", "modified": "2016-12-05T00:16:44.288Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "piezo-powered-tambourine"}, {"website": "", "description": "<p>PingPongPlus is a digitally enhanced version of the classic ping-pong game. It is played with ordinary, un-tethered paddles and balls, and features a \"reactive table\" that incorporates sensing, sound, and projection technologies. Projectors display patterns of light and shadow on the table; bouncing balls leave images of rippling water; and the rhythm of play drives accompanying sound and visuals. In the process, this project  explores new ways to couple athletic recreation and social interaction with engaging digital enhancements.</p>", "people": ["ishii@media.mit.edu"], "title": "PingPongPlus", "modified": "2016-12-05T00:16:44.315Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "pingpongplus"}, {"website": "", "description": "<p>PingPong++ (PingPongPlusPlus) builds on PingPongPlus (1998), a ping pong table that could sense ball hits, and reuse that data to control visualizations projected on the table. We have redesigned the system using open-source hardware and software platforms so that anyone in the world can build their own reactive table. We are exploring ways that people can customize their ping pong game experience. This kiosk allows players to create their own visualizations based on a set of templates. For more control of custom visualizations, we have released a software API based on the popular Processing language to enable users to write their own visualizations. We are always looking for collaborators! Visit pppp.media.mit.edu to learn more.</p>", "people": ["daniell@media.mit.edu", "x_x@media.mit.edu", "ishii@media.mit.edu"], "title": "PingPongPlusPlus", "modified": "2016-12-05T00:16:44.338Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "pingpongplusplus"}, {"website": "", "description": "<p>Pintail is a travel companion app for guided storytelling. It will start by capturing your travel plan so that it can nudge you with personalized story-creation triggers at the right context. Pintail would act as a work-in-progress scrapbook from the moment a trip is planned. It will provide users the structure and tools for storytelling while taking into account the short attention span of today's audience. Pintail would use priming as a technique by showing the user what others feel or have drawn about the places he or she is visiting. Some of the content of Pintail prompts would be automatically collected from travel review sites. Users can then use the Pintail story-creation tools to reflect and create their own stories. Pintail would also attempt to balance between the story creation activity and the actual trip experience.</p>", "people": ["sujoy@media.mit.edu", "geek@media.mit.edu"], "title": "Pintail", "modified": "2016-12-05T00:16:44.365Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "pintail"}, {"website": "", "description": "<p>The pinwheels project continues the study of Ambient Fixtures to communicate digital information at the periphery of human perception through ambient media. This project takes fields of pinwheels and explores what arrangements create interfaces that are intuitive and informative for the user while being an ambient source of information. Its other focus is on finding interesting application domains and ways to map computationally the information to the pinwheels. Its current application domains include stock market activity monitoring, Web site hits tracking, natural wind movement, and network activity monitoring.</p>", "people": ["ishii@media.mit.edu"], "title": "pinwheels", "modified": "2016-12-05T00:16:44.390Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "pinwheels"}, {"website": "", "description": "<p>Pitcher is an efficient and flexible real-time software-based pitch shifter. It goes well beyond an octave shift with no strong timbre artifact. It applies particularly well to the singing voice, or to instruments such as the Hyperviolin. It is now being networked for collaborative new music experiences.</p>", "people": ["tristan@media.mit.edu", "tod@media.mit.edu"], "title": "Pitcher", "modified": "2016-12-05T00:16:44.428Z", "visibility": "PUBLIC", "start_on": "2001-12-31", "location": "E15-445", "groups": ["opera-of-the-future", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "pitcher"}, {"website": "", "description": "<p>The Internet has disrupted the aid sector like so many other industries before it. In times of crisis, donors are increasingly connecting directly with affected populations to provide participatory aid. The Digital Humanitarian Marketplace aggregates these digital volunteering projects, organizing them by crisis and skills required to help coordinate this promising new space.</p>", "people": ["ethanz@media.mit.edu", "stempeck@media.mit.edu"], "title": "Digital Humanitarian Marketplace", "modified": "2016-12-05T00:16:44.562Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2014-04-01", "slug": "digital-humanitarian-marketplace"}, {"website": "", "description": "<p>The placebo effect is not limited to sugar pills in clinical trials of medications. In fact, placebos are a specific example of the affect of meaning on health. All medicines work in two parallel ways, as a direct result of the active medication, and indirectly as a placebo response. Expectations and beliefs lead to measurable and real health outcomes, and one important way that expectations and beliefs are manipulated is by pricing, branding, and marketing. These are powerful factors that can affect how people respond to medicines. This project investigates how differences in pricing, branding, and other marketing factors affect people\ufffds response to medication. By understanding this response, we can understand how people respond to drugs in the real world outside of the clinical trial.</p>", "people": [], "title": "Placebos & Marketing", "modified": "2016-12-05T00:16:44.446Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-320A", "groups": ["erationality"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "placebos-marketing"}, {"website": "", "description": "<p>This top-secret project is aimed at improving human cognition and happiness, by empowering people to control their lives.</p>", "people": ["esb@media.mit.edu"], "title": "Plasma Planning", "modified": "2016-12-05T00:16:44.611Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-435", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "plasma-planning"}, {"website": "", "description": "<p>The Out for Change Transformative Media Organizing Project (OCTOP) links LGBTQ, Two-Spirit, and allied media makers, online organizers, and tech-activists across the United States. In 2013-2014, we are conducting a strengths/needs assessment of the media and organizing capacity of the movement, as well as offering a series of workshops and skillshares around transmedia organizing. The project is guided by a core group of project partners and advisers who work with LGBTQ and Two-Spirit folks. The project is supported by faculty and staff at the MIT Center for Civic Media, Research Action Design and by the Ford Foundation's Advancing LGBT Rights Initiative.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "elplatt@media.mit.edu"], "title": "Out for Change: Transformative Media Organizing Project", "modified": "2016-12-05T00:16:44.784Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "out-for-change-transformative-media-organizing-project"}, {"website": "", "description": "<p>As RFID technology matures, and PDAs such as smart phones become ubiquitous, the time is right to look at the future of RFID tags beyond the inventory list. This project examines using smart phones as a mediator between RFID tags and smart appliances, making the use of RFID tags seamless and intuitive, and supporting applications using everyday objects in everyday scenarios.</p>", "people": ["pattie@media.mit.edu"], "title": "Plunge-n-Play", "modified": "2016-12-05T00:16:44.725Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "Pond", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "plunge-n-play"}, {"website": "", "description": "<p>Plethora creates simultaneous, localized, personal broadcasting networks that allow audiences to form on-the-fly and build their own media streams. The display ranges from personal devices such as phones, to embedded screens in kitchens. It uses Bluetooth LE beacons to emulate local broadcast stations that signal the proximity. Plethora inverts the relationship between mobility and the multiplicity of screens with which we interact on a moment-to-moment basis. It also considers media as migrating away from larger, high definition screens to a universe of personal, anytime, anywhere representations.</p>", "people": ["weller@media.mit.edu", "lip@media.mit.edu"], "title": "Plethora", "modified": "2016-12-05T00:16:44.818Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2016-01-01", "slug": "plethora"}, {"website": "", "description": "<p>PlusShorts is a Java applet designed to encourage and facilitate online collaboration between videomakers. The system uses punctuation as an iconic system for describing and augmenting edited video, with the intention of providing users with a level of interactivity that is both playful and meaningful.</p>", "people": ["gid@media.mit.edu"], "title": "PlusShorts", "modified": "2016-12-05T00:16:44.844Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-351", "groups": ["interactive-cinema"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "plusshorts"}, {"website": "", "description": "<p>Point & Shoot Data explores the use of visible light as a wireless communication medium for mobile devices. A snap-on case allows users to send messages to other mobile devices based on directionality and proximity. No email address, phone number, or account login is needed, just point and shoot your messages! The project enables infrastructure-free, scalable, proximity-based communication between two mobile devices.</p>", "people": ["lip@media.mit.edu", "trich@media.mit.edu"], "title": "Point & Shoot Data", "modified": "2016-12-05T00:16:45.032Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "point-shoot-data"}, {"website": "", "description": "<p>Imagine a room filled with electronic devices\ufffdappliances, thermostats, computers, portable music players, PDAs\ufffdall of which can communicate wirelessly. This may not even require an act of imagination. Now imagine yourself with a handheld device in the middle of all this wanting to use it to communicate with just one of these devices. How will you do it? Will you select its name from a menu on a screen? Will you tap on an icon? Either way your attention will be focused on the device in your hand, not the true target of your intention. Your selection will be a selection by metaphor. Why not simply point at the thing you want to communicate with? Pointing is a natural extension of the human capacity to focus attention. It establishes a spatial axis relative to an agent, unambiguously identifying anything in line-of-sight without a need to name it. This brings our interactions with electronic devices closer to our interactions with physical objects, which we name only when we have to. Pointable Computing proposes that a visible laser be coupled to a handheld device and cheap sensors attached to potential objects of communication. Information may be conveyed along a laser beam, establishing a channel of communication along a perceivable axis of pointing. Ultimately, the goal of this project is\ufffdthrough a subtle change in the focus of interaction\ufffdto foster decentralized computation in which individual computational objects scattered in the environment may be controlled directly rather than through a single hub. This kind of electronic autonomy curtails the ability of faceless authorities or proprietary standards to shape the future of computation.</p>", "people": [], "title": "Pointable Computing", "modified": "2016-12-05T00:16:45.069Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-301", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "pointable-computing"}, {"website": "", "description": "<p>We introduce polarization field displays as an optically efficient design for dynamic light field display using multi-layered LCDs. Such displays consist of a stacked set of liquid crystal panels with a single pair of crossed linear polarizers. Each layer is modeled as a spatially controllable polarization rotator, as opposed to a conventional spatial light modulator that directly attenuates light. We demonstrate that such displays can be controlled, at interactive refresh rates, by adopting the SART algorithm to tomographically solve for the optimal spatially varying polarization state rotations applied by each layer. We validate our design by constructing a prototype using modified off-the-shelf panels. We demonstrate interactive display using a GPU-based SART implementation supporting both polarization-based and attenuation-based architectures.</p>", "people": ["raskar@media.mit.edu", "gordonw@media.mit.edu", "naik@media.mit.edu"], "title": "Polarization Fields: Glasses-Free 3DTV", "modified": "2016-12-05T00:16:45.164Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2015-09-15", "slug": "polarization-fields-glasses-free-3dtv"}, {"website": "", "description": "<p>The term \"polynym\" represents a theory that involves categorical division of a topic. A polynym knowledge base can enhance cultural disambiguation for automatic machine translation.</p>", "people": ["walter@media.mit.edu"], "title": "Polynyms", "modified": "2016-12-05T00:16:45.134Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-320", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "polynyms"}, {"website": "", "description": "<p>Mobility in urban environments is often coupled with a quest for efficiencies such as a nearby parking spot, a ride to some destination, a date for dinner, a ticket to the theater, or any other kind of service that one might need while on the move. The idea is an ad hoc, mobile \"Craig's List.\" We demonstrate a system that provides an easy way to advertise services or goods offered or sought by means of mobile phones and ultra-mobile PCs, and we are concerned with issues of ensuring trust, security, and anonymity for all participants.</p>", "people": ["ypod@media.mit.edu", "lip@media.mit.edu"], "title": "PolyQuest", "modified": "2016-12-05T00:16:45.192Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "polyquest"}, {"website": "", "description": "<p>We are applying learnings from the SNAPSHOT study to the problem of changing behavior, exploring the design of user-centered tools which can harness the experience of collecting and reflecting on personal data to promote healthy behaviors--including stress management and sleep regularity. We draw on commonly used theories of behavior change as the inspiration for distinct conceptual designs for a behavior-change application based on the SNAPSHOT study. This approach will enable us to compare the types of visualization strategies that are most meaningful and useful for acting on each theory.</p>", "people": ["picard@media.mit.edu", "sataylor@media.mit.edu", "akanes@media.mit.edu"], "title": "SNAPSHOT Expose", "modified": "2018-03-08T15:48:06.097Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["affective-computing", "advancing-wellbeing"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "snapshot-expose"}, {"website": "", "description": "<p>Portrait of Cati 2 is an homage to the project \"Portrait of Cati\" done by Stefan Agamanolis. This portrait reacts differently to a man or a woman. While reacting to a woman in a more natural way, Cati's image is more of woman's icon as a reaction to a man.</p>", "people": ["sajid@media.mit.edu", "stefan@media.mit.edu", "pattie@media.mit.edu"], "title": "Portrait of Cati 2", "modified": "2016-12-05T00:16:45.332Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "portrait-of-cati-2"}, {"website": "", "description": "<p>Each day we walk by silent buildings and lots. Absent landlords, hibernating developers, abandoned lots\ufffdtheir impact as part of the patchwork is subtle but real. Be they eyesore or brownfield, spaces are reactive. Unused spaces trap the vitality of the neighborhoods they surround. For the last 15 years, Vail Court has fallen into disrepair. The 24-unit housing complex is uninhabitable and unsafe as a structure. Conversation continues within the community, but how does the conversation flow back out\ufffdbeyond city councilors to the decision-making owners? Postmarked asked neighbors and frequent passersby to fill postcards that were mailed directly to the property owners. These postcards captured the lived experience and hopes of those who share Vail Court in common. The postcards and process were documented online before being mailed\ufffdtwo cards per day\ufffdwith an invitation for owners to respond to the community online.</p>", "people": ["ethanz@media.mit.edu"], "title": "Postmarked", "modified": "2016-12-05T00:16:45.379Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "postmarked"}, {"website": "", "description": "<p>The Powers Sensor Chair gives visitors a special glimpse into Tod Machover\ufffds robotic opera \"Death and the Powers,\" by providing a new way to explore the sonic world of the opera. A solo participant sitting in a chair discovers that when she moves her hands and arms, the air in front of her becomes an instrument. With a small, delicate gesture, a sharp and energetic thrust of her hand, or a smooth caress of the space around her, she can use her expressive movement and gesture to play with and sculpt a rich sound environment drawn from the opera, including vocal outbursts and murmurs and the sounds of the show\ufffds special Hyperinstruments. This installation explores the body as a subtle and powerful instrument, providing continuous control of continuous expression, and incorporates alum Elena Jessop\ufffds high-level analysis frameworks for recognition and extension of expressive movement.</p>", "people": ["ejessop@media.mit.edu", "tod@media.mit.edu"], "title": "Powers Sensor Chair", "modified": "2016-12-05T00:16:45.403Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "powers-sensor-chair"}, {"website": "", "description": "<p>With Story Trail, people record a piece of story on top of a toy trail piece. As a stuffed toy moves along the trail, it tells stories that were recorded on the trail. Each trail piece can be moved around or re-recorded to change both the physical and metaphoric direction of the story.</p>", "people": [], "title": "Story Trail", "modified": "2016-12-05T00:16:45.429Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-320", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "story-trail"}, {"website": "", "description": "<p>We have developed a system to recognize posture patterns and associated affective states in real time, in an unobtrusive way, from a set of pressure sensors on a chair. This system discriminates states of children in learning situations, such as when the child is interested, or is starting to take frequent breaks and looking bored. The system uses pattern recognition techniques, while watching natural behaviors, to \"learn\" what behaviors tend to accompany which states. The system thus detects the surface-level behaviors (postures) and their mappings during a learning situation in an unobtrusive manner so that they don't interfere with the natural learning process. Through the chair, we can reliably detect nine static postures, and four temporal patterns associated with affective states.</p>", "people": ["picard@media.mit.edu", "win@media.mit.edu"], "title": "Posture Recognition Chair", "modified": "2016-12-05T00:16:45.454Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "posture-recognition-chair"}, {"website": "", "description": "<p>'PreCursor' is an invisible layer that hovers in front of the screen and enables novel interaction that reaches beyond the current touchscreens. Using a computer mouse provides two levels of depth when interacting with content on a screen: one can either just hover or click. Hovering allows receiving short descriptions, while clicking selects or performs an action. PreCursor provides this missing sense of interaction to touchscreens. PreCursor technology has the potential to expand beyond a basic computer screen. It can also be applied to mobile touchscreens, to objects in the real world, or can be the launching pad for creating a 3D space for interaction.</p>", "people": ["pranav@media.mit.edu", "pattie@media.mit.edu"], "title": "PreCursor", "modified": "2016-12-05T00:16:45.535Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "precursor"}, {"website": "", "description": "<p>Media Cloud is a system that facilitates massive content analysis of news on the Web. Developed by the Berkman Center for Internet and Society at Harvard University, Media Cloud already analyzes content in English and Russian. During the last months, we have been working on support for Portuguese content. We intend to analyze the online debate on the most controversial and politically hot topics of the Brazilian Civil Rights Framework for the Internet, namely network neutrality and copyright reform. At the same time, we are writing a step-by-step guide to Media Cloud localization. In the near future, we will be able to compare different media ecosystems around the world.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "Media Cloud Brazil", "modified": "2016-12-05T00:16:45.553Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "media-cloud-brazil"}, {"website": "", "description": "<p>Young children learn language not through listening alone, but through active communication with a social actor. Cultural immersion and context are also key in long-term language development. We are developing robotic conversational partners and hybrid physical/digital environments for language learning. For example, the robot Sophie helped young children learn French through a food-sharing game. The game was situated on a digital tablet embedded in a cafe table. Sophie modeled how to order food and as the child practiced the new vocabulary, the food was delivered via digital assets onto the table's surface. A teacher or parent can observe and shape the interaction remotely via a digital tablet interface to adjust the robot's conversation and behavior to support the learner. More recently, we have been examining how social nonverbal behaviors impact children's perceptions of the robot as an informant and social companion.</p>", "people": ["cynthiab@media.mit.edu", "jakory@media.mit.edu", "sooyeon6@media.mit.edu"], "title": "Robotic Language Learning Companions", "modified": "2017-05-31T18:10:02.374Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "robotic-language-learning-companions"}, {"website": "", "description": "<p>People use cues for determining appropriate moments for interruption. If somebody just finished talking on the telephone, that action might be a good indication to knock at their door. What cue detail level is necessary for people to make appropriate interruption decisions? We explored if humans are capable of inferring interruption times with an experiment that presented a series of screenshots from people navigating Websites. Subjects were asked to select three possible interruption times based on mouse activity. Mouse movement trajectories can indicate how information is interpreted while browsing a Webpage. Slow and arched trajectories as users move their mouse would indicate an ambiguous state of mind.</p>", "people": [], "title": "Predicting Web Interruptibility", "modified": "2016-12-05T00:16:45.640Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "predicting-web-interruptibility"}, {"website": "", "description": "<p>We are developing tabletop and desktop printers capable of directly printing both structure and function in two and three dimensions. To date, we have printed actuators, logic, and display media. Such printers may be useful as personal fabricators, giving great design freedom and fabrication capability to the individual, as well as a means of ultra-cheap manufacture of oligo-layered electromechanical and electronic structures.</p>", "people": ["jacobson@media.mit.edu", "neilg@media.mit.edu"], "title": "Printed Electro-Mechanical Systems (PEMS)", "modified": "2016-12-05T00:16:45.608Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "--Choose Location", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "printed-electro-mechanical-systems-pems"}, {"website": "", "description": "<p>Presentations in virtual environments tend to suffer from a number of common problems: presenters are easily overwhelmed by unfamiliar audience activity, communication among audience members is often difficult and distracting to the presenter, and managing what you're looking at is difficult for both presenters and audience members.  In this project, we designed a set of standalone components for Sun Microsystem's Project Wonderland virtual environment that can be combined into a system that addresses these core issues with presentations by taking advantage of the spatial properties of virtual worlds and creating presentation experiences that are more than recreations of face to face experiences. This work was done at Sun Labs in collaboration with the Project Wonderland research group.</p>", "people": ["geek@media.mit.edu"], "title": "Presentation Spaces", "modified": "2016-12-05T00:16:45.666Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "presentation-spaces"}, {"website": "", "description": "<p>We are developing materials and methods for the printing of high-quality, inorganic semiconductor devices. By exploiting the reduced melting point and high solubility of nanoparticles, we have demonstrated that inorganic materials can be processed at plastic-compatible temperatures. Coupled with our efforts in sub-micron printing, microelectronic devices produced from nanoparticle-based inks promise to enjoy the processing advantages usually associated with organic materials while retaining the performance advantages typically associated with inorganic materials.</p>", "people": ["jacobson@media.mit.edu", "hamad@media.mit.edu"], "title": "Printed Electronics", "modified": "2016-12-05T00:16:45.693Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-015", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "printed-electronics"}, {"website": "", "description": "<p>TinkRBook is a storytelling system that introduces a new concept of reading, called textual tinkerability. Textual tinkerability uses storytelling gestures to expose the text-concept relationships within a scene. Tinkerability prompts readers to become more physically active and expressive as they explore concepts in reading together. TinkRBooks are interactive storybooks that prompt interactivity in a subtle way, enhancing communication between parents and children during shared picture-book reading. TinkRBooks encourage positive reading behaviors in emergent literacy: parents act out the story to control the words onscreen, demonstrating print referencing and dialogic questioning techniques. Young children actively explore the abstract relationship between printed words and their meanings, even before this relationship is properly understood. By making story elements alterable within a narrative, readers can learn to read by playing with how word choices impact the storytelling experience. Recently, this research has been applied in developing countries.</p>", "people": ["cynthiab@media.mit.edu", "dnunez@media.mit.edu"], "title": "TinkRBook: Reinventing the Reading Primer", "modified": "2017-05-31T18:21:26.258Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-468", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2014-12-31", "slug": "tinkrbook-reinventing-the-reading-primer"}, {"website": "", "description": "<p>Microencapsulated electrophoretic displays offer low power, flexibility, high contrast, bistability, and flexibility for a wide range of applications including electronic books, signs, and portable handheld devices.</p>", "people": ["jacobson@media.mit.edu", "hamad@media.mit.edu"], "title": "Printed Displays", "modified": "2016-12-05T00:16:45.730Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-015", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "printed-displays"}, {"website": "", "description": "<p>This work presents an architecture for developing Web-based design tools that  automatically generate searchable tutorials for every document created. The goal of these tools is to integrate Web-based tutorial annotation, search, and viewing software within the creative tools themselves. By making the process of creating  tutorial material implicit, a large, searchable, and application-specific knowledge bank would be generated on the Web. This knowledge bank, when searched from within the creative tool itself, allows quick access to a wide array of commands,  gestures, and annotations. The work includes an analysis and evaluation of current tutorial techniques, and a synthesis of design principles for integrating an evolving, collaborative knowledge base of tutorials into digital art  and design tools.   </p>", "people": [], "title": "Process Makes Perfect", "modified": "2016-12-05T00:16:45.817Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "process-makes-perfect"}, {"website": "", "description": "<p>We are developing tomographic techniques for image synthesis on displays composed of compact volumes of light-attenuating material. Such volumetric attenuators recreate a 4D light field or high-contrast 2D image when illuminated by a uniform backlight. Since arbitrary views may be inconsistent with any single attenuator, iterative tomographic reconstruction minimizes the difference between the emitted and target light fields, subject to physical constraints on attenuation. For 3D displays, spatial resolution, depth of field, and brightness are increased, compared to parallax barriers. We conclude by demonstrating the benefits and limitations of attenuation-based light field displays using an inexpensive fabrication method: separating multiple printed transparencies with acrylic sheets.</p>", "people": ["raskar@media.mit.edu", "gordonw@media.mit.edu", "naik@media.mit.edu"], "title": "Layered 3D: Glasses-Free 3D Printing", "modified": "2016-12-05T00:16:45.881Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2015-09-15", "slug": "layered-3d-glasses-free-3d-printing"}, {"website": "", "description": "<p>We propose a flexible light-field camera architecture that represents a convergence of optics, sensor electronics, and applied mathematics. Through the co-design of a sensor that comprises tailored, angle-sensitive pixels and advanced reconstruction algorithms, we show that, contrary to light-field cameras today, our system can use the same measurements captured in a single sensor image to recover either a high-resolution 2D image, a low-resolution 4D light field using fast, linear processing, or a high-resolution light field using sparsity-constrained optimization.</p>", "people": ["raskar@media.mit.edu", "gordonw@media.mit.edu", "naik@media.mit.edu"], "title": "A Switchable Light-Field Camera", "modified": "2016-12-05T00:16:46.070Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2015-09-15", "slug": "a-switchable-light-field-camera"}, {"website": "", "description": "<p>As part of the Toys to Think With project, we built several strings of Programmable Beads: computational jewelry that children (of all ages) can play with and learn from. Each bead is a tiny computer, able to talk to its neighbors and to change color. The Programmable Beads were an attempt to extend classic bead play activity to include exploration of dynamic patterns  and we hoped that Programmable Beads, like the non-programmable kind, would be particularly attractive to girls, as girls as a group have usually been under-served by developers of technology.   \n</p>", "people": ["borovoy@media.mit.edu", "mres@media.mit.edu"], "title": "Programmable Beads", "modified": "2016-12-05T00:16:46.092Z", "visibility": "PUBLIC", "start_on": "1995-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "1999-09-01", "slug": "programmable-beads"}, {"website": "", "description": "<p>Project Voyager uses wirelessly networked devices to create Personal Shopping Assistants that deliver compelling Web services to customers in a supermarket.  Our research explores how to associate \"virtual\" services, such as personalized discounts, a recipe finder, a product recommendation agent, and mobile point of sales with products in a store. As such, Project Voyager aspires to build an \"Internet Presence\" for people, places, and things, thus building a bridge between the virtual and physical worlds we inhabit.</p>", "people": ["mike@media.mit.edu"], "title": "Project Voyager", "modified": "2016-12-05T00:16:45.981Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-468", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "project-voyager"}, {"website": "", "description": "<p>Promiserver provides toolkits and services for online micro-contracts that are socially constructed, community enforced, and procedurally executed. The project aims to offer a sensible, lightweight, and agile promise system as an alternative to traditionally heavy legal commitments, and to facilitate new models of collaborative business by reducing transaction costs and improving market fluidity.</p>", "people": ["holtzman@media.mit.edu"], "title": "Promiserver", "modified": "2016-12-05T00:16:46.133Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "promiserver"}, {"website": "", "description": "<p>This project creates an environment that allows young children access to programming. They can program a robot and see its movements but are also presented with a more abstract, graphical representation of the physical movement. In addition to the graphical image, Logo code that enables the movement is generated. Having the three versions of the same program allows children to transition from a concrete method of programming a robot to a symbolic programming language. The children can program with any of the methods and experiment with the relationships between physical and graphical representations.</p>", "people": ["cavallo@media.mit.edu"], "title": "Programming Continuum", "modified": "2016-12-05T00:16:46.156Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["future-of-learning-2"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "programming-continuum"}, {"website": "", "description": "<p>Drivers spend a significant amount of time multi-tasking while they are behind the wheel. These dangerous behaviors, particularly texting while driving, can lead to distractions and ultimately to accidents. Many in-car interfaces designed to address this issue still neither take a proactive role to assist the driver nor leverage aspects of the driver's daily life to make the driving experience more seamless. In collaboration with Volkswagen/Audi and the SENSEable City Lab, we are developing AIDA (Affective Intelligent Driving Agent), a robotic driver-vehicle interface that acts as a sociable partner. AIDA elicits facial expressions and strong non-verbal cues for engaging social interaction with the driver. AIDA also leverages the driver's mobile device as its face, which promotes safety, offers proactive driver support, and fosters deeper personalization to the driver. </p>", "people": ["cynthiab@media.mit.edu"], "title": "AIDA: Affective Intelligent Driving Agent", "modified": "2017-05-31T18:24:21.710Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-468", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "aida-affective-intelligent-driving-agent"}, {"website": "", "description": "<p>We have trouble controlling our consumer impulses, and there's a gap between our decisions and the consequences. When we pull a product off the shelf, do we know our bank-account balance, or whether we're over budget for the month? Our existing senses are inadequate to warn us. The Proverbial Wallet fosters a financial sense at the point of purchase by embodying our electronically tracked assets. We provide tactile feedback reflecting account balances, spending goals, and transactions as a visceral aid to responsible decision-making.</p>", "people": ["daniell@media.mit.edu", "jkestner@media.mit.edu", "holtzman@media.mit.edu"], "title": "Proverbial Wallets", "modified": "2016-12-05T00:16:46.178Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "proverbial-wallets"}, {"website": "", "description": "<p>Money is a wonderful invention. It lets people specialize, save, and trade. At the same time, money is very abstract, making it very hard to think about the \"shadow value\" of money (all other possible uses for money). In this project, we are investigating the ways in which individuals think about money, and in particular about the ways in which the psychology of money deviates from standard utility.</p>", "people": [], "title": "Psychology of Money", "modified": "2016-12-05T00:16:46.262Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-314", "groups": ["erationality"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "psychology-of-money"}, {"website": "", "description": "<p>Would you know if a dear, but seldom seen, friend happened to be on the same train as you? The Proximeter is both an agent that tracks the past and future proximity of one\ufffds social cloud, and an instrument that charts this in an ambient display. By reading existing calendar and social-network feeds of others, and abstracting these into a glanceable pattern of paths, we hope to nuture within users a social proprioception and nudge them toward more face-to-face interactions when opportunities arise.</p>", "people": ["jkestner@media.mit.edu", "holtzman@media.mit.edu"], "title": "Proximeter: An Ambient Social Navigation Instrument ", "modified": "2016-12-05T00:16:46.281Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "proximeter-an-ambient-social-navigation-instrument"}, {"website": "", "description": "<p>In a digital, immersive world, e-book readers and flat displays are taking over what was once the realm of books. The reBook project attempts to revitalize traditional book production by adding a digital interface fabrication step to the conventional print and bind process. A reBook is a fusion of a 'real book' and an e-book; the cover is embedded with a memory module, wireless microprocessor, paper-based keypad, and flexible display. The reBook's coupling of digital content within a familiar design allows for reinterpretation of classic book functionality. We are exploring how new interactions such as search, bookmarking, physical copy and paste, and annotations could emerge with such a product.</p>", "people": ["pattie@media.mit.edu", "linder@media.mit.edu"], "title": "reBook", "modified": "2016-12-05T00:16:46.327Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "rebook"}, {"website": "", "description": "<p>How can one understand and visualize the lifestyle of a person on the other side of the world? Puzzlaef attempts to tackle this question through a mobile picture puzzle game, which users collaboratively solve with pictures from their lifestyles.</p>", "people": ["geek@media.mit.edu"], "title": "Puzzlaef", "modified": "2016-12-05T00:16:46.433Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "puzzlaef"}, {"website": "", "description": "<p>Qooqle allows people to reshape their interactions with computing and reorganize the world\ufffds information through their casual conversations and habitual gestures. Qooqle combines mobile, cloud, and social media to draw people closer to computing and make computers more invisible. The multi-modal user interface of Qooqle allows people to engage with one another and the information world more naturally.</p>", "people": ["holtzman@media.mit.edu"], "title": "Qooqle", "modified": "2016-12-05T00:16:46.462Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "qooqle"}, {"website": "", "description": "<p>Online games bring geographically separated people together in a virtual world. Board games bring people together in the physical world. QRtcullis is a game that does both. It is a place-based, multi-player, social game that can be played on any smartphone, tablet, or computer. It is a remix of the fort-explorer genre, updated to have a connection to the physical world through QR code-based markers. Play can be affected by real-world environment factors. Each fort can be populated with various objects that either help or hinder other players. Completing the fort may require solving a maze or other puzzles, answering riddles, or finding answers to questions about the real world. The goal is to find and complete as many forts as possible. QR codes will be placed throughout the Media Lab and on a screen in the Viral Spaces area where you can see and interact with all the players.</p>", "people": ["holtzman@media.mit.edu", "lip@media.mit.edu"], "title": "QRtcullis", "modified": "2016-12-05T00:16:46.483Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["viral-communications", "information-ecology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "qrtcullis"}, {"website": "", "description": "<p>When bits meet atoms, new modes of computation, communication, and collaboration arise from the transition to a different world of physics: quantum mechanics. How do we build computers from atoms and molecules? Can we construct a universal, digital abstraction for describing quantum systems? These are the challenges facing our project.</p>", "people": ["neilg@media.mit.edu", "ike@media.mit.edu"], "title": "quanta", "modified": "2016-12-05T00:16:46.500Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "--Choose Location", "groups": ["quanta"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "quanta"}, {"website": "", "description": "<p>QUANTIFY is a generalized framework and JavaScript library to allow rapid multi-dimensional \"measurement\" of subjective qualities of media. The goal is to make qualitative metrics quantized. For everything from measuring emotional responses of content to the cultural importance of world landmarks, QUANTIFY helps to elicit the raw human subjectivity that fills much of our lives, and makes it programmatically actionable. </p>", "people": ["hidalgo@media.mit.edu", "lip@media.mit.edu", "kzh@media.mit.edu", "trich@media.mit.edu"], "title": "QUANTIFY", "modified": "2016-12-05T00:16:46.569Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "quantify"}, {"website": "", "description": "<p>What new computation, communication, and cryptography capabilities are enabled by use of quantum physics?  We have shown that absolutely secure digital signatures can be created using quantum states; stored programs can also be encoded in quantum states, such that they can only be executed once (quantum states collapse after measurement). Currently, we are working on control techniques for quantum systems, and new quantum algorithms based on symmetry transforms.</p>", "people": ["ike@media.mit.edu"], "title": "Quantum Information Science", "modified": "2016-12-05T00:16:46.589Z", "visibility": "PUBLIC", "start_on": "2002-09-01", "location": "E15-427", "groups": ["quanta"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "quantum-information-science"}, {"website": "", "description": "<p>Inspired by the Danish fairy tale \"The Emperor's New Clothes\" and Lady Gaga\ufffds Orbit dress, we have designed and implemented a costume, The Queen\ufffds New Clothes, which appears plain to the naked eye but exhibits changing patterns on photos taken at different times and locations. The process of making this costume has taken us on a journey of exploring the digital aspect and dual status of fashion, fashion as a dynamically changing and embodied visual communication tool, and the relationship between the fashion trendsetter and the audience.</p>", "people": ["holtzman@media.mit.edu"], "title": "Queen's New Clothes", "modified": "2016-12-05T00:16:46.522Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "queens-new-clothes"}, {"website": "", "description": "<p>Sonic Artifacts is a physical music project that affords artists a mode to interact with their fans in a new way. Alongside traditional audio, the devices will provide hardware actuation in sync with the music, as well as sensor input so that listeners can play along with the composition. For example, 90 percent of a song would be pre-recorded, but the artist would create software synthesizers on the device that will take user input to change certain aspects of the music in real time. So each time the listener engages with the artifact, the song will be different. </p><p>The research will culminate in a platform for music artists to create interactive physical instances of their work. Spotify and similar platforms are great for consumption of music, but there is something special about having a physical representation of the music. Sonic Artifacts will push the medium of recorded music further, allowing for intentional (artists' design parameter space to be explored) interaction with their listeners.</p><p>My lens is that of a touring musician. Musicians need things to sell at their merchandise table. CD sales are on the decline in the US, while vinyl is increasing. Either way I believe there is a demand for physical representations of music and also room to innovate away from static recorded music towards a platform that allows for custom aesthetically tuned (to the music itself) instances is of vital importance. Eventually a method to broadcast and share individual instances will be implemented.</p>", "people": [], "title": "Sonic Artifacts", "modified": "2016-12-05T00:16:46.546Z", "visibility": "LAB", "start_on": "2016-09-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2017-06-01", "slug": "sonic-artifacts"}, {"website": "", "description": "<p>We are flooded with information every instant; new email alerts, latest twitter updates, weather alerts, sports and the list goes on. While these alerts may intrude activities we're immersed in, there is still a desire to stay in touch with this ongoing noise. Radio Messenger is a new audio interface designed to deliver ambient and social information in a way to minimize the amount of time a user gets distracted from his primary task. This is achieved by interleaving this information with music listened to by the user in manner similar to a traditional radio. This will enable the user to listen and assimilate the alerts while tending to other tasks.  </p>", "people": ["geek@media.mit.edu"], "title": "Radio Messenger", "modified": "2016-12-05T00:16:46.624Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "radio-messenger"}, {"website": "", "description": "<p>QuitoLab will incorporate both architectural and CityScope LEGO models of the historic core of Quito to engage local and visiting communities in experiencing and understanding the city in creative, multisensory ways. The goal of the QuitoLab project is to use multiscalar models as educational and community-building tools to present a multidimensional image of the city, its history, and its potential for future development. Quito will be one of the first case studies for CityScope, Changing Places\ufffd platform for participatory urban design using LEGOs. CityScope uses 3D mapping technology to project urban data onto reconfigurable LEGO models. It creates a tangible, interactive platform that allows expert and non-expert stakeholders to understand and make informed decisions about the interaction of architecture, space use, mobility modes, energy and water networks, urban food production, movement of goods, data flows, and other urban systems.</p>", "people": ["ramiroal@media.mit.edu", "kll@media.mit.edu"], "title": "QuitoLab", "modified": "2016-12-05T00:16:46.641Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "quitolab"}, {"website": "", "description": "<p>What can you learn from listening to the radio all day? We are working on a music and language acquisition system that constantly monitors five genres of radio and extracts information about the incoming artists from the Web. The language data is fused with the music data to create machine representations of music description, which is then bootstrapped into lexical relations. The machine listener concurrently extracts musical structure from the audio with a view towards synthesis and intelligent retrieval.</p>", "people": ["bv@media.mit.edu"], "title": "Radio, Radio", "modified": "2016-12-05T00:16:46.677Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-491", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "radio-radio"}, {"website": "", "description": "<p>How many decisions does it take before you hear a desired piece of music on your iPod? First, you are asked to pick a genre, then an artist, then an album, and finally a song. The more songs you own, the tougher the choices are. To resolve these issues, we have turned the modern music player into an old analog radio tuner, the Radio-ish Media Player. No LCDs, no favorite channels, just a knob that will help you surf through channel after channel accompanied by synthesized noise. Radio-ish is our attempt to revive the lost art of channel surfing in the old analog radio tuner. Let music find you: your ears will tell you if the music is right. This project is not only a retrospective design, but also our reflection on lost simplicity in the process of digitalization. A mobile phone version is also available for demo.</p>", "people": ["geek@media.mit.edu"], "title": "Radio-ish Media Player", "modified": "2016-12-05T00:16:46.694Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-310", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "radio-ish-media-player"}, {"website": "", "description": "<p>RadioActive is a mobile-based, asynchronous, audio-messaging platform. It features one-to-many discussions (think Usenet/Newsgroups) using speech instead of text. There are many design challenges in building a large-scale asynchronous audio conversation space.  How do you browse and navigate through the potentially large chat space? How do you skim messages? Leave responses? We are using a novel visual representation and combined interaction technique to deal with such issues.</p>", "people": ["judith@media.mit.edu"], "title": "RadioActive", "modified": "2016-12-05T00:16:46.712Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-390", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "radioactive"}, {"website": "", "description": "<p>The Rainwater Wall augments and demonstrates the natural process of rainwater collection and dispersal. It is a freestanding wall that holds water storage bubbles and planters. Rainwater is collected on the roof, distributed by tubing to storage bubbles, and released to plants or returned to the earth. The wall constantly changes due to water retention and plant growth. The bubble wall is a manifestation of the temporal and physical processes of the exterior environment in the form of an interior micro-environment.</p>", "people": [], "title": "Rainwater Wall", "modified": "2016-12-05T00:16:46.739Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "Upper Atrium", "groups": [], "published": true, "active": false, "end_on": "2009-01-01", "slug": "rainwater-wall"}, {"website": "", "description": "<p>Can robots learn to be social? Can they do that in a structured way? This project uses the DragonBot platform and state-of-the-art artificial curiosity algorithms to explore the possibility of robots learning to behave socially, similar to children. The robot reacts to people and receives internal rewards whenever the social interaction succeeds. Initially, the robot learns which behavior best initiates social interaction and later learns which behavior maintains that interaction for the longest period. The goal is to build a brain-inspired hierarchical curiosity-driven social behavior architecture, in which the robot autonomously learns a growing repertoire of social skills.</p>", "people": ["cynthiab@media.mit.edu", "ggordon@media.mit.edu"], "title": "Curious Social Robot", "modified": "2017-05-31T18:36:24.278Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2015-08-31", "slug": "curious-social-robot"}, {"website": "", "description": "<p>\"At the frontier, our liberty is stripped away...we enter the universe of control....We submit to scrutiny, to inspection, to judgment. We must be passive, docile. To be otherwise is to be suspect.\" (Salman Rushdie, Step Across This Line) Random Search is a subtle, reactive undergarment. It logs, shares, and analyzes the experience of invasive airport searches on behalf of our silent, abiding, fearful bodies.</p>", "people": ["csik@media.mit.edu"], "title": "Random Search", "modified": "2016-12-05T00:16:46.785Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "random-search"}, {"website": "", "description": "<p>Rapidnition is a new way of thinking about gesturally controlled interfaces.  Rather than forcing users to adapt their behavior to a predefined gestural interface, Rapidnition frees users to define their own gestures, which the system rapidly learns. The machine learning algorithms at the core of Rapidnition enable it to quickly infer a user\ufffds gestural vocabulary, using a small number of user-demonstrated examples of each gesture. Rapidnition is capable of recognizing not just static postures but also dynamic temporal gestures. In addition, Rapidnition allows the user to define complex, nonlinear, continuous-mapping spaces. Rapidnition is currently being applied to the real-time recognition of musical gestures to rigorously test both the discrete and continuous recognition abilities of the system.</p>", "people": ["joep@media.mit.edu"], "title": "Rapidnition: Rapid User-Customizable Gesture Recognition", "modified": "2016-12-05T00:16:46.877Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "rapidnition-rapid-user-customizable-gesture-recognition"}, {"website": "", "description": "<p>Reach merges inherently local communications with user requests or offers of services. It is built atop data from services users already use, like Facebook and Google Latitude. Reach is intended to demonstrate a flexible, attractive mobile interface that allows users to discover \"interesting\" aspects of the environment and to call upon services as needed. These can range from a broadcast offer to serve as a triage medic, to a way to share a cab or get help for a technical service problem like plugging into a video projector.</p>", "people": ["lip@media.mit.edu"], "title": "Reach", "modified": "2016-12-05T00:16:46.903Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "reach"}, {"website": "", "description": "<p>The mismatch between the supply of graduates\u2019 skills and the needs of the labor market has become increasingly obvious and problematic over the past years. This has prompt policymakers, educators, employers and applicants to reevaluate the role of higher education system. But how do each of these actors perceive higher education? How similar are, according to them, the different degree programs and institutions?</p><p>In this project, we use a data-driven approach to unveil the structure of similarities between degree programs as perceived from the candidates. To that end, we use applicants\u2019 preferences to higher education in Chile and Portugal between the years of 2007 and 2014 as a proxy to measure the similarity between each pair of degree programs. We find that:</p><ol><li>The two structures share the same topological features, despite coming from two different political and social-economical contexts;</li><li>We quantify the mismatch between the current state of the art classification used by educators and policymakers and the structure identify</li><li>We find the existence of strong spatial patterns in the assortment of gender, application scores, demand and unemployment levels; and</li><li>We find that structure of similarities encapsulates non-trivial information about the nature of each degree program, allowing us to predict with high accuracy the level of unemployment by just taking into account the relative position of a degree program in the higher education option set.&nbsp;</li></ol><p>Currently, we are preparing a manuscript to present our findings.&nbsp;</p>", "people": ["flaviopp@media.mit.edu", "ccandiav@media.mit.edu", "hidalgo@media.mit.edu"], "title": "Mapping Higher Education Option Space", "modified": "2017-10-17T18:44:40.535Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2017-12-31", "slug": "mapping-higher-education-network"}, {"website": "", "description": "<p>Current 3-D laser scanning systems require a lengthy interval to complete each scan and require a clear line-of-sight, thereby severing the physical model from its CAD representation, and not allowing most handheld interaction because of occlusion. This project proposes 3-D volumetric sensing using real-time tag tracking that allows the user to manipulate the physical model object while the tags are being scanned. This approach provides a seamless link between the real object and the virtual object, effectively converging the design and modeling processes for artists, architects, and engineers. The tag detection scheme uses magnetically coupled resonators similar to those developed for the Swept RF Tagging project. The current areas of investigation include reducing the dimensions of the tags, improving the signal-to-noise ratio of location measurement of tags in each axis, and increasing the number of trackable tags to form meaningful shapes and contours. Demonstrative content is being developed in collaboration with the MIT School of Architecture and Urban Planning.</p>", "people": ["joep@media.mit.edu"], "title": "Real-Time 3-D Volumetric Sensing", "modified": "2016-12-05T00:16:46.957Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "real-time-3-d-volumetric-sensing"}, {"website": "", "description": "<p>Recap uses Glue to automatically create video casts, allowing users to specify how much time they have available and then intelligently filling that chunk of time with the top trending stories of the day.</p>", "people": ["lip@media.mit.edu", "dahlseng@media.mit.edu"], "title": "Recap", "modified": "2016-12-05T00:16:47.091Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "recap"}, {"website": "", "description": "<p>Recast is a media curation and distribution platform that enables anyone to create and distribute \"news programs\" that represent their views of the world from their own perspective. Recast provides a visual scripting interface similar to Scratch, where users can combine a series of logical blocks to query specific scene elements that present their views by drawing from arbitrary video contents, and constructing a story sequence. Recast uses the Constellation system as a backend for querying video content, and uses the Media Matrix as a content distribution platform.</p>", "people": ["lip@media.mit.edu"], "title": "Recast", "modified": "2016-12-05T00:16:47.119Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "recast"}, {"website": "", "description": "<p>Human beings have long shaped the physical environment to reflect designs of form and function. As an instrument of control, the human hand remains the most fundamental interface for affecting the material world. In the wake of the digital revolution, this is changing, bringing us to reexamine tangible interfaces. What if we could now dynamically reshape, redesign, and restructure our environment using the functional nature of digital tools? To address this, we present Recompose, a framework allowing direct and gestural manipulation of our physical environment. Recompose complements the highly precise, yet concentrated affordance of direct manipulation with a set of gestures, allowing functional manipulation of an actuated surface.</p>", "people": ["dlakatos@media.mit.edu", "ishii@media.mit.edu"], "title": "Recompose", "modified": "2016-12-05T00:16:47.152Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["viral-communications", "tangible-media"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "recompose"}, {"website": "", "description": "<p>Red Ink is an open-source web service for ad hoc groups to share and interpret their collective financial data. It includes interfaces for defining transactions upon which to aggregate, methods of visualization, and public and private web publishing.</p>", "people": ["csik@media.mit.edu"], "title": "Red Ink", "modified": "2016-12-05T00:16:47.241Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-001", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "red-ink"}, {"website": "", "description": "<p>Urban educational systems face myriad challenges. In developing countries, these challenges are accentuated by lack of resources and diminishing public confidence. Approaches to schools, informal learning centers, and community development need to change, but we lack concrete models and ideas. This project engages with several cities around the world to re-think, re-invent, and re-invigorate the learning environment for all children. We are creating centers of innovation for children, challenging mindsets about what education must be, and providing concrete examples of new learning. We are presently working in S\ufffdo Paulo, Curitiba, Campinas, Manaus, and Salvador (Brazil), where children are designing solutions for the problems of their cities and communities.</p>", "people": ["cavallo@media.mit.edu", "papert@media.mit.edu"], "title": "Redesigning Urban Learning Environments", "modified": "2016-12-05T00:16:47.297Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["future-of-learning-2"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "redesigning-urban-learning-environments"}, {"website": "", "description": "<p>The Rehab Sound Sculptor is a motion-tracking interface that allows the researcher to quickly map a subject's movement to any aspect of sound control. The motivation for developing this interface is that it allows us to determine which attributes of sound are potentially more useful than others for providing feedback in motor rehabilitation. There has been a significant body of work, particularly with Parkinson's disease, that has shown the rehabilitative benefit of auditory feedback during physical therapy. However, little work has been done to determine exactly which features of sound are responsible for this benefit. With the Rehab Sound Sculptor interface, we can investigate the relative contributions of specific sound attributes to rehabilitation.</p>", "people": ["tod@media.mit.edu"], "title": "Rehab Sound Sculptor", "modified": "2016-12-05T00:16:47.450Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "rehab-sound-sculptor"}, {"website": "", "description": "<p>With the Relational Pillow project, we are trying to provide a simple, intimate, and personable communication medium between loved ones. The pillows are capable of sensing touch information, and displaying incoming touch data as a pattern of lights that show the outline \"drawn\" upon the remote pillow. Pillows can connect to each other over the network so that this sense of touch can be shared across long distances. The physical sensation of holding a pillow and interacting with it builds upon the idea of using the natural features of the object in order to acheive a deeper connection between the users, without interfering in the communication process itself.</p>", "people": ["sajid@media.mit.edu", "pattie@media.mit.edu"], "title": "Relational Pillow", "modified": "2016-12-05T00:16:47.476Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "relational-pillow"}, {"website": "", "description": "<p>Relief is an actuated tabletop display, able to render and animate 3D shapes with a malleable surface. It allows users to experience and form digital models such as geographical terrain in an intuitive manner. The tabletop surface is actuated by an array of motorized pins, which can be addressed individually and sense user input such as pulling and pushing. Our current research focuses on utilizing freehand gestures for interacting with content on Relief.</p>", "people": ["daniell@media.mit.edu", "ishii@media.mit.edu"], "title": "Relief", "modified": "2016-12-05T00:16:47.517Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "relief"}, {"website": "", "description": "<p>Remembrance Agents are a new class of applications that watch over a user's shoulder and suggest information relevant to the current situation. While query-based memory aids help with direct recall, remembrance agents are an augmented associative memory. For example, a database or search engine can answer direct questions like \"how do I get to Central Square?\" Remembrance Agents instead offer the kind of information you didn't even know enough to ask about, like \"there's a great coffee-shop just around the corner.\"  \n</p>", "people": ["pattie@media.mit.edu"], "title": "Remembrance Agents", "modified": "2016-12-05T00:16:47.542Z", "visibility": "PUBLIC", "start_on": "1994-12-31", "location": "E15-305A", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "remembrance-agents"}, {"website": "", "description": "<p>Relational Agents are computational artifacts designed to build and maintain long-term, social-emotional relationships with their users. Central to the notion of relationship is that it is a persistent construct, spanning multiple interactions. Thus, Relational Agents are explicitly designed to remember past history and manage future expectations in their interactions with users. Since face-to-face conversation is the primary context of relationship-building for humans, our work focuses on Relational Agents as a specialized kind of embodied conversational agent (animated humanoid software agents that use speech, gaze, gesture, intonation, and other nonverbal modalities to emulate the experience of human face-to-face conversation). One major achievement was the development of a Relational Agent for health behavior change, specifically in the area of exercise adoption. A study involving 100 subjects interacting with this agent over one month demonstrated that the relational agent was respected more, liked more, and trusted more, and that these ratings were maintained over time (unlike for the non-relational agent, where they were not only significantly lower overall, but also declined over time.) People also expressed significantly greater ratings of perceived caring by the agent, and significantly more desire to keep working with the relational agent after the termination of the study.   </p>", "people": ["bickmore@media.mit.edu", "picard@media.mit.edu"], "title": "Relational Agents", "modified": "2016-12-05T00:16:47.580Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "relational-agents"}, {"website": "", "description": "<p>With skin conductance sensors, we map out what frustrates and excites customers as they shop\ufffdfrom layout to wanting to touch the product. Our work has helped a variety of large retailers innovate on what it means to shop. Findings have focused on reducing the stress of choices and learning while surprising customers in new ways. With the sensor technology we can pinpoint moments when customers are overwhelmed and then build out new ways to make retail engaging again.</p>", "people": ["picard@media.mit.edu", "hedman@media.mit.edu"], "title": "Reinventing the Retail Experience ", "modified": "2016-12-05T00:16:47.497Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "reinventing-the-retail-experience"}, {"website": "", "description": "<p>Remix and Robo are two sampler/sequencer controllers that allow children to engage in (1) storytelling, and (2) competitive endeavors with a modular robotic toy. We base this work on Topobo, a constructive assembly system with kinetic memory (the ability to record and play back physical motion). Remix is a tangible interface used to sample, organize, and manipulate several kinetic records. Robo is a modified game controller used to capture robotic motions, adjust global motion parameters, and execute kinetic recordings in real time. Remix and Robo facilitate new kinds of applications that build on children\ufffds varying interests and social motivations. Whereas children typically play with building toys because of the joy of model-making or solving engineering problems, Remix and Robo allow users to cast their creations in performances and competitions. By introducing an expanded social scope, our work motivates learners to focus and reflect upon their understanding of dynamic physics concepts with Topobo. </p>", "people": ["ishii@media.mit.edu"], "title": "Remix and Robo Topobo", "modified": "2016-12-05T00:16:47.632Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "remix-and-robo-topobo"}, {"website": "", "description": "<p>Remnant is a greeting card that merges the affordances of physical materials with the temporal malleability of digital systems to create, enshrine, and reinforce the very thing that makes a greeting personal: the hand of the sender. The card records both the timing and the form of the sender's handwriting when it is first used. At a later time, collocated output recreates the handwriting, allowing the invisible, memorized hand of the sender to write his or her message directly in front of the recipient.</p>", "people": ["sajid@media.mit.edu", "pattie@media.mit.edu"], "title": "Remnant: Handwriting Memory Card", "modified": "2016-12-05T00:16:47.656Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "remnant-handwriting-memory-card"}, {"website": "", "description": "", "people": [], "title": "Maggie Test", "modified": "2018-12-21T20:20:40.280Z", "visibility": "PUBLIC", "start_on": "2018-12-21", "location": "", "groups": ["city-science"], "published": false, "active": false, "end_on": "2018-12-22", "slug": "maggie-test"}, {"website": "", "description": "<p>Re\ufffdplay is a self-documenting construction kit for children, allowing them both to share their designs with others and reflect on their own design process. Re\ufffdplay consists of a set of angular construction pieces that can sense their connection and orientation. A virtual model is rendered in real time as a design is constructed, and an on-screen playback interface allows users to view models from multiple perspectives and watch how a design was assembled.</p>", "people": ["ttseng@media.mit.edu", "mres@media.mit.edu"], "title": "Re\ufffdplay", "modified": "2016-12-05T00:16:47.840Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "replay"}, {"website": "", "description": "<p>This project investigates relationships between and within bodies. Architectural bodies and living bodies communicate in this project through sound, infrasound, and touch.</p>", "people": ["monster@media.mit.edu", "csik@media.mit.edu"], "title": "ReSound", "modified": "2016-12-05T00:16:47.785Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "resound"}, {"website": "", "description": "<p>The experience of being in a crowd is visceral. We feel a sense of connection and belonging through shared experiences like watching a sporting event, speech, or performance. In online environments, though, we are often part of a crowd without feeling it. ROAR is designed to allow very large groups of distributed spectators to have meaningful conversations with strangers or friends while creating a sense of presence of thousands of other spectators. ROAR is also interested in creating opportunities for collective action among spectators and providing flexible ways to share content among very large groups. These systems combine to let you feel the roar of the crowd even if you're alone in your bedroom.</p>", "people": ["geek@media.mit.edu"], "title": "ROAR ", "modified": "2016-12-05T00:16:47.950Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "roar"}, {"website": "", "description": "<p>WordPlay is a mutual tabletop interface for the generating, organizing, and exploring new ideas. Participants add terms to the table by speaking, modify the properties (size, position, rotation) of the terms with their fingers, and find related concepts through a simple set of gestures (stretching, merging, circling).</p>", "people": ["pattie@media.mit.edu"], "title": "WordPlay", "modified": "2016-12-05T00:16:47.982Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-320B", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "wordplay"}, {"website": "", "description": "<p>People's daily lives are impacted by a great deal of dynamic environmental information, such as weather and traffic. Though most of this information is now available on the Internet, there isn't an easy way to access it while mobile. In addition, people do not always need to know this kind of information unless there is a significant change that may impact their current or future activities. Ringing in the Rain proposes a distributed, multi-agent architecture that uses GPS-enabled cell phones to build a mobile service development framework. The goal of this framework is to build mobile services to deliver timely changes in environmental information that could impact a user's current or future activities. A weather warning system is developed based on this framework as a demonstration to inform people to leave or detour in advance to avoid being caught in the rain.</p>", "people": ["geek@media.mit.edu"], "title": "Ringing in the Rain", "modified": "2016-12-05T00:16:48.085Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "ringing-in-the-rain"}, {"website": "", "description": "<p>Robo Topobo is a controller that children can use to save, replay, and adjust playback of up to four Topobo recordings. Robo introduces improvisational performance, typical of video games, to a dynamic construction kit. With Topobo you can bring your creations to life, and with Robo you can direct them in a performance. To use Robo, a child builds a creation and physically programs it by twisting its body around. She can save a motion by pressing \"record\" and then one of four playback buttons. Pressing that same playback button will cause the recording to spontaneously play back. She can also play the motion backwards and two joysticks can change the speed and size of the motion. We are using Robo to create Topobo performances and to do \"battle bots\" style robot design competitions.</p>", "people": ["ishii@media.mit.edu"], "title": "Robo Topobo", "modified": "2016-12-05T00:16:48.109Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "robo-topobo"}, {"website": "", "description": "<p>RoBallet is a research project that brings together the arts, learning, and technology. Children extend the joy and creativity of expression through dance. They use technology to augment their choreography by programming interactive robots, animation, music, light, and images in their dance space. They outfit their bodies and the environment with computational devices and sensors so that their bodies can generate the activity of the space. Central to this vision is the insistence that children should control technology to serve their imaginations, and not be driven by it. The experience results in a deeper understanding of the three themes of RoBallet: arts as expression, technology as a means to serve expression, and learning as \"hard fun\"\ufffdcreative and enjoyable yet disciplined and purposeful.</p>", "people": ["cavallo@media.mit.edu"], "title": "RoBallet", "modified": "2016-12-05T00:16:48.148Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-443", "groups": ["future-of-learning"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "roballet"}, {"website": "", "description": "<p>In order to build machines with common sense we need to find ways to integrate a wide variety of specific ways to represent knowledge, reason with that knowledge, and arrange such resources into a larger architecture. To study this problem we are building a rich, simulated environment in which a pair of simulated robots live and perform various tasks: walking about, manipulating objects, building structures together, and conversing with each other about events in their world.</p>", "people": ["minsky@media.mit.edu"], "title": "Robots with Common Sense", "modified": "2016-12-05T00:16:48.320Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-385", "groups": ["society-of-mind"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "robots-with-common-sense"}, {"website": "", "description": "<p>DragonBot is a new platform built to support long-term interactions between children and robots. The robot runs entirely on an Android cell phone, which displays an animated virtual face. Additionally, the phone provides sensory input (camera and microphone) and fully controls the actuation of the robot (motors and speakers). Most importantly, the phone always has an Internet connection, so a robot can harness cloud-computing paradigms to learn from the collective interactions of multiple robots. To support long-term interactions, DragonBot is a \"blended-reality\" character: if you remove the phone from the robot, a virtual avatar appears on the screen and the user can still interact with the virtual character on the go. Costing less than $1,000, DragonBot was specifically designed to be a low-cost platform that can support longitudinal human-robot interactions \"in the wild.\"</p>", "people": ["cynthiab@media.mit.edu"], "title": "DragonBot: Android Phone Robots for Long-Term HRI", "modified": "2017-05-31T18:40:07.639Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "dragonbot-android-phone-robots-for-long-term-hri"}, {"website": "", "description": "<p>Rope Revolution is a rope-based gaming system for collaborative play. After identifying popular rope games and activities from around the world, we developed a generalized, tangible rope interface that includes a compact motion-sensing and force-feedback module that can be used for a variety of rope-based games, such as rope jumping, kite flying, and horseback riding. Rope Revolution is designed to foster both co-located and remote collaborative experiences by using actual rope to connect players in physical activities across virtual spaces.</p>", "people": ["liningy@media.mit.edu", "ishii@media.mit.edu", "sdg1@media.mit.edu"], "title": "Rope Revolution", "modified": "2016-12-05T00:16:48.372Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "rope-revolution"}, {"website": "", "description": "<p>Strata/ICC is a computationally-augmented, two-meter-tall physical model of a 54-story skyscraper, serving as an interactive display of electricity consumption, water consumption, network utilization, and other kinds of building infrastructure.  Our approach pushes information visualizations into the physical world, with a vision of transforming large-scale physical models into new kinds of interaction workspaces.</p>", "people": ["ishii@media.mit.edu"], "title": "Strata/ICC", "modified": "2016-12-05T00:16:48.345Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-441", "groups": ["tangible-media", "personal-fabrication"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "strataicc"}, {"website": "", "description": "<p>RunLog is a simple Web application that keeps track of how much a user runs. For frequent runners, RunLog can be a useful tool to keep track of running goals. But RunLog was built to motivate people who spend most of their day working in front of a computer. Many people start running to get in shape as a New Year's resolution, and hopefully RunLog will help users maintain goals through the support of an online social network.</p>", "people": [], "title": "RunLog", "modified": "2016-12-05T00:16:48.458Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "runlog"}, {"website": "", "description": "<p>Safe & Sound is an application that enables a parent to define a \"secure zone\" for a child. A secure zone is an area where the parent considers the child to be safe. If the child ventures out of this zone, the parent will be alerted. He can then decide whether to place a call to the child or to log the child's coordinates (latitude, longitude, direction heading, and speed). The child will also receive an audio alert indicating that he is out of the defined boundary.</p>", "people": ["geek@media.mit.edu"], "title": "Safe & Sound", "modified": "2016-12-05T00:16:48.485Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "safe-sound"}, {"website": "", "description": "<p>Learning is rooted in people and culture and bears fruit through the construction process, with shoots that branch into new areas. These principles of learning are experientially based, differing markedly from the concepts that require a disconnected accumulation of chunks of knowledge. In order for learning to become truly rooted, a person must have a deep emotional attachment to the subject area. Rooting and the possibilities for branching flow from a better understanding of affect, comfort, culture, and motivation. We have found that when individuals participate in shared activity, they develop new ideas for themselves (fruits) and the collaborative process transforms and shapes the organization of the community as a whole (shoots). Rather than thinking of learning and expertise as a body of acquired skills and knowledge unique to an individual, we adopt a more \"situated learning\" perspective, where the unit of analysis for expertise is the community itself. We are building technological affordances to rooted knowledge that serve as mentors and that foster the creative and idiosyncratic connections (new shoots) to learning that help community members progress through apprenticeship, guided participation, and participatory appropriation.</p>", "people": ["cavallo@media.mit.edu", "walter@media.mit.edu"], "title": "Roots, Fruits, and Shoots", "modified": "2016-12-05T00:16:48.435Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "Pond", "groups": ["electronic-publishing", "future-of-learning-2", "gray-matters"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "roots-fruits-and-shoots"}, {"website": "", "description": "<p>Rumble is an ambient notification platform to help you learn the locations of invisible urban infrastructures. Some example infrastructures you might want to learn are: subway lines, free WiFi hotspots, public restrooms, and so on. Android-based software monitors your location in the background; when you are near a resource you wanted to know about, a wearable peripheral device vibrates unobtrusively. If you decide you want to learn more about a certain feature, you can press the wearable against your head and hear information via a bone-conduction driver. The work's motivation is to help people optimize their everyday urban experience.</p>", "people": ["tod@media.mit.edu"], "title": "Rumble", "modified": "2016-12-05T00:16:48.547Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "rumble"}, {"website": "", "description": "<p>The Salvage Table is a reuse system that educates participants about electronic items left for recycling. The Salvage Table breaks down complex circuitry into core, reusable pieces and provides information on how to reuse them. While the table is a main feature of the current trial implementation, a core component is crowd-sourcing. Users can visit salvage.media.mit.edu, browse through the items that have been left at the Salvage Table, and add missing meta-information (e.g., components on the boards, ways to use the chips or components). A long-term goal is to influence company policy for manufacturing parts so that modularization and reuse are prioritized. Eventually, companies may even re-harvest parts from older models for use in other products or in research labs. Produce less, salvage more.\n</p>", "people": ["holtzman@media.mit.edu"], "title": "Salvage", "modified": "2016-12-05T00:16:48.571Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "salvage"}, {"website": "", "description": "<p>The U.S. banking crisis and economic downturn has led to many people to seek assistance from social service agencies who'd not previously sought assistance. Same Boat lets people who've discovered helpful social services to share their discoveries with others who are in the same boat. Using the technologies in our What's Up toolkit, Same Boat is currently in the process of co-development in Wisconsin Falls, Wisconsin in partnership with the Community Foundation of South Wood County, Wisconsin. </p>", "people": ["ethanz@media.mit.edu", "leob@media.mit.edu"], "title": "Same Boat", "modified": "2016-12-05T00:16:48.622Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "same-boat"}, {"website": "", "description": "<p>Sanctuary is an educational game to be played in pairs. It addresses topics in high-school biology and mathematics, and encourages players to become collaborative scientists with asymmetric interfaces and tools.</p>", "people": ["jhaas@media.mit.edu", "klopfer@media.mit.edu"], "title": "Sanctuary", "modified": "2016-12-05T00:16:48.651Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "sanctuary"}, {"website": "", "description": "<p>Sand:stone is a piece developed for the 7th Annual New York Digital Salon and subsequent installation in galleries across Europe. The physical installation is a box of sand with some stones in it. On a screen on the far side, there is a statue on a pedestal, reminiscent of Rodin's Thinker. The sound of wind blows by the desolate statue. As the participant moves the stones, a strain of music unfolds. The statue stretches and raises its arms in response to the movement of the stones. As different arrangements of stones are created, the statue takes on different feelings: a young man breathing in a bracing wind, a lonesome traveler bowed down by rain, a farmer bending to embrace the earth, a tortured zealot being burned at the stake. Through this installation we hope to investigate how character may be conveyed through music, light, and camera.\n</p>", "people": [], "title": "sand:stone", "modified": "2016-12-05T00:16:48.675Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "sandstone"}, {"website": "", "description": "<p>We provide a software and hardware toolkit for creating an on-body network of tactile and ambient information accessories, connecting people physically with information accessed via the Internet. My Ears Are Burning uses the toolkit to make a user aware of attention being paid to her online presence. Heating elements placed on the user's ears are activated when, for example, her Web page is accessed or she is tagged in a photograph on Facebook. The toolkit hardware consists of a Bluetooth module outfitted with simple-to-use I/O pins for connecting input sensors and output actuators. The software component resides on a cell phone that acts as a router between the Bluetooth modules and the Internet. This platform is also used for the Proverbial Wallets project.</p>", "people": ["jkestner@media.mit.edu", "holtzman@media.mit.edu"], "title": "My Ears Are Burning", "modified": "2016-12-05T00:16:48.727Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "my-ears-are-burning"}, {"website": "", "description": "<p>This project is a novel concept for a social TV application, targeting the demographic of viewers enjoying live sports events, such as road bicycle racing. We intend to enhance the viewing experiences of spectators with sensor-fitted bikes tied to an interactive biking environment on television. The system enables a new form of personalized, physical, and virtual-reality interaction between viewers and a TV program, as well as interactions within or among communities of friends. We have created a prototype, My Second Bike, which uses a 3-D mirrored world environment (Google Earth) to visually represent participating spectators, competing athletes, and outdoor bikers. We contend that the system has the potential to attract and support a large user base on account of its scalability, ease of deployment, and ability to promote audience participation in live sports events on TV.</p>", "people": ["geek@media.mit.edu"], "title": "My Second-Bike", "modified": "2016-12-05T00:16:48.753Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "my-second-bike"}, {"website": "", "description": "<p>Complicated models that can only be understood by experts block the communication process when decision makers have to choose between the different policies to be implemented.  SARS-TA is a new methodology for explaining and communicating the consequences of the various alternatives that governments will face in the battle against SARS. It is designed to provide both understanding and motivation to those in government who must make difficult decisions in a timely fashion by demonstrating the likely consequences of a lack of preparedness or delays in the implementation of specific recommendations.</p>", "people": [], "title": "SARS-TA", "modified": "2016-12-05T00:16:48.778Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-320G", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "sars-ta"}, {"website": "", "description": "<p>With the Scratch Board, people can use sensors to control interactive stories and games that they create with the Scratch programming language. By connecting the physical and virtual, the Scratch Board extends the range of what people can design -- and extends what they learn in the process. The Scratch Board comes with several built-in sensors: a light sensor, sound sensor, touch sensor, and slider. It also has four ports where you can plug in your own resistance-based sensors. For example, you can create a Scratch program that controls music and animation on the computer based on your interactions with sensors connected to the Scratch Board.</p>", "people": ["millner@media.mit.edu", "jmaloney@media.mit.edu", "mres@media.mit.edu", "bss@media.mit.edu"], "title": "Scratch Board", "modified": "2016-12-05T00:16:48.808Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "scratch-board"}, {"website": "", "description": "<p>Computer-assisted crafting is one type of software that can prove relevant for non-industrial crafting communities, such as Santa Clara del Cobre in Mexico. We developed artisanal pattern CAD software, based on the Adolfo Best Maugard drawing technique (Best Maugard, 1923, 1927), to teach Mexican-style drawing to children. Developed with sculptors James Metcalf and Ana Pellicier and the artisans of Santa Clara, this software enabled users to create primitive  graphic motifs and combine them into patterns. This environment provided alternate ways of exploring variations in the creation of motifs and their arrangement into patterns. The software was developed using a vector drawing program, Sketch, as a platform. It was designed not to replace traditional ways of crafting but instead to offer new ways to explore the creation and combination of traditional patterns.</p>", "people": ["mres@media.mit.edu"], "title": "Craft Computing", "modified": "2016-12-05T00:16:48.837Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "craft-computing"}, {"website": "", "description": "<p>Micro-Macro Fluidic Fabrication (MMFF) enables the control of mechanical properties through the design of non-linear lattices embedded within multi-material matrices. At its core it is a hybrid technique that integrates molding, casting, and macro-fluidics. Its workflow allows for the fabrication of complex matrices with geometrical channels injected with polymers of different pre-set mechanical combinations. This novel fabrication technique is implemented in the design and fabrication of a midsole running shoe. The goal is to passively tune material stiffness across surface area in order to absorb the impact force of the user's body weight relative to the ground, and enhance the direction of the foot-strike impulse force relative to the center of body mass.</p>", "people": ["neri@media.mit.edu"], "title": "Micro-Macro Fluidic Fabrication of a Mid-Sole Running Shoe", "modified": "2016-12-05T00:16:48.864Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2014-12-12", "slug": "micro-macro-fluidic-fabrication-of-a-mid-sole-running-shoe"}, {"website": "", "description": "<p>Audio recordings are an archive of a performance.  As an archive, they cannot be changed; they embody certain choices on the part of the performers and the producers. Thus, a recording only allows certain interpretations to be heard at a later date, even if the source material itself is open to a plethora of expositions.  We are developing tools that allow a single recording to encapsulate multiple interpretations: a new phrase here, and entirely new performance there.  These new mutable recordings will give performers the chance to present a variety of interpretations to the listener, while allowing the listener to have new experiences each time she listens to the piece.</p>", "people": ["tod@media.mit.edu"], "title": "Mutable Recordings", "modified": "2016-12-05T00:16:48.888Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "mutable-recordings"}, {"website": "", "description": "<p>Multicasting is one way to converge the delivery of broadcast video with the Internet. Last year, we built a protocol that used every receiver as a shared memory to allow personal music from an arbitrarily large set of channels. This year, we are adding scalable video to the multicast system to allow graceful degradation and variable resolution viewing. With SMD, on-demand delivery and time-shifting become not only feasible, but practical.</p>", "people": ["lip@media.mit.edu"], "title": "Scalable Multicasting", "modified": "2016-12-05T00:16:48.919Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-344", "groups": ["media-and-networks"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "scalable-multicasting"}, {"website": "", "description": "<p>To adapt this vision for video to the rapidly evolving Internet, we are developing an architecture for the Shareable Media project which is scaleable and which will allow a range of users to augment and to play with the shared media objects. This architecture provides standardized APIs, which invite users to create their own interface applications for wired and wireless devices using both broadband and narrowband connections. This builds on I-View, a system developed by Pengkai Pan.</p>", "people": ["gid@media.mit.edu"], "title": "Scalable Architecture for Shareable Media", "modified": "2016-12-05T00:16:48.941Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-351", "groups": ["interactive-cinema", "gray-matters"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "scalable-architecture-for-shareable-media"}, {"website": "", "description": "<p>A technology that is thousands of years old, knitting is imbued with tradition, myth, and storytelling\u2014to knit is to embody the work of our ancestors. In a hand knitted piece, each loop passes through needles and fingers, leaving small clues about its maker. Depending on our mood and the day, we might knit relaxed loops, tense knots, or daydream and slip a stitch. Unlike machines, we humans have unique quirks and tendencies, and this extends to our knitting. With the proliferation of factory mass production,&nbsp;knits have become standardized, with anonymous stitches that lack any mark of a sentient maker.</p><p>During this project, I spent a month working in a factory in China to learn about manufacturing and labor. Between rows of hundreds of identical machines, there are still many humans scurrying around to fix jammed needles, trim loose threads, stack and transport finished pieces. I was struck by how these workers live in service of machines, their labor and sweat as ghosts within the identical and perfect end products. This resulting project attempts to reintegrate emotional mark of the worker back into the process of mass production, to begin telling a story of each individual in a landscape of anonymous labor.</p><p>Automation has many functions and plays a critical role in our technological advancement, but is it possible to embed automation with gestures as intimate as the mark of a brushstroke in a painting? Taking the cognitive signatures of a human via their EEG signals, I translated these brain activities back into the knitting process. Depending on the EEG signal, the tensility of the weaving was programmed to vary, reflecting the mark of the worker's \u201ccognitive\u201d hand. Depending on the mental stress levels over a day of work, the tensility varies and the fabric ripples in empathy. &nbsp;</p><p>This project stitches a portrait of the factory worker, through their fluctuating mental states throughout the day\u2014capturing moments of frustration, focus, and meditative work flow. The resulting fabrics tell a story, and each one is unique to the worker and particular moment in time.</p><p>There are many ways humans express themselves\u2014what does this expression look like in the age of mechanical production? Is there a way to insert the mark of being human into the process? This project offers a moment of reflection\u2014for both the consumer and the worker\u2014to reflect on the labor,&nbsp;both mechanical and human, that is involved in our economy.</p><p>Learn more:&nbsp;<a href=\"https://ani-liu.com/mind-in-machine\">https://ani-liu.com/mind-in-machine</a></p>", "people": ["wonder@media.mit.edu"], "title": "Mind in the Machine: Psyche in the age of mechanical production", "modified": "2019-03-26T19:13:50.509Z", "visibility": "PUBLIC", "start_on": "2017-06-02", "location": "", "groups": ["hacking-manufacturing"], "published": true, "active": false, "end_on": "2017-08-02", "slug": "mind-in-the-machine"}, {"website": "", "description": "<p>Electronic commerce on the Web is thriving, but consumers still have trouble finding products to meet their needs and desires. We introduce a novel recommender system technique which works even when users don\ufffdt necessarily know exactly what they're looking for. Users describe a goal for a real-life scenario, e.g., \"I want something elegant to wear for my boss's birthday party.\" A common-sense reasoning system maps between the stated goals and possibly relevant characteristics of the product. Reasoning is based on an 800,000-sentence common-sense knowledge base, and spreading activation inference. Scenario-oriented recommendation breaks down boundaries between products' categories, finds the \"first example\" for existing techniques like collaborative filtering, and helps promote independent brands. We describe our scenario-oriented fashion recommendation system, What Am I Gonna Wear?.</p>", "people": ["lieber@media.mit.edu"], "title": "Scenario-Oriented Recommendation", "modified": "2016-12-05T00:16:49.063Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-383", "groups": ["software-agents", "sociable-media"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "scenario-oriented-recommendation"}, {"website": "", "description": "<p>Scents Sense attempts to incorporate the recent developments in the field of electronic smell into the home. As the heart of the home, the kitchen presents an ideal setting for experimenting with this field. From preventing baked goods from burning (Burning Scentsations) to detecting and identifying spoiled produce, the possibilities are almost endless. A major goal of the project is to further ongoing smell research with the creation of the .snf file, a universal file format that will assist with the identification and classification of any sample being tested.</p>", "people": ["mike@media.mit.edu"], "title": "Scents Sense", "modified": "2016-12-05T00:16:49.088Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-068", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "scents-sense"}, {"website": "", "description": "<p>The SCOOT Behavior system is a rich and flexible behavior architecture for constructing directable autonomous animated characters. Derived from studies of animal behavior and Marvin Minsky's \"Society of Mind\", the SCOOT behavior system comprises a few simple \"building blocks,\" and can be used to easily implement arbitrary models of action selection, behavioral motivations, emotions, and learning. The SCOOT behavior system allows us to easily construct characters that have distinct personalities and respond \"in character\" to the situations they encounter at any given moment.  It is the underlying behavior architecture used in Swamped!</p>", "people": [], "title": "SCOOT Behavior System", "modified": "2016-12-05T00:16:49.111Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "scoot-behavior-system"}, {"website": "", "description": "<p>A character's thoughts, intentions, and feelings are ultimately revealed through its motion. This project seeks to make it possible to combine key-framed animation, procedural animation, and user-driven animation in a seamless and expressive manner, such that the character always moves \"in character.\" In particular, it focuses on the twin problems of spatial blending (i.e., compositing the motion of motor skills that affect different parts of the body simultaneously), and temporal blending (i.e., transitioning smoothly from one motor skill to another over time). In addition, borrowing ideas from Rose and Cohen, we are exploring the use of multi-target motion interpolation so as to be able to interpolate in \"adverb space,\" (i.e., given a sample of an \"very happy walk\" and a \"very sad walk\" be able to generate a \"slightly sad walk.\") The SCOOT motor system was used to implement the motor systems of the individual characters in Swamped!</p>", "people": [], "title": "SCOOT Motor System", "modified": "2016-12-05T00:16:49.158Z", "visibility": "PUBLIC", "start_on": "1996-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "1998-09-01", "slug": "scoot-motor-system"}, {"website": "", "description": "<p>muStick is a tribal instrument turned digital. It is a simple stick to which sensors are attached. They detect the thumping of the stick against the ground and tapping on the stick. The angle at which the stick is held is also measured. These signals are tranformed into MIDI signals, and turn ?Stick into a simple yet powerful music controller.</p>", "people": ["tod@media.mit.edu"], "title": "muStick", "modified": "2016-12-05T00:16:49.188Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "mustick"}, {"website": "", "description": "<p>It has become increasingly common for new dance theater works to combine the movement of live dancers with the movement of computer-generated graphics projected onto the stage. Such works typically either manipulate carefully pre-made motion-capture material or, alternatively, a variety of indirect sensing technologies on the dancers themselves. What happens if the computers can see a high-resolution, real-time motion capture of the dancers in the theatre as the dance unfolds? How should such hybrid, collaborative pieces be made? Two new pieces are being created to help answer these questions. \"How long will the subject linger on the edge of the volume?\" is a new piece with the Trisha Brown Dance Company, and \"22\" is being created with Bill T. Jones.</p>", "people": ["tod@media.mit.edu"], "title": "New Works for Dance Theater", "modified": "2016-12-05T00:16:49.251Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-450", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "new-works-for-dance-theater"}, {"website": "", "description": "<p>Despite recent widespread interest in hobbyist electronics and the maker movement, the design of printed circuit boards (PCBs) remains an obscure and often intimidating activity. This project attempts to introduce PCB design and production to new audiences by creating examples, activities, and other resources that provide context and motivation for those practices. We've developed a series of interactive lights that demonstrate the creation of useable products with simple circuits. These examples introduce novices to the space of possibilities and provide them with a starting point for creating their own designs. In workshops, novices design, produce, assemble, and program their own electronic circuits. These workshops provide an entry point to understanding the way that electronic products are made and an opportunity for discussion and reflection about how more people might get involved in their production.</p>", "people": ["mellis@media.mit.edu", "mres@media.mit.edu"], "title": "Novice Design of Interactive Products", "modified": "2016-12-05T00:16:49.276Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "novice-design-of-interactive-products"}, {"website": "", "description": "<p>Open Sources visualizes the roles and relationships among developers in free software communities. It is designed to help members of an open-source community quickly and easily answer questions such as: Is \"A\" prolific and reliable, frequently contributing code that remains part of the project? Is \"B\" a pruner, someone who comes in and mostly deletes other users' code?  Do some of those deleted coders then retaliate by undoing \"B\"'s work? Does \"C\" discuss plans before committing code? Is \"D\"'s work frequently discussed in messages?  Do you always talk about your code before you commit it, or do particular individuals always discuss your code months later?</p>", "people": ["judith@media.mit.edu"], "title": "Open Sources", "modified": "2016-12-05T00:16:49.298Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-390", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "open-sources"}, {"website": "", "description": "<p>For new members of a work community, it can be difficult to learn the social landscape of the organization. Where are the casual meeting areas? When are people likely to be socially active? Are there spontaneous gatherings happening in parts of the workspace? OPENSPACE enables awareness of presence and activity by combining a grid of motion sensors with data logging, pattern analysis, and a variety of visualization techniques. With OPENSPACE, we aim to increase the social awareness of the larger spaces in which we work.</p>", "people": ["holtzman@media.mit.edu"], "title": "OPENSPACE", "modified": "2016-12-05T00:16:49.324Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "openspace"}, {"website": "", "description": "<p>This project explores the relationship between empathy and civic engagement. We have designed and implemented a seven-part workshop to foster mutual understanding, collaborative problem-solving, and self-expression. The curriculum (which employs Scratch as a central tool) builds capacities in three areas: programming, storytelling, and perspective-taking. Throughout the workshop, participants use a variety of tools and techniques to engage in acts of personal expression by creating rich, interactive, multi-threaded narratives.</p>", "people": ["kbrennan@media.mit.edu", "mres@media.mit.edu"], "title": "Say What?!", "modified": "2016-12-05T00:16:49.348Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "say-what"}, {"website": "", "description": "<p>The Arduino platform makes prototyping and tinkering with electronics open to more people, but its complicated programming language is a barrier to entry.  Building on the graphical-blocks programming language developed in the Scratch project, we are creating a new, more accessible way to program the Arduino, so that more people can become \"makers\" with electronics.</p>", "people": ["mellis@media.mit.edu", "leah@media.mit.edu", "mres@media.mit.edu", "ericr@media.mit.edu"], "title": "Scratch for Arduino", "modified": "2016-12-05T00:16:49.398Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-001", "groups": ["high-low-tech", "lifelong-kindergarten"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "scratch-for-arduino"}, {"website": "", "description": "<p>As Scratch proliferates through the world, there is a growing need to support learners. But for teachers, educators, and others who are primarily concerned with enabling Scratch learning, there is a disconnect between their needs and the resources that are presently available through the Scratch Web site. ScratchEd is an online environment for Scratch educators to share stories, exchange resources, ask questions, and find people.</p>", "people": ["kbrennan@media.mit.edu", "mres@media.mit.edu"], "title": "ScratchEd", "modified": "2016-12-05T00:16:49.456Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "scratched"}, {"website": "", "description": "<p>To serve us well, robots and other agents must understand our needs and how to fulfill them. To that end, our research develops robots that empower humans by interactively learning from them. Interactive learning methods enable technically unskilled end-users to designate correct behavior and communicate their task knowledge to improve a robot's task performance. This research on interactive learning focuses on algorithms that facilitate teaching by signals of approval and disapproval from a live human trainer. We operationalize these feedback signals as numeric rewards within the machine-learning framework of reinforcement learning. In comparison to the complementary form of teaching by demonstration, this feedback-based teaching may require less task expertise and place less cognitive load on the trainer. Envisioned applications include human-robot collaboration and assistive robotic devices for handicapped users, such as myolectrically controlled prosthetics.</p>", "people": ["cynthiab@media.mit.edu"], "title": "Robot Learning from Human-Generated Rewards", "modified": "2017-05-31T18:41:05.939Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2014-05-01", "slug": "robot-learning-from-human-generated-rewards"}, {"website": "", "description": "<p>What if everyone could create their own interactive content in virtual worlds? We are putting the playful and intuitive features of Scratch into a new programming language for Second Life. We hope to make it easier for everyone to create their own interactive virtual pets, dancefloors, games, clothing, houses, and whatever else they can imagine.</p>", "people": ["mres@media.mit.edu", "ericr@media.mit.edu"], "title": "Scratch Worlds", "modified": "2016-12-05T00:16:49.435Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "scratch-worlds"}, {"website": "", "description": "<p>ScratchR is a platform for sharing programmable media online, allowing people to publish their own interactive stories, games, and animations. ScratchR is the engine behind the Scratch online community, a social network of young programmers. Unlike other user-generated content communities, ScratchR makes it easy to reuse other people's creations to foster collaborative learning. ScratchR allows members to rate, comment, tag, and create galleries. ScratchR is to programmable media what YouTube is to videos.</p>", "people": ["mres@media.mit.edu"], "title": "ScratchR", "modified": "2016-12-05T00:16:49.480Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "scratchr"}, {"website": "", "description": "<p>Screen interactions have been shown to contribute to increases in stress, anxiety, and deficiencies in breathing patterns. Since better respiration patterns can have a positive impact on wellbeing, ScreenSpire improves respiration patterns during information work using subliminal biofeedback. By using subtle graphical variations that are tuned to attempt to influence the user subconsciously, user distraction and cognitive load are minimized. To enable a truly seamless interaction, we have adapted an RF-based sensor (ResMed S+ sleep sensor) to serve as a screen-mounted contact-free and respiration sensor. Traditionally, respiration sensing is achieved with either invasive or on-skin sensors (such as a chest belt); having a contact-free sensor contributes to increased ease, comfort, and user compliance, since no special actions are required from the user.</p>", "people": ["changzj@media.mit.edu", "achituv@media.mit.edu", "pattie@media.mit.edu"], "title": "ScreenSpire", "modified": "2017-08-25T11:59:51.368Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces", "advancing-wellbeing"], "published": true, "active": false, "end_on": "2016-12-31", "slug": "screenspire"}, {"website": "", "description": "<p>RoomScope is an immersive real-time architectural sensor visualization using Microsoft HoloLens.</p>", "people": ["honghaod@media.mit.edu"], "title": "RoomScope", "modified": "2017-04-05T18:40:23.388Z", "visibility": "PUBLIC", "start_on": "2017-02-05", "location": "", "groups": ["changing-places"], "published": true, "active": false, "end_on": "2017-10-31", "slug": "roomscope"}, {"website": "", "description": "<p>One of the most fundamental and important characteristics of children is their curiosity. Can an interaction with a robot elicit, guide, and promote curiosity? In this project we use the DragonBot platform and a tablet app in a language educational setting with children. We test the hypothesis that a personalized, curiosity-driven behavior of a robot behaving as a younger peer, can affect a child's own curiosity. We use an in-house developed app in which the child's interaction with figures is automatically transformed into a spoken and written story. The social robot reacts to the story in an emotional way and then asks the child questions about it. The child and robot then switch roles, so that the robot tells the story and the child asks questions. The research question is whether the robot's curiosity-driven behavior affects the child's curiosity.</p>", "people": ["cynthiab@media.mit.edu", "ggordon@media.mit.edu"], "title": "Social Robot as a Younger Curious Peer", "modified": "2017-05-31T18:41:35.438Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2017-05-15", "slug": "social-robot-as-a-younger-curious-peer"}, {"website": "http://zensei.technology", "description": "<p><span style=\"font-size: 18px; font-weight: normal;\">We introduce </span>Zensei<span style=\"font-size: 18px; font-weight: normal;\">, an implicit sensing system that leverages bio-sensing, signal processing, and machine learning to classify </span>uninstrumented<span style=\"font-size: 18px; font-weight: normal;\"> users by their body\u2019s electrical properties. </span>Zensei<span style=\"font-size: 18px; font-weight: normal;\"> could allow many objects to recognize users. E.g., phones that unlock when held, cars that automatically adjust mirrors and seats, or power tools that restore user settings.</span><br></p><p>We introduce wide-spectrum bioimpedance hardware that measures both amplitude and phase. It extends previous approaches through multi-electrode sensing and high-speed wire- less data collection for embedded devices. We implement the sensing in devices and furniture, where unique electrode configurations generate characteristic profiles based on user\u2019s unique electrical properties. Finally, we discuss results from a comprehensive, longitudinal 22-day data collection experiment with 46 subjects. Our analysis shows promising classification accuracy and low false acceptance rate.</p><p>More information at <a href=\"http://zensei.technology\">http://zensei.technology</a></p><p>This project was completed in collaboration with <a href=\"http://london.takram.com\">Takram London</a> and <a href=\"http://atap.google.com/\">Google ATAP</a>.</p>", "people": ["raskar@media.mit.edu", "munehiko@media.mit.edu", "olwal@media.mit.edu", "rohan@media.mit.edu"], "title": "Zensei: Embedded, Multi-Electrode Bioimpedance Sensing for Implicit, Ubiquitous User Recognition", "modified": "2017-05-09T18:23:47.552Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2017-12-01", "slug": "zensei"}, {"website": "", "description": "<p>Drivers spend a significant amount of time multi-tasking while they are behind the wheel, especially parent drivers that attend to child passengers. These dangerous behaviors, particularly texting while driving, can lead to distractions and ultimately to accidents. Many in-car interfaces do not assist the driver with the task of entertaining the passengers. In a collaboration with Volkswagen/Audi and the SENSEable City Lab, we are developing PANDA (parental affective natural driver assistant), a robotic driver-vehicle interface that acts as a sociable partner and assists parent drivers. PANDA elicits facial expressions for engaging social interaction with the driver and passengers. PANDA uses car entertainment to entertain and engage the children in educational games while in the car and frees the parent to focus on the task of driving.</p>", "people": ["cynthiab@media.mit.edu", "michalg@media.mit.edu"], "title": "PANDA: Parental Affective Natural Driver Assistant", "modified": "2017-05-31T18:41:51.451Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2015-05-01", "slug": "panda-parental-affective-natural-driver-assistant"}, {"website": "", "description": "<p>Can you feel empathy for a robot? The electric parrot project is about designing a robot that can engender empathy. For this we are constructing a novel zoomorphic robot that can create its own life story: that is to say it can experience the world, be changed by the experience, and communicate the experience to us. We aim to show through psychometric tests that giving the robot an implicit life story will invoke empathy. Such a robot can subsequently be used for empathy intervention. </p>", "people": ["cynthiab@media.mit.edu", "palash@media.mit.edu"], "title": "Electric Parrot", "modified": "2017-05-31T18:44:43.634Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2016-08-31", "slug": "electric-parrot"}, {"website": "", "description": "<p>Most sensory data has limited utility without location information, and manual node localization becomes impossible for large, inaccessable, or mobile sensor deployments. Thus, autonomous localization is crucial for many sensor-network applications. We are developing a distributed-localization algorithm for the PLUG indoor sensor network by analyzing commonly detected sound, light, and vibration sensory data from naturally occuring phenomena. The system enters active mode when its sensed region stays relatively silent and stable (assumed to be unoccupied). Otherwise, it stays in passive mode, with each node estimating its location by collecting sensory data and comparing it to synchronized data from other neighborhood nodes. In active mode, each node ocassionally generates predefined mimics of natural phenomena such as sonic transients, or manipulates an attached light source. The main features of this approach are distributed properties, lack of heavy infrastructure, unobtrusive exploitation of background phenomena, and application of junction trees for message passing.</p>", "people": ["joep@media.mit.edu"], "title": "Sensor Network Localization from Natural Phenomena", "modified": "2016-12-05T00:16:49.770Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "sensor-network-localization-from-natural-phenomena"}, {"website": "", "description": "<p>The Sensor Network Application Retasking Framework (or SNARF) is a collection of tools for embedding a variant of the Python programming language into a sensor network. The goal is to make programming and interacting with sensor networks significantly easier. SNARF builds upon PyMite, an open source project for porting Python to microcontrollers.</p>", "people": ["joep@media.mit.edu"], "title": "Sensor Network Application Retasking Framework", "modified": "2016-12-05T00:16:49.748Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "sensor-network-application-retasking-framework"}, {"website": "", "description": "<p>We are developing low-cost, distributed sensor networks for grassroots environmental monitoring. </p>", "people": ["csik@media.mit.edu"], "title": "Sensible Cities", "modified": "2016-12-05T00:16:49.802Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-020D", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "sensible-cities"}, {"website": "", "description": "<p>This project explores the wide-scale distribution of low-power, low-cost sensor nodes that can measure temperature, humidity, light levels, and human presence. These sensor nodes will enable buildings to react quickly and effectively to the changing needs of their inhabitants, automatically controlling, for example, heating/air conditioning, windows (opening and shades), and lighting. Total building power consumption can be reduced, and repair requests can be made automatically.</p>", "people": ["geppetto@media.mit.edu", "joep@media.mit.edu"], "title": "Sensor-Enabled Active Buildings", "modified": "2016-12-05T00:16:49.924Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "sensor-enabled-active-buildings"}, {"website": "", "description": "<p>Senspectra is a computationally augmented physical modeling toolkit designed for sensing and visualization of structural strain. The system functions as a decentralized sensor network; it consists of nodes that are each embedded with computational capabilities and a full-spectrum LED, which communicate with neighbor nodes to determine a network topology through a system of flexible joints. Each joint uses a simple optical occlusion technique as an omnidirectional bend-sensing mechanism to sense and communicate mechanical strain between neighboring nodes, while also serving as a data and power bus between nodes. The Senspectra infrastructure provides a flexible, modular, sensor-network platform whose primary application focuses on coupling physical-digital modeling techniques utilized in the architecture and industrial design disciplines with systems for structural engineering analysis, offering an intuitive approach for physical realtime finite element analysis.</p>", "people": ["ishii@media.mit.edu"], "title": "Senspectra", "modified": "2016-12-05T00:16:49.951Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "senspectra"}, {"website": "", "description": "<p>Pigeons can perform impressive feats when trained with classical chaining, but these apparently sophisticated performances diminish in accuracy if the context changes or chained behavior is interrupted. Pigeons cannot easily start chaining in the middle of the sequence, omit steps, or modify the chained sequence to adapt to new conditions. We are studying the grey parrot's ability to transfer serial learning from an initial context to new contexts with minimal retraining, and to acquire knowledge beyond simple stimulus-response. While similar to classic operant-conditioning studies, here the parrot must demonstrate versatility in his behavior.</p>", "people": ["impepper@media.mit.edu"], "title": "Serial TrHacking", "modified": "2016-12-05T00:16:49.979Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-318", "groups": ["pet-projects"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "serial-trhacking"}, {"website": "", "description": "<p>Interactive teaching tools strive to make subject-material more interesting through a hands-on approach to learning. Using video projection and shadow tracking, we want to expand the hands-on interaction approach to the whole body. Vehicles, based on ideas from the book \"Vehicles\" by Valentino Braitenberg, is a system for programming characters that can respond to sensed stimuli in a virtual environment. These characters are constructed similarly to the vehicle metaphor in the book, by connecting simple sensors to effectors. Users can then directly interact with their creations by stepping in front of the projected environment and casting a shadow. By using sensors and effectors with different behaviors in virtual environments with varying rules, we believe this will be a good tool for teaching science as well as some basic computer programming concepts.</p>", "people": ["pattie@media.mit.edu"], "title": "Shadow Vehicles", "modified": "2016-12-05T00:16:50.061Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "shadow-vehicles"}, {"website": "", "description": "<p>Shake and Play is a family of building blocks for young children that encourage new forms of expressive and exploratory play. Some bricks can light up in any color, and others make sounds. All respond to different sensors, such as shaking, tilting, loudness and brightness. They also can record a sound or a pattern of colors and play it back.</p>", "people": ["mres@media.mit.edu", "ericr@media.mit.edu"], "title": "Shake and Play", "modified": "2016-12-05T00:16:50.090Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "Cube", "groups": [], "published": true, "active": false, "end_on": "2008-01-01", "slug": "shake-and-play"}, {"website": "", "description": "<p>We are exploring ways to encode information exchange into preexisting natural interaction patterns, both between people and between a single user and objects with which he or she interacts on a regular basis. Two devices are presented to provoke thoughts regarding these information interchange modalities: a pair of gloves that requires two users to complete a \"secret handshake\" in order to gain shared access to restricted information, and a doorknob that recognizes the grasp of a user and becomes operational only if the person attempting to use it is authorized to do so.</p>", "people": ["vmb@media.mit.edu"], "title": "ShakeOnIt", "modified": "2016-12-05T00:16:50.148Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "shakeonit"}, {"website": "", "description": "<p>In most \ufffddrop-on-demand\ufffd inkjet control schemes, a superheated bubble of liquid is used to propel a droplet or a piezoelectric crystal physically squeezes out a droplet at high speeds. These models rely on a reservoir of print media that is always \ufffdopen\ufffd on one end for the droplet outlet. This makes the design of the system difficult for two reasons: the pore has to be small enough to hold back low-viscosity liquids by surface tension alone (~10um diameter), and the open nozzle leaves the ink exposed and prone to drying out.  We propose a new deposition mechanism based around a nozzle that is \ufffdplugged\ufffd by an actuating \ufffdstopper\ufffd made of shape memory wire backed by a positive internal fluid pressure. When the wire is actuated, the stopper is removed and the pressure of the fluid pushes one or more droplets out until the stopper is replaced.</p>", "people": ["jacobson@media.mit.edu", "neilg@media.mit.edu", "neri@media.mit.edu"], "title": "Shape Memory Inkjet", "modified": "2016-12-05T00:16:50.171Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "shape-memory-inkjet"}, {"website": "", "description": "<p>In this installation you play the role of a shepherd who must herd a bunch of sheep with attitude through a course using Duncan H. Terrier (a.k.a. Duncan of Innisfree) as your sheepdog. Duncan and the sheep are fully autonomous, although Duncan responds to voice commands such as \"bye,\" \"away,\" \"steady,\" and \"down.\" In fact, Duncan can be trained to respond to any set of utterances, in any accent, because rather than using speech recognition, he is looking at an acoustic pattern. Both the sheep and Duncan have minds of their own: for example, if the sheep get frightened, they may bolt or charge.</p>", "people": [], "title": "sheep/dog: Trial by Eire", "modified": "2016-12-05T00:16:50.294Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-320", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "sheepdog-trial-by-eire"}, {"website": "", "description": "<p>A small number of studies support the notion of a functional relationship between movement stereotypy and arousal in individuals with ASD, such that changes in autonomic activity either precede or are a consequence of engaging in stereotypical motor movements. Unfortunately, it is difficult to generalize these findings as previous studies fail to report reliability statistics that demonstrate accurate identification of movement stereotypy start and end times, and use autonomic monitors that are obtrusive and thus only suitable for short-term measurement in laboratory settings. The current investigation further explores the relationship between movement stereotypy and autonomic activity in persons with autism by combining state-of-the-art ambulatory heart rate monitors to objectively assess arousal across settings; and wireless, wearable motion sensors and pattern recognition software that can automatically and reliably detect stereotypical motor movements in individuals with autism in real time.</p>", "people": ["mgoodwin@media.mit.edu", "picard@media.mit.edu"], "title": "Sensor-Enabled Measurement of Stereotypy and Arousal in Individuals with Autism", "modified": "2016-12-05T00:16:50.248Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "sensor-enabled-measurement-of-stereotypy-and-arousal-in-individuals-with-autism"}, {"website": "", "description": "<p>Expressive interactive characters call for expressive camera and lighting control. To show off our digital actors to best advantage, we are working on a real-time autonomous character, the ShutterBug, who will be the cinematographer in our virtual world. By combining the same techniques that we use to build our synthetic characters with inspiration from traditional film making, we seek to bring to life a dynamic camera system with an organic feel. Our autonomous camera creature captures the actions and emotions of our on-screen actors in an interesting and comprehensible manner, while simultaneously facilitating user interaction in the world. Current projects include adding an emotional system to the ShutterBug, and continuing our work in automatic lighting design. This work forms the basis of the camera control and lighting used in Swamped!\n</p>", "people": [], "title": "ShutterBug: An Autonomous Camera Creature", "modified": "2016-12-05T00:16:50.369Z", "visibility": "PUBLIC", "start_on": "1995-12-31", "location": "", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "shutterbug-an-autonomous-camera-creature"}, {"website": "", "description": "<p>NewsClouds presents a visual exploration of how the news reporting of an event evolves over time. Each \"cloud\" represents a publication and each competing news organization usually emphasizes different aspects of that same story. Using the time sliders, that evolution becomes evident. In addition, each word or phrase can be expanded to show its links and context. We are building an archive of events associated with ongoing US election developments.</p>", "people": ["thariq@media.mit.edu", "lip@media.mit.edu"], "title": "NewsClouds", "modified": "2018-10-09T01:51:14.404Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2018-06-06", "slug": "newsclouds"}, {"website": "", "description": "<p>Like people, dogs and cats live among technologies that affect their lives. Yet little of this technology has been designed with pets in mind. We are developing systems that interact intelligently with animals to entertain, exercise, and empower them. Currently, we are developing a laser-chasing game, in which dogs or cats are tracked by a ceiling-mounted webcam, and a computer-controlled laser moves with knowledge of the pet's position and movement. Machine learning will be applied to optimize the specific laser strategy. We envision enabling owners to initiate and view the interaction remotely through a web interface, providing stimulation and exercise to pets when the owners are at work or otherwise cannot be present.</p>", "people": ["cynthiab@media.mit.edu"], "title": "Animal-Robot Interaction", "modified": "2017-05-31T18:44:59.591Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2014-05-31", "slug": "animal-robot-interaction"}, {"website": "", "description": "<p>Shybot is a personal mobile robot designed to both embody and elicit reflection on shyness behaviors. Shybot is being designed to detect human presence and familiarity from face detection and proximity sensing in order to categorize people as friends or strangers for interaction. Shybot also can reflect elements of the anxious state of its human companion through LEDs and a spinning propeller. We designed this simple social interaction to open up a new direction for intervention for children living with autism. We hope that from minimal social interaction, a child with autism or social anxiety disorders could reflect on and more deeply attain understanding about personal shyness behaviors, as a first step toward helping to make progress in developing greater capacity for complex social interactions.</p>", "people": ["picard@media.mit.edu"], "title": "ShyBot", "modified": "2016-12-05T00:16:50.395Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "shybot"}, {"website": "", "description": "<p>Signs is an instant messenger design philosophy and implementation. It aims to reduce confusion while increasing expressive power through a suite of novel design features.  These features provide native structural elements in the conversation that free the user from sub-optimally performing meta-linguistic tasks in the same text-only chat space.  Specifically, Signs aims to facilitate repair of sequencing problems, reduce ambiguity, and use the persistent chat log as a backdrop for new communication acts. </p>", "people": ["judith@media.mit.edu"], "title": "Signs", "modified": "2016-12-05T00:16:50.448Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-318", "groups": [], "published": true, "active": false, "end_on": "2010-01-01", "slug": "signs"}, {"website": "", "description": "<p>Collaborative group situations present opportunities for designing meaningful whole-body input and interaction techniques. In this project we focus on the action of sharing information in a group through posters and signs held up by its members and translating that action meaningfully based on the group context. Live audience events like reality-TV shows, sports events, and classrooms are example scenarios where this action is most commonly elicited. We attempt to automate the process of recognizing whenever people hold their signs up and feed this information back into the system or to remote audiences. In classroom situations, this tool would offer a quick way of gauging solutions to short questions which can then be projected back into the classroom. In the live-audience case, it offers a way of engaging remote spectators that can vote on all aggregated signs and projecting relevant information back into the live-audience space.</p>", "people": [], "title": "Signs-Up!", "modified": "2016-12-05T00:16:50.423Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2013-09-01", "slug": "signs-up"}, {"website": "", "description": "<p>Communication is an integral task for any distributed work group. As the work environment becomes less structured, it becomes harder for teams that are not co-located to coordinate and communicate about essential matters that might be integral to their productivity. More often than not, communication is not the only task in which an individual is engaged, but one that occurs in parallel and is closely dependent on many others. This project proposes a wireless, mobile, context-sensitive application for voice messaging and communication specifically designed for distributed work groups. The purpose of the system is to allow groups to communicate easily on a dedicated, ongoing communication channel. Groups can be composed of several mobile users using Pocket PCs and users connected over a telephone.</p>", "people": ["geek@media.mit.edu"], "title": "SimPhony: Voice Group Communication", "modified": "2016-12-05T00:16:50.497Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-344", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "simphony-voice-group-communication"}, {"website": "", "description": "<p>Singing Fingers allows children to fingerpaint with sound. Users paint by touching a screen with a finger, but color only emerges if a sound is made at the same time. By touching the painting again, users can play back the sound. This creates a new level of accessibility for recording, playback, and remixing of sound.</p>", "people": ["silver@media.mit.edu", "mres@media.mit.edu", "ericr@media.mit.edu"], "title": "Singing Fingers", "modified": "2016-12-05T00:16:50.520Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "singing-fingers"}, {"website": "", "description": "<p>The goal of this project is to allow designers to continue using the traditional drawing tools they love, but to digitally enhance the drawing space so that designers can explore and enhance their work in more resourceful ways. More specifically, we are looking at an application for architects. There is a natural connection between architects and this research, due to their work with layers of transparencies to sketch out their designs. Because architects work so frequently with layers, they end up dealing with tens of thousands of sketches, and organizing and utilizing such a large collection of sketches is difficult. This project proposes to digitally capture and organize the layers that architects create, in order to help architects document and utilize their creative process without losing the tactile media to which they are accustomed. Using a video camera and a projector installed in the sketch space, we can capture and track the layers as they are being developed. We are also designing ways to graphically and spatially organize the captured layers, and to allow architects to pull out and work with the old layers easily. Once the prototype is complete, we plan to evaluate it by inviting architects to use the system in their design studios. </p>", "people": ["ishii@media.mit.edu"], "title": "Sketch Interface", "modified": "2016-12-05T00:16:50.564Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "sketch-interface"}, {"website": "", "description": "<p>A large portion of popular media is remixed: existing media content is spliced and re-ordered in a manner that serves a specific narrative. Super Cut Notes is a semi-comical content remix tool that allows a user to splice and combine the smallest bits of media: words. By tapping into the dataset of our group's <a href=\"https://www-prod.media.mit.edu/projects/superglue/overview/\">SuperGlue platform</a>, it has access to a huge dictionary of words created by SuperGlue's transcription module. Users are able to input a text of any length, choose video-bits of individual words that match their text, and create a video of their combination\u2014in the style of cut-and-pasted ransom notes.</p>", "people": ["weller@media.mit.edu", "lip@media.mit.edu"], "title": "Super Cut Notes", "modified": "2018-10-09T01:52:24.513Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2017-06-06", "slug": "super-cut-notes"}, {"website": "", "description": "<p>Skeleton is a set of Cocoa libraries optimized for fast development of research, and for professional applications dealing with the analysis of musical signals. Based on fundamentals of psychoacoustics, perception, and learning, the underlying framework consists of machine-listening and machine-learning tools, supported by flexible data structures and visualizations. Designed essentially as an alternative to more generic and slower tools such as Matlab, Skeleton should provide a robust, efficient, specific and yet open programming environment for deconstructing, structuring, and labeling audio, or for generating personalized music. Skeleton aims to combine various solid scientific approaches to music listening, learning, and cognition, to enable the development of consistent creative audio applications, and enhance music-making. Current applications include beat matching, music mosaicing, compression, and music textures.</p>", "people": ["tristan@media.mit.edu", "tod@media.mit.edu"], "title": "Skeleton", "modified": "2016-12-05T00:16:50.638Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "skeleton"}, {"website": "", "description": "<p>Skellig is an opera with music by Tod Machover and a libretto based on the best-selling novel for young people by David Almond. It premiered in the UK in November 2008. Besides blending acoustics and electronics, natural noise, and soaring melodies, Skellig also presents several live performance breakthroughs. A non-professional teenage chorus is used throughout, blended seamlessly with high-level professionals; this chorus is guided by an interactive \"sonic score\" that provides auditory cues, textures to imitate, and electronic reinforcement for the entire 100-minute show. In addition, specially designed \"ambisonics\" were developed to allow sound to emanate from the stage and engulf the audience in all dimensions, the first time such a technique has been used in a full-scale theatrical setting.</p>", "people": ["tod@media.mit.edu"], "title": "Skellig: A \"Surround\" Opera", "modified": "2016-12-05T00:16:50.665Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "Hyperinstruments offices", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "skellig-a-surround-opera"}, {"website": "", "description": "<p>Imagine opening your eyes and being awake for only half an hour at a time. This is the life that robots traditionally live. This is due to a number of factors, such as battery life and wear on prototype joints. Roboticists have typically muddled though this challenge by crafting handmade perception and planning models of the world, or by using machine learning with synthetic and real-world data, but cloud-based robotics aims to marry large distributed systems with machine learning techniques to understand how to build robots that interpret the world in a richer way. This movement aims to build large-scale machine learning algorithms that use experiences from large groups of people, whether sourced from a large number of tabletop robots or a large number of experiences with virtual agents. Large-scale robotics aims to change embodied AI as it changed non-embodied AI.</p>", "people": ["cynthiab@media.mit.edu", "ndepalma@media.mit.edu"], "title": "Cloud-HRI", "modified": "2017-05-31T18:45:21.362Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2017-05-30", "slug": "cloud-hri"}, {"website": "", "description": "<p>The skin is a versatile organ, capable of fine temporal and spatial discriminations. Touch is considered by many to be the most important of our senses, for without it we can have no sense of the physical boundaries of our own bodies or our surrounding physical environment. The Skinscape project introduces tools and a wearable that allow us to compose and perceive compositions designed for the skin. The wearable consists of a number of vibrotactile stimulators designed into a body suit. The stimulators are sequenced to play over the body according to the composer's intention. Cutaneous Grooves is a series of tactile and musical compositions designed to demonstrate the perceptual power of the system.</p>", "people": ["gid@media.mit.edu"], "title": "Skinscape: A Tool for Tactile Composition", "modified": "2016-12-05T00:16:50.693Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "", "groups": ["interactive-cinema", "gray-matters"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "skinscape-a-tool-for-tactile-composition"}, {"website": "", "description": "<p>Human success is largely attributed to our collective capacity to accumulate knowledge. This capacity, a.k.a. our Social Collective Memory, is constrained by a trade-off between knowledge creation and knowledge preservation. Through knowledge creation, populations bring about new ideas and through knowledge preservation\u2014which usually entails communication. They keep ideas from being lost once individuals who posses them are no longer available. Here we study how communication shapes the size, volatility, longevity, and redundancy of our Social Collective Memory. We use a simple model to identify optimal levels of communication that maximize the Social Collective Memory and minimize the knowledge gap between individuals and the population as a whole. We find that when innovation is difficult, communication increases the Social Collective Memory and longevity of ideas, while reducing its volatility. Further, we find that the optimal level of communication needed to maximize the Social Collective Memory decreases with population size and are the lowest for the small-world communication network topology. Our findings contribute to a better understanding of the role of communication in creating human collective memory.</p>", "people": ["isal@media.mit.edu"], "title": "Social Collective Memory", "modified": "2017-10-13T15:45:53.850Z", "visibility": "LAB-INSIDERS", "start_on": "2017-02-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2017-12-31", "slug": "social-collective-memory"}, {"website": "", "description": "<p>In this research project, we rethink the mobility dynamics in the city context, both from a physical and cognitive perspective, considering a bus line as a starting point to redesign multimodal urban transportation system. We consider a bus line as a complex environment composed of the bus as a public transportation system; the bus stop, where people meet, wait, connect; the city as the urban/architectural/cultural environment; the passengers with their needs; and the information/communication systems and devices. With this perspective, we explore how wireless interactive media technologies can be integrated into every level of a complex bus line, including physical and virtual public spaces and the information/communication system with passengers and urban environment, with the aim of increasing smart mobility and a more efficient information distribution.</p>", "people": ["federico@media.mit.edu"], "title": "Smart Mobility", "modified": "2016-12-05T00:16:50.770Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "smart-mobility"}, {"website": "", "description": "<p>Sunlight is one of the most fundamental elements in nature to which we have free access in urban environments. It is also at the core of how we experience the physical world. What if we could engage sunshine in the digital age, to tame, modify, and bend light to our will? LightByte, a massive interactive sun pixel facade, modifies the sun's rays at your whim into intricate shapes. It turns sunlight into an expressive medium to carry information, communicate ideas, and shape your own shadows.</p>", "people": ["aithpao@media.mit.edu", "kll@media.mit.edu"], "title": "LightByte: Animate the Sunlight", "modified": "2018-04-03T20:34:13.521Z", "visibility": "LAB", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "OLD_lightbyte-animate-the-sunlight"}, {"website": "", "description": "<p>In the future, we will be able to paint pixels onto nearly any surface\ufffdbut what will we use them for? Our smart megapixel display project aims to create a 100 megapixel display surface that is aware of the people around it and where they are looking. We will use this platform to explore collaborative work, software visualization, adaptive information display, and multi-resolution display.</p>", "people": ["dsmall@media.mit.edu"], "title": "Smart Megapixel Display", "modified": "2016-12-05T00:16:50.817Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "--Choose Location", "groups": ["design-ecology"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "smart-megapixel-display"}, {"website": "", "description": "<p>This project examines the creation of modular computational elements which  can be used to build smart rooms, linked meeting rooms, and other sensor- and display-equipped intelligent spaces. These units are intended  to tile the walls of a room, and act as a scalable, self-organizing system. Each tile incorporates networked communications, sensing, intelligence, and actuation/display. Cells coordinate their operations in order to provide complex sensing and display applications.  Most recently this platform has been the basis for a gesture-based distributed telecollaboration and simulation system. This is a joint project between the Media Laboratory and the Information and Communications University (ICU) in Korea.</p>", "people": ["vmb@media.mit.edu"], "title": "Smart Architectural Surfaces", "modified": "2016-12-05T00:16:50.843Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "Garden Conference Room", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "smart-architectural-surfaces"}, {"website": "", "description": "<p>The Smart Motorcycles project is aimed at instrumenting motocycles in ways that increase safety and reduce the chance of accidents to motocycle drivers. Radar, sonar, laser, GPS, and computer-vision technology are used to accumulate data about road conditions, moving objects, and terrain. A computer with a flat-panel, highy visible display is added on top of the tank. A wearable computer is integrated into the helmet. Biosensors are embedded into the suit or jacket of the driver to monitor body data. The Smart Motorcycle technology extends the work of the sensor fusion project\ufffdoriginally developed for Smart Rooms\ufffdinto a new application area.</p>", "people": ["sandy@media.mit.edu", "gid@media.mit.edu"], "title": "Smart Motorcycles", "modified": "2016-12-05T00:16:50.921Z", "visibility": "LAB", "start_on": "2000-01-01", "location": "E15-384A", "groups": [], "published": true, "active": false, "end_on": "2000-09-01", "slug": "smart-motorcycles"}, {"website": "", "description": "<p>SnapN'Share, the first sample application that makes use of the Comm.unity platform, runs on wireless mobile devices and stationary computers, and enables users to generate and seamlessly share content with groups and communities as they come within close proximity of their peers. The system offers local filtering of content, exposing users to what is locally relevant, and also allowing new introductions to occur in the physical space. It enables communication based on groups and context\ufffd\"Virtual Spaces\" are created for different social groups (friends, family, classmates). The system allows for the easy creation of ad hoc or time-limited groups (e.g., meeting attendees or people going on a trip together). SnapN'Share includes multiple features related to trust and identity, allowing the differentiation between strangers and familiar peers, and configuration of varying degrees of trust and privacy. Finally, the system offers all of the inherent features of the Comm.unity platform.</p>", "people": ["lip@media.mit.edu", "nadav@media.mit.edu"], "title": "SnapN'Share", "modified": "2016-12-05T00:16:51.210Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "snapnshare"}, {"website": "", "description": "<p>SMPL Services is a collection of basic software technologies built around the SMPL core that allows applications to go beyond the level of a single device. The first generation of distributed SMPL services encompasses data storage, user authentication, media rendering, annotation, sharing, search, interaction harvesting, and project management.</p>", "people": [], "title": "SMPL Services", "modified": "2016-12-05T00:16:51.238Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "smpl-services"}, {"website": "", "description": "<p>The Internet supports many great tools for communicating at a distance in order to maintain personal relationships and build social networks.  However, these tools rarely help us realize which relationships are strained by lack of attention. Social Garden explores using virtual plants as a metaphor for relationships, encouraging us to tend to our social connections as we do our gardens. By tracking and analyzing communications through email, instant messaging, social websites, SMS, and phone, Social Garden proposes to give feedback on how our relationships are flourishing or wilting, and organizes our social circles. We also explore the garden metaphor as a practical interface to browse and manage conversations and contacts.</p>", "people": ["jkestner@media.mit.edu", "holtzman@media.mit.edu"], "title": "Social Garden", "modified": "2016-12-05T00:16:51.322Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "social-garden"}, {"website": "", "description": "<p>We are using non-invasive measurement of social signals found in voice, body movement, and location to quantify symptoms in neurological disorders such as Parkinson's Disease.</p>", "people": ["sandy@media.mit.edu"], "title": "Social Signals in Biomedicine", "modified": "2016-12-05T00:16:51.495Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "social-signals-in-biomedicine"}, {"website": "", "description": "<p>Social Transactions is an application that allows communities of consumers to collaboratively sense the market from mobile devices, enabling more informed financial decisions in a geo-local and timely context. The mobile application not only allows users to perform transactions, but also to inform, share, and purchase in groups at desired times. It could, for example, help people connect opportunistically in a local area to make group purchases, pick up an item for a friend, or perform reverse auctions. Our framework is an Open Transaction Network that enables applications from restaurant menu recommendations to electronics purchases.  We tested this with MIT's TechCASH payment system to investigate whether shared social transactions could provide just-in-time influences to change behaviors.</p>", "people": ["lip@media.mit.edu"], "title": "Social Transactions/Open Transactions", "modified": "2016-12-05T00:16:51.580Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "social-transactionsopen-transactions"}, {"website": "", "description": "<p>SonicLink is a fully decentralized, proximal communication framework for personal devices to seamlessly discover, connect, and interact with arbitrary public installations (such as digital billboards). It establishes connections based on audio proximity when you are not on the same network:  being near in physical space does not mean you are near in \"network space.\"  SonicLink uses near-ultrasonic acoustic signals that permit devices and installations to discover each other. It also exploits peer-to-peer proximal wireless networking techniques for establishing a high-bandwidth, low-latency link between the device and the installation. Possible uses include \ufffdborrowing\ufffd a large-screen TV and a camera in the public space for personal video conferencing, presenting personal notifications to a public display, and taking over neon lights for visualizing music on a phone.</p>", "people": ["lip@media.mit.edu"], "title": "SonicLink", "modified": "2016-12-05T00:16:51.732Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "soniclink"}, {"website": "", "description": "<p>Building on well-developed data visualization techniques, audio is used to enhance understanding by improving the brain's ability to parse information in the scientific process. Sonification (aka auditory display) is the use of non-speech audio to convey information. In collaboration with the Media Lab, NASA is interested in using the human auditory system's powers of organizing and deconstructing sound for the purposes of scientific research and exploratory data analysis. Faced with increasingly voluminous and complex multimodal data and computations, researchers are seeking novel ways to optimize the conversion of information into knowledge. Several sonifications are under study, including interactive tours through the solar system, spectral data from Mars, sensor data from the Media Lab, new augmentations of the traditional orrery, and enhanced exploration of the Mandelbrot set. The eventual goal is to generalize these new sonification techniques for the display, exploration, and analysis of any large dataset.</p>", "people": ["joep@media.mit.edu", "bv@media.mit.edu"], "title": "Sonic Scientist", "modified": "2016-12-05T00:16:51.635Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "sonic-scientist"}, {"website": "", "description": "<p>The purpose of the Software Perception project is to develop fluid knowledge representations for the concepts needed to understand and create programs. These representations will power a visual editor to assist a person synthesizing new code through a visual dialogue. The system utilizes simulations to build models of many existing real-world metaphors used for programming models. In the other direction, visualization involves creating real-world analogues to abstract processes. The goal is to teach the system programming concepts and concepts based on real-world experiences, and the relation between the two.</p>", "people": [], "title": "Software Perception", "modified": "2016-12-05T00:16:51.682Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-305A", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "software-perception"}, {"website": "", "description": "<p>In this project, human-built spaces such as parks are acoustically analyzed and then permanently and publicly labeled.</p>", "people": ["csik@media.mit.edu"], "title": "Sonic Authority", "modified": "2016-12-05T00:16:51.710Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-020A", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "sonic-authority"}, {"website": "", "description": "", "people": [], "title": "DoppelMarsh", "modified": "2017-07-19T16:03:17.569Z", "visibility": "PUBLIC", "start_on": "2017-06-07", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2017-08-10", "slug": "doppelmarsh"}, {"website": "", "description": "<p>Sound designers, audio professionals, and musicians often spend time and energy looking for the right sound for a particular piece of music or sonic environment. Current sound synthesizers either contain numerous sound presets that are laborious to parse, or batteries of parameters to tweak without straightforward connections to one's intuitive expectation. We propose a sound retrieval and modification engine based on everyday words like \"bright,\" \"warm,\" and \"fat.\" The perceptual sound synthesis engine is informed by a survey of musicians and listeners worldwide and can also be customized. This system allows dynamic tagging of sound material from online libraries, and \"sound sculpting\" based on common verbal descriptors instead of obscure numerical parameters.</p>", "people": ["bv@media.mit.edu"], "title": "Sound Design with Everyday Words", "modified": "2016-12-05T00:16:51.756Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-310", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "sound-design-with-everyday-words"}, {"website": "", "description": "<p>This project will build comfortable technology and tools that a wearer can use to record their own speech in natural social settings, play it back, and learn about and reflect on how he sounds. The tools will include automated analysis of pitch patterns, loudness, and timing, to help highlight these non-linguistic features of speech. The toolkit can be used during, or in conjunction with, speech-language therapy for individuals on the autism spectrum to explore and reflect on natural social situations. It will also enable autism researches to collect speech data outside of laboratory settings to quantify progress with language and communication in natural social contexts. \n</p>", "people": ["mgoodwin@media.mit.edu", "picard@media.mit.edu", "mehoque@media.mit.edu"], "title": "SoundAffects", "modified": "2016-12-05T00:16:51.822Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-443", "groups": [], "published": true, "active": false, "end_on": "2009-09-01", "slug": "soundaffects"}, {"website": "", "description": "<p>Sourcemap is the social network for supply chains, connecting producers, manufacturers, and consumers for end-to-end visibility. Consumers use Sourcemap to learn where things come from and what they're made of, including their social and environmental impact. Manufacturers use Sourcemap to trace products down to raw materials and plan more resilient, efficient supply chains. Visit www.sourcemap.com for more information.</p>", "people": ["ishii@media.mit.edu", "amerigo@media.mit.edu"], "title": "Sourcemap", "modified": "2016-12-05T00:16:51.894Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "E15-348", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "sourcemap"}, {"website": "", "description": "<p>Soundforms is a tabletop interface for collaborative composition. The shapes and sizes of objects are used as simple icons for controlling sound output, affording a playful interface for people to experiment together in a short sound composition.</p>", "people": ["pol@media.mit.edu", "pattie@media.mit.edu"], "title": "SoundForms", "modified": "2016-12-05T00:16:51.859Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "soundforms"}, {"website": "", "description": "<p>SoundStrand is a music composition toy. It comprises a set of building blocks, each containing a musical motif. The blocks can be connected to each other to create a musical theme. They can also be manipulated with three degrees of freedom: elongation changes the rhythmic distribution of the notes; bending changes the direction of the melody; and twisting changes the harmonic context.</p>", "people": ["tod@media.mit.edu"], "title": "SoundStrand", "modified": "2016-12-05T00:16:51.976Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "soundstrand"}, {"website": "", "description": "<p>Internet spam is an overwhelming disease of the 21st century. With very elaborate devices, we go to great length to avoid seeing spam: junk-mail filters, screening services, spam databases. SP4M attempts to turn the relationship we have with spam around. A microcontroller-based Webserver retrieves spam sent over the Internet in real time, and remotely feeds a continuous shuffled stream of it to public displays. Spam is no longer meaningless, time-consuming junk, but rather a diagnosis of cultural values and an alternate representation of societies. </p>", "people": ["csik@media.mit.edu"], "title": "SP4M", "modified": "2016-12-05T00:16:52.041Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "sp4m"}, {"website": "", "description": "<p>SparkInfo enables users to create, exchange, and augment their audiovisual elements in ways that are personally unique and sociable. SparkInfo provides a social space for the co-creation of audiovisual and multimedia resources. </p>", "people": ["holtzman@media.mit.edu"], "title": "SparkInfo", "modified": "2016-12-05T00:16:52.128Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "sparkinfo"}, {"website": "", "description": "<p>Space Saver is a system for importing physical spaces into virtual environments. The project provides tools that lower the complexity barrier to representing our physical spaces in a virtual world, in turn adding new levels of relevance to virtual representations. The system operates by amassing readings from a distance-sensing camera into models that can be imported into Second Life and other 3-D modeling software.</p>", "people": ["holtzman@media.mit.edu"], "title": "Space Saver", "modified": "2016-12-05T00:16:52.101Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "space-saver"}, {"website": "", "description": "<p>Sparkler is the opening piece for Toy Symphony concerts composed by Tod Machover. A simple, sinuous melody weaves throughout the piece, beginning and ending in calmness, but rapidly and delicately undulating along the way. This melody varies continuously\ufffdpulling in its wake tiny fragments of remembered classical and popular tunes; expanding into more and more intricate textures; and finally being subsumed in a series of dense sonic masses at the work\ufffds culminating moment. Sparkler explores many different relationships between orchestra and technology. Three keyboards\ufffdeach with a specially designed controller for shaping timbre and articulation\ufffdplay and modulate much of the electronics. Unlike many previous works where only solo instruments are amplified or electronically processed, here microphones capture the entire orchestral sound, which is analyzed live into \ufffdperceptual parameters\ufffd through software written by Tristan Jehan. These instrumental sound masses\ufffdwhich are performed with a certain freedom by players and conductor\ufffdgenerate and control (pushing, pulling, twisting, and morphing) complex electronic extensions, turning the whole ensemble into a kind of \ufffdhyperorchestra.\ufffd Sparkler reflects the energy, innocence, imagination, vulnerability, and rapid passing of childhood.</p>", "people": ["tristan@media.mit.edu", "tod@media.mit.edu"], "title": "Sparkler", "modified": "2016-12-05T00:16:52.158Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "sparkler"}, {"website": "", "description": "<p>SpeakCup is a digital voice recorder. Rather than being controlled via buttons and blinking lights, SpeakCup uses shape to proscribe function. It takes the form of a small rubber disc with holes in its center on one side. When the holes are pressed in, forming a small cup, SpeakCup absorbs and contains sound. When the holes are pressed out, the stored sounds (embodied as fluxuating lights) are released.</p>", "people": ["ishii@media.mit.edu"], "title": "SpeakCup", "modified": "2016-12-05T00:16:52.255Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "speakcup"}, {"website": "", "description": "<p>We are developing a tool to allow users to place and recall virtual sound objects (music files, phone numbers) at 3-D spatial coordinates around us. As these locations are in fact imaginary, it frees us to explore visually independent interaction through audio. Two turns of the wrist, for pitch and yaw orientation, and a scroll of the thumb to provide a distance value, and we are exploring different notions of \ufffdfile paths\ufffd and \"memory addressing.\"</p>", "people": ["geek@media.mit.edu"], "title": "Spatial-Audio Interface", "modified": "2016-12-05T00:16:52.280Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "spatial-audio-interface"}, {"website": "", "description": "<p>Speakeasy is a community-based telephone service that connects multilingual volunteers, non-English speaking immigrants, and social service agencies.  </p>", "people": [], "title": "Speakeasy", "modified": "2016-12-05T00:16:52.303Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "speakeasy"}, {"website": "", "description": "<p>The ability to write has become completely identified with intellectual power, conflating the visual word with the nature of knowledge. One effect of graphocentrism is the limited path to literacy and learning. We propose some ways to re-value modes of composition that are closer to spontaneous oral language than to writing. We define a new term, spriting (speak + write = sprite), which serves some or all of the functions of writing (permanence, possibilities of editing, indexing, and scanning) but in a spoken modality, without the difficult transition to a different form of representation such as writing. We explore how spriting might support literacy learning and offer different ways to compose, using a design research intervention with novel spriting technology in two culturally and socio-economically diverse schools.  This work has implications for literacy learning, educational policy, writing software and speech dictation recognition technology, and Digital Talking Books (DTB) standards development.</p>", "people": ["papert@media.mit.edu"], "title": "Speaking on the Record", "modified": "2016-12-05T00:16:52.354Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-443D", "groups": ["future-of-learning"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "speaking-on-the-record"}, {"website": "", "description": "<p>We are adapting the video data collection and analysis technology derived from the Human Speechome Project for the retail sector through real-world deployments. We are developing strategies and tools for the analysis of dense, longitudinal video data to study behavior of and interaction between customers and employees in commercial retail settings. One key question in our study is how the architecture of a retail space affects customer activity and satisfaction, and what parameters in the design of a space are operant in this causal relationship.</p>", "people": ["dkroy@media.mit.edu", "decamp@media.mit.edu"], "title": "Speechome Video for Retail Analysis", "modified": "2016-12-05T00:16:52.428Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-441", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "speechome-video-for-retail-analysis"}, {"website": "", "description": "<p>spekter is an 8-bit analog input and output device capable of processing up to eight analog audio (ex: sensor) inputs simultaneously. The heart of the device is an off-the-shelf microprocessor core, featuring built-in Internet connectivity and a substantial amount of on-board memory. The intention is to create an easy and intuitive access to analog streams of data, such as audio. To this end, the device features an LED panel and a selector knob which allow the user to preview a value at any selected channel without the aid of a computer interface. On the flipside, a great degree of control over incoming data is afforded by a C-based programming environment that is interfaced to the device via serial connection. This combination provides for a powerful i/o tool intended as a means to streamline hardware development in the Aesthetics + Computation group.</p>", "people": [], "title": "spekter", "modified": "2016-12-05T00:16:52.403Z", "visibility": "PUBLIC", "start_on": "2001-12-31", "location": "E15-305A", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "spekter"}, {"website": "", "description": "<p>SpendTrend is a ReflectOn designed to allow its user to reflect on spending habits. Often we spend based on the momentary buying capability, without considering the long-term outcomes. Most would not think twice about a $4 Starbucks latte, but over a year this amount becomes non-trivial. Likewise, we often fail to think about savings in the long term. By making trends visible at the moment of purchase, SpendTrend attempts to make users mindful of their behavior and long-term goals. SpendTrend is built into a credit card, and has embedded processing and communication. The SpendTrend reader informs the card of the details of the purchases. With accurate, fine-grained information about purchases, the card then computes and display feedback, while also acting as a collection device for receipt data. The card harvests power directly from the reader, and has no explicit charging needs.</p>", "people": ["sajid@media.mit.edu", "pattie@media.mit.edu"], "title": "SpendTrend: Reflecting on Spending Habits", "modified": "2016-12-05T00:16:52.468Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "spendtrend-reflecting-on-spending-habits"}, {"website": "", "description": "<p>Control of quantum systems is becoming increasingly relevant in magnetic resonance imaging, quantum computation, and quantum instrumentation. We are investigating the application of techniques from classical control theory to the quantum domain. Demonstrating open loop control via a proof of principle nuclear magnetic resonance experiments, we are working toward broad applications of control techniques to quantum mechanical engineering.</p>", "people": ["neilg@media.mit.edu"], "title": "Spingineering", "modified": "2016-12-05T00:16:52.600Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2002-01-01", "slug": "spingineering"}, {"website": "", "description": "<p>Spinner is a Lab-wide sensor network platform designed to detect and capture fragmented events of human behavior that can be collected and sequenced into a cohesive narrative conveying a larger overall meaning. This project also looks at the development of parametric models of narrative that can be mapped on to sensor-detectable elements of human activity.</p>", "people": ["joep@media.mit.edu"], "title": "Spinner", "modified": "2016-12-05T00:16:52.622Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-351", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "spinner"}, {"website": "", "description": "<p>StackAR explores the augmentation of physical objects within a digital environment by abstracting interfaces from physical to virtual implementations. StackAR is a Lilypad Arduino shield that enables capacitive touch and light base communication with a tablet. When pressed against a screen, the functionality of StackAR extends into the digital environment, allowing the object to become augmented by the underlying display. This creates an augmented breadboard environment where virtual and physical components can be combined and prototyped in a more intuitive manner.</p>", "people": ["holtzman@media.mit.edu"], "title": "StackAR", "modified": "2016-12-05T00:16:52.665Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "stackar"}, {"website": "", "description": "<p>StoryBeads are an interactive wearable for constructing narratives through collecting, trading and sequencing images. Individual beads, labeled with metadata descriptions, act as containers for collections of images and related text. As images travel between users, the beads enable a transaction, which preserves the historical context, a small step toward memory of use.</p>", "people": ["barbara@media.mit.edu", "gid@media.mit.edu"], "title": "StoryBeads", "modified": "2016-12-05T00:16:52.753Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-427", "groups": ["interactive-cinema", "toys-of-tomorrow", "broadercasting"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "storybeads"}, {"website": "", "description": "<p>The Strata project explores the design of layered, electronically-augmented physical models that serve as tangible interfaces to specific dataspaces.  Strata's first application is an interface that embodies the physical structure of the Media Lab's home building.  This takes the form of a five-layer, translucent acrylic model woven with embedded lights, sensors, and computation.  This tangible interface will allow the building's networking and facilities staff to access, monitor, and control a dynamic representation of the building's physical and digital infrastructure.</p>", "people": ["ishii@media.mit.edu", "jpatten@media.mit.edu"], "title": "Strata", "modified": "2016-12-05T00:16:52.843Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-441", "groups": ["tangible-media", "personal-fabrication"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "strata"}, {"website": "", "description": "<p>Storytelling Engines are computer programs that manipulate media objects based on their knowledge of content, rules for story progression, and potential for audience interaction. They can be designed to configure a presentation on-the-fly (selecting one material at a time), or they can be designed to select, assemble, and present a composite stream of scenes. Example engines include: ConTour, a system that uses the principle of a spreading activation network combined with positive-to-negative keyword weightings to locate the next relevant content segment; and Agent Stories, a system that assembles a story presentation based on the preferred characteristics of a storytelling agent.</p>", "people": ["gid@media.mit.edu"], "title": "Storytelling Engines", "modified": "2016-12-05T00:16:52.923Z", "visibility": "PUBLIC", "start_on": "1986-01-01", "location": "E15-489", "groups": ["interactive-cinema", "gray-matters"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "storytelling-engines"}, {"website": "", "description": "<p>We are collecting ever more data about our environment, from above using high-end satellite networks, as well as from below with the cheap sensors built into our cellphones. This coverage has the potential to drastically change our perception of the fragile world we live in, and to make sense of such data we commonly use visualization techniques, enabling public discourse and analysis. However, when it comes to understanding complex Earth systems and causality chains\ufffdfor instance the dramatic events leading up to the 2011 environmental disaster in Japan\ufffdcan we find more engaging ways of interacting with geospatial data that go beyond static visualization? Stratarium is an interaction space that integrates various interface techniques to allow users to explore a layered model of our Earth in the form of part-physical, part-digital representations, and to understand the relationships between the planet\ufffds surface shape and sensor data resulting from natural phenomena.</p>", "people": ["ishii@media.mit.edu"], "title": "Stratarium", "modified": "2016-12-05T00:16:52.955Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "stratarium"}, {"website": "", "description": "<p>Stress OutSourced (SOS) is a peer-to-peer network that allows anonymous users to send each other therapeutic massages to relieve stress. By applying the emerging concept of crowdsourcing to haptic therapy, SOS brings physical and affective dimensions to our already networked lifestyle while preserving the privacy of its members. SOS is an exploration and illustration of a new field of haptic social networking.</p>", "people": ["x_x@media.mit.edu", "ishii@media.mit.edu"], "title": "Stress OutSourced", "modified": "2016-12-05T00:16:52.985Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "stress-outsourced"}, {"website": "", "description": "<p>New, streaming, sound-encoding technologies are starting to fill the demand for low-bandwidth transmission of audio over the Internet, but such information-theoretic encoding schemes often don't allow for high-quality audio transmission or client-side sound manipulation. We are developing NetSound, a structured audio coding/decoding scheme (sort of a \"Postscript for audio\") that allows sound descriptors, algorithms, models, and schedulers to be sent efficiently over the Internet, and then altered, personalized, and reconstructed in real time at the receiver site with high fidelity. A set of tools based on this concept has been proposed by the Music, Mind and Machine group and accepted by the MPEG Consortium as part of the MPEG-4 international standard. We continue to develop and refine Structured Audio techniques in collaboration with researchers and industrial labs around the world.</p>", "people": ["bv@media.mit.edu"], "title": "Structured Audio", "modified": "2016-12-05T00:16:53.019Z", "visibility": "PUBLIC", "start_on": "1996-01-01", "location": "E15-401", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "structured-audio"}, {"website": "", "description": "<p>Swamped! is an interactive experience in which instrumented plush toys are  used as a tangible, iconic interface for directing autonomous animated  characters. Each character has a distinct personality and decides in real  time what it should do based on its perception of its environment, its  motivational and emotional state, and input from its \"conscience,\" the  guest. By manipulating a stuffed animal corresponding to the character the  guest can influence how a given character acts and feels.    For example, the guest could have the character wave at another character  by waving its arm, make it squawk by squeezing it, or direct the  character's attention by moving the stuffed animal's head. The characters  incorporate a novel model of behavior and emotion, multi-target motion  interpolation and new techniques for real time graphics. Automatic camera  and lighting control helps reveal the emotional content of each scene.    By combining research in autonomous character design, automatic camera  control, sympathetic interfaces and action interpretation, Swamped! seeks  to create an evocative and novel experience.\n</p>", "people": [], "title": "SWAMPED!", "modified": "2016-12-05T00:16:53.081Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "1998-12-30", "slug": "swamped"}, {"website": "", "description": "<p>Super Cilia Skin is a multi-modal interactive interface, conceived as a computationally enhanced membrane coupling tactile-kinesthetic input with tactile and visual output. An array of individual actuators (cilia) uses changes in orientation to display images or physical gestures as physical or tactile information.</p>", "people": ["ishii@media.mit.edu"], "title": "Super Cilia Skin", "modified": "2016-12-05T00:16:53.048Z", "visibility": "PUBLIC", "start_on": "2002-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "super-cilia-skin"}, {"website": "", "description": "<p>Current methods to assess depression and then ultimately select appropriate treatment have many limitations. They are usually based on having a clinician rate scales, which were developed in the 1960s. Their main drawbacks are lack of objectivity, being symptom-based and not preventative, and requiring accurate communication. This work explores new technology to assess depression, including its increase or decrease, in an automatic, more objective, pre-symptomatic, and cost-effective way using wearable sensors and smart phones for 24/7 monitoring of different personal parameters such as physiological data, voice characteristics, sleep, and social interaction. We aim to enable early diagnosis of depression, prevention of depression, assessment of depression for people who cannot communicate, better assignment of a treatment, early detection of treatment remission and response, and anticipation of post-treatment relapse or recovery.</p>", "people": ["picard@media.mit.edu", "sfedor@media.mit.edu"], "title": "Objective Asessment of Depression and its Improvement", "modified": "2018-05-01T21:02:35.703Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["affective-computing", "advancing-wellbeing"], "published": true, "active": false, "end_on": "2019-01-01", "slug": "objective-asessment-of-depression-and-its-improvement"}, {"website": "", "description": "<p>Syncwalk is a platform for sound design in a geographic space: you place songs from your music library onto a map, and then you go out into the world and, as you walk from one place to another, you hear the songs you\ufffdve placed along the way. The project explores different ways to design and share experiences centered around, but not wholly determined by, the spaces we transit and inhabit.</p>", "people": ["tod@media.mit.edu"], "title": "Syncwalk", "modified": "2016-12-05T00:16:53.224Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "syncwalk"}, {"website": "", "description": "<p>This is a collection of smaller projects currently being pursued by the Synthetic Characters group. (1) C4.3 is the latest iteration of our toolkit for building adaptive and expressive synthetic characters. All of our projects are done using this Java- and C++-based toolkit, and the toolkit evolves as the lessons from the projects are incorporated into it. The system runs on Mac OS X and Windows 2000. (2) New Languages from Old are extensions to the Java programming language and the Java runtime custom-made for the problems faced by behavior designers. (3) Sequence, Sequences and More Sequences is a unification and diversification of our current sequence-learning strategies for new applications to behavior creation and motion understanding. (4) Stochastic Convolution Rendering creates new, painterly, expressive, and emphatically non-photorealistic graphics within our C4/magic graphics system. (5) Experiments in Intelligent Form are early sketches in force-field-based motor representations for synthetic characters, found far away from engineering concerns for optimality or the aesthetic concerns of 3-D computer graphics. (6) Expressive Inverse Kinematics: while the theory of inverse kinematics is well-understood, its application in real-time settings for synthetic characters that must always \"move in character\" remains problematic. In this project we are investigating new representations and approaches to inverse kinematics that allow the algorithm to take advantage of previously sampled motion. The hope is that by doing so, the algorithm can infer joint constraints, as well as the quality of movement that defines the character.</p>", "people": [], "title": "Synthetic Characters Skunk Works", "modified": "2016-12-05T00:16:53.249Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-441", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "synthetic-characters-skunk-works"}, {"website": "", "description": "<p>Over two million speech-disabled people in the US alone rely on Augmentative and Alternative Communication (AAC) devices to help express their everyday thoughts, needs, and ideas. Some of these devices now support extensible vocabularies; however, the means of extension are too cumbersome to permit new symbols to be defined on demand, for immediate use in an utterance. On the time scale of a conversation, this shortcoming limits AAC users to a relatively small, fixed vocabulary. Symbolsnare is a gaze-speech interface enabling an AAC user to instantly extend the vocabulary of his or her device. New symbols are captured from the environment and imported into the user's AAC using an eye-tracking digital capture system triggered by button presses or simple vocalisations. By opening the vocabulary of AAC devices to the world, Symbolsnare empowers the speech-disabled to discuss anything they can see.</p>", "people": ["dkroy@media.mit.edu"], "title": "Symbolsnare", "modified": "2016-12-05T00:16:53.200Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-4FL", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "symbolsnare"}, {"website": "", "description": "<p>We are working on a new physical interactive system that makes it easier for kids to explore the behavior of dynamic systems. System Blocks, a set of computationally enhanced children's blocks made of wood and electronics, enables children to create and interact with systems that simulate real-life dynamic behavior, such as population growth or the delicate equilibrium of an ecosystem.</p>", "people": ["mres@media.mit.edu"], "title": "System Blocks", "modified": "2016-12-05T00:16:53.328Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-120B", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "system-blocks"}, {"website": "", "description": "<p>The Toy Interface Construction Kit Learning Environment (T.I.C.K.L.E.) is a universal construction kit for the rest of us. It doesn't require 3D printers or CAD skills. Instead, it's a DIY social process for creating construction interoperability.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "jnmatias@media.mit.edu", "ericr@media.mit.edu"], "title": "T.I.C.K.L.E.", "modified": "2016-12-05T00:16:53.467Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "tickle"}, {"website": "", "description": "<p>T(ether) is a novel spatially aware display that supports intuitive interaction with volumetric data. The display acts as a window affording users a perspective view of three-dimensional data through tracking of head position and orientation. T(ether) creates a 1:1 mapping between real and virtual coordinate space, allowing immersive exploration of the joint domain. Our system creates a shared workspace in which co-located or remote users can collaborate in both the real and virtual worlds. The system allows input through capacitive touch on the display with a motion-tracked glove. When placed behind the display, the user\ufffds hand extends into the virtual world, enabling the user to interact with objects directly.</p>", "people": ["dlakatos@media.mit.edu", "ishii@media.mit.edu"], "title": "T(ether)", "modified": "2016-12-05T00:16:53.367Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["viral-communications", "tangible-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "tether"}, {"website": "", "description": "<p>T+1 is an application that creates an iterative structure  to help groups organize their interests and schedules.  Users receive instructions and send their personal information through mobile devices at discretized time steps, orchestrated by a unique, adaptive scheduling engine. At each time-step, T+1 takes several relevant factors of human interactions (such as participants' interests, opinions, locations, and partner matching schedules), and then computes and optimizes the structure and format of a group interactions for the next interval. T+1 facilitates consensus formation, better group dynamics, and more engaging user experiences by using a clearly visible and comprehensible process. We are planning to deploy the platform for both academic and political discussions, and will analyze how user opinions and interests evolve in time to understand its efficacy.</p>", "people": ["borovoy@media.mit.edu", "lip@media.mit.edu"], "title": "T+1", "modified": "2016-12-05T00:16:53.397Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "t1"}, {"website": "", "description": "<p>A low-cost UV-Vis spectrometer is being constructed for widespread use in developing nations. Consumer digital cameras have driven down the price of digital imaging chips; the spectrometer uses digital imaging to obtain a spectrum. This image is then sent to a computer and processed to produce an absorbance spectrum. Currently, the spectrometer distinguishes between colors and varying concentrations of test material and produces spectral data consistent with a commercial UV-Vis spectrometer. When finished, the spectrometer will be capable of food analysis and chemical and biochemical experiments. Recent and future work includes the development of IR spectrometers, FT-IR spectrometers, Raman spectrometers, and mass spectrometers.</p>", "people": ["neilg@media.mit.edu"], "title": "Table-Top Chemistry", "modified": "2016-12-05T00:16:53.507Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "table-top-chemistry"}, {"website": "", "description": "<p>Traditional NMR spectrometers are powerful and versatile, but cost hundreds of thousands of dollars and are complicated to operate. We are working on designing a table-top NMR system based on small permanent magnets and specialized electronics with a target price of around a thousand dollars. The original motivation for this was table-top quantum computing, but simple inexpensive NMR spectrometers would be broadly useful in medicine and other fields.</p>", "people": ["neilg@media.mit.edu"], "title": "Table-Top NMR", "modified": "2016-12-05T00:16:53.532Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-023", "groups": ["physics-and-media", "silicon-biology", "personal-fabrication"], "published": true, "active": false, "end_on": "2003-12-30", "slug": "table-top-nmr"}, {"website": "", "description": "", "people": ["narula@media.mit.edu"], "title": "Auditable Logs", "modified": "2017-04-05T05:23:05.553Z", "visibility": "LAB-INSIDERS", "start_on": "2016-09-05", "location": "", "groups": ["digital-currency-initiative-dci"], "published": false, "active": false, "end_on": "2017-05-26", "slug": "auditable-logs"}, {"website": "", "description": "<p>taggyMedia is an online interactive system that provides easier and more interesting ways of media search and exploration. By constucting a semantic network using a social tagging scheme, we are able to design a system that provides better tagging interface, and a variety of ways of exploration based on concepts, emotions, scenarios, etc. The system improves the current \"keyword-matching\" searching scheme to a much more intuitive and explorative one.</p>", "people": ["pattie@media.mit.edu"], "title": "taggyMedia", "modified": "2016-12-05T00:16:53.625Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "taggymedia"}, {"website": "", "description": "<p>These \"autonomous sensing devices,\" which combine sensors as information providers with wireless communication systems, have enormous implications for end users, and therefore tremendous market potential in key sectors such as healthcare, education, food quality control, and environmental monitoring. This National Centre for Sensor Research (NCSR)-Media Lab Europe (MLE)-Higher Education Authority of Ireland (HEA) project focuses on education (sensors integrated with LEGO Mindstorms and Crickets) and the environment (such as wireless networked data loggers for oxygen, carbon dioxide, temperature, and PH). (Funded by the Higher Education Authority of Ireland.)</p>", "people": [], "title": "Sensors for Science Education", "modified": "2016-12-05T00:16:53.696Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "MLE", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "sensors-for-science-education"}, {"website": "", "description": "<p>TalkBack is an answering machine with a little more smarts. It allows you to respond to your messages naturally as you get them. Next time you have a long list of messages on your answering machine, you do not have to remember all of them and respond to each one separately. Instead, you can record your responses as they come and send your responses back to the person who left you the message.</p>", "people": ["geek@media.mit.edu"], "title": "TalkBack", "modified": "2016-12-05T00:16:53.754Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "talkback"}, {"website": "", "description": "<p>The talking trivet and oven mitt are digital enhancements of common household objects. By communicating important messages about a food's temperature, they make far better tools than a simple thermometer. The trivet uses an embedded computer to read time and temperature. It tells a user whether food needs re-warming (is under 90 degrees F); is hot and ready to take out of the oven and eat, but not overcooked; or is so hot that it will catch on fire (above 454 degrees F). This high-temperature alert provides an added safety feature for the home.</p>", "people": [], "title": "Talking Trivet/Oven Mitt", "modified": "2016-12-05T00:16:53.663Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-320", "groups": ["context-aware-computing", "counter-intelligence"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "talking-trivetoven-mitt"}, {"website": "", "description": "<p>Takeover TV heralds a new era of bar patronage where you and your like-minded friends are in charge of the screens. When you check in at a location, your likes and dislikes automatically influence what is being shown on local displays.  If you want more control, start a vote to pick a new show using your beer glass\ufffdor your iPhone. Create season-premiere nights for your favorite shows, or work with friends to define the types of shows that play at your local bars. Sick of watching sports?  Assemble enough fans of your favorite show at the local pub and take over the TV.</p>", "people": ["holtzman@media.mit.edu"], "title": "Takeover TV", "modified": "2016-12-05T00:16:53.721Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "takeover-tv"}, {"website": "", "description": "<p>What if the taste of your toothpaste told you the weather? Leave your smart phone weather app in your pocket.  Tastes Like Rain dynamically alters the flavor and color of your morning toothpaste to give you today's temperature and weather. No LCD required.</p>", "people": ["holtzman@media.mit.edu"], "title": "Tastes Like Rain", "modified": "2016-12-05T00:16:53.782Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "tastes-like-rain"}, {"website": "", "description": "<p>Tangible Viewpoints is a storytelling application for interactive character-driven narratives that was designed for the TViews platform. The different segments of a multiple-point-of-view story are organized according to the character viewpoint they represent, as well as their place in the overall narrative. These segments can consist of various types of media (video, audio, images, text), and can present character development, action, and location with as much complexity as any scene of a film or chapter in a book. Graspable pawns are used to navigate through the multiple-viewpoint story. When a pawn is placed on the sensing surface, the story segments associated with its character's point-of-view are projected around it. Users select segments to be displayed on a nearby monitor, causing the narrative to advance and new segments to become available. An aura is also projected around each pawn to give a visual representation of the prominence of that viewpoint in the current telling of the story. Changes in the story space are reflected by dynamic changes in the projected graphics.</p>", "people": ["ishii@media.mit.edu", "gid@media.mit.edu"], "title": "Tangible Viewpoints", "modified": "2016-12-05T00:16:53.922Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-368", "groups": ["tangible-media", "interactive-cinema"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "tangible-viewpoints"}, {"website": "", "description": "<p>The Tangible StorySims project creates narrative simulation games that can be played out on the TViews display and interaction platform. The first of these, entitled \"The Blackjack Hack,\" is based on the story of a group of MIT students who made millions playing blackjack in casinos around the country. Viewers control the spotters on a blackjack card-counting team, and try to maximize their winnings as they engage in narrative play and social interaction around the platform.</p>", "people": ["gid@media.mit.edu"], "title": "Tangible StorySims", "modified": "2016-12-05T00:16:53.893Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "tangible-storysims"}, {"website": "", "description": "<p>New technologies for learning need to be accompanied by new ways of instruction. Teacher's LAB is an effort to bring digital technologies to teachers, to help them plan classroom activities and design learning interactions. Rather than making \"lesson plans\" that focus on procedural steps, Teacher's LAB allows teachers to create planning documents that focus on student interactions. What are good questions to ask students? How can they be encouraged to conduct self-directed learning? The hope is that such tools will encourage teachers to adopt new practices for teaching and instruction.</p>", "people": [], "title": "Teacher's LAB", "modified": "2016-12-05T00:16:54.051Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-001", "groups": ["critical-computing"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "teachers-lab"}, {"website": "", "description": "<p>Many of us who enjoy what we do were motivated by an event or person that inspired us to learn, challenge, and question. We are building applications that attempt to convey that epiphany through cooperative learning and exploration. This year's work centers around ChessMaze, a strategic thinking game built in collaboration with International Chess Grand Master Maurice Ashley. ChessMaze challenges novices to learn the game as a team, and to develop generalizable strategic thinking. While playing their solo games, individual players contribute to a team database, where the same moves are grouped together, sorted based on popularity, and presented back to the community of players on a heat map.</p>", "people": ["lip@media.mit.edu"], "title": "Teamification: ChessMaze", "modified": "2016-12-05T00:16:54.080Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "teamification-chessmaze"}, {"website": "", "description": "<p>As a direct way to visualize the impact of who you are (and what you eat), the electro-optic response of this display is controlled by the risk-factor levels in the bloodstream as an in situ form of risk-factor monitoring. Would you eat as many cheeseburgers if you could really SEE what they do to you?</p>", "people": ["mlj@media.mit.edu"], "title": "Tattooed Displays", "modified": "2016-12-05T00:16:54.028Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2008-01-01", "slug": "tattooed-displays"}, {"website": "", "description": "<p>Musical interaction occurs through one of three modalities: performing, listening, or composing. This project investigates new ways in which computers and associated technologies may be applied to music education through each of these modalities. This project will apply current pedagogic theory and practice to the music toys developed for the \"Toy Symphony\" project. We will perform a major assessment of these instruments from an educational perspective, including design and implementation of workshop activities with children using the instruments, and evaluation of the learning outcomes in a variety of educational and cultural settings. (Funded by the Higher Education Authority of Ireland.)</p>", "people": ["tod@media.mit.edu"], "title": "Technology and Creative Music Education", "modified": "2016-12-05T00:16:54.106Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-441", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "technology-and-creative-music-education"}, {"website": "", "description": "<p>The Telecorrelator allows people to view multiple perspectives of real-time news events aligned by content as well as time. It reveals emphasis and timing and allows participants to discover points of view and important events. Currently, the Telecorrelator aligns broadcast news from four broadcast sources to visually reveal the emphasis and time  allocation they devote to a succession of events.</p>", "people": ["lip@media.mit.edu"], "title": "Telecorrelator", "modified": "2016-12-05T00:16:54.212Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "telecorrelator"}, {"website": "", "description": "<p>Establishing telecenters (community computer centers that often provide a range of services) in developing countries requires careful evaluation of alternative hardware configurations, power sources and telecommunication services. Quite often the goal is to find a configuration whose recurrent cost can be sustained by the target community. The Telecenter Cost Estimator is a visual tool for exploring the impact of alternative telecenter configurations on capital and ongoing costs.</p>", "people": [], "title": "Telecenter Cost Estimator", "modified": "2016-12-05T00:16:54.241Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-391", "groups": ["edevelopment", "e-markets"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "telecenter-cost-estimator"}, {"website": "", "description": "<p>Mediated communication between remote social spaces is a relatively new concept. One current example of this interaction is video conferencing among people within the same organization. Large-scale video conferencing walls have begun to appear in public or semi-public areas such as workplace lobbies and kitchens. These connections provide a link via audio and/or video to another space within the organization. When placed in these spaces, they are often designed for casual encounters among people within that community. Thus far, communicating via these systems has not met expectations. We are exploring a different approach to linking spaces through the use of what we are defining as a social catalyst. These catalysts are incorporated into our installation, Telemurals. An ethnography study of Telemurals will be conducted between two graduate dormitories to see how the catalysts affect interaction.</p>", "people": ["judith@media.mit.edu"], "title": "Telemurals", "modified": "2016-12-05T00:16:54.269Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "telemurals"}, {"website": "", "description": "<p>We are developing a system that allows a group of distributed participants to direct the actions of a remote actor. This remote \"TeleActor\" is a telepresence agent, taking commands from the directors and sending audio and video back to them from the remote location. Research goals include designing interfaces for large groups of cooperating and competing individuals, developing complex telepresence command and feedback systems, and advancing our understanding of the interactions between the TeleActor and the real world. Our current focus is on TeleActor as a media agent for news broadcasting.</p>", "people": ["judith@media.mit.edu", "monster@media.mit.edu"], "title": "TeleAction", "modified": "2016-12-05T00:16:54.178Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-449", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "teleaction"}, {"website": "", "description": "<p>TeleStory is a tangible interface for children to learn vocabulary through gestural experimentation. Children pair characters in a story and influence their actions through gestural input on the Siftables platform. TeleStory allows children to influence the design of a television animation and learn the meanings of words by triggering actions in the narrative.</p>", "people": ["pattie@media.mit.edu"], "title": "TeleStory", "modified": "2016-12-05T00:16:54.456Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "telestory"}, {"website": "", "description": "<p>We are exploring ways to build sensors using a variety of crafting and needlework techniques, using affordable and available materials such as conductive threads, yarns, fabrics, and paints. These materials are used to sew, knit, crochet, embroider, and laminate, creating a range of textile-based sensors.</p>", "people": ["leah@media.mit.edu"], "title": "Textile Sensors", "modified": "2016-12-05T00:16:54.611Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "textile-sensors"}, {"website": "", "description": "<p>Affective Remixer is a real-time music-arranging system that reacts to immediate affective cues from a listener. Data was collected on the potential of certain musical dimensions to elicit change in a listener\ufffds affective state using sound files created explicitly for the experiment through composition/production, segmentation, and re-assembly of music along these dimensions. Based on listener data, a probabilistic state transition model was developed to infer the listener\ufffds current affective state. A second model was made that would select music segments and re-arrange ('re-mix') them to induce a target affective state. </p>", "people": ["picard@media.mit.edu"], "title": "The Affective Remixer: Personalized Music Arranging", "modified": "2016-12-05T00:16:54.709Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "the-affective-remixer-personalized-music-arranging"}, {"website": "", "description": "<p>Fifty years ago, Doug Engelbart created a conceptual framework for augmenting human intellect in the context of problem-solving. We expand upon Engelbart's framework and use his concepts of process hierarchies and artifact augmentation for the design of personal intelligence augmentation (IA) systems within the domains of memory,  decision making, motivation, and mood. We propose a systematic design methodology for personal IA devices, to organize existing IA research within a logical framework, and to uncover underexplored areas of IA that could benefit from the invention of new artifacts.</p>", "people": ["pattie@media.mit.edu"], "title": "The Design of Artifacts for Augmenting Intellect", "modified": "2016-12-05T00:16:54.740Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "the-design-of-artifacts-for-augmenting-intellect"}, {"website": "", "description": "<p>The Alphabots are trans-fictional(xF) mobile and modular semi-autonomous robotic symbol set characters designed to play with preschool aged children (three to six years old).\ufffdIn support of early development goals (literacy, numeracy and shape recognition) educators and parents can take an active role in co-designing playful learning interactions both on and off-screen.</p>", "people": ["cynthiab@media.mit.edu"], "title": "The Alphabots", "modified": "2016-12-05T00:16:54.768Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "the-alphabots"}, {"website": "", "description": "<p>The future of mobile communications will certainly be one where devices will opportunistically connect to a variety of networks, obtaining and providing services on the fly. Our portable devices will carry our identity and act as intermediaries between users and their rich environments. The Amulet embodies this vision. We are developing this new device which instantiates a user's identity and provides a computing model that allows the device to securely exploit its context and surrounding infrastructure in an open, secure, and adaptable manner.</p>", "people": ["lip@media.mit.edu"], "title": "The Amulet", "modified": "2016-12-05T00:16:54.838Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "the-amulet"}, {"website": "", "description": "<p>The Chandelier is a large-scale robotic musical instrument that is being developed for \"Death and the Powers.\" Its 48 strings can be actuated both through powerful electromagnets, and tactilely (plucked like a harp or bowed like a cello). With the strings driven by electromagnets, the tactile player can also repeatedly damp strings or create overtones by carefully touching the strings' anti-nodes, creating a new intimacy between players, who play not just the same instrument, but the same strings. The Chandelier is composed of many systems\ufffdlogic for control of music and lighting, networked servers, and playable interfaces\ufffdall built around an elegant, articulated skeletal structure which allows changes to the length, angle, and tensions of the strings. We are currently experimenting with playing it through new types of interfaces to take advantage of its unusual tuning and sonorities.</p>", "people": ["tod@media.mit.edu"], "title": "The Chandelier", "modified": "2016-12-05T00:16:54.899Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "the-chandelier"}, {"website": "", "description": "<p>This project is exploring instrumenting everyone (The Flying Karamazov Brothers) and everything (their juggling clubs) in a large space (a stage), in order to let them work collaboratively as a giant musical instrument, display information in their juggling, and allow them to juggle in different worlds.</p>", "people": ["monster@media.mit.edu", "neilg@media.mit.edu"], "title": "The Flying Karamazov Brothers", "modified": "2016-12-05T00:16:55.016Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "the-flying-karamazov-brothers"}, {"website": "", "description": "<p>Two robot arms are in constant motion and hard at work. From a distance, they can be seen considering their tasks, communicating with each other, and struggling to make sense of their abstract mission. Participants are encouraged to approach the hands to engage in a dialog with the robots to offer assistance or advice. The project demonstrates a chatter system to simulate affective conversation and a parametric animation engine to provide dynamic, autonomous character-driven movement in the robots. </p>", "people": ["cynthiab@media.mit.edu", "dnunez@media.mit.edu"], "title": "The Helping Hands", "modified": "2016-12-05T00:16:55.045Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "the-helping-hands"}, {"website": "", "description": "<p>The Feast is an immersive, multi-sensory experience in four acts. Each act explores a different theme of eating, from mysterious to playful to communal. The Feast is not a normal dinner where people passively eat what's in front of them. In the late hours of the night, we will explore the active, creative role of the diner and build community around food and companionship.</p>", "people": ["patorpey@media.mit.edu", "pip@media.mit.edu", "tod@media.mit.edu", "benb@media.mit.edu"], "title": "The Other Feast", "modified": "2016-12-05T00:16:55.236Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "the-other-feast"}, {"website": "", "description": "<p>Telepresent robots are often pitched as a technology to extend the influence of those who already have money and power. We want to use robotic telepresence for the public good\ufffdbroadening access, supporting public interest reporting, and funding access initiatives.</p>", "people": ["ethanz@media.mit.edu", "jnmatias@media.mit.edu"], "title": "The People's Bot", "modified": "2016-12-05T00:16:55.263Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "the-peoples-bot"}, {"website": "", "description": "<p>In the spirit of Richard Feynman's playful approach to the world of ideas and things, we are developing a set of tools to enable everyone to have \"The Pleasure of Finding Things Out\" through rich Constructionist activities, in which one \"finds things out\" by \"making things\" they care about. Children and adults alike can use Crickets, LogoChips, and Towers to embed computational \"intelligence\" into the world of things, and in the process develop new ways of making and thinking about behaviors and functions. They can also use design tools like LaserLogo to create and shape the things in the world around them in new and more effective ways using computer-controlled, rapid-prototyping tools, such as laser-cutters, 3-D printers, machining tools, and embroidery, sewing, and pattern making machines. With these tools, the next generation will treat the whole world as a medium of expression: they will design patterns to put onto the surfaces of things, cut things out of a wide range of materials, and embed intelligence in them. In the process, they will discover new ways of mixing form and function by actively meshing the worlds of physical and computational ideas and artifacts together.</p>", "people": [], "title": "The Pleasure of Making Things", "modified": "2016-12-05T00:16:55.288Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-344", "groups": ["personal-fabrication"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "the-pleasure-of-making-things"}, {"website": "", "description": "<p>The Reflectory is a storytelling and learning interface for children whose goal is to provide an environment in which children generate their own educational content by sharing personal written stories with a peer. Two children use the Reflectory to write and to exchange letters that together spark the appearance of an historical biography relevant to both children's stories. To create a feeling of privacy and security, the Reflectory's touch-sensitive screen has been encased in a traditional hard-cover volume. Opening this innocuous-looking book reveals a visually rich and engaging screen onto which children write their stories directly, using a stylus. The Reflectory will continue to develop as we work with children to discover how technologies that encourage and enhance collaborative storytelling can be powerful tools for self-expression, personalized learning, and language development.</p>", "people": [], "title": "The Reflectory", "modified": "2016-12-05T00:16:55.310Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-320", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "the-reflectory"}, {"website": "", "description": "<p>Like many institutions, MIT is thinking about how to involve the MIT community in its space in Second Life. The Projects is a collaborative building system in Second Life, consisting of configurable pods owned by MIT community members that connect together into a high-level structure. The design of these pods and their interconnection has a number of important challenges unique to working in a three-dimensional, virtual environment like Second Life: presence, history, search, and navigation. Based on sensors embedded in the pods and rules about how pods can be connected, we can provide socially based services that will form the backbone of this virtual architecture.</p>", "people": ["judith@media.mit.edu"], "title": "The Projects", "modified": "2016-12-05T00:16:55.409Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "the-projects"}, {"website": "", "description": "<p>While technologists scramble to develop technologies for production and storage of environmentally friendly electricity, we must consider our personal roles in conserving energy. Thighmaster balances comfort and discomfort to achieve sustainable change. In addition to potentially decreasing a user's energy use, Thighmaster can also relieve the less easily measured\ufffdbut no less real\ufffdfeeling of individual powerlessness in the face of accelerated climate change. The system consists of a personal techno-garter, inspired by the \"Opus Dei\" cilice popularized in Dan Brown's novel The DaVinci Code, worn on the thigh, that communicates wirelessly to a set of low-power sensors measuring the wearer's personal energy consumption. If the wearer's electricity use exceeds a certain limit, the device plunges stainless-steel thorns into the thigh to remind users of both their complicity in the planet's demise and mortality.</p>", "people": ["csik@media.mit.edu"], "title": "Thighmaster", "modified": "2016-12-05T00:16:55.569Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-020D", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "thighmaster"}, {"website": "", "description": "<p>Search engines often return millions of results to a query. Organizing these results is a challenge, particularly for visual imagery. Search engines typically use textual annotations rather than the visual characteristics of the images to perform the search. ThoughtSort uses gaze detection and inferred points of interest to dynamically adjust the results of a search query. The user implicitly steers the system by showing more visual interest in some results than others. With ThoughtSort, search becomes a more dynamic experience as results self-adjust before the user's eyes. This application is part of a framework which aims to provide developers with the necessary tools to create dynamic and considerate content that can adjust to the natural responses of the user.</p>", "people": ["holtzman@media.mit.edu"], "title": "ThoughtSort", "modified": "2016-12-05T00:16:55.687Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "thoughtsort"}, {"website": "", "description": "<p>Much social interaction goes on at a door's threshold: you poke your head in for a quick chat with a colleague, or you pass through to enter a meeting or to follow up a previous conversation. Sensors and a voice recognition system outside and inside the digitally enhanced threshold indicate a person's approach and identity. This \"smart\" threshold can also be used to help manage meeting size or meeting scheduling. It does so by using electronically enhanced doormats that would know and communicate how many people enter or leave a room. Voice interface can identify visitors and reschedule them if the room's occupant is not there. A graphical interface in the room can let the room's owner approve the entrance of a visitor.</p>", "people": [], "title": "Threshold", "modified": "2016-12-05T00:16:55.711Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2001-09-01", "slug": "threshold"}, {"website": "", "description": "<p>More than a billion people a month visit YouTube to watch videos. Sometimes, those billion people watch the same video. What We Watch is a browser for trending YouTube videos. Some videos trend in a single country, and some find regional audiences.  Others spread across borders of language, culture, and nation to reach a global audience.  What We We watch lets us visualize and explore the connections between countries based on their video viewing habits.</p><p><a href=\"http://whatwewatch.mediameter.org/\">http://whatwewatch.mediameter.org/</a><br></p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "elplatt@media.mit.edu"], "title": "What We Watch", "modified": "2016-12-05T00:16:55.775Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2016-01-01", "slug": "what-we-watch"}, {"website": "", "description": "<p>The Tiles were a construction kit, in which powerful computation and communication were built into kid-scale blocks. Children experimented with dynamic patterns, wrote programs that moved from Tile to Tile, and  created rich arrangements of overlapped atoms and bits. This project grew out of our research into mobile code approaches to network programming. Our  goal was to create \"communities of toys,\" creating new possibilities for interactive play. \n</p>", "people": ["mres@media.mit.edu"], "title": "Tiles", "modified": "2016-12-05T00:16:55.873Z", "visibility": "PUBLIC", "start_on": "1995-01-01", "location": "E15-120h", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "1999-09-01", "slug": "tiles"}, {"website": "", "description": "<p>TinyProjector is a very small portable character projector, based on inexpensive laser diodes, projecting a single line of text onto nearby walls and tables. TinyProjector is useful for projecting text from portable and wearable devices such as cell phones and PocketPCs that will be connected, for example, via a wireless serial connection.</p>", "people": ["geek@media.mit.edu", "stefanm@media.mit.edu"], "title": "TinyProjector", "modified": "2016-12-05T00:16:55.921Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "tinyprojector"}, {"website": "", "description": "<p>Distributed meetings present a set of interesting challenges to staying engaged and involved. Because one person speaks at a time, it is easy (particularly for remote participants) to disengage from the meeting undetected. However, non-speaking roles in a meeting can be just as important as speaking ones, and if we could give non-speaking participants ways to contribute, we could help support better-run meetings of all kinds. Tin Can collects background tasks like taking notes, managing the agenda, sharing relevant content, and tracking to-dos in a distributed interface that uses meeting participants' phones and laptops as input devices, and represents current meeting activities on an iPad in the center of the table in each meeting location. By publicly representing these background processes, we provide meeting attendees with new ways to contribute and be recognized for their non-verbal participation.</p>", "people": ["geek@media.mit.edu"], "title": "Tin Can", "modified": "2016-12-05T00:16:55.897Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "tin-can"}, {"website": "", "description": "<p>Classroom discussions may not seem like an environment that needs a new kind of supporting technology. But we've found that augmenting classroom discussions with an iPad-based environment to help promote discussion, keep track of current and future discussion topics, and create a shared record of class keeps students engaged and involved with discussion topics, and helps restart the discussion when conversation lags. Contrary to what you might expect, having another discussion venue doesn't seem to add to student distraction; rather it tends to focus distracted students on this backchannel discussion. For the instructor, our system offers powerful insights into the engagement and interests of students who tend to speak less in class, which in turn can empower less-active students to contribute in a venue in which they feel more comfortable.</p>", "people": ["geek@media.mit.edu"], "title": "Tin Can Classroom", "modified": "2016-12-05T00:16:55.989Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "tin-can-classroom"}, {"website": "", "description": "<p>Topobo is a 3D constructive assembly system embedded with kinetic memory\ufffdthe ability to record and play back physical motion. Unique among modeling systems is Topobo\ufffds coincident physical input and output behaviors. By snapping together a combination of passive (static) and active (motorized) components, users can quickly assemble dynamic, biomorphic forms such as animals and skeletons, animate those forms by pushing, pulling, and twisting them, and observe the system repeatedly playing back those motions. For example, a dog can be constructed and then taught to gesture and walk by twisting its body and legs. The dog will then repeat those movements.</p>", "people": ["ishii@media.mit.edu"], "title": "Topobo", "modified": "2016-12-05T00:16:56.107Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "topobo"}, {"website": "", "description": "<p>The Touch-Phone was developed to explore the use of objects to mediate the emotional exchange in interpersonal communication. Through an abstract visualization of screen-based color changes, a standard telephone is modified to communicate how it is being held and squeezed. The telephone receiver includes a touch-sensitive surface which conveys the user's physical response over a computer network. The recipient sees a small colored icon on his computer screen which changes in real time according to the way his conversational partner is interacting with the telephone object.  \n</p>", "people": ["picard@media.mit.edu"], "title": "Touch-Phone", "modified": "2016-12-05T00:16:56.209Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-383", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "touch-phone"}, {"website": "", "description": "<p>One of the most difficult aspects of using Radio-Frequency Identification chips is determining what tag is being read when. By sending the energy, as well as the data, to interrogate a tag through the human body, it is possible to associate gestures with tags. This allows user interfaces to be based on manipulating tangible icons, and makes it possible to pick up the bit associated with an object at the same time you pick up its atoms.</p>", "people": ["neilg@media.mit.edu"], "title": "Touch Tags", "modified": "2016-12-05T00:16:56.178Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "touch-tags"}, {"website": "", "description": "<p>Trackmate is an open-source initiative to create an inexpensive, do-it-yourself tangible tracking system. The Trackmate tracker allows any computer to recognize tagged objects and their corresponding position, rotation, and color information when placed on a surface. With over 280 trillion possible visual markers, each tagged object can be unique in the world and work seamlessly across any Trackmate system.  Trackmate sends all object data via LusidOSC (a protocol layer for unique spatial input devices), allowing any LusidOSC-based application to use the spatial object information.</p>", "people": ["daniell@media.mit.edu", "ishii@media.mit.edu", "labrune@media.mit.edu"], "title": "Trackmate", "modified": "2016-12-05T00:16:56.231Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "trackmate"}, {"website": "", "description": "<p>Toy Symphony combines children, virtuosic soloists, composers, and symphony orchestras around the world to alter radically how children are introduced to music, as well as to redefine the relationship between professional musicians and young people. A complete set of Music Toys will be distributed to children in each host city (including Berlin, Dublin, Glasgow, Manchester/London, and Tokyo), where children will be mentored to create their own sounds and compositions for toys and traditional instruments. A pedagogy for using these Music Toys to teach and to instill a love for musical creativity will also be developed. Final concerts will be presented in each host city including children's compositions and specially commissioned works by young composers, to be performed by children, soloists, and orchestra, playing Music Toys, Hyperinstruments, and traditional instruments.</p>", "people": ["laird@media.mit.edu", "ariane@media.mit.edu", "tristan@media.mit.edu", "tod@media.mit.edu"], "title": "Toy Symphony", "modified": "2016-12-05T00:16:56.277Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-483", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "toy-symphony"}, {"website": "", "description": "<p>Touch\ufffdSensitive is a wearable device that leverages stress and provides comfort for people on the move. It is determined by how the body reacts to the bag-load user's support, and we position this research within wearable devices that consider body specifics in their design. A series of observations using ethnographic methods informed us of how our apparel can soothe people who suffer from bag overload and can connect to the technologies users carry.</p>", "people": ["ishii@media.mit.edu"], "title": "Touch\ufffdSensitive", "modified": "2016-12-05T00:16:56.253Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-350", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "touchsensitive"}, {"website": "", "description": "<p>We are designing and testing an automatic system to calculate the usefulness of items on Web pages. Using this system, Web sites can adjust the order of presentation, for example of reviews on a book-selling site, so that the most interesting and useful reviews filter to the top. The system works by observing the dwell-times and locations of the cursor while users browse the site, and so requires no deliberate action on the part of its users or administrators. The system can be used passively to gather information, or to feed the interest levels of past users forward in order to improve the experience of future visitors.</p>", "people": ["sajid@media.mit.edu", "holtzman@media.mit.edu", "pattie@media.mit.edu"], "title": "Tracking Usefulness", "modified": "2016-12-05T00:16:56.309Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "tracking-usefulness"}, {"website": "", "description": "<p>The sense of agency (SoA) describes the feeling of being the author and in control of one's movements. It is closely linked to automated aspects of sensorimotor control and understood to depend on one's ability to monitor the details of one's movements. As such SoA has been argued to be a critical component of self-awareness in general and contribute to presence in virtual reality environments in particular. A common approach to investigating SoA is to ask participants to perform goal-directed movements and introducing spatial or temporal visuomotor mismatches in the feedback. Feedback movements are traditionally either switched with someone else's movements using a 2D video-feed or modified by providing abstracted feedback about one's actions on a computer screen. The aim of the current study was to quantify conscious monitoring and the SoA for ecologically valid, three dimensional feedback of the participants' actual limb and movements. This was achieved by displaying an Infra-Red (IR) feed of the participants' upper limbs in an augmented virtuality environment (AVE) using a head-mounted display (HMD). Movements could be fed back in real-time (46ms system delay) or with an experimental delay of up to 570ms. As hypothesized, participant's SoA decreased with increasing temporal visuomotor mismatches (p&lt;;.001), replicating previous findings and extending them to AVEs. In-line with this literature, we report temporal limits of 222\u00b160ms (50% psychometric threshold) in N=28 participants. Our results demonstrate the validity of the experimental platform by replicating studies in SoA both qualitatively and quantitatively. We discuss our findings in relation to the use of virtual and mixed reality in research and implications for neurorehabilitation therapies.</p>", "people": [], "title": "The Temporal Limits of Agency for Reaching Movements in Augmented Virtuality", "modified": "2017-04-07T12:16:14.654Z", "visibility": "PUBLIC", "start_on": "2016-08-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2016-09-02", "slug": "the-temporal-limits-of-agency-for-reaching-movements-in-augmented-virtuality"}, {"website": "", "description": "<p>The Trainer Piano actively moves your fingers as you learn how to play. Employing active magnetic force below the keys of a working piano, the Trainer Piano provides users with kinesthetic input that augments their normal motor learning process. By providing a \"feel\" for what a user is supposed to play, the Trainer Piano minimizes the amount of time necessary to learn new motor patterns and acquire new motor skills. The core idea behind this project lies in the hypothesis that computer-controlled force can be used to teach students how to play an instrument at a faster and more efficient rate than would occur in an unaided learning environment.</p>", "people": ["tod@media.mit.edu"], "title": "Trainer Piano", "modified": "2016-12-05T00:16:56.360Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "trainer-piano"}, {"website": "", "description": "<p>\"Trash Talk\" is a distributed mesh network of decidedly inexpensive speaker nodes that play audio messages. Each node contains a microphone and speaker, and can be used to program the message that will be rebroadcast by the network. With several Trash Talk nodes, one can easily and quickly deploy a freely accessible messaging platform in public spaces. Nodes are intended to be cheap enough that the deployments can be considered disposable, and usable in spaces where authoritarian presences restrict democratic speech.</p>", "people": ["geek@media.mit.edu"], "title": "Trash Talk", "modified": "2016-12-05T00:16:56.407Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-320", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "trash-talk"}, {"website": "", "description": "<p>Bringing broadcast to the Internet redefines it to a participatory medium. Centralized distribution does not scale for technical and economic reasons; instead, we distribute distribution to the receivers. We do this with Rewire, a dynamic protocol with a completely distributed architecture that has evolved from the VidTorrent project. The protocol builds an elastic, self-healing overlay that scales organically, with no upfront investment. Latency and effects on the network are minimized with gradual self-optimization.</p>", "people": ["lip@media.mit.edu"], "title": "TV, Rewired!", "modified": "2016-12-05T00:16:56.502Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "tv-rewired"}, {"website": "", "description": "<p>TViews is an integrated display and interaction table designed for collaborative exploration of multimedia applications in home and classroom environments. The platform is extensible at both hardware and software levels, and invites a variety of interactive applications including media content browsing, multi-viewpoint storytelling, and narrative simulation games. This work was supported by Samsung Electronics.</p>", "people": ["gid@media.mit.edu"], "title": "TViews", "modified": "2016-12-05T00:16:56.530Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "tviews"}, {"website": "", "description": "<p>Twinkle lets you program using crayons, LEGO bricks, or anything that has colors, like a striped shirt or fall leaves. Compose a song with markers, program a robot by drawing instructions on paper, or create a custom interface just by doodling.</p>", "people": ["silver@media.mit.edu", "mres@media.mit.edu", "ericr@media.mit.edu"], "title": "Twinkle", "modified": "2016-12-05T00:16:56.560Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "twinkle"}, {"website": "", "description": "<p>TxTMob is a text-message service developed for political activists engaged in direct action and mass mobilizations. Approximately 6,000 protestors used TxTMob during the Democratic and Republican National Conventions of 2004. TxTMob was also used by demonstrators in Kiev during Ukraine's Orange Revolution, and by activists during George W. Bush's 2005 inauguration in Washington, DC.</p>", "people": [], "title": "TxTMob", "modified": "2016-12-05T00:16:56.618Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "txtmob"}, {"website": "", "description": "<p>We have turned our building into a living laboratory by distributing 45 Ubiquitous Media Portals throughout our facility. These sensor-rich platforms capture video and stereo audio in addition to measuring nearby motion, temperature/humidity, light levels, and IR detection of active badges. The portals also work as Zigbee base stations, enabling communication and localization of various low-power, wireless, wearable devices that our group and others have developed for on-body sensing and identification/tracking of people. They also feature a small touch-screen display and speaker, allowing them to render and present dynamic graphics and information.</p>", "people": ["nanwei@media.mit.edu", "joep@media.mit.edu"], "title": "Ubiquitous Media Portals", "modified": "2016-12-05T00:16:56.685Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "ubiquitous-media-portals"}, {"website": "", "description": "<p>uCom (the \"u\" stands for \"ubiquitous\") is a follow-on to our iCom (Media Lab/Media Lab Europe, 1999) system for connecting architectural spaces to enable collaboration by distributed groups. uCom takes advantage of input/output resources (e.g., displays, cameras, speakers, sensors) already in place; it is not restricted to one \"window\" in one location, but rather creates multiple video and audio portals between the spaces; it scales in richness as the number of input and output devices increase; and it has both a real-world and a virtual-world presence. The virtual-world model can be accessed by users in other places, or for replay of past events. uCom also enables the aggregating of sensor data to allow processes to draw higher-level inferences (e.g., for understanding the actions of the community of users), monitoring individual users (e.g., for health or disability reasons), or augmenting the space with additional virtual information.</p>", "people": ["vmb@media.mit.edu"], "title": "uCom", "modified": "2016-12-05T00:16:56.752Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "Garden Conference Room", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "ucom"}, {"website": "", "description": "<p>The aim of UniPlug is to make it *really easy* to configure and update systems of networked devices without the need for manual intervention by administrative staff. Currently, a lot of time and manual effort goes into configuring and installing systems. UniPlug includes a safety framework to ensure that a new device added to a system does not decrease the safety of the system. UniPlug devices are context oriented. They convey context information and seek configuration and behaviour information that the user has specified previously. The user uses a web browser or an application on a device such as a cellphone to configure device behaviour. Since UniPlug is a generic framework, it could be used in an Operating Room, and at the same time be used for configuring devices in a barber's salon or the phone system of an enterprise.</p>", "people": ["lip@media.mit.edu"], "title": "UniPlug", "modified": "2016-12-05T00:16:56.916Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-468d", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "uniplug"}, {"website": "", "description": "<p>Everyone knows the world is not a perfect place.  Love and slack\nthough, fill in many painful gaps.  In this project, an electronic device\nnegotiates the pathologically noisy environment of New York City,\nexposing its rhythm and harmony.  Successor to noise cancellation headphones, Universal Slack draws on numerous Media Lab technologies\nto turn the sound around us into music.  Comfortably integrated \ninto the wearer's bodyspace, worn like an iPod, it analyzes audio, finds \norder, reduces noise, harmonizes and soothes.\n\n</p>", "people": ["csik@media.mit.edu"], "title": "Universal Slack", "modified": "2016-12-05T00:16:56.955Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-020A", "groups": [], "published": true, "active": false, "end_on": "2005-01-01", "slug": "universal-slack"}, {"website": "", "description": "<p>Fashion signals are important in displaying one\ufffds quality of access to information. Electronic fashions (information that may circulate within blog circles or online communities) experience rapid fashion cycles as the information is easily disseminated and regenerated. Physical fashions (such as clothing) are unable to update as quickly as information flow due to their physical structure. Urbanhermes is an augmented messenger bag that aims to incorporate the fluid, expressive signals of electronic fashion into the constrained, material-based environment of physical fashion. The bag is able to change its dynamic, temporal display within the context of its social environment, providing a versatile means for face-to-face signaling. By accelerating the physical fashion cycle, Urbanhermes facilitates more meaningful and communicative representations of self-identity.</p>", "people": ["judith@media.mit.edu"], "title": "Urbanhermes", "modified": "2016-12-05T00:16:56.862Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "urbanhermes"}, {"website": "", "description": "<p>Beginning in the early twentieth century, composition branched out into a variety of new representations, the most common being the graphical score.  Cage's _Variations II_ is a prime example, utilizing only dots and lines as its basis.  A potential downside to these new methods of representation is the intense burden it places on the performer to define the parameters for the realization.  We have created an interactive version of Cage's piece, called here _Variations 10b_, where a performer can change the score and get immediate feedback as to the result.  We hope that both listeners and performers will develop a more nuanced understanding of the score through the use of the interface.</p>", "people": ["tod@media.mit.edu"], "title": "Variations 10b", "modified": "2016-12-05T00:16:57.094Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "variations-10b"}, {"website": "", "description": "<p>Increasingly, software agents will need to detect certain kinds of semantically significant information that appear amidst a larger body of relatively unstructured information, for applications such as Web page analysis and data mining. Currently, the only way to express patterns of text is through grammars (a set of textual rules), which drive parsers (the recognition procedures). But grammars are difficult and error-prone to write. We are experimenting with an agent that learns recognition rules by example. The agent generates a set of hypotheses about the interpretation of the examples and dynamically displays them to the user, who may choose among them and edit them incrementally. This agent brings the power of parsing technology into the hands of non-expert users.</p>", "people": ["lieber@media.mit.edu"], "title": "Teaching Agents to Recognize Text", "modified": "2016-12-05T00:16:57.256Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "teaching-agents-to-recognize-text"}, {"website": "", "description": "<p>The Victorian Laptop explores the interaction between physical objects and virtual worlds, conversational storytelling, and the use of computers to encourage and enhance communication. It merges (and extends) the capabilities of modern laptop computers with the aesthetic and evocative  qualities of an antique writing box. It makes readable the memories and histories that are associated with old and valued objects as it links the present-day traveler to those who have stood before in the same spot.</p>", "people": [], "title": "Victorian Laptop", "modified": "2016-12-05T00:16:57.184Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "victorian-laptop"}, {"website": "", "description": "<p>Can display nanostructures be applied to all surfaces in a room?  We explore growing them (like mold), and their interactions with light and the presence of electric fields. Beamed standing waves can selectively excite and not-excite nano-surface areas to create the display. These nano-surfaces could also function as holographic displays, or displays that have complex lenses embedded into them that allow light to be \"thrown\" to other areas of the room, onto bodies, or yes, thin air.</p>", "people": ["mlj@media.mit.edu"], "title": "Video Displays on Every Surface", "modified": "2016-12-05T00:16:57.283Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2008-01-01", "slug": "video-displays-on-every-surface"}, {"website": "", "description": "<p>Long-distance families are increasingly staying connected with free video conferencing tools. However, the tools themselves are not designed to accommodate children's or families' needs. We explore how play can be a means for communication at a distance. Our Video Play prototypes are simple video-conferencing applications built with play in mind, creating opportunities for silliness and open-ended play between adults and young children. They include simple games, such as Find It, but also shared activities like book reading, where users' videos are displayed as characters in a story book.</p>", "people": ["ishii@media.mit.edu"], "title": "Video Play", "modified": "2016-12-05T00:16:57.304Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "video-play"}, {"website": "", "description": "<p>This work exploits the spontaneity and connectivity speed afforded by mobile devices, combined with the organizational structure of a Weblog, where brief captured experiences can be expanded and further reflected upon. A group of friends are using their cell phones to correspond and exchange mediated messages with one of their friends who is in the hospital undergoing a medical procedure. In introducing this approach to documenting the recovery process, we strive to create an evolving, \"architectured,\" story process that is built up of many voices and interpretations. On an immediate level, there is a desire to connect and extend our physical reach and to provide comfort and companionship to a friend, which is met by the friend's wish to help remove the alienating distance of the hospital and its attendant medical procedures. The Weblog additionally makes visible the stages of recovery, delineating individual progress made in the context of the continuing day-to-day goings-on of the rest of the group. This experiment will be extended as we introduce MOVITS, a custom-designed application for the cell phone that offers a methodology for gently interrogating lived experiences, where the time-lag between experience, interpretation, representation, and display becomes, on some levels, miniscule.</p>", "people": ["gid@media.mit.edu"], "title": "Video Weblogs", "modified": "2016-12-05T00:16:57.328Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-320B", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "video-weblogs"}, {"website": "", "description": "<p>The vinylMP3 project brings together new digital technology with old vintage analog technology. It was designed to look, sound, and behave just like a normal old vintage record player, but is, in fact, a transparent, physically-controlled, computer-based MP3 player.</p>", "people": ["tristan@media.mit.edu", "tod@media.mit.edu"], "title": "VinylMP3", "modified": "2016-12-05T00:16:57.416Z", "visibility": "PUBLIC", "start_on": "2001-12-31", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "vinylmp3"}, {"website": "", "description": "<p>Broadband architecture is optimized for asymmetric, client-server computing applications. While ample downstream capacity is offered to subscribers, the upstream capacity is limited. As a result, emerging peer-to-peer and dynamic applications cannot easily reach residential end-users; innovation is limited in the edge of the network. The objective of the Viral Broadband project is to build a viral network that compliments existing broadband infrastructure in order to provide symmetric networking capabilities.  We experiment with\nparasitic wireless network infrastructure deployed from the bottom up\nby end users.</p>", "people": ["lip@media.mit.edu"], "title": "Viral BroadBand", "modified": "2016-12-05T00:16:57.448Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-495", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "viral-broadband"}, {"website": "", "description": "<p>Streetscore is a computer vision algorithm that estimates people's perception of urban environments. We have used Streetscore to create high resolution maps of urban perception with the goal of studying the social impact of urban perception, and also, to study urban change.</p>", "people": ["naik@media.mit.edu"], "title": "Streetscore", "modified": "2016-12-05T00:16:57.478Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": "2016-10-22", "slug": "streetscore-new"}, {"website": "", "description": "<p>Virtual Gaza is a space where ordinary Palestinians under siege can describe their experiences in their own words, and where the destruction of the Gaza strip can be documented by those experiencing it directly. The diary entries, photographs, and video material gathered have been contributed by residents of Gaza.</p>", "people": ["csik@media.mit.edu"], "title": "Virtual Gaza", "modified": "2016-12-05T00:16:57.561Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "virtual-gaza"}, {"website": "", "description": "<p>The virtual messenger system acts as a portal to subtly communicate messages and pass information between the digital, virtual, and physical worlds, using the Media Lab\ufffds Glass Infrastructure system. Users who opt into the system are tracked throughout the Media Lab by a multimodal sensor network. If a participating user approaches any of the Lab\ufffds Glass Infrastructure displays they are met by their virtual personal assistant (VPA), who exists in DoppelLab\ufffds virtual representation of the current physical space. Each VPA acts as a mediator to pass on any messages or important information from the digital world to the user in the physical world. Participating users can interact with their VPA through a small subset of hand gestures, allowing the user to read any pending messages or notices, or inform their virtual avatar not to bother them until later.  </p>", "people": ["joep@media.mit.edu"], "title": "Virtual Messenger", "modified": "2016-12-05T00:16:57.587Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "virtual-messenger"}, {"website": "", "description": "<p>Canopy Climb is a virtual climbing interface for a rainforest tree. Pulling on the ropes allows the visitor to 'climb' up and down the tree. At the base of the ropes is a log which has a scroll mouse. Stepping on this log will display more information about the organisms in the tree. This is written in HTML, so that museums and schools can easily create their own vertical interfaces such as a journey to the bottom of the sea, to outer space, down a mine shaft, or up a skyscraper. New plastic, vacuum-formed molds for the base of the tree have been made as an exploration of low-cost rapid prototyping tools.</p>", "people": [], "title": "Virtual Tree", "modified": "2016-12-05T00:16:57.609Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "virtual-tree"}, {"website": "", "description": "<p>Virtual Towing allows a city to rapidly collect and redistribute vehicles, by enabling a single driver to tow multiple vehicles without mechanically connecting them. Given the principles of one-way vehicle sharing in the CityCar concept, the need to stabilize the city's vehicle fleet metabolism is critical to properly balancing vehicle demand and location. Virtual towing builds upon the (digital) wheel robot architecture and combines a multitude of sensing technologies in order to maximize security and efficiency. The key issues of adoption and system autonomy are being explored in this project. A small-scale, radio-controlled vehicle has been built to test the control of the wheel robots and will serve as a testing platform for virtual towing.</p>", "people": ["ypod@media.mit.edu", "rchin@media.mit.edu"], "title": "Virtual Towing", "modified": "2016-12-05T00:16:57.680Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "virtual-towing"}, {"website": "", "description": "<p>Computer vision is a class of technologies that lets computers use cameras to automatically stitch together panoramas, reconstruct 3D geometry from multiple photographs, and even tell you when the water's boiling. For decades, this technology has been advancing mostly within the confines of academic institutions and research labs. Vision on Tap is our attempt to bring computer vision to the masses.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "Vision on Tap", "modified": "2016-12-05T00:16:57.704Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-320", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "vision-on-tap"}, {"website": "", "description": "<p>The VisiGeek displayer enables the visual comparison of otherwise non-visual datasets. The visualizer highlights differences in the data using patterns, letter-shapes and motion. Applications of this type of visualization could include affinity-matching when searching for newsgroups or chat rooms to join, or visual sorting tasks such as identifying a desirable place to live using a back-end service such as a demographic database. \n\nThe demonstration project's immediate goal is to allow a viewer to visually compare \"Geek Codes\" of two individuals to uncover differences and similarities.</p>", "people": ["judith@media.mit.edu"], "title": "VisiGeek", "modified": "2016-12-05T00:16:57.648Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "visigeek"}, {"website": "", "description": "<p>Visual Who is a tool for visualizing an electronic community. Using data such as mailing list subscriptions, it creates a spring-based model of the patterns of affiliation within the community. By varying the groups chosen as anchor points, the user can explore interactively the underlying social and organizational structures. The goal of Visual Who is to make the social patterns of an electronic community visible. A large-scale installation version of the project is being built in collaboration with the Responsive Environments group. There, the visualization is displayed on the LaserWall and music and sound accompany the motion of the anchors and names.</p>", "people": ["judith@media.mit.edu"], "title": "Visual Who", "modified": "2016-12-05T00:16:57.780Z", "visibility": "PUBLIC", "start_on": "1993-12-31", "location": "E15-449", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "visual-who"}, {"website": "", "description": "<p>Nutritional assessment is an extremely important problem for every American. Studies suggest that as many as 90% of Americans fall short of Vitamins D&amp;E as a result of their regular dietary habits, and up to 50% of Americans can\u2019t get enough Vitamin A &amp; Calcium.</p><p>This problem is even more prominent in less wealthy communities, where not only food budget is more limited, but education in basic nutritional facts also more lacking. Even if full records of daily food intake are available, knowledge about nutrients in foods is needed to reflect on recent food consumption and subsequently act to nutritionally complement recent habits.</p><p>Therefore, there are two major obstacles stopping many ordinary Americans from healthily managing their diets. The first is recording dietary intake, and the second is interpreting nutritional profiles from the foods you\u2019re eating. It\u2019s after these two steps that insight into nutritional intake can be inferred and insights into dietary balance can be made. We set out to drastically lower the efforts involved in both steps by utilizing machine learning technologies. Image recognition technologies will be utilized to allow easy recording of dietary intake via photos, and nutrition data will be subsequently inferred based on USDA\u2019s nutritional database, which also serves as the basis for nutritional evaluations from dietary records by nutritionists.</p><p>The result is a machine agent that can assist users&nbsp;to&nbsp;keep track of their dietary habits, and feedback nutritional deficiencies and suggestions to improve current diet to the user when needed.</p><p><br></p><p><br></p>", "people": ["lukeglw@media.mit.edu"], "title": "Olive - AI Nutrition Management", "modified": "2019-02-14T16:55:54.219Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2017-06-09", "slug": "olive-ai-nutrition-management"}, {"website": "", "description": "<p>An interconnected musical network for a museum setting, Voice Patterns allows for up to four participants to create and transform musical motives with their voices, and to share these motives with peers by synchronizing play patterns.</p>", "people": ["tod@media.mit.edu"], "title": "Voice Patterns", "modified": "2016-12-05T00:16:57.839Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-441", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "voice-patterns"}, {"website": "", "description": "<p>A wall that responds to its surroundings. By listening and interacting with the people around it the wall retains a memory of its experiences. The wall communicates with its visitors through audible sound and infrasound, sometimes speaking with itself to remember and feel. \n</p>", "people": ["monster@media.mit.edu", "csik@media.mit.edu"], "title": "Wall of Sound", "modified": "2016-12-05T00:16:57.885Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "wall-of-sound"}, {"website": "", "description": "<p>How will life change as our surfaces become dynamic information displays?  How will we use space tastefully and what kinds of interactions will be possible? Wall Paper explores the ability to selectively display information based on proximity and user attention, making it possible to delve deeper into information as people interact with the wall.  By intelligently switching modes based on user behavior, the wall avoids being the cause of information overload despite the vast amount of information it can provide.</p>", "people": ["holtzman@media.mit.edu"], "title": "Wall Paper", "modified": "2016-12-05T00:16:57.921Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "wall-paper"}, {"website": "", "description": "<p>Communication is much more than the direct transfer of information. It is an interactive collaborative act including potentially rich verbal and non-verbal cues. To date, telecommunication has focused on verbal communication at a distance, restricting the non-verbal situational information. With the existing infrastructure, it is necessary to first contact a person before receiving feedback on what they are doing. Current communication devices do not provide a persistent sense of presence of those with whom we connect, nor do they create opportunities for additional communication with them. WatchMe\ufffda platform for mobile communication and awareness in the form of a watch\ufffdaddresses these limitations. </p>", "people": ["geek@media.mit.edu"], "title": "WatchMe", "modified": "2016-12-05T00:16:58.029Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-384C", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "watchme"}, {"website": "", "description": "<p>In a high-population-density city, inhabitants must be prepared to defend their own personal space. Technologies that increase personal productivity are on the rise, even though they may intrude on others. The unavoidable reaction is to create technologies that counteract other people\ufffds devices. Wave Bubble counters the all-too-familiar annoyance of loud ring tones and overt cell-phone conversations in public. This wearable device blocks all radio frequencies in the PCS/GSM cellular phone bands. By activating it, the user creates a bubble of low-reception in their personal space, which repels cell-phone users (or, if they choose, deactivates RFID tag readers or GPS tracking devices). When worn openly, wearers make the statement that they reject the intrusion of these technologies into their everyday lives.</p>", "people": ["csik@media.mit.edu"], "title": "Wave Bubble", "modified": "2016-12-05T00:16:58.076Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-020D", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "wave-bubble"}, {"website": "", "description": "<p>Wearable Body Organs are a series of digitally controlled, electromechanical wearable devices that respond to a set of psychologically and socially critical needs. Each device is adaptable to its current user and situation, and in this way WBOs are made by users and their ever-changing influences. ScreamBody, HoldBody, CryBody, FightBody\ufffdrather than being hidden and made to go unnoticed as may be the trend with our ever-shrinking mobile devices, WBOs are designed for visible, active, personal, and social engagement.</p>", "people": ["monster@media.mit.edu", "csik@media.mit.edu"], "title": "Wearable Body Organs", "modified": "2016-12-05T00:16:58.225Z", "visibility": "PUBLIC", "start_on": "1997-01-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "wearable-body-organs"}, {"website": "", "description": "<p>We are exploring the use of wearable objects for capturing emotion. When the user experiences a particular emotion, she initiates the wearable object to generate unique haptic sensations that come to be associated with that particular emotion. We explore the use of these haptic-emotion capture devices triggered by natural gestures such as the knee-slapping funny gesture and the congratulatory high-five gesture.</p>", "people": ["pattie@media.mit.edu"], "title": "Wearables for Emotion Capture", "modified": "2016-12-05T00:16:58.477Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "wearables-for-emotion-capture"}, {"website": "", "description": "<p>This project presents a weight-changing device based on the transfer of mass. We chose liquid metal and a bi-directional pump to inject into or remove from from the mass of a target object. The liquid metal is about six times heavier than water, and is thus suitable for effective mass transfer. We also combine the device with a dynamic volume changing function to achieve programmable mass/volume at the same time. We explore three potential applications enabled by weight-changing devices. This technique opens up a new direction in human science regarding user interfaces with dynamic weight/volume changing.</p>", "people": ["liningy@media.mit.edu", "ishii@media.mit.edu"], "title": "Weight and Volume Changing Device", "modified": "2016-12-05T00:16:58.541Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "weight-and-volume-changing-device"}, {"website": "", "description": "<p>What's Up is a neighborhood news system that combines power of the telephone and of the Web to make it easier for youth to organize community events and find out what is happening in the place where they live. By dialing a central number, youth can send and receive voicemail messages, publish audio community announcements, create voicemail groups, add events to a shared community calendar and more. When field tested in Lawrence, MA, What's Up helped increase awareness of and accessibility to important local youth resources, provided youth with opportunities to express their opinions about their neighborhoods and, with that, contributed to transform Lawrence into a community that is both friendlier and more empowering for young people.</p>", "people": ["csik@media.mit.edu", "mres@media.mit.edu", "leob@media.mit.edu"], "title": "What's Up", "modified": "2016-12-05T00:16:58.731Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-020A", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "whats-up-2"}, {"website": "", "description": "<p>The Wetpaint project investigates new interfaces for exploring the history of a work of visual art. We are seeking intuitive metaphors for touch-screen interaction, including virtually scraping through the multispectral scans of an ancient painting, and pulling, stretching, and tearing through a virtual canvas. The interaction techniques refined through Wetpaint are being built into a Web-based tool for leveraging collective intelligence toward the pursuit of art diagnostics.</p>", "people": ["x_x@media.mit.edu", "ishii@media.mit.edu", "amerigo@media.mit.edu"], "title": "Wetpaint", "modified": "2016-12-05T00:16:58.609Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-348", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "wetpaint"}, {"website": "", "description": "<p>The system consists of virtual representations of a handful of people that react to what a person is currently reading, writing, or talking about. These \"digital personas\" are constructed automatically by analyzing personal texts (such as interviews/blogs/messages posted by the person being modeled) using natural language processing techniques and the Open Mind Common Sense knowledge base. The \"What Would They Think?\" system has several applications. For example, it can help a person form a deep understanding of a community that is new to them by constantly showing them the attitudes and disagreements of strong personalities of  that community. Other applications include virtual \"mentors\" or \"guides,\" and knowledge-sharing in communities and workgroups.</p>", "people": ["pattie@media.mit.edu"], "title": "What Would They Think?", "modified": "2016-12-05T00:16:58.673Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "what-would-they-think"}, {"website": "", "description": "<p>What's Up is a set of tools designed to allow people in a small geographic community to share information, plan events, and make decisions, using media that's as broadly inclusive as possible. The platform incorporates low-cost LED signs, online and paper event calendars, and a simple yet powerful phone system that is usable with the lowest-end mobile and touch tone phones. </p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "leob@media.mit.edu"], "title": "What's Up", "modified": "2016-12-05T00:16:58.859Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "whats-up"}, {"website": "", "description": "<p>\"Where The Hel\" is a pair of helmets: plain and funky. The funky helmet is 3D printed; the plain helmet visualizes proximity to the funky helmet as a function of signal strength, via an LED light strip. The funky helmet contains an Xbee and a GPS Radio. Its position is tracked via a web app. The wearer of the plain helmet can track the funky one via the web app and the LED strip on his helmet. These helmets are potential iterations towards a more developed HADR (Humanitarian Assistance and Disaster Relief) helmet system.</p>", "people": ["holtzman@media.mit.edu"], "title": "Where The Hel", "modified": "2016-12-05T00:16:59.058Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "where-the-hel"}, {"website": "", "description": "<p>While computer data can live virtually anywhere, we are still faced with the mundane tasks of document management such as uploading, sharing, and syncing between locations. Window Wallet aims to remove the burden of managing your data across screens, computers, and devices by turning your portable device into a virtual wallet of data and software. This project looks at developing an interface that facilitates this process, acting on your mobile device as a virtual conduit between local data and data in the cloud. It allows you both to access and transfer documents independent of your physical location.</p>", "people": ["holtzman@media.mit.edu"], "title": "Window Wallet", "modified": "2016-12-05T00:16:59.219Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "window-wallet"}, {"website": "", "description": "<p>This project describes the design and development of a system that will help to increase safety and security using cell phones. This system has two main components: a master phone application to assist people who need to take care of their loved ones, and a slave phone application to provide help to care-recipients who need attention from their caregivers. This system applies location awareness (GPS), awareness of social activities (communication activity and proximity with close peers), and peer-to-peer data communication as its core technologies.</p>", "people": ["geek@media.mit.edu"], "title": "Will You Help Me?", "modified": "2016-12-05T00:16:59.239Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "will-you-help-me"}, {"website": "", "description": "<p>WISE is a Web system in which children can engage in conversation with interactive storytellers. These storytellers draw children out, listen to their personal stories, and then offer a traditional story in return. Children can also create their own WISE characters online and share them with others around the world.</p>", "people": [], "title": "WISE", "modified": "2016-12-05T00:16:59.278Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "wise"}, {"website": "", "description": "<p>This work has a double focus. First, considerable research has centered on designing computer tools and interfaces, \"smart rooms,\" and computer-based interactive systems for human adults and children, but few people realize the potential for extending this work to include animals. After many years of studying the intelligence of animals, we believe that such an extension to the animal kingdom is not only justified, but viable. Toward this end, we have been designing \"Pet Projects,\" a collection of computer-based tools to enhance pet-human interactions and the lives of pet animals. Second, these tools can be used to explore further the cognitive abilities of animals. Such work is important because understanding how creatures with brains so different from humans can process information can help us design better artificial learning systems.</p>", "people": ["impepper@media.mit.edu"], "title": "Woofers & Tweeters", "modified": "2016-12-05T00:16:59.340Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-325", "groups": ["synthetic-characters", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "woofers-tweeters"}, {"website": "", "description": "<p>We are exploring the work piece as an interface to digital fabrication and digital modeling. Possible advantages include a more intuitive workflow and a simpler learning curve. As the first step, we are putting together a prototype of a PCB workbench that allows the user to bring up information about a feature on the PCB from the linked CAD data, by pointing a custom pointing device to the desired trace or component.</p>", "people": ["pragun@media.mit.edu", "joep@media.mit.edu"], "title": "Work Piece as Interface", "modified": "2016-12-05T00:16:59.367Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "work-piece-as-interface"}, {"website": "", "description": "<p>World Lens informs users about newsworthy events that are both popular and obscure. It is a front page that is both navigable and scalable, allowing one to discover as well as track ongoing events. We array in-depth news information across a large multitouch display organized by time, coverage, and geography. Elements are drawn from blogs, the web, newspapers, magazines, and television. Each is presented by a front page that tells the literal story. Readers can fly through the news space, mark items for interest, and activate each. News data is gathered and analyzed by our Glue system, which generates frame-by-frame metadata for video and page analysis for other online material.</p>", "people": ["lip@media.mit.edu"], "title": "WorldLens", "modified": "2016-12-05T00:16:59.435Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "worldlens"}, {"website": "", "description": "<p>This project investigates soft mechanisms, origami, and fashion. We created a modified Miura-fold skirt that changes shape through pneumatic actuation. In the future, our skirt could dynamically adapt to the climatic, functional, and emotional needs of the user; for example, it might become shorter in warm weather.</p>", "people": ["perovich@media.mit.edu", "pip@media.mit.edu", "kll@media.mit.edu", "vmb@media.mit.edu"], "title": "Awakened Apparel", "modified": "2016-12-05T00:16:59.501Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "awakened-apparel"}, {"website": "", "description": "<p>The WoW Pod is an immersive architectural space that provides and anticipates all life needs of the World of Warcraft player. Outfitted with toilet throne, hydration system, and meals at the ready, the WoW Pod makes daily human function possible without ever stepping away from the game. The project received grants from the Council for the Arts at MIT and the SHASS's Peter de Florez Fund for Humor.</p>", "people": ["ishii@media.mit.edu"], "title": "WoW Pod", "modified": "2016-12-05T00:16:59.468Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "wow-pod"}, {"website": "", "description": "<p>Pipettes are the equivalent in biology of the keyboard for computer science: a key tool that enables interface with the subject matter. In the case of the pipette, it enables the scientist to move precise amounts of liquids. Pipette design hasn't changed in over 30 years. We've designed a new type of pipette that allows wireless, context-aware operation.</p>", "people": ["jacobson@media.mit.edu", "fracchia@media.mit.edu"], "title": "Context-Aware Pipette", "modified": "2016-12-05T00:16:59.612Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2015-12-01", "slug": "context-aware-pipette"}, {"website": "", "description": "<p>We are developing techniques to use a focused ion beam to program the fabrication of nanowires-based nanostructures and logic devices.</p>", "people": ["jacobson@media.mit.edu"], "title": "NanoFab", "modified": "2016-12-05T00:16:59.650Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-015", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "nanofab"}, {"website": "", "description": "<p>Electrical stimulation (FES) is the current clinical stimulation modality used to restore function and provide therapy in a variety of clinical applications.&nbsp;However, its clinical implementation is riddled with challenges of fatigue, reverse order recruitment of motor units, and diffuse/non-specific excitation of surrounding tissues.&nbsp;</p><p>Optogenetics is a recently evolving field, which involves genetically altering cells so that they can be activated with light. Optogenetic techniques have largely been used to probe neural circuits and study the brain's function. Work in our lab has focused on implementing optogenetics as a stimulation modality for peripheral tissues. Under optogenetic stimulation, many of the challenges associated with electrical stimulation are overcome.&nbsp;</p><p>Most recently, we have utilized optogenetics in a closed-loop system to control a murine hind limb to follow desired movement patterns, mimicking climbing stairs and walking.&nbsp;&nbsp;In an ideal future, similar techniques may be used to restore functional movement in patients with paralysis or other motor impairments. We demonstrate that our methods outperform traditional electrical stimulation methods by having less fatigue and smoother movement.&nbsp; This system is the first proof-of-principle for peripheral limb control using closed-loop optogenetics and can perform with greater than 95% accuracy.</p>", "people": ["hherr@media.mit.edu", "shriyas@media.mit.edu", "bmaimon@media.mit.edu"], "title": "Closed-Loop Optogenetics for Peripheral Nerve Control", "modified": "2019-01-31T15:21:42.923Z", "visibility": "PUBLIC", "start_on": "2017-09-01", "location": "", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "closed-loop-optogenetics-for-peripheral-nerve-control"}, {"website": "", "description": "<p>A designated room in the Media Lab will be equipped with a module that includes an occupancy sensor, speakers, and processor. This apparatus will produce an acoustic space that reflects the activity of squirrels when they don\ufffdt detect human presence. It is possible to trick your imaginary friend into coming out when room is occupied, but the occupant must be very still and silent.</p>", "people": [], "title": "Your Imaginary Friend", "modified": "2016-12-05T00:16:59.843Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "Bartos Theater", "groups": [], "published": true, "active": false, "end_on": "2009-01-01", "slug": "your-imaginary-friend"}, {"website": "", "description": "<p>Coordination of and communication between large numbers of individuals, especially in situations that are prone to change rapidly, requires a common output and a recognizable input. Shake4Action looks at how we can organize large groups by augmenting SMS, email, and phone calls with mobile gestures. This project builds a platform to receive information of varying types (such as keywords, touch tones, and gestures), and return information that can be re-interpreted on output by each participant.</p>", "people": ["holtzman@media.mit.edu"], "title": "Shake4Action: Gestural Mobile Coordination", "modified": "2016-12-05T00:16:59.955Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "shake4action-gestural-mobile-coordination"}, {"website": "", "description": "<p>We present 'Smell Camera' innovative way of capturing memories in the form of smell which can be recorded, stored and played in future.</p><p>System Design:&nbsp;</p><p>The device consists of a hand-held pneumatic pump which is controlled by the user's phone. The user can record memories through this arrangement. The smells are encapsulated in a gelatin capsule which can be preserved in an air-tight personalized accessory. &nbsp;Whenever the user wants to experience the same environment, feeling or evoke the same emotions, the user can play the smell.&nbsp;</p><p><br></p>", "people": [], "title": "Smell Camera", "modified": "2017-03-28T05:24:52.754Z", "visibility": "PUBLIC", "start_on": "2016-03-30", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2017-03-31", "slug": "smell-camera"}, {"website": "", "description": "<p>We present 'Smell Camera' innovative way of capturing memories in the form of smell which can be recorded, stored and played in future.</p><p>System Design: </p><p>The device consists of a hand-held pneumatic pump which is controlled by the user's phone. The user can record memories through this arrangement. The smells are encapsulated in a gelatin capsule which can be preserved in an air-tight personalized accessory.  Whenever the user wants to experience the same environment, feeling or evoke the same emotions, the user can play the smell.&nbsp;</p>", "people": [], "title": "Smell Camera: Record, Play, Rewind", "modified": "2017-03-28T05:34:21.858Z", "visibility": "LAB-INSIDERS", "start_on": "2016-03-30", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2017-03-30", "slug": "smell-camera1"}, {"website": "", "description": "<p>Mind-Theoretic Planning (MTP) is a technique for robots to plan in social domains. This system takes into account probability distributions over the initial beliefs and goals of people in the environment that are relevant to the task, and creates a prediction of how they will rationally act on their beliefs to achieve their goals. The MTP system then proceeds to create an action plan for the robot that simultaneously takes advantage of the effects of anticipated actions of others and also avoids interfering with them.</p>", "people": ["cynthiab@media.mit.edu"], "title": "Mind-Theoretic Planning for Robots", "modified": "2017-05-31T18:46:51.951Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2014-05-31", "slug": "mind-theoretic-planning-for-robots"}, {"website": "", "description": "<p>What would you like to \"build with biology\"? The goal of the GeneFab project is to develop technology for the rapid fabrication of large DNA molecules, with composition specified directly by the user. Our intent is to facilitate the field of synthetic biology as it moves from a focus on single genes to designing complete biochemical pathways, genetic networks, and more complex systems. Sub-projects include: DNA error correction, microfluidics for high throughput gene synthesis, and genome-scale engineering (rE. coli).</p>", "people": ["jacobson@media.mit.edu"], "title": "GeneFab", "modified": "2016-12-05T00:17:00.317Z", "visibility": "PUBLIC", "start_on": "2002-09-01", "location": "E15-015", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "genefab"}, {"website": "http://actionpath.org", "description": "<p>Action Path is a mobile app to help people learn about and engage with issues in their community. The app uses push notifications tied to geography that invite people to provide meaningful feedback on nearby issues as they traverse the city. Most platforms for civic engagement, whether online or offline, are inconvenient and disconnected from the source of the issues they are meant to address. Action Path addresses barriers to effective civic engagement by inviting people's input, converting individual actions into collective action, and providing context and a sense of efficacy.</p>", "people": ["emreiser@media.mit.edu", "ethanz@media.mit.edu", "rahulb@media.mit.edu", "erhardt@media.mit.edu"], "title": "Action Path", "modified": "2018-04-27T14:33:05.572Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2017-09-01", "slug": "action-path"}, {"website": "http://mmodm.co/", "description": "<p>MMODM is an online drum machine based on the Twitter streaming API, using tweets from around the world to create and perform musical sequences together in real time. Users anywhere can express 16-beat note sequences across 26 different instruments, using plain-text tweets from any device. Meanwhile, users on the site itself can use the graphical interface to locally DJ the rhythm, filters, and sequence blending. By harnessing this duo of website and Twitter network, MMODM enables a whole new scale of synchronous musical collaboration between users locally, remotely, across a wide variety of computing devices, and across a variety of cultures.</p>", "people": ["basheer@media.mit.edu", "joep@media.mit.edu", "tod@media.mit.edu", "ddh@media.mit.edu"], "title": "MMODM: Massively Multiplayer Online Drum Machine", "modified": "2017-05-17T22:49:11.048Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["responsive-environments", "tangible-media"], "published": true, "active": false, "end_on": "2017-06-01", "slug": "mmodm"}, {"website": "http://irawinder.com/blog/riyadh", "description": "<p>We recently led a workshop in Saudi Arabia, with staff from the Riyadh Development Authority, to test a new version of our CityScope platform. With only an hour to work, four teams of five professionals competed to develop a redevelopment proposal for a neighborhood near the city center. The platform evaluated their designs according to energy, daylighting, and walkability.</p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu", "mkh@media.mit.edu"], "title": "CityScope Riyadh", "modified": "2018-06-26T14:28:55.162Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": "2016-09-01", "slug": "cityscope-mark-iva-riyadh"}, {"website": "https://github.com/iashris/adwy", "description": "<p>Identity is fluid. As we travel this journey of life, we get imprinted and we forget parts of ourselves. Older people's memories are erased and new memories are formed. As we go through this fluid process without actively knowing it, it can be helpful to pause, spend time with ourselves, and ponder where we started and where we are headed by reliving past conversations.&nbsp;</p><p>A Date With Yourself is an interactive experience that lets you ponder the digital footprints you've left behind over the years on social media and encourages you to reflect on how you've changed as a person. Our prototype analyses a user's Facebook Archive to compute how their online persona has evolved over the years.</p><p>Our analysis looked for emotions like happiness, surprise,&nbsp;disappointment, and anger. Only messages written by the researcher were analyzed\u2014not those of the friends spoken to. Every message was decontextualized and when knitted together, created moments of the researcher's life where she was unhappy, inspired, or nostalgic, creating a realization of how our interactions with others have changed over time, we can learn about how they have moulded us.</p>", "people": ["ashris@media.mit.edu", "jsirera@media.mit.edu"], "title": "A Date with Yourself", "modified": "2019-01-30T06:34:01.207Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-05-31", "slug": "a-date-with-yourself"}, {"website": "", "description": "<p>Near-unity polarization of nuclear systems in atoms has been demonstrated via optical pumping. This relies on pumping specific absorption lines of atoms. Molecules are required to provide enough qubits for computations. Molecular absorption spectra are much more broadband, and therefore will require a more complex pumping signal in order to attain polarization. We will be investigating the feasibility of using shaped ultrafast optical pumping to improve nuclear polarization. High nuclear spin polarization is a route to increasing the number of qubits available and thereby the usefulness of NMR quantum computers.</p>", "people": ["neilg@media.mit.edu"], "title": "Shaped Ultrafast Pulse Optical Pumping of NMR Systems", "modified": "2016-12-05T00:17:00.385Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "shaped-ultrafast-pulse-optical-pumping-of-nmr-systems"}, {"website": "", "description": "<p>The \"Barcelona\" demo is an independent prototype designed to model and simulate human interactions within a Barcelona-like urban environment. Different types of land use (residential, office, and amenities) are configured into urban blocks and analyzed with agent-based techniques.</p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu", "gowharji@media.mit.edu", "csmuts@media.mit.edu", "ryanz@media.mit.edu"], "title": "CityScope Barcelona", "modified": "2018-04-27T14:41:51.014Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": "2016-01-01", "slug": "cityscope-barcelona"}, {"website": "https://dmsm.github.io/scissors-congruence/", "description": "<p>The remarkable Wallace\u2013Bolyai\u2013Gerwein theorem states that any simple polygon can be cut into finite pieces and rearranged to form any other simple polygon of equal area. We present an interactive application that visualizes the beautiful, constructive proof of this theorem. The interface allows the user to input their own initial and terminal polygons, and then will rigidly transform their initial polygon to their terminal polygon.</p>", "people": [], "title": "Visualizing Scissors Congruence", "modified": "2018-11-21T17:16:01.155Z", "visibility": "PUBLIC", "start_on": null, "location": "", "groups": [], "published": true, "active": false, "end_on": "2016-06-13", "slug": "scissor-congruence"}, {"website": "", "description": "<p><b>ATEN\u00c7\u00c3O:</b>&nbsp;Saiu o <b>resultado do Desafio Aprendizagem Criativa Brasil 2018</b>! Clique <a href=\"https://www.media.mit.edu/posts/resultado-do-desafio-aprendizagem-criativa-brasil-2018/\">aqui</a> para conhecer os fellows e os projetos selecionados!</p><p>----</p><p>O Desafio Aprendizagem Criativa Brasil \u00e9 uma iniciativa da Funda\u00e7\u00e3o Lemann e do MIT Media Lab que visa fomentar a implementa\u00e7\u00e3o de solu\u00e7\u00f5es inovadoras \u2013 novas tecnologias, produtos e servi\u00e7os \u2013 que ajudem a tornar a educa\u00e7\u00e3o brasileira mais m\u00e3o na massa, significativa, colaborativa e l\u00fadica.</p><p>O Desafio tamb\u00e9m tem como objetivo identificar, conectar e apoiar indiv\u00edduos brasileiros \u2013 artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decis\u00e3o \u2013 que possam ter um papel-chave no avan\u00e7o de pr\u00e1ticas de Aprendizagem Criativa, especialmente no que se refere a projetos m\u00e3o na massa envolvendo programa\u00e7\u00e3o e constru\u00e7\u00e3o no mundo f\u00edsico, em escolas p\u00fablicas (de Educa\u00e7\u00e3o Infantil ao Ensino M\u00e9dio) e ambientes de aprendizagem n\u00e3o formais de todo o Brasil.</p><p>Os representantes dos projetos selecionados ganhar\u00e3o uma&nbsp;<i>Creative Learning Fellowship&nbsp;</i>para ajudar a implementar seu trabalho.&nbsp;</p><p><b>As inscri\u00e7\u00f5es v\u00e3o at\u00e9 o&nbsp;dia&nbsp; 9 de fevereiro e devem ser feitas \u00fanica e exclusivamente atrav\u00e9s do formul\u00e1rio abaixo.</b></p><p>Clique <a href=\"https://docs.google.com/document/d/1N6AgIZc7W6544cJKw5dsVmNSTgTzt1LAyMOiDDSKaqU/edit?usp=sharing\">aqui</a> para a&nbsp;<b>chamada de projetos</b>&nbsp;completa.<br></p><p>Clique <a href=\"https://www.questionpro.com/t/AN4HdZbPeN\">aqui</a>&nbsp;para o&nbsp;<b>formul\u00e1rio de inscri\u00e7\u00e3o</b>.</p><p>Clique&nbsp;<a href=\"https://docs.google.com/document/d/1LpYYcImuoZCeIm7XRcE1bBP62wdDgLQIfPmVN0_7AxM/edit?usp=sharing\">aqui</a>&nbsp;para respostas \u00e0s&nbsp;<b>perguntas mais frequentes</b>.</p><p><b>Aten\u00e7\u00e3o</b>: &nbsp;esta p\u00e1gina ser\u00e1 atualizada periodicamente com mais informa\u00e7\u00f5es sobre o Desafio. Discuss\u00f5es sobre o edital est\u00e3o ocorrendo no&nbsp;&nbsp;<a href=\"https://forum.aprendizagemcriativa.org/t/chamada-de-projetos-desafio-aprendizagem-criativa-brazil-2018/\">f\u00f3rum da Rede Brasileira de Aprendizagem Criativa</a>.&nbsp;</p>", "people": ["mres@media.mit.edu", "leob@media.mit.edu"], "title": "Desafio Aprendizagem Criativa Brasil 2018", "modified": "2018-12-03T22:52:00.488Z", "visibility": "PUBLIC", "start_on": "2018-01-22", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "desafio-aprendizagem-criativa-brasil-2018"}, {"website": "", "description": "<p>EcoFlux provides a new way for people to experience the inner working of ecosystems by using augmented or virtual reality to explore unseen ecological processes.&nbsp;This project used a human-centered design and development process to create a proof-of-concept technology that visualizes models of ecosystem processes using real-time, sensor-based data.&nbsp;</p><p>EcoFlux builds on the existing MIT Media Lab project DoppelMarsh, is a virtual environment that changes in response to real-time environmental conditions captured by the distributed sensor network at the Tidmarsh wetland site. EcoFlux is the first of its kind to visualize models of molecular motion and carbon flow in 3D, within the context of the physical site, and driven by real-time data.&nbsp;</p><p>Whether experienced remotely or integrated on site, EcoFlux can be used to inspire curiosity for visitors, enhance scientific understanding for researchers, and promote community development by demonstrating the value of ecological restoration. As environmental sensing becomes more ubiquitous in our daily lives, this thesis provides a foundation for making meaning from this wealth of information and harnessing human sensory systems to encourage new insights.&nbsp;</p>", "people": [], "title": "EcoFlux: Ecosystem Visualizations Driven by Real-Time, Sensor-Based Data", "modified": "2017-10-13T01:51:36.482Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2017-06-01", "slug": "ecoflux"}, {"website": "", "description": "<p>Tourism is Cuba\u2019s third largest source of foreign currency, behind the two dominant industries of sugar and tobacco. The number of visitors so far in 2016 jumped 13.5 percent over the year to 1.5 million tourists. However, the hunt for novelty in Cuba doesn\u2019t conceal its disquieting poverty and struggles: lack of regulation on sex work, ungovernable black markets and creaky infrastructure. Everyday in Havana, we inevitably enjoyed the material comforts like all the other tourists while investigating the power relations of the nation as the artists.</p><p>This project calls attention to the actions of each individual, using the nation\u2019s subsidized, soft, sweet, round daily buns as the vehicle. Made of imported flour, sugar, dry yeast and water, the daily bun tastes nothing but pale flour. Collaborating with IFF (International Flavor and Fragrance), we designed the bread with a complete flavor that balances the nuances of gasoline, sweat and white ginger flower (the national flower of Cuba).</p>", "people": ["xxxxxxin@media.mit.edu"], "title": "Havana Bread", "modified": "2017-11-30T15:56:33.024Z", "visibility": "PUBLIC", "start_on": "2017-03-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2017-05-31", "slug": "havana-bread"}, {"website": "", "description": "<p>Complex and expensive medical devices are mainly used in medical facilities by health professionals. IDA is an attempt to disrupt this paradigm and introduce a new type of device: easy to use, low cost, and open source. It is a digital stethoscope that can be connected to the Internet for streaming physiological data to remote clinicians. Designed to be fabricated anywhere in the world with minimal equipment, it can be operated by individuals without medical training.</p>", "people": ["picard@media.mit.edu", "yadid@media.mit.edu"], "title": "IDA: Inexpensive Networked Digital Stethoscope", "modified": "2017-07-27T04:58:24.482Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "ida-inexpensive-networked-digital-stethoscope"}, {"website": "", "description": "<p>Slap Snap Tap combines wearable sensors with physical block programming to enable enhanced expression through movement. By slapping on a set of sensor straps, snapping in code that links movement triggers to sound actions, and tapping the sensors to activate a play experience, users can combine motion and sound in creative ways. A dancer can create music through movement; an athlete can add emphasis to her performance; demonstrators can synchronize and amplify a chant; and anyone can create sound effects for life moments. Slap Snap Tap is a method of the Slay Play endeavor which aims to broaden participation in computational creation by using movement as a pathway into computational thinking.</p>", "people": ["ethanz@media.mit.edu", "joyab@media.mit.edu", "mres@media.mit.edu"], "title": "Slap Snap Tap", "modified": "2017-03-31T19:41:01.413Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["civic-media", "lifelong-kindergarten"], "published": true, "active": false, "end_on": "2016-12-31", "slug": "slap-snap-tap"}, {"website": "", "description": "<p>We used 15 months of data from 1.5 million people to show that four points--approximate places and times--are enough to identify 95 percent of individuals in a mobility database. Our work shows that human behavior puts fundamental natural constraints on the privacy of individuals, and these constraints hold even when the resolution of the dataset is low. These results demonstrate that even coarse datasets provide little anonymity. We further developed a formula to estimate the uniqueness of human mobility traces. These findings have important implications for the design of frameworks and institutions dedicated to protecting the privacy of individuals.</p>", "people": ["hidalgo@media.mit.edu", "yva@media.mit.edu"], "title": "The Privacy Bounds of Human Mobility", "modified": "2016-12-05T00:17:00.666Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["collective-learning", "human-dynamics"], "published": true, "active": false, "end_on": "2016-10-22", "slug": "the-privacy-bounds-of-human-mobility"}, {"website": "", "description": "<p>People's tastes are their systems of aesthetic judgment and preference. Taste systems are generative; understanding the tastes of friends makes it possible to anticipate what book or movie they would like, or how they might feel about a particular news topic. We are pioneering a computational approach to automatically modeling the tastes of individuals and whole communities by mining their texts; blogs, social network profiles, and homepages; and analyzing the attitudes and preferences contained therein. By also applying machine-learning techniques, we generalize these models so that they can predict how a person would feel about hypotheticals, and even forecast tomorrow's trends.</p>", "people": ["pattie@media.mit.edu"], "title": "Taste Modeling and Forecasting", "modified": "2018-12-04T14:44:11.037Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "taste-modeling-and-forecasting"}, {"website": "", "description": "<p>&nbsp;Research in Social Psychology has demonstrated that people's cognition is heavily influenced by an individual's sense of identity, which, in turn, is determined in part by their group relations.</p><p>Using large amounts of data from online social systems, this project aims to uncover the role played in online polarization by this mechanism, especially in conjunction with the currently emerging plethora of alternative news sources.</p>", "people": ["monsted@media.mit.edu"], "title": "Mechanisms of Political Polarization in Online Systems", "modified": "2018-05-07T17:26:55.950Z", "visibility": "PUBLIC", "start_on": "2018-05-01", "location": "", "groups": ["scalable-cooperation"], "published": false, "active": false, "end_on": "2018-12-03", "slug": "Pol2"}, {"website": "", "description": "<p>Mobility on Demand (MoD) systems are fleets of lightweight electric vehicles at strategically distributed electrical charging stations throughout a city. MoD systems solve the \"first and last mile\" problem of public transit, providing mobility between transit station and home/workplace. Users swipe a membership card at the MoD station and drive a vehicle to any other station (one-way rental). The Velib' system of 20,000+ shared bicycles in Paris is the largest and most popular one-way rental system in the world. MoD systems incorporate intelligent fleet management through sensor networks, pattern recognition, and dynamic pricing, and the benefits of Smart Grid technologies include intelligent electrical charging (including rapid charging), vehicle-to-grid (V2G), and surplus energy storage for renewable power generation and peak sharing for the local utility. We have designed three MoD vehicles: CityCar, RoboScooter, and GreenWheel bicycle. (Continuing the vision of William J. Mitchell.)</p>", "people": ["kll@media.mit.edu", "bmander@media.mit.edu", "rchin@media.mit.edu", "kdinakar@media.mit.edu"], "title": "_Mobility on Demand Systems", "modified": "2016-12-08T20:04:57.913Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-001", "groups": ["changing-places"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "OLD_mobility-on-demand-systems"}, {"website": "", "description": "<p>Suppose that one could put a connected, portable computer into the hands of all the children of the world who lack access to the kind of education that would prepare them for participation in a modern, knowledge-based society. Would you use the computers to put the children through the standard curriculum of \"developed\" countries? This project\u2014a complex undertaking\u2014sets out to design radically different approaches. Some of its components, such as theoretical studies of the epistemology of learning, extend work we have pursued in the past. A component not undertaken before is developing a scientific basis for the design and implementation of a whole essentially new curriculum\u2014or more likely, the process through which such a curriculum will evolve. Experiences such as the pilot program, which implemented a 1:1 computer infrastructure in a small rural community in Costa Rica, are been developed and studied.</p>", "people": ["cavallo@media.mit.edu", "papert@media.mit.edu", "calla@media.mit.edu", "mbletsas@media.mit.edu"], "title": "1:1 Laptops: Learning with the Hundred-Dollar Laptop", "modified": "2017-09-21T20:54:54.899Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2005-01-01", "slug": "11-laptops-learning-with-the-hundred-dollar-laptop"}, {"website": "", "description": "<p>The Context Manager is a generalized framework for connecting a variety of pervasive sensor technologies and output modalities to intelligent context-aware services. It focuses on binding together technologies that can be used to sense activity and interaction to services in a device- and collocation-agnostic way, and then use the context information, along with other personalization data that may be part of the user's profile, to provide highly relevant, just-in-time information and recommendations. The output is produced in a generalized form that can then be rendered into different modalities, such as WAP (cell phone output), audio, or a regular computer screen. The Context Manger can also push highly relevant information to the user, consolidating the point-of-contact between a multitude of services and the user in a consistent, manageable way. This infrastructure will be part of several projects, including ReachMedia, Invisible Media, and the UbER-Badge Personalization Project.</p>", "people": ["sajid@media.mit.edu", "pattie@media.mit.edu"], "title": "xLink: Extensible Context Management Layer", "modified": "2018-12-04T14:52:42.080Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "xlink-extensible-context-management-layer"}, {"website": "", "description": "<p>This work explores a technological answer to the perennial question \"What's for dinner?\" Few of us know with great certainty the exact food we crave; instead, we stew on the question and explore the nature of cravings with imaginative descriptions: \"I feel like something light, fresh, sophisticated, not too mushy; something influenced by Thai or Indian, something aromatic.\" Synaesthetic Recipes is a visual search program that allows imaginative textual descriptions, using them to drive recipe recommendations. A database of 100,000 recipes is automatically annotated with common sense about food. An artificial intelligence robotic reader reads each recipe, and based on flavors of the ingredients and cooking procedures predicts how a food will look, taste, and smell. We are translating recipes into the rich descriptive vernacular of how people naturally conceptualize their cravings for food.</p>", "people": ["pattie@media.mit.edu"], "title": "Synaesthetic Recipes", "modified": "2018-12-04T15:32:05.948Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "synaesthetic-recipes"}, {"website": "", "description": "<p>Architectural robotics enable a hyper-efficient, dynamically reconfigurable co-working space that accommodates a wide range of activities in a small area.</p>", "people": ["hlarrea@media.mit.edu", "alonsolp@media.mit.edu", "kll@media.mit.edu"], "title": "City Office", "modified": "2017-10-18T00:44:53.325Z", "visibility": "GROUP", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["city-science", "changing-places"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "cityoffice"}, {"website": "", "description": "", "people": [], "title": "PERSONAlised Mental Health Monitoring System", "modified": "2017-11-19T20:03:01.123Z", "visibility": "PUBLIC", "start_on": "2014-05-05", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2017-07-24", "slug": "personalised-mental-health-monitoring-system"}, {"website": "", "description": "", "people": [], "title": "CityScope Shanghai", "modified": "2019-01-31T17:23:22.615Z", "visibility": "GROUP", "start_on": "2017-03-01", "location": "", "groups": ["city-science"], "published": false, "active": false, "end_on": "2019-03-31", "slug": "cityscope-shanghai"}, {"website": "", "description": "<p>The Relative Size of Things is a low-cost 3D scanner for the microscopic world. It combines a webcam, a three-axis computer-controlled plotter, and image processing to merge hundreds of photographs into a single three-dimensional scan of surface features which are invisible to the naked eye. </p>", "people": ["marcelo@media.mit.edu", "pattie@media.mit.edu"], "title": "The Relative Size of Things", "modified": "2016-12-05T00:17:00.732Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "the-relative-size-of-things"}, {"website": "", "description": "<p>MaKey MaKey lets you transform everyday objects into computer interfaces. Make a game pad out of Play-Doh, a musical instrument out of bananas, or any other invention you can imagine. It's a little USB device you plug into your computer, and you use it to make your own switches that act like keys on the keyboard: Make + Key = MaKey MaKey!  It's plug and play. No need for any electronics or programming skills. Since MaKey MaKey looks to your computer like a regular mouse and keyboard, it's automatically compatible with any piece of software you can think of. It's great for beginners tinkering and exploring, for experts prototyping and inventing, and for everybody who wants to playfully transform their world.  </p>", "people": ["silver@media.mit.edu", "mres@media.mit.edu", "ericr@media.mit.edu"], "title": "MaKey MaKey", "modified": "2016-12-05T00:17:00.828Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "makey-makey"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: normal;\">The boom in Chinese housing prices in recent years has given rise to intensive concern about&nbsp;</span><span style=\"font-size: 18px; font-weight: normal;\">&nbsp;the economic fundamentals of housing prices. This project mainly focuses on giving deep insight into&nbsp;</span>housing prices from the perspective of&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">industry composition, especially by checking the characteristics (agglomeration, innovation, diversity, and so on) of a city's position in the industry space.</span><span style=\"font-weight: normal; font-size: 16px;\">&nbsp;</span></p>", "people": ["hidalgo@media.mit.edu", "sunlijun@media.mit.edu", "lduan@media.mit.edu"], "title": "Industry space and housing prices", "modified": "2017-04-03T00:02:27.435Z", "visibility": "PUBLIC", "start_on": "2017-02-10", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "industry-space-and-housing-price"}, {"website": "", "description": "<p>The vast amounts of user-generated content on the Web produce information overload as frequently as they provide enlightenment. Twitter Weather reduces large quantities of text into meaningful data by gauging its emotional content. This Website visualizes the prevailing mood about top Twitter topics by rendering a weather-report-style display. Comment Weather is its counterpart for article comments, allowing users to gauge sentiment without leaving the page. Supporting Twitter Weather is a user-trained Web service that aggregates and visualizes attitudes on a topic.</p>", "people": ["jkestner@media.mit.edu", "holtzman@media.mit.edu"], "title": "Twitter Weather", "modified": "2016-12-05T00:17:00.873Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "twitter-weather"}, {"website": "", "description": "<p>Some of the world's most pressing problems can be traced to inequity between people, both in the present and over time. Long periods of inequity can lead to both social and environmental degradation. In its 2011 Human Development Report (HDR), the United Nations incisively examines some of the complex relationships between socioeconomic equity and environmental sustainability. To examine some of the patterns in this report, we created \ufffdNetworks in Equity and Sustainability,\ufffd a visualization tool that shows, via a network graph, how nations are multi-dimensionally linked. Examining linkages with this tool can illuminate potential partnerships between cultures. As the tool is expanded in the future, it will support intercultural discussions in the global effort for a world that is more equal today and more sustainable over time.</p>", "people": ["holtzman@media.mit.edu", "hidalgo@media.mit.edu"], "title": "UN HDR: Networks in Sustainability and Equity", "modified": "2016-12-05T00:17:00.970Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["collective-learning", "information-ecology"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "un-hdr-networks-in-sustainability-and-equity"}, {"website": "", "description": "<p>This project, a collaboration between the Media Lab, the United States Postal Service, and the Lear Corporation, explores the application of Media Lab technologies in the context of a widely-used postal delivery truck, the two ton. The intention is to improve driver performance and to address a variety of safety issues.</p>", "people": [], "title": "USPS Postal Concept Truck", "modified": "2016-12-05T00:17:01.012Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-489", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "usps-postal-concept-truck"}, {"website": "", "description": "<p>The Village Area Network (VAN) provides information and communication services to an entire rural community, supporting a range of roaming and fixed services. Schools, rural clinics, government offices, police stations, agricultural cooperatives, and local residences will be empowered with access to information and new means to communicate with each other and the world. The first VAN is being implemented in the rural community of Bohechio, Dominican Republic. Last year, a LINCOS telecenter was installed in Bohechio. The VAN will be centered around a LINCOS multipurpose community telecenter and will extend the LINCOS facilities with mobile handheld and fixed wireless services in strategic local centers such as schools and hospitals.</p>", "people": [], "title": "Village Area Network", "modified": "2016-12-05T00:17:01.120Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "", "groups": ["edevelopment"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "village-area-network"}, {"website": "", "description": "<p>The computer's promise as a fundamentally different medium for learning will not be realized merely by re-hosting existing content onto new technological platforms; rather, its potential is in its use as a medium for construction, design, expression, and debugging. Areas once thought to be beyond the grasp of young learners are attainable when we shift from paper and pencil and static representational forms to computational and dynamic forms. We can enable children to connect on a personally meaningful basis with deep mathematical and scientific ideas through the development of new content and new tools. Our work in RoBallet and movement is one example of this work.</p>", "people": ["cavallo@media.mit.edu", "papert@media.mit.edu", "calla@media.mit.edu"], "title": "Content for the Digital Age", "modified": "2016-12-05T00:17:01.228Z", "visibility": "LAB", "start_on": "2003-01-01", "location": "E15-443", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "content-for-the-digital-age"}, {"website": "", "description": "<p>The Lifelong Kindergarten group is collaborating with the Museum of Science in Boston to develop materials and workshops that engage young people in \"maker\" activities in Computer Clubhouses around the world, with support from Intel. The activities introduce youth to the basics of circuitry, coding, crafting, and engineering. In addition, graduate students are testing new maker technologies and workshops for Clubhouse staff and youth. The goal of the initiative is to help young people from under-served communities gain experience and confidence in their ability to design, create, and invent with new technologies.</p>", "people": ["chrisg@media.mit.edu", "mellis@media.mit.edu", "ria@media.mit.edu", "ttseng@media.mit.edu", "jieqi@media.mit.edu", "mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "Start Making!", "modified": "2016-12-05T00:17:01.293Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2016-08-31", "slug": "start-making"}, {"website": "", "description": "<p>The LEVI (Low-Error Voter Interface) ballot design seeks to create a new ballot for voting systems that is flexible, efficient, and easy to use. We have incorporated new navigation features that serve as a visual representation of a voter's status within the voting session. In addition to this, we are exploring the use of audio verification as a tool to further improve voter confidence and accuracy.</p>", "people": [], "title": "Ballot Design with Audio Verification", "modified": "2016-12-05T00:17:01.359Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "ballot-design-with-audio-verification"}, {"website": "", "description": "<p>This project uses gaze detection and eye tracking to create information displays that know when they are being read. By modeling reading behavior in real time, we can create information that continuously reacts to and is informed by the previous actions of the reader. In the future, we hope to expand this application to address multiple simultaneous readers gazing at multiple displays.</p>", "people": ["dsmall@media.mit.edu"], "title": "Information Looks Back", "modified": "2016-12-05T00:17:01.394Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-301", "groups": ["design-ecology"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "information-looks-back"}, {"website": "", "description": "<p>The Nanogate is a micro electromechanical systems (MEMS) device capable of accurately and repeatedly controlling a nanometer gap between two very flat surfaces. The nanometer gap is achieved using a circular lever-fulcrum structure made from patterned silicon and Pyrex wafers adhered to each other via anodic bonding. The distance in the center gap can be varied by pushing on the outside edge. The separation distance between the silicon and Pyrex wafers can range from a few nanometers to several microns, while the aspect ratio between the width of the gap can be as large as 10^6. The Nanogate has promising applications both as a tunable capacitor for frequency-agile wireless networks and as a nanoscale instrument for studying molecular-scale phenomena. We are developing a key technology for the Nanogate to measure the displacement of the nanometer gap using capacitive sensing.</p>", "people": ["joep@media.mit.edu"], "title": "Instrumentation for the Nanogate", "modified": "2016-12-05T00:17:01.421Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-344", "groups": ["responsive-environments", "silicon-biology"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "instrumentation-for-the-nanogate"}, {"website": "", "description": "<p>The advent of Python-based networks creates new opportunities for mobile interaction. A compelling application would be interactive graphic control of audio on one device by deft gestures on a remote device. We have developed a stand-alone frequency modulation synthesizer using a Csound audio engine, Python, along with PyGTK and Cairo graphic control, and will extend this to other audio processing algorithms. This development, currently in a Linux environment, can be ported to networked platforms like mobile phones, handheld devices, and the $100 laptop.</p>", "people": ["bv@media.mit.edu"], "title": "Interactive Graphic Control of Networked Audio", "modified": "2016-12-05T00:17:01.460Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "interactive-graphic-control-of-networked-audio"}, {"website": "", "description": "<p>Creating smart 3-D models in the CATIA environment allows designers and engineers to explore and test the feasible envelopes in component-assembly systems. Parametric models are created based on design rules at both the component and master-assembly level. Once the subcomponents are created, assemblies test associative properties such as connection points, packaging, manufacturability, structural integrity, and substitutive affinity. By mapping all the possible solutions in an unbiased and systematic open-tree structure, conventions can be questioned and design opportunities investigated.</p>", "people": ["rchin@media.mit.edu"], "title": "Exploring the Solution Space: Parametric Design", "modified": "2016-12-05T00:17:01.492Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "exploring-the-solution-space-parametric-design"}, {"website": "", "description": "<p>This visual exploration exposes the slant of five online news sources regarding two mass shooting incidents: the 2017 Las Vegas shooting and the 2012 Sandy Hook incident.&nbsp; We show underlying facts, context, and the evolution of editorial slant derived from the event.</p><p>We designed and built a Chrome extension that lets the user highlight the text of an online article with three colors according to the amount of slant they believe the text contains. We used three categories: \u201cfacts\u201d for clearly factual and unbiased text, \u201ccontext\u201d for topic that may be factual but has been conveniently chosen to surround a subject, and \u201cslant\u201d for clearly biased text/opinionated text. </p><p>The Chrome plugin is storing the highlighted text as well as the corresponding URL to a database, which we are using to identify the cumulative metrics per news source for the timepoints we are examining.</p>", "people": ["kalli@media.mit.edu"], "title": "Slantometer", "modified": "2018-10-13T16:10:52.283Z", "visibility": "LAB-INSIDERS", "start_on": "2017-09-04", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2018-06-06", "slug": "slantometer"}, {"website": "", "description": "<p>Place Pulse is a crowdsourcing effort to map urban perception. By asking users to select images from a pair, Place Pulse collects the data needed to evaluate people's perceptions of urban environments. This data is also the data used to train Streetscore.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">Place Pulse was developed by Phil Salesses as part of his requirement to complete his master's thesis. The present version of Place Pulse was re-engineered by Daniel Smilkov and Deepak Jagdish.</span></p>", "people": ["smilkov@media.mit.edu", "salesses@media.mit.edu", "djagdish@media.mit.edu"], "title": "Place Pulse", "modified": "2018-05-07T18:06:16.218Z", "visibility": "PUBLIC", "start_on": "2010-08-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2014-08-01", "slug": "place-pulse-new"}, {"website": "", "description": "<p>Tod Machover's latest opera premiered at the Houston Grand Opera in April and May 1999. It was Machover's first opera for traditional operatic forces, including ten solo singers, chorus, and orchestra. In addition, special keyboard-based hyperinstruments were developed to allow rich and sophisticated electronic sounds to be blended with the performers in the pit and on stage during the entire 2.5 hour performances. In addition, experiments were conducted to distribute electronic sound via multiple small, high-quality loudspeakers, placed in the pit with the orchestra. The result was an enhanced opera that sounded acoustic and physical while extending the sonic and dramatic expression of the operatic medium.</p>", "people": ["tod@media.mit.edu"], "title": "Resurrection", "modified": "2016-12-05T00:17:01.615Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-494", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "resurrection"}, {"website": "", "description": "<p>Dating practices have been indelibly marked by the pace and communication opportunities available online. We are examining this phenomenon to see what it suggests about decision-making, attraction, and human attachment. To do so, we are using a few converging approaches: data analysis of online behavior, data visualizations, and direct experiments. Unlike other decision-making domains, online behaviors each result in a trace: button clicks, pages visited. Even from anonymous histories, we can begin to reconstruct the decision-making processes and variables that inform such decisions. Our laboratory experiments complement our descriptive data analysis through direct examination of the trends and patterns we see occurring. Through these approaches, we hope to understand predictors of successful relationships and to develop methods to improve the process.</p>", "people": [], "title": "CheckMate", "modified": "2016-12-05T00:17:01.678Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-318", "groups": ["erationality"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "checkmate"}, {"website": "https://kalli-retzepi.com/", "description": "<p>In Cuba, the Internet has another name, and it fits in the palm of your hand. It is a hard drive called \u201cEl Paquete Semanal\u201d (the Weekly Package)\u2014a collection of one terabyte of information: shows, movies, music, PDFs, downloaded onto hard drives and distributed door-to-door and operated by its users. In the rest of the world we have Facebook.  For the past thirty years, social networks have demonstrated their power and usefulness to link us together and place communications in everyone\u2019s hands.  At the same time, they have matured from being operated by the people who use them (e.g., Cuba, the WELL, BBSes), to large-scale commercial organizations operated on those users\u2019 behalf (e.g., Facebook).  As Nicholas Negroponte said: in the early days of media, the users were the inventors\u2014now they are two separate classes.&nbsp;</p><p>The central question addressed in this thesis is whether the design and underlying technology of entry points to a network change the way people interact with it and the experience they have. To explore this question, we designed and engineered a set of playful physical objects which function as nodes of a hyper-local network. Information bestowed upon this network remains within these nodes, cryptographically secure, and accessible only to local community members who are aware of the network's existence and mode of operation. This network was tested by deploying the node-objects in four real world locations, where participants could leave and retrieve audio messages from and to the nodes.</p>", "people": ["kalli@media.mit.edu", "lip@media.mit.edu"], "title": "Topophonia", "modified": "2019-05-13T13:46:04.394Z", "visibility": "LAB-INSIDERS", "start_on": "2018-10-15", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2019-05-08", "slug": "internet-as-an-object"}, {"website": "", "description": "<p>SilverStringers is a community-centric approach to news coverage and presentation, tapping into the strength of the older generation to develop techniques for the next-generation coverage of cities and towns. Persons over the age of 50 have unparalleled wisdom about the communities where they have lived and/or worked. SilverStringers is intended to train and equip men and women in this age group to be reporters, photographers, illustrators, editors, and designers of a localized computer network. We are interested in enhancing grass-roots communications while at the same time discovering new models for media coverage. Often, communities have only an oral basis, but no other platform for information sharing, storytelling, and exchanging past experiences. Improved communication promotes and tightens relationships, and can even create bridges between generations. HDL is a tool that aims to provide such a platform, by taking advantage of the feedback obtained from the SilverStringers (and other users of its predecessor project, Pluto). Using XML/XSL, HDL offers the ability to deliver and create content for devices other than desktop computers, such as mobile phones, PDAs, and set-top boxes. Another new feature is the ability to include streaming video and audio content in the stories. The development of an HDL audio version is underway. The publications that are created will be regularly broadcast on specific days and times, can be 15 to 30 minutes long, and can have target audiences that are urban or rural.</p>", "people": ["driscoll@media.mit.edu", "holtzman@media.mit.edu", "walter@media.mit.edu"], "title": "HDL/Silverstringers", "modified": "2019-02-06T21:25:42.074Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-320B", "groups": ["electronic-publishing", "gray-matters"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "hdlsilverstringers"}, {"website": "", "description": "<p>The robotic facade is conceived as a mass-customizable module that combines solar control, heating, cooling, ventilation, and other functions to serve an urban apartment. It attaches to the building \"chassis\" with standardized power, data, and mechanical attachments to simplify field installation and dramatically increase energy performance. The design makes use of an articulating mirror to direct shafts of sunlight to precise points in the apartment interior. Tiny, low-cost, easily installed wireless sensors and activity recognition algorithms allow occupants to use a mobile phone interface to map activities of daily living to personalized sunlight positions. We are also developing strategies to control LED luminaires to turn off, dim, or tune the lighting to more energy-efficient spectra in response to the location, activities, and paths of the occupants. </p>", "people": ["kll@media.mit.edu"], "title": "_Context-Aware Dynamic Lighting", "modified": "2016-12-08T20:40:51.604Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "OLD_context-aware-dynamic-lighting"}, {"website": "", "description": "<p>Sculptural artifacts that model and reveal the embedded history of human thought and scientific principles hidden inside banal digital technologies. These artifacts provide alternative ways to engage and understand the deepest interior of our everyday devices, below the circuit, below the chip. They build a sense of the machines within the machine, the material, the grit of computation.</p>", "people": ["tjlevy@media.mit.edu", "slavin@media.mit.edu"], "title": "beneath the chip", "modified": "2016-12-05T00:17:01.746Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2015-09-01", "slug": "beneath-the-chip"}, {"website": "", "description": "<p>Chit Chat Club is an experiment in bringing people together in both immediate and online spaces. It consists of a set of communication portals in the form of human-scale sculptures; these give the remote user equivalent presence among the co-located participants. Each sculpture features a different interaction style, allowing us to evaluate the social effect of different interfaces.</p>", "people": ["judith@media.mit.edu"], "title": "Telematic Sculptures (Chit Chat Club II)", "modified": "2016-12-05T00:17:01.780Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "telematic-sculptures-chit-chat-club-ii"}, {"website": "", "description": "<p>This project explores the utility of voice in a range of applications offering services to users of the telephone network. Topics being examined include voice mail, speech synthesis of e-mail, access to calendars and Rolodexes, and speech-based user interface to call-processing features, such as variable call forwarding. Visual- (on the workstation) and speech- (over the telephone) based applications offer differing views of the same underlying databases in an office environment.</p>", "people": ["geek@media.mit.edu"], "title": "Telephone-Based Voice Services", "modified": "2016-12-05T00:17:01.816Z", "visibility": "PUBLIC", "start_on": "1988-12-31", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "1997-12-30", "slug": "telephone-based-voice-services"}, {"website": "", "description": "<p>The goal of the TeleReporter project is to design a system that allows for complete and open interaction between news providers and the news audience. The audience will, as a group, be able to \"direct\" the flow of a live news report by offering suggestions and requests on what actions to take, who to talk to, or what questions to ask. The TeleReporter is an extension of the Tele-Actor system, an ongoing research project in telepresence in development at the Media Laboratory and the University of California at Berkeley. The Tele-Actor interface allows remote users to share control of a human \"Tele-Actor,\" sending commands decided by majority vote to the actor and following progress through a live video feed streaming from a camera on the actor's person.</p>", "people": ["judith@media.mit.edu", "monster@media.mit.edu"], "title": "TeleReporter", "modified": "2016-12-05T00:17:01.837Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-449", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "telereporter"}, {"website": "", "description": "<p>Aerial imaging and sensor nodes each present a unique view point into the world. Using Unmanned Aerial Vehicles (UAVs) to navigate the same space shared by sensor networks, this project aims explore interaction between two historically disparate systems for purposes of both immersive experiences and scientific research. Users will remotely navigate landscapes using latest generation of head-mounted dsplays and see real-time sensor data as an augmented overlay with a real-time video stream from the UAV.</p>", "people": ["joep@media.mit.edu", "ddh@media.mit.edu", "vram@media.mit.edu"], "title": "Quadra-Sense: Confluence of UAVs, Enviornmental Sensor Networks, and Augmented Reality", "modified": "2017-05-17T23:11:48.250Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2016-01-01", "slug": "quadra-sense-confluence-of-uavs-enviornmental-sensor-networks-and-augmented-reality"}, {"website": "", "description": "<p>Real-time geospatial data is visualized on an exhibition-scale 3D city model. The model is built of LEGO bricks, and visualization is performed by an array of calibrated projectors. Through computation, GIS data is \"LEGO-tized\" to create a LEGO abstraction of existing urban areas. Data layers include mobility systems, land use, social media, business activity, windflow simulations, and more.</p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu", "mkh@media.mit.edu", "csmuts@media.mit.edu"], "title": "CityScope Mark I: Real-Time Data Observatory", "modified": "2018-01-08T21:22:13.710Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": "2016-01-01", "slug": "cityscope-mark-i-real-time-data-observatory"}, {"website": "", "description": "<p><b>&nbsp;Technical Summary</b></p><p>Sepsis is a life-threatening disease and one of the major causes of death in hospitals. Imaging of microcirculatory dysfunction is a promising approach for automated diagnosis of sepsis. We report a machine learning classifier capable of distinguishing non-septic and septic images from dark field microcirculation videos of patients. The classifier achieves an accuracy of 89.45%. The area under the receiver operating characteristics of the classifier was 0.92, the precision was 0.92 and the recall was 0.84. Codes representing the learned feature space of trained classifier were visualized using t-SNE embedding and were separable and distinguished between images from critically ill and non-septic patients. Using an unsupervised convolutional autoencoder, independent of the clinical diagnosis, we also report clustering of learned features from a compressed representation associated with healthy images and those with microcirculatory dysfunction. The feature space used by our trained classifier to distinguish between images from septic and non-septic patients has potential diagnostic application.</p>", "people": [], "title": "Machine Learning Algorithms for Classification of Microcirculation Images from Septic and Non-Septic Patients", "modified": "2018-10-22T16:12:34.980Z", "visibility": "LAB", "start_on": "2018-03-01", "location": "", "groups": ["health-0-0"], "published": false, "active": false, "end_on": "2018-10-17", "slug": "machine-learning-algorithms-for-classification-of-microcirculation-images-from-septic-and-non-septic-patients"}, {"website": "", "description": "<p>The crystal oscillator inside a quartz wristwatch vibrates at 32,768 times per second. This is too fast for a human to perceive, and it's even more difficult to imagine its interaction with the mechanical circulation of a clock. 32,768 Times Per Second is a diagrammatic, procedural, and fully functional sculpture of the electro-mechanical landscape inside a common wristwatch. Through a series of electronic transformations, the signal from a crystal is broken down over and over, and then built back up to the human sense of time.</p>", "people": ["tjlevy@media.mit.edu", "slavin@media.mit.edu"], "title": "32,768 Times Per Second", "modified": "2016-12-05T00:17:01.854Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "32768-times-per-second"}, {"website": "", "description": "<p>MITes (MIT environmental sensors) are low-cost, wireless devices for collecting data about human behavior and the state of the environment. Nine versions of MITes have now been developed, including MITes for people movement (3-axis accelerometers), object movement (2-axis accelerometers), temperature, light levels, indoor location, ultra-violet light exposure, heart rate, haptic output, and electrical current flow.  MITes are being deployed to study human behavior in natural settings. We are also developing activity recognition algorithms using MITes data for health and energy applications. (A House_n Research Consortium Initiative funded by the National Science Foundation.)</p>", "people": ["kll@media.mit.edu", "intille@media.mit.edu"], "title": "MITes: Portable Wireless Sensors for Studying Behavior in Natural Settings", "modified": "2017-10-13T16:20:40.650Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "1CC-4th Floor", "groups": ["city-science"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "mites-portable-wireless-sensors-for-studying-behavior-in-natural-settings"}, {"website": "", "description": "<p>We introduce a family of fragile electronic musical instruments designed to be \u201cplayed\u201d through the act of destruction. Each Fragile Instrument consists of an analog synthesizing circuit with embedded sensors that detect the destruction of an outer shell, which is destroyed and replaced for each performance. Destruction plays an integral role in both the spectacle and the generated sounds.</p>", "people": ["joep@media.mit.edu", "tod@media.mit.edu", "x_x@media.mit.edu", "ddh@media.mit.edu"], "title": "Fragile Instruments", "modified": "2017-05-17T23:43:56.207Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2017-12-01", "slug": "fragile-instruments"}, {"website": "", "description": "<p>TextNet is a \"distributed\" wiki. While normal wikis are designed for groups to create a single document collaboratively, TextNet is built for authors writing multiple linked documents and texts with overlapping or partially overlapping content. TextNet helps writers to compute, track, and merge differences between diverged documents. The intention is to help writers easily collaborate where possible\u2014while differing where necessary\u2014in an ad hoc and lightweight manner. The project builds on thinking and tools from the world of distributed source management and version-control systems used in the free and open-source software community.</p>", "people": ["csik@media.mit.edu", "walter@media.mit.edu"], "title": "TextNet", "modified": "2019-02-06T21:44:32.812Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "textnet"}, {"website": "http://deepempathy.mit.edu/", "description": "<h2><i>What would&nbsp;<b>your city</b>&nbsp;look like after a disaster?</i>&nbsp;</h2><p>Deep Empathy&nbsp;is&nbsp;a collaboration between the&nbsp;<a href=\"http://www.media.mit.edu/groups/scalable-cooperation\">Scalable Cooperation</a>&nbsp;group and the&nbsp;<a href=\"https://www.unicef.org/innovation\">UNICEF Innovation Office</a>&nbsp;to pursue a scalable way to increase empathy.&nbsp;</p><p>The brutal, six-year-old Syrian war has affected more than 13.5 million people in Syria , including 80% of the country's children\u20148.4 million young lives shattered by violence and fear. Hundreds of thousands of people have been displaced and their homes destroyed.&nbsp;</p><p>But people generate a response that statistics can't. And technologists\u2014through tools like AI\u2014have opportunities to help people see things differently. We wondered: \"Can AI increase empathy for victims of far-away disasters?\" This question led us to create a provocation for the research community to examine how AI can create narratives to tell the stories of some of the world's most intractable problems.</p>", "people": ["cebrian@media.mit.edu", "dubeya@media.mit.edu", "pinary@media.mit.edu", "irahwan@media.mit.edu", "nobradov@media.mit.edu"], "title": "Deep Empathy", "modified": "2017-12-04T19:54:02.578Z", "visibility": "PUBLIC", "start_on": "2017-12-01", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": "2018-05-01", "slug": "deep-empathy"}, {"website": "", "description": "<p>The pioneering research of the Electronic Publishing group has explored a \"constructionist\" approach to the development of personal and community publishing tools in order to help increase the level of personal and community participation and appropriation. In the early days of the Internet we built systems to fine-tune and prioritize information based on criteria such as timeliness, importance, and relevance. Still, the expectations of the Internet consumer closely aligned with those of the traditional media consumer\u2014the need for an editor, whether human or machine, to reduce complexity and expose the essence of available information. But we are beginning to expect more than just efficient access to ideas. Our goal is to expand scope rather than restrict it; to that end, we are exploring a unique feature of electronic media: you can tinker under the hood because every Web browser allows any content to reveal its inner structure. We are attempting to build representations of domain knowledge, but also local knowledge about people, local cultures, and norms. We aim to make the means of expression accessible without diminishing quality or complexity.</p>", "people": ["walter@media.mit.edu"], "title": "Electronic Publishing", "modified": "2019-02-06T21:44:53.165Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2006-09-01", "slug": "electronic-publishing"}, {"website": "", "description": "<p>Many people play strategy games against computer opponents, but don't have a clear understanding of how they function. Building your own Deep Blue is a formidable challenge, but is perhaps a good way to learn the computer science and mathematics of decision making. Since most people aren't computer scientists, they need a way to describe game strategies so they can be executed by a computer they can play against. This research features a graphic toolkit for designing strategy-games and computer opponents, tailored to support learning artificial intelligence techniques for deciding which next moves are better than others.  The demonstration of the toolkit is complemented by initial results of a study of how kids articulate their strategies and modify them after seeing their effect on game play.</p>", "people": ["walter@media.mit.edu"], "title": "Who's Got Game?", "modified": "2019-02-06T21:45:08.761Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-320G", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "whos-got-game"}, {"website": "https://medium.com/mit-media-lab/becoming-someone-54ed1798a1b7", "description": "<p>Becoming Someone is a multi-modal concept that lives across Medium, Instagram, and human minds. It is comprised of The Ever Contracting Void, &nbsp;an Instagram-based container by Micah Epstein commissioned by Manuel Cebrian and Iyad Rahwan as the visual metaphor for the &nbsp;essay Becoming Someone. The Ever Contracting Void is a media container that exists within &nbsp;<a href=\"https://www.instagram.com/directory.of.worlds/\">The Directory of Worlds</a>, an installment in Micah\u2019s ongoing Instagram installation. The Directory is a series of accounts, called \"worlds,\" which serve as spaces for exploration and reprieve from Instagram\u2019s visual overload and hyper-targeted marketing. Where his previous worlds were an opportunity, the Void is a threat. The Void contains nothing but the source code of popular social media websites, which use machine learning to drive the internet ever inwards upon the individual. Trapped in its center is an individual, whose very identity hinges on the viewer double-tapping the screen to like the image. What remains, when the Void closes upon the individual, leaving nothing but pristine white grids and information superhighways?</p><p>Read the articulation <a href=\"https://medium.com/mit-media-lab/becoming-someone-54ed1798a1b7\">here</a> and explore the visual container on Instagram <a href=\"http://instagram.com/ever.contracting.void\">here</a>.&nbsp;</p>", "people": ["cebrian@media.mit.edu", "irahwan@media.mit.edu"], "title": "Becoming Someone", "modified": "2018-04-23T17:54:11.668Z", "visibility": "PUBLIC", "start_on": "2018-04-05", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": "2018-04-26", "slug": "becoming-someone"}, {"website": "http://maggic.ooo", "description": "<p><span style=\"font-weight: normal;\"><i>I am a traveling saleswoman, a nomadic cuntress performing with sex hormones.</i>&nbsp;</span></p><p>Exhibited at Raumschiff Gallery during Ars Electronica in Linz, Hormone Microperformance is an installation of hormonal shrines situated next to the \"freak-science\" experimentation process from which it originates. Hormones, when isolated from the body, act as pheromones that can influence the mind and behavior through chemical signaling. Using urine samples given by the other artists of the show, sex hormones were extracted and connected to oxygen masks for the audience to inhale and experience. What they experience is a microcolonization of the mind that is both ancient and evolutionary, but imperceptible to the naked eye.<br></p>", "people": ["maggic@media.mit.edu"], "title": "Hormone Microperformance", "modified": "2017-10-11T20:29:21.605Z", "visibility": "PUBLIC", "start_on": "2016-09-07", "location": "", "groups": ["design-fiction"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "microperformativity-sex-hormones"}, {"website": "", "description": "<p>This project explores how television fans appropriate video for personal expression and how technology can support such creative appropriation. Televisions do not have an equivalent to a Web browser's \"view source\" option; however, programs can be structured by their transcripts, which are embedded as closed captions in the signal of most shows. With our talkTV video-editing software, rearranging lines of dialogue automatically creates new scenes, thereby enabling television viewers to become authors and editors.</p>", "people": ["walter@media.mit.edu"], "title": "talkTV", "modified": "2019-02-06T21:45:19.869Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-320G", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "talktv"}, {"website": "", "description": "<p>SoundFORMS creates a&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">new method for composers of electronic music to&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">interact with their compositions. Through the use of a&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">pin-based shape-shifting display, synthesized&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">waveforms are projected in three dimensions in real&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">time affording the ability to hear, visualize, and interact&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">with the timbre of the notes. Two types of music&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">composition are explored: generation of oscillator&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">tones, and triggering of pre-recorded audio samples.&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">The synthesized oscillating tones have three timbres:&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">sine, sawtooth and square wave. The pre-recorded</span></p><p>audio samples are drum tracks. Through the use of a&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">gestural vocabulary, the user can directly touch and&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">modify synthesized waveforms.</span></p>", "people": ["ishii@media.mit.edu", "ddh@media.mit.edu", "tice@media.mit.edu"], "title": "SoundFORMS: Manipulating Sound Through Touch", "modified": "2017-05-26T15:59:57.300Z", "visibility": "PUBLIC", "start_on": "2016-02-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2016-06-01", "slug": "soundform"}, {"website": "", "description": "<p>Nothing triggers memories like smell.  Momentary, fleeting, and at times unexpected, one scent can conjure up the warmth of a grandparent, or the heat of a first kiss.   </p><p>Certain botanicals are known for their olfactive properties.  Evolved to seduce pollinators and to proliferate the plant's own genes, the fragrance of flowers have also become entangled in our human dance of seduction.  </p><p>In the art of perfumery, we have long extracted the scents of flowers to apply to ourselves- what if we did the contrary and engineered a plant to emit the odor profile of a person instead?   Could we design new rituals for mourning, new biologies for remembering? </p><p>This project is the speculative design of a plant that smells like a person who is emotionally significant to me, but has passed away.  </p><p>Commercial agendas often drive the progress of certain trajectories of engineering.  This project explores the an alternative design of plants that is not driven nor thoroughly integrated in capitalist production.  Exploring emotions such as loneliness, isolation, and feelings of guilt and anxiety towards human impacts on the environment, the function of these inquiries is to reflect on past, current, and future trajectories of human influences on plant life.</p>", "people": ["sputniko@media.mit.edu", "wonder@media.mit.edu"], "title": "Forget Me Not: The Botany of Desire & Loss", "modified": "2017-10-11T20:31:21.574Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["design-fiction"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "biome-botany-for-sensorial-memory-2"}, {"website": "", "description": "<p>TeleScrapbook and TelePostcard are pairs of wirelessly connected electronic scrapbooks and electronic greeting cards, respectively. These electronic pages use I/O Stickers\ufffdadhesive electronic sensors and actuators\ufffdto allow users to design their own interfaces for remote communication. This project combines the creative affordances of traditional paper craft with electronic interactivity and long-distance communication. The simple, low-bandwidth connections made from these sensors and actuators leave room for users to design not only the look and function of the pages, but also the signification of the connections. By attaching I/O Stickers to these special books and greeting cards, users can invent ways to communicate with long-distance loved ones with personalized messages that are also connected in real time.</p>", "people": ["cynthiab@media.mit.edu", "leah@media.mit.edu", "jieqi@media.mit.edu"], "title": "TeleScrapbook and TelePostcard", "modified": "2016-12-05T00:17:02.152Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["high-low-tech", "personal-robots"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "telescrapbook-and-telepostcard"}, {"website": "", "description": "<p>eRadio: Empowerment Through Community Electronic Radio is a set of logistics and electronic tools designed to help underserved communities that suffer from multiple communication gaps (lack of content, audience, means, and motivation, stemming from underestimation of their resources and capabilities). The logistics intend to develop a community-specific iterative process of self-discovery and empowerment, bringing together eTools, applications, procedures, and people, and making use of inquiry, discovery, and ethno-methodological strategies and techniques to get the community to generate content \"a voice\" and to broadcast and Webcast it so as to multiply participation, generate more feedback, and further the iterative process. Inter-community and diaspora-community involvement is also foreseen. The tool, named \"eAudioKit,\" is a low-cost, user-friendly data and audio processor, fitted with specific software and open-source applications (eTools), organized, integrated, and customized for recording, editing, encoding, archiving, and uploading small audio pieces and audio projects or radio programs.</p>", "people": ["walter@media.mit.edu"], "title": "eRadio: Empowerment Through Community Electronic Radio", "modified": "2019-02-06T21:47:11.178Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-320B", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "eradio-empowerment-through-community-electronic-radio"}, {"website": "", "description": "<p>Every Sign of Life challenges assumptions about how we might think and feel about personal health monitoring. It is an exploration of how to make information collected by personal health-monitoring devices fun and engaging, and consequently more useful to the non-specialist. The approach is to design and build computer games based on such information. The goal is self-efficacy; to implicitly make people take care of their own health by altering their habits and by health-aware planning of their lives. This work tests the hypothesis that fun (the fun of learning, achieving, competing) is a way to achieve this goal. One research focus explores the basic architecture for personal health monitoring systems, which has led to a new approach to design of sensor peripherals and wearable computer components called \"Extremity Computing.\" This approach is used to redefine biosensor monitoring from periodic to continuous (ultimately saving data over a lifetime). Another research focus explores adding implicit biofeedback to computer games, which has led to a new genre of games that straddles the boundary between sports and computer games called \"bio-analytical\" games.</p>", "people": ["walter@media.mit.edu"], "title": "Every Sign of Life", "modified": "2019-02-06T21:47:29.955Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["electronic-publishing", "gray-matters"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "every-sign-of-life"}, {"website": "", "description": "<p>Email lists are useful for communicating with groups, but it is difficult to refine the intended audience. When planning a surprise party, for example, it is time consuming to delete the guest of honor's address from the multiple e-mail lists one wants to contact. We have built a tool that makes it easy to modify e-mail lists and, thus, contact specific audiences. The tool provides the computational mechanism for solving 'e-mail equations'. An equation for the aforementioned example could be ((partyPeople@mit.edu &amp; dancers@mit.edu) - joe@mit.edu). This equation would resolve to address all recipients of two e-mail lists except for joe@mit.edu. E-mail list addition, subtraction, intersection and xor can be computed with our tool.</p>", "people": ["walter@media.mit.edu"], "title": "Email Equations", "modified": "2019-02-06T21:47:58.684Z", "visibility": "PUBLIC", "start_on": "2001-12-31", "location": "E15-311", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "email-equations"}, {"website": "", "description": "<p>This paper explores how an actuated pin-based shape display may serve as a platform on which to build musical instruments and controllers. We designed and prototyped three new instruments that use the shape display not only as an input device, but also as a source of acoustic sound. These cover a range of interaction paradigms to generate ambient textures, polyrhythms, and melodies. This paper first presents existing work from which we drew interactions and metaphors for our designs. We then introduce each of our instruments and the back-end software we used to prototype them. Finally, we offer reflections on some central themes of NIME, including the relationship between musician and machine.</p>", "people": ["rebklein@media.mit.edu", "thomassl@media.mit.edu", "pewebb@media.mit.edu", "joep@media.mit.edu", "tod@media.mit.edu", "akito@media.mit.edu", "x_x@media.mit.edu", "ishii@media.mit.edu", "ddh@media.mit.edu"], "title": "Kinephone: Exploring the acoustic musical potential of an actuated pin-based shape display", "modified": "2017-05-18T01:07:33.691Z", "visibility": "PUBLIC", "start_on": "2016-05-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2016-08-01", "slug": "kinephone"}, {"website": "", "description": "<p>SilverWire is a tool for increasing socialization and augmenting communication among communities that actively publish content on the World Wide Web. It is designed to build bridges across community boundaries by comparing the works of different community publishers and by introducing members of different communities to each other. SilverWire also provides a way for conventional media companies to better serve these emerging community publications by integrating professionally created information feeds with community publications. The goal of the SilverWire system is to be a Web intermediary that makes communities more aware of other communities doing similar (or interestingly different) things.</p>", "people": ["walter@media.mit.edu"], "title": "SilverWire", "modified": "2019-02-06T21:48:54.409Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-001", "groups": ["electronic-publishing", "gray-matters"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "silverwire"}, {"website": "http://rnd.studio/project/amoeba-wall", "description": "<p>Amoeba Wall: a context aware wall system. Amoetecture is a set of amoeba-like dynamic spatial elements, including transformable floors, ceilings, tables, chairs, and workstations. We focus on designing architecture robotics and platforms that enable a hyper-efficient and dynamically reconfigurable co-working space that accommodates a wide range of activities in a small area.</p><p><strong>&nbsp;Award</strong></p><p>-<a href=\"https://www.designboom.com/design/2019-a-design-award-and-competition-call-for-entries-02-07-2019/\"><strong>A' Design Award&nbsp;</strong><strong>2017 -&nbsp;</strong><strong>Gold Prize</strong></a></p><p>-Honorable Mention - Tomorrow Workplace Competition by METROPOLIS</p><p><b>Publication</b></p><p>-<a href=\"http://www.academia.edu/35684876/Amoeba_Wall\">H Deng, H Ho, L Alonso, X Li, J Angulo, K Larson Amoeba Wall - PASAJES - archquitectura NO.143, pp8-9</a></p>", "people": ["alonsolp@media.mit.edu", "honghaod@media.mit.edu", "oi7@media.mit.edu"], "title": "Amoetecture", "modified": "2019-02-11T15:09:42.775Z", "visibility": "PUBLIC", "start_on": "2016-04-22", "location": "", "groups": ["city-science", "changing-places"], "published": true, "active": false, "end_on": "2017-11-30", "slug": "_amoeba"}, {"website": "", "description": "<p>SHARE is a robotic cognitive architecture focused on manipulating and understanding the phenomenon of shared attention during interaction. SHARE incorporates new findings and research in the understanding of nonverbal referential gesture, visual attention system research, and interaction science. SHARE's research incorporates new measurement devices, advanced artificial neural circuits, and a robot that makes its own decisions.</p>", "people": ["cynthiab@media.mit.edu", "ndepalma@media.mit.edu"], "title": "SHARE: Understanding and Manipulating Attention Using Social Robots", "modified": "2017-10-15T17:57:01.084Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2017-05-31", "slug": "share-understanding-and-manipulating-attention-using-social-robots"}, {"website": "https://github.com/shayneobrien", "description": "<p>Talk radio exerts significant influence on the political and social dynamics of the United States, but labor-intensive data collection and curation processes have prevented previous works from studying its content at scale. Over the past year, the Laboratory for Social Machines and Cortico have created a talk radio ingest system to record and automatically transcribe audio from more than 160 stations around the country. Using these transcripts, we propose novel compression-based methods for unsupervised summarization of spoken opinion in conversational dialogue. By relying on an unsupervised framework that obviates the need for labeled data, the summarization task becomes largely agnostic to human input beyond necessary decisions regarding model architecture, input data, and output length. As a result, trained models are able to produce a more accurate depiction of opinion. Using the outputs of my proposed methods, we conduct a case study to examine the variability of public opinion across America. In the interests of reproducibility and further research, we open-source all code and data used.&nbsp;</p>", "people": ["dkroy@media.mit.edu", "shayneob@media.mit.edu"], "title": "Spoken Opinion Summarization", "modified": "2019-04-17T00:15:52.766Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2019-06-07", "slug": "spoken-opinion-summarization"}, {"website": "http://oi7.me", "description": "<p>Ant-Based Modeling explores the possibility of&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">implementing agent-based modeling with living ants&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">and external stimuli such as electromagnetic radiations,&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">magnetic fields, and electric fields. In an experiment&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">with fire ants, we discovered that ultraviolet and&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">infrared lights can affect their behavior in the form of&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">attraction and dispersion towards the light,&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">respectively. The video highlights some of the LEGOmade&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">landscapes we use in our explorations and how&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">the behavior of ants can be influenced by ultraviolet&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">light to achieve certain purposes such as exploring a&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">new area or dragging a ping pong ball to a specific&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">location. These experiments have allowed us to learn&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">more about ants and have inspired us to explore novel&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">forms of human-ant interaction.</span></p>", "people": ["m_kayser@media.mit.edu", "oi7@media.mit.edu", "javierhr@media.mit.edu", "csmuts@media.mit.edu"], "title": "Ant-Based Modeling", "modified": "2018-04-26T15:20:39.904Z", "visibility": "PUBLIC", "start_on": "2016-04-22", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2017-04-22", "slug": "ant-based-modeling"}, {"website": "", "description": "<p>We add a sonic dimension to print news so that it conveys some of the vividness of video but without the advance commitment by the reader to sit through a whole report. Sounds are linked to elements on the page and are invoked automatically by eye tracking or manually as a click. We suggest two categories of sound: those that are informative and those that are indicative. The former can be quotes or actualities, the latter reflect impact and reveal who else is reading the story.\n                    \n                </p>", "people": [], "title": "SoundsAlive", "modified": "2016-12-05T00:17:02.301Z", "visibility": "LAB", "start_on": "2016-04-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2017-01-01", "slug": "soundsalive"}, {"website": "", "description": "<p>The massive sensor network assumed by most advanced scenarios of ubiquitous computing must first be innocously deployed in devices used for other purposes. Accordingly, this project has developed a sensor node embedded into a common power strip. This device has access to power (and potentially networking) through its cord, can control and measure the detailed current profile consumed by devices plugged into its outlets, supports an ensemble of sensors (microphone, light, temperature, and vibration sensors are intrinsic, and other sensors such as thermal motion and cameras can be added easily), and hosts an RF network that can connect to other PlugPoint sensors and small nearby wireless sensors. We are now exploring a host of ubiquitous computing applications that can be developed on these devices when they are deployed across the Media Lab.</p>", "people": ["geppetto@media.mit.edu", "joep@media.mit.edu"], "title": "PlugPoint\u2014A Platform for Ubiquitous Sensor Networks", "modified": "2017-10-13T18:26:49.483Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "plugpointa-platform-for-ubiquitous-sensor-networks"}, {"website": "", "description": "<p>Slurp is a tangible interface for manipulating abstract digital information as if it were water. Taking the form of an eyedropper, Slurp can extract (slurp up) and inject (squirt out) pointers to digital objects. We have created Slurp to explore the use of physical metaphor, feedback, and affordances in tangible interface design when working with abstract digital media types. Our goal is to privilege spatial relationships between devices and people while providing new physical manipulation techniques for ubiquitous computing environments. Current applications include tangible/graphical mediation, locative-media, multi-display reaching, and physicalization for online retail.</p>", "people": ["ishii@media.mit.edu"], "title": "Slurp", "modified": "2019-02-15T03:08:31.211Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "slurp"}, {"website": "", "description": "<p>Discreet Log Contracts (<a href=\"https://adiabat.github.io/dlc.pdf\">DLC</a>) are a new type of smart contract which limit the information gained&nbsp;and influence of oracles, and can run on the very limited scripting system present in Bitcoin, without&nbsp;the need for more complex languages such as in Ethereum.&nbsp;</p><p>DLC works by precomputing a wide range of potential outcomes for a given contract, and when the oracle announces an event, the event-dependent correct outcome can be published. There are a number of applications where this model applies, and the one that we\u2019ll be starting with first is <b>Bitcoin settled dollar futures</b>. This use case introduces a useful tool to mitigate the volatility of Bitcoin.</p>", "people": ["adragos@media.mit.edu", "narula@media.mit.edu", "tdryja@media.mit.edu", "joe@media.mit.edu"], "title": "Building smart contracts with Bitcoin", "modified": "2018-05-01T20:05:09.760Z", "visibility": "PUBLIC", "start_on": "2018-01-01", "location": "", "groups": ["digital-currency-initiative-dci"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "bitcoin-settled-dollar-futures"}, {"website": "", "description": "<p>SmileTracker is a system designed to capture naturally occurring instances of positive emotion during the course of normal interaction with a computer. A facial expression recognition algorithm is applied to images captured with the user's webcam. When the user smiles, both a photo and a screenshot are recorded and saved to the user's profile for later review. Based on positive psychology research, we hypothesize that the act of reviewing content that led to smiles will improve positive affect, and consequently, overall wellbeing.</p>", "people": ["jaquesn@media.mit.edu", "picard@media.mit.edu", "cvx@media.mit.edu"], "title": "SmileTracker", "modified": "2016-12-05T00:17:02.371Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["affective-computing", "advancing-wellbeing"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "smiletracker"}, {"website": "", "description": "<p>How to generate almost anything is a collaborative project between humans and AI, run by Pinar Yanardag Delul of the Scalable Cooperation group. The project combines expertise in making (inspired by Neil Gershenfeld\u2019s How To Make Almost Anything class) from around MIT with generative adversarial neural networks (GANS). Each project chooses a focus for the human-machine collaboration \u2014 from music and fashion to pizza and perfume \u2014 to create outcomes that otherwise might never have been imagined!</p><p>To read more about the project, it's fully documented at:&nbsp;https://medium.com/@howtogeneratealmostanything, and is written about here:&nbsp;https://venturebeat.com/2018/09/10/mit-students-use-ai-to-cook-up-pizza-recipes/</p>", "people": ["agnescam@media.mit.edu", "pinary@media.mit.edu"], "title": "How To Generate Almost Anything", "modified": "2018-09-17T15:49:17.289Z", "visibility": "PUBLIC", "start_on": "2018-07-01", "location": "", "groups": ["scalable-cooperation", "center-for-bits-and-atoms-1"], "published": true, "active": false, "end_on": "2018-09-30", "slug": "how-to-generate-almost-anything"}, {"website": "", "description": "<p>The availability of cheap LEDs and diode lasers in a variety of wavelengths enables creation of simple and cheap spectroscopic sensors for specific tasks such as food shopping and preparation, healthcare sensing, material identification, and detection of contaminants or adulterants. </p>", "people": [], "title": "Simple Spectral Sensing", "modified": "2016-12-05T00:17:02.537Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "simple-spectral-sensing"}, {"website": "", "description": "<p>This work focuses on the creation and prototyping of novel, mobile, interactive story experiences. The stories we create inhabit public space and invite the audience to participate and influence the story's progress as they move in, pass through, and experience that space. These stories are dynamic, responsive, and evolve as the audience plays with them in the public space. The stories are context aware: they are affected by changes to the surrounding environment, such as weather conditions, varying levels of audience activity, and time shifting, all of which influence the story through sensor-triggered story interfaces. The stories are organic, changing and growing as the audience becomes author and adds to the narrative. This collaboration (Trinity College Dublin and Media Lab Europe) brings together expertise in content creation, story design, wireless networks, and sensor technologies in the creation of these exploratory and novel story platforms. The project has implications outside its own application area as the provision of ever-changing information delivered to people in a meaningful and unobtrusive manner as they go about their daily lives is a challenging issue.</p>", "people": ["gid@media.mit.edu"], "title": "Sin Sceal Eile (That's Another Story)", "modified": "2017-09-13T21:00:16.905Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-441", "groups": ["interactive-cinema"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "sin-scal-eile-thats-another-story"}, {"website": "http://udayan-u.com", "description": "<p>DropletIO uses aqueous droplets as a form of programmable material for human-material interaction. We demonstrate how fluids in our environment can be &nbsp;programmed droplet-wise to move, merge, and split. Through these operations we can then digitally regulate fluid properties\u2014smell, color, chemical, and biological characteristics. When seamlessly integrated into a range of everyday objects and spaces, droplets become ubiquitous displays and other interactive elements aiding creative activity such as painting, storytelling, and art. Beyond this, programmable droplets have applications in digital biology and chemistry.</p>", "people": ["udayan@media.mit.edu", "ishii@media.mit.edu", "ken_n@media.mit.edu"], "title": "Droplet IO: Programmable Droplets for Human-material Interaction", "modified": "2017-10-05T21:52:41.593Z", "visibility": "LAB-INSIDERS", "start_on": "2016-10-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2017-04-06", "slug": "droplet-io"}, {"website": "", "description": "<p>Mumble Melody uses musically altered sensory feedback as a potential treatment for stuttering.<br></p><p>Several studies have shown improvement in speech fluency with delayed and pitch-altered auditory feedback.&nbsp; In this project, we use sensory/auditory alterations that stimulate both the right (and left) hemispheres as a means of reducing the auditory feedback-mediated errors in basal ganglia-related motor selection&nbsp; &nbsp; &nbsp;<br></p><p>Stuttering is a condition characterized by involuntary, periodic disturbances in speech fluency, usually via speech sound repetitions, blockages, or prolongations. A host of other secondary features also accompany the condition (e.g. tongue thrusting, eye blinking, body movements), although these are not considered \u201ccore\u201d features. Stuttering improves when an individual\u2019s speech is played back to him or her in an altered manner, most famously when delayed by fractions of second, but also when the frequency is shifted, when masked with white noise, and when reading in choral speech. This rather interesting phenomenon of altered feedback-induced fluency is theorized to result from a reduced ability to detect small errors in articulation that occur in stuttering, which reduces its inhibition on speech initiation and output via the feedback mechanism.</p><p>In people who stutter, there is both structural and functional evidence of atypical hemispheric lateralization of speech and language. People who stutter, when speaking fluently, tend to activate the right hemisphere a during speech tasks. The white matter integrity is disrupted on the left. This rightward shift of speech function may be compensatory (as opposed to causal). Trials comparing fluent versus non-fluent trials in people who stutter reveal the former to associated with activity in the right hemisphere and latter with the left hemisphere. In addition, white matter integrity is negatively correlated with severity of dysfluency on the left, and positively correlated on the right. The overall notion is that stuttering is associated with atypical left-sided speech mechanisms, and that this can be overcome, at least partially, when the right hemisphere is able to effectively compensate. &nbsp;While most prominently explored in stuttering, the idea that left hemisphere lesions can be overcome by shifting the motor control of speech to the right is supported by other studies in post-stroke aphasia.</p><p>In this light, altered auditory feedback\u2014a fluency-inducing intervention in stuttering\u2014is associated with activity in right hemispheric sensory, motor, and language areas. In addition, singing\u2014another fluency-evoking task\u2014is known to activate right hemisphere motor areas compared with non-musical speech production.</p>", "people": ["rebklein@media.mit.edu"], "title": "Mumble Melody", "modified": "2018-10-22T16:47:03.821Z", "visibility": "PUBLIC", "start_on": "2018-03-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2019-01-01", "slug": "mumble-melody"}, {"website": "", "description": "<p>This category of Music Toy is being designed as an inexpensive, stand-alone, sound-producing, and manipulating device that gains power when networked with other Simple Things. Each Simple Thing is hand-held, and contains several buttons and a single continuous pressure controller. Sound memory and a simple processor allow the storage of waveforms, and their modulation or manipulation. IR links allow sound to be exchanged from one Simple Thing to another, as well as for Simple Things to be synchronized in large numbers. We are conducting experiments in the use of large numbers of Simple Things in physical proximity, and as remote music controllers via the Internet. In addition, we are exploring the use of Simple Things as a means of bringing personalized music and musical information between public performance experience spaces and home.</p>", "people": ["tod@media.mit.edu"], "title": "Simple Things", "modified": "2016-12-05T00:17:02.564Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-483", "groups": ["opera-of-the-future", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "simple-things"}, {"website": "", "description": "<p>The human singing voice is the oldest musical instrument, yet it is one of the most difficult to simulate convincingly. Using signal processing and pattern-recognition techniques in combination with prior knowledge of the musical score, we are attempting to extract control parameters which capture the most perceptually significant features. This parameterized, structured model could then be efficiently transmitted and re-synthesized, resulting in a high-quality recreation of the original performance.</p>", "people": ["bv@media.mit.edu"], "title": "Singing Voice Parameterization and Re-synthesis", "modified": "2016-12-05T00:17:02.684Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "--Choose Location", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "singing-voice-parameterization-and-re-synthesis"}, {"website": "", "description": "<p>Computational Food is a series of experiments around the shape-changing nature of food and its associated unique sensory experiences. We used food&nbsp;as a <i>medium</i>&nbsp;and a <i>platform</i> to develop dynamic, shape-changing prototypes that are edible or that enhance the culinary experience.</p>", "people": ["inamura@media.mit.edu", "djain@media.mit.edu", "amores@media.mit.edu", "viirj@media.mit.edu"], "title": "Computational Food", "modified": "2019-04-29T22:10:01.940Z", "visibility": "PUBLIC", "start_on": "2014-09-16", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2014-12-31", "slug": "computational-food"}, {"website": "", "description": "<p>A design project done in collaboration with the MIT Media Lab and the Laboratorio para la Ciudad (Laboratory for the City), Mexico City's experimental office for civic innovation and urban creativity, A-pops is a networked learning experience across Mexico City that supports young learners in engaging in emergent and playful opportunities in and beyond their local communities. In line with the \"Playful City\" goal, this project aims to embed playful learning experiences across Mexico City that are creative, collaborative, and public, by leveraging existing public spaces throughout neighborhoods and micro-communities across the city. By embedding a variety of playful learning experiences across a variety of locations, a wide range of learners have the ability to easily and socially engage in transformative experiences that support key skills in design, collaboration, creativity, programming, and learner agency.</p>", "people": ["jgroff@media.mit.edu", "gbernal@media.mit.edu", "thomassl@media.mit.edu", "minasg@media.mit.edu"], "title": "A-pops", "modified": "2018-10-20T18:17:15.133Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten", "ml-learning"], "published": true, "active": false, "end_on": "2017-06-01", "slug": "a-pops"}, {"website": "", "description": "<p>Media Meter Focus shows focus mapping of global media attention. What was covered in the news this week? Did the issues you care about get the attention you think they deserved? Did the media talk about these topics in the way you want them to? The tool-set also shows news topics mapped against country locations.</p>", "people": ["ethanz@media.mit.edu", "a_hashmi@media.mit.edu"], "title": "Media Meter Focus", "modified": "2016-12-05T00:17:02.737Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["civic-media", "future-of-news"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "media-meter-focus"}, {"website": "http://drugastefania.com", "description": "<p>The main purpose of this project is to develop a hands-on learning activity around tinkering with microbiological bricks to be play-tested with kids and educators. </p><p class=\"\"><span style=\"font-size: 18px; font-weight: normal;\" class=\"\">I am exploring how microorganisms can create generative art and music and &nbsp;how we could transform any affordable webcam into a &nbsp;microscope that can connect directly to the computer. I am also looking at how this live stream from microcosmos world could &nbsp;be visualized in many different ways by applying multiple filters and image transformations such that it provides us with many new perspective about the world around us. This live connection to living organisms enables us also to imagine new ways of interacting and tinkering with them. &nbsp;Some initial explorations lead to the creation of a bio-kaleidoscope that is alive (literally).&nbsp;</span></p><p class=\"\"><span style=\"font-size: 18px; font-weight: normal;\" class=\"\">I hope this project will &nbsp;explore new ways in which kids could  play with the micro-cosmos around us, explore the magic at the intersection of digital, physical and bio tinkering and discover how using familiar things, such as webcams, to do unfamiliar actions can boost our creative confidence.&nbsp;</span></p>", "people": ["sdruga@media.mit.edu"], "title": "BioBricks", "modified": "2017-06-05T21:44:02.634Z", "visibility": "PUBLIC", "start_on": "2016-11-01", "location": "", "groups": ["lifelong-kindergarten"], "published": false, "active": false, "end_on": "2017-09-30", "slug": "biobricks"}, {"website": "", "description": "<p>The Passing On project uses data from 20 years of <i>New York Times</i> stories about society's heroes, leaders, and visionaries to \"reader-source\" improvements to Wikipedia. As readers explore compelling stories about notable women, content is generated to create new content and inspire the public to contribute those stories to Wikipedia.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "jnmatias@media.mit.edu"], "title": "Readersourcing", "modified": "2016-12-05T00:17:02.779Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2016-10-01", "slug": "readersourcing"}, {"website": "", "description": "<p>The \"300M IT Edition\" project is a collaboration between the Media Lab, MIT's Aero/Astro Department, DaimlerChrysler, and Motorola. Using the project platform\ufffda fully instrumented version of the Chrysler 300M\ufffdwe collected data on the driver's mental workload and emotional state. Several demonstrations showed examples of how an intelligent cockpit could improve driving. For example, it could remind drivers to drive their best, and make bicycling near the vehicle safer.</p>", "people": [], "title": "300M IT Edition", "modified": "2016-12-05T00:17:02.853Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "300m-it-edition"}, {"website": "", "description": "<p>The CalTech/MIT Voting Technology Project was established by Caltech president David Baltimore and MIT president Charles Vest in December 2000 to prevent a recurrence of the problems that threatened the 2000 U.S. Presidential Election. Specific tasks of the project include evaluating the current state of reliability and uniformity of U.S. voting systems; establishing uniform attributes and quantitative guidelines for performance and reliability of voting systems; and proposing specific uniform guidelines and requirements for reliable voting systems.</p>", "people": [], "title": "CalTech/MIT Voting Technology Project", "modified": "2016-12-05T00:17:02.832Z", "visibility": "PUBLIC", "start_on": "2000-09-01", "location": "E15-301", "groups": [], "published": true, "active": false, "end_on": "2003-09-01", "slug": "caltechmit-voting-technology-project"}, {"website": "", "description": "<p>Recent findings in affective neuroscience and psychology indicate that human affect and emotional experience play a significant and useful role in human learning and decision-making. Most machine-learning and decision-making models, however, are based on old, purely cognitive models, and are slow, brittle, and awkward to adapt. We aim to redress many of these classic problems by developing new models that integrate affect with cognition. Ultimately, such improvements will allow machines to make smarter and more human-like decisions for better human-machine interaction.</p>", "people": ["picard@media.mit.edu"], "title": "Affective-Cognitive Framework for Machine Learning and Decision Making", "modified": "2016-12-05T00:17:02.896Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "affective-cognitive-framework-for-machine-learning-and-decision-making"}, {"website": "", "description": "<h2>Overview</h2><p>We have created a new customizable, multi-user research-through-play platform designed to facilitate social skill development for children with ASD. Through the highly motivating, individualized play environment, children can progress at their own pace while practicing social skills. Early results suggest that SPRING\u2019s novel multi-player environment elicits social interaction in a way that can engage learners with very different interests.</p><h2>Background</h2><p>Practitioners have long explored using motivating, personalized reinforcement to achieve developmental goals for children with Autism&nbsp;Spectrum Disorder (ASD) (Koegel &amp; Mentis, 1985\u037e Vismara &amp; Lyons, 2007). SPRING\u2013Smart Platform for Research, Intervention, and Neurodevelopmental Growth\u2013 is a customizable, interactive, research-through-play platform, built to systematically probe the effects of these reinforcement modalities on learning and physiological regulation (Johnson &amp; Picard, 2017). SPRING has shown promise in facilitating increased engagement and skill development for children with autism and other neuro-differences, but it has lacked multi-user functionality and built-in means to prompt social skills, such as joint attention, turn taking, and cooperative play\u2013until&nbsp;now. Adding this functionality allows practitioners to customize the reinforcement and developmental challenge of each individual SPRING unit while simultaneously encouraging social engagement by linking the units over a virtual network.</p><h2>Objectives</h2><p>Here, we present a Multi-SPRING system designed to</p><ol><li>Stimulate early social experiences, such as joint attention, turn taking, and cooperative play\u037e</li><li>Facilitate simultaneous play between individuals with different skill levels, such as typical and autistic peers or siblings, while providing personalized reinforcement tuned to each individual\u2019s motivating interests\u037e</li><li>Reduce anxiety associated with unaided social interactions and extend engagement in a multi-person activity\u037e and</li><li>Passively capture time-synchronized, quantitative measures of users\u2019 affective states via wearable physiological sensors and data of users\u2019 play progressions via SPRING.</li></ol><h2>Methods</h2><p>Each SPRING unit has a removable, modular center that can be adapted to the needs of an individual child by inserting different physical modules. An integrated smartphone and embedded LEDs allow user-selected customization of motivating reinforcement, such as favorite video clips, images, music, or colorful light displays. The smartphone also enables scaffolded difficulty levels within a single module so each child can be met with the \u201cjust right\u201d challenge. Embedded digital sensors capture and store time-stamped data of a child\u2019s interaction with SPRING. Paired with wearable physiological sensors, these data allow multimodal analysis of a child\u2019s affective state and learning progression.</p><p>The new Multi-SPRING system links multiple SPRING units in realtime through a virtual private room, much like a chat room. This method enables social multiplayer interactions, such as turn taking and cooperative play, while continuously capturing activity data logs from every child for future study.</p>", "people": ["ktj@media.mit.edu"], "title": "Multi-SPRING", "modified": "2018-06-11T14:03:21.507Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["affective-computing", "ml-learning"], "published": true, "active": false, "end_on": "2018-08-01", "slug": "multi-spring"}, {"website": "", "description": "<p>\n                    Driven female professionals often choose to pursue their careers in lieu of having children. For many of them, strategies of surrogacy or freezing eggs are popular options not only because of available technological advancements, but also because of shifts in cultural perspective enabled by a new biotechnical regime. The dichotomy that forces an \"either/or\" divide between motherhood and career can be seen as a modern form of regulatory control on women. The question of reproduction becomes a matter of our bio-techno-capitalist society as a confine of women's voices and freedom. Companies such as Facebook and Apple have recently offered to pay female employees to freeze their eggs so they can continue with their careers, without interrupting their dreams of having children. However, there still remain many ethical, social, and political dilemmas which exist with surrogacy, questions that must be posed to the public. \n                </p>", "people": ["sputniko@media.mit.edu", "dkc@media.mit.edu"], "title": "-", "modified": "2017-10-11T20:30:51.528Z", "visibility": "LAB", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["design-fiction"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "digital-pregnancy-through-domestic-objects"}, {"website": "", "description": "<p>SkillGrinder facilitates knowledge sharing and learning in existing communities, making it easier to identify a community members' strengths, expertise, and skills, as well as areas of interest for new learning. </p>", "people": ["jgroff@media.mit.edu", "klopfer@media.mit.edu"], "title": "SkillGrinder", "modified": "2018-06-20T19:59:23.353Z", "visibility": "LAB", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["ml-learning"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "skillgrinder"}, {"website": "", "description": "<p>&nbsp;When the body senses itself internally and localizes its actions, it provides the basis for a material sense of self-existence. At the same time, the mind registers the sense of an agency with free will, the sense of being, the cause of voluntary action. Among all interoceptive experiences, respiration is the only one that we can regulate directly. There are many&nbsp;psychophysical breathing exercises&nbsp;to help self-regulation and reflection, that, combined with meditation and yoga, are designed to restore natural, smooth breathing appropriate to the physical needs of the body.&nbsp;</p>", "people": ["xxxxxxin@media.mit.edu", "pattie@media.mit.edu"], "title": "Masque", "modified": "2018-10-20T21:47:28.195Z", "visibility": "PUBLIC", "start_on": "2017-01-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-05-31", "slug": "masque"}, {"website": "http://www.valentinheun.com", "description": "<p>The Reality Editor is a web browser for the physical world: Point your phone or tablet at a physical object and an interface pops up with information about that object as well as services related to that object. The Reality Editor platform is open and entirely based on web standards making it easy for anyone to create Reality Editor enabled objects as well as Reality Editor applications that integrate the physical and digital world in one experience.<br></p><p>Reality Editor version 2.0<br></p><p>&nbsp;<b><a href=\"http://realityeditor.org\">Reality Editor &nbsp;version 2.0</a>&nbsp;</b>is now available for download and adds the following features:</p><ul><li><b>World Wide Web</b> conform content creation.<br></li><li><span style=\"font-size: 18px;\"><b>Spatial Search</b> -&nbsp;</span>Instantly browse through relevant information in the physical world around you. you to browse reality.<br></li><li><span style=\"font-size: 18px;\"><b>Bi-Directional AR</b> - A real-time interactions system.</span></li><li><b style=\"font-size: 18px;\">Private and Decentralized</b><span style=\"font-size: 18px;\"> infrastructure for connecting the IoT objects.</span><br></li><li><span style=\"font-size: 18px;\"><b>Logic Crafting</b> - A visual programming language designed for Augmented Reality.<br></span></li></ul><p>The &nbsp;Reality Editor works on iOS and you can get it <a href=\"https://itunes.apple.com/us/app/reality-editor/id997820179\"><b>here</b></a>.&nbsp;<span style=\"font-size: 18px;\">Try</span><span style=\"font-size: 18px;\">&nbsp;it out with our <a href=\"https://www.dropbox.com/s/3vd1d2v9bmm7e7s/Reality%20Editor.dmg?dl=1\"><b>Starter App</b></a> and some Philips Hue Lights or the Lego WeDo 2.0. Learn more about Logic Crafting in our <a href=\"http://realityeditor.org/getting-started/\"><b>User Interface 101</b></a>.</span></p><p></p>", "people": ["benolds@media.mit.edu", "pattie@media.mit.edu", "heun@media.mit.edu"], "title": "Reality Editor 2.0", "modified": "2018-10-11T18:46:45.692Z", "visibility": "PUBLIC", "start_on": "2017-05-22", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2017-12-31", "slug": "reality-editor-20"}, {"website": "", "description": "<p>&nbsp;We present<b> a mobile heart rate regulator</b>\u2014ambienBeat\u2014which provides <b>closed-loop ambient biofeedback via subliminal tactile stimulus</b> based on a user's heartbeat rate variability (HRV). We applied the principle of interpersonal physiological synchronization to achieve our goal of effortless regulation of HRV, which is tightly coupled with mental stress levels. ambienBeat provides various patterns of subliminal tactile stimuli, which mimics the feeling of a heartbeat pulse, to guide a user's HRV to resonate with its rhythmic, tactile patterns. The strength and rhythmic patterns of tactile stimulation are controlled to a level below the cognitive threshold of an individual's tactile sensitivity on their wrist so as to minimize task disturbance. Here we present an acoustically noiseless, soft voice-coil actuator to render the ambient tactile stimulus and present the system and implementation process. We evaluated our system by comparing it to ambient auditory and visual guidance. Results from the user study shows that the subliminal tactile stimulation was effective in guiding a user's HRV to resonate with ambienBeat to either calm or boost the heart rate using minimal cognitive load.&nbsp;</p>", "people": ["yun_choi@media.mit.edu"], "title": "ambienBeat: Mobile tactile biofeedback for subliminal heart rate rhythmic regulation", "modified": "2019-04-22T16:40:11.719Z", "visibility": "LAB-INSIDERS", "start_on": "2019-01-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2019-05-31", "slug": "ambienbeat"}, {"website": "", "description": "<p>Following the 2016 election, the entirety of the nation became conscious of its polarization. According to a study by the National Bureau of Economic Research*, polarization has increased among Americans since 1990. The study observes, however, that in eight of the nine measures of polarization, older individuals (70+ age group) show higher rates of increase in polarization than other age groups. This age group also utilizes social media less than other age-groups. Could it be that social media is not the root cause of polarization?</p><p>In order to explore this further, we looked at polarization through talk radio, which is commonly thought to have political influence.&nbsp;</p>", "people": ["anderton@media.mit.edu", "kalli@media.mit.edu", "lip@media.mit.edu", "hbedri@media.mit.edu"], "title": "Radio Days", "modified": "2019-04-11T15:26:19.908Z", "visibility": "PUBLIC", "start_on": "2018-10-12", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "radio-days"}, {"website": "", "description": "<p>Pure networks and pure hierarchies both have distinct strengths and weaknesses. These become glaringly apparent during disaster response. By combining these modes, their strengths (predictability, accountability, appropriateness, adaptability) can be optimized, and their weaknesses (fragility, inadequate resources) can be compensated for. Bridging these two worlds is not merely a technical challenge, but also a social issue.</p>", "people": ["ethanz@media.mit.edu", "bl00@media.mit.edu"], "title": "Mixed-Mode Systems in Disaster Response", "modified": "2016-12-05T00:17:03.349Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2016-01-01", "slug": "mixed-mode-systems-in-disaster-response"}, {"website": "", "description": "<p>How can we show our 16-megapixel photos from our latest trip on a digital display? How can we create screens that are visible in direct sunlight as well as complete darkness? How can we create large displays that consume less than 2W of power? How can we create design tools for digital decal application and intuitive-computer aided modeling? We introduce a display that is high-resolution but updates at a low frame rate: a slow display. We use lasers and monostable light-reactive materials to provide programmable space-time resolution. This refreshable, high-resolution display exploits the time decay of monostable materials, making it attractive in terms of cost and power requirements. Our effort to repurpose these materials involves solving underlying problems in color reproduction, day-night visibility, and optimal time sequences for updating content.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "Slow Display", "modified": "2017-04-03T18:23:43.031Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "slow-display"}, {"website": "", "description": "<p><b>ATEN\u00c7\u00c3O:</b> Saiu o <b>resultado do Desafio Aprendizagem Criativa Brasil 2017</b>! Clique <a href=\"https://www.media.mit.edu/posts/resultado-do-desafio-aprendizagem-criativa-brasil-2017/\">aqui</a> para conhecer os fellows e projetos selecionados!</p><p>----</p><p>&nbsp;Desafio Aprendizagem Criativa Brasil \u00e9 uma iniciativa da Funda\u00e7\u00e3o Lemann e do MIT Media Lab que visa fomentar a implementa\u00e7\u00e3o de solu\u00e7\u00f5es inovadoras \u2013 novas tecnologias, produtos e servi\u00e7os \u2013 que ajudem a tornar a educa\u00e7\u00e3o brasileira mais m\u00e3o na massa, significativa, colaborativa e l\u00fadica.</p><p>O Desafio tamb\u00e9m tem como objetivo identificar, conectar e apoiar indiv\u00edduos brasileiros \u2013 artistas, pesquisadores, educadores, desenvolvedores de tecnologia, empreendedores e tomadores de decis\u00e3o \u2013 que possam ter um papel-chave no avan\u00e7o de pr\u00e1ticas de Aprendizagem Criativa, especialmente no que se refere a projetos m\u00e3o na massa envolvendo programa\u00e7\u00e3o e constru\u00e7\u00e3o no mundo f\u00edsico, em escolas p\u00fablicas (de Educa\u00e7\u00e3o Infantil ao Ensino M\u00e9dio) e ambientes de aprendizagem n\u00e3o formais de todo o Brasil.</p><p>Os representantes dos projetos selecionados ganhar\u00e3o uma <i>Creative Learning Fellowship </i> para ajudar a implementar seu trabalho.&nbsp;</p><p><b>As inscri\u00e7\u00f5es v\u00e3o at\u00e9 o dia 5 de fevereiro de 2017 e devem ser feitas \u00fanica e exclusivamente atrav\u00e9s do formul\u00e1rio abaixo.</b><br></p><p>Clique <a href=\"https://www.dropbox.com/s/taxr6e878q1e5h6/Desafio%20Aprendizagem%20Criativa%20Brasil%20-%2020170125a.pdf?dl=1\">aqui</a>&nbsp;para a <b>chamada de projetos</b> completa.<br></p><p>Clique <a href=\"https://www.tfaforms.com/4597543\">aqui</a> para o <b>formul\u00e1rio de inscri\u00e7\u00e3o</b>.</p><p>Clique <a href=\"https://docs.google.com/document/d/1N2GKlkc_t83Kp0V4dgR_PzeCp5FJvhov-93OnBXpY0Y/edit?usp=sharing\">aqui</a> para respostas \u00e0s <b>perguntas mais frequentes</b>.</p><p><b>Aten\u00e7\u00e3o</b>: &nbsp;esta p\u00e1gina ser\u00e1 atualizada periodicamente com mais informa\u00e7\u00f5es sobre o Desafio. Discuss\u00f5es sobre o edital est\u00e3o ocorrendo no <a href=\"http://forum.aprendizagemcriativa.org/t/chamada-de-projetos-desafio-aprendizagem-criativa-brazil/238\">f\u00f3rum da Rede Brasileira de Aprendizagem Criativa</a>.</p>", "people": ["mres@media.mit.edu", "leob@media.mit.edu"], "title": "Desafio Aprendizagem Criativa Brasil 2017", "modified": "2018-01-22T15:34:04.822Z", "visibility": "PUBLIC", "start_on": "2017-01-13", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2017-12-31", "slug": "desafio-aprendizagem-criativa"}, {"website": "", "description": "<p>Microbes are the foundation upon which life on Earth depends: they set the boundaries of habitability for all plants and animals and create half of the oxygen we breathe. Ocean-dwelling microbes regulate the global climate and could hold the secrets to the origin of life. Put simply, we wouldn\u2019t be here without microbes, yet most people don't realize how ubiquitous and important they are.<br>&nbsp;<br>The Micronauts project sought to build an emotional bridge over this gap. Microscopic creatures are, by definition, typically hidden from view, and the challenge of seeing them and perceiving their importance prevents emotional involvement and investment.&nbsp;We built an interactive installation that invites people from a general audience to step into the&nbsp;microscopic world. Visitors who step into the interactive projection represent sources of food or light, which allow for the microorganisms to live and reproduce. The reproduction and death rates for the visualization are based on mathematical models generated from the analysis of samples from Cape Cod. This kind of interactive interface has a special appeal to introduce children to Science concepts that are more abstract or far from their everyday experience. The exhibit happened during the&nbsp;<a href=\"https://www.media.mit.edu/events/allhandsondeck/\">2018 National Ocean Exploration Forum</a>&nbsp;on November 8 and 9, at the MIT Media Lab.</p><h1>Team Members</h1><ul><li>Jeffrey Marlow (Harvard)</li><li>Benjamin Bray (MIT Sea Grant)</li><li>Keith Ellenbogen (MIT Sea Grant)</li><li>Craig McLean (MIT/WHOI Joint Program)</li><li>Raquel Fornasaro (Contemporary Artist)</li><li>Mark Adams (National Park Service/Painter)</li><li><a href=\"https://www.media.mit.edu/people/carolx/overview/\">Caroline Rozendo</a>&nbsp;(MIT Media Lab, Object-Based Media)</li></ul>", "people": ["carolx@media.mit.edu"], "title": "Micronauts", "modified": "2019-04-22T16:59:56.008Z", "visibility": "PUBLIC", "start_on": "2018-07-01", "location": "", "groups": ["object-based-media", "open-ocean"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "micronauts"}, {"website": "", "description": "<p>Hybrid Radio: A parasitic molecular infrastructure</p><p>This work opens a dialogue around the possibilities of re-thinking radio communication as an open tool for transmitting and receiving in order to create streams for civic communication, engagement, and expression. This text explores the history of radio, as well as&nbsp;free radio&nbsp;theories around the world, and proposes to re-appropriate the space of the airwaves that has been drastically regulated, privatized and institutionalized. Radio acts as an invisible and mobile architecture, having the characteristic of breaking down boundaries, territories, and walls. Understanding radio as a parasitic system can provide a setting to grow in an organic and molecular way. The objective is to explore the potentials in radio infrastructure, its invisibility and the possible ways of using it to foster expression, and trigger discussions about decentralized communication networks and open streams of coexistence.</p><p>Hybrid Radio was curated by Nomeda and Gediminas Urbonas for the&nbsp;<a href=\"http://www.swamp.lt/\">Swamp Pavilion</a>&nbsp;(Lithuanian Pavilion) at the&nbsp;<a href=\"http://www.labiennale.org/en\">Venice Architecture Biennale 2018</a></p><p>Tardigrade Radio, The Radio for Almost Invisible Beings</p><p>This project is in the form of an installation and a series of community engagement and hybrid-radio building workshops. This project has the objective of deploying an open radio infrastructure and a layered invisible architecture of sound in the city. By combining sonic compositions and narratives from an interspecies perspective, the Tardigrade Radio works as a platform to give voice to different organisms (micro/macro/meta), humans, non-human agents, and matter.</p><p>This piece proposes to re-appropriate the airwaves space, which has been drastically regulated, privatized and institutionalized. Radio pieces are caught by a \u201cparasitic radio receiver module\u201d built with biolab tools an installed in public spaces. Sonic compositions and narratives are taken from the inside of research laboratories to common spaces, blurring the boundaries between inside and outside, lab and nature, fiction and reality. The first installation consists of 1 \u201cparasitic radio receiver module\u201d and three sonic pieces inspired by the Lab Tardigrade\u2019s story and environment. These compositions are meant to be absorbed in a non-linear narrative dispersed on space. Future iterations consider the installation of many \u201cparasitic radio receiver modules,\" creating a large-scale choreography of radio transmitters and receivers. The piece uses radio as a spatial medium for mobile spaces, transversal structures, and build layered invisible architectures.</p>", "people": ["nicolelh@media.mit.edu"], "title": "Hybrid Radios", "modified": "2018-10-19T19:28:33.774Z", "visibility": "PUBLIC", "start_on": "2017-11-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2018-11-30", "slug": "hybrid-radios"}, {"website": "", "description": "<p>The onset of Mixed Reality as a platform offers the opportunity to create new, playful paradigms for building and fostering creativity. The Holobits application leverages the tried and tested features of physical block building platforms like LEGO and introduces the benefits of building in mixed environments to support making and storytelling. The proposed system combines the hand tracking capabilities of the Leap Motion with the spatial mapping of Hololens to enable hands-on building experiences with virtual blocks, denoted as \u201cbits.\u201d These blocks have different attributes and characteristics that determine how they look and behave within the mixed reality building space. The platform also allows users to share their creative building process in a frame-by-frame fashion that enables remixing and reflection on every play session. Holobits allows users to record their interactions with their creations to make animated environments with ease and support storytelling. Another way to enable collaboration is to let kids share models (or download someone else's creation in your space), allowing multiple users to build in a shared physical space or over distances, where a player can \u201chop\u201d into the physical space of the Hololens user using virtual reality. Last but not least, we intend to integrate the Scratch visual programming toolkit into the Holobits platform to allow users to orchestrate their virtual creations and create the ultimate interactive stories.</p>", "people": ["afuste@media.mit.edu", "pe25171@media.mit.edu", "cdvm@media.mit.edu", "pattie@media.mit.edu", "minakhan@media.mit.edu"], "title": "Holobits: Creativity and fun in Mixed Reality", "modified": "2018-11-30T17:21:40.954Z", "visibility": "PUBLIC", "start_on": "2017-02-13", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-05-31", "slug": "holobits"}, {"website": "", "description": "<p>Bitcoin generates net-new value from \"mining\" in a distributed network. In this work, we explore solar micro-mining rigs that transform excess energy capacity from renewable energy (hard to trade) into money (fungible). Each rig runs a small Bitcoin miner and produces Bitcoin dust for micropayments. We envision these micro-miners populating a highly distributed network, across rooftops, billboards, and other outdoor spaces. Where systematic or environmental restrictions limit the ability to freely trade the underlying commodity, micro-mining produces new economic viability. Renewable energy-based, micropayment mining systems can broaden financial inclusion in the Bitcoin network, particularly among populations that need a currency for temporary store of value and must rely on flexible electricity off the grid (e.g., unbanked populations in the developing world). This exploration seeds a longer-term goal to enable open access to digital currency via account-free infrastructure for the public good.</p>", "people": ["aekblaw@media.mit.edu", "lip@media.mit.edu"], "title": "Solar Micro-Mining", "modified": "2018-10-08T01:32:51.238Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["viral-communications", "digital-currency-initiative-dci"], "published": true, "active": false, "end_on": "2017-06-06", "slug": "solar-micro-mining"}, {"website": "", "description": "<p>Recent developments in wearable robots and human augmentation open up new possibilities of designing computational interfaces integrated to the body. Particularly, supernumerary robot is a recently established field of research that investigates a radical idea of adding robotic limbs to users. Such augmentations, however, pose a limit in how much we can add to the body due to weight or interference with other body parts. To address that, we explore the use of soft robots as supernumerary robotic fingers. We present a pair of soft robotic fingers driven by cables and servomotors, and applications using the robotic fingers in various contexts.&nbsp;</p>", "people": ["sangwon@media.mit.edu", "yuhanhu@media.mit.edu"], "title": "Hand Development Kit", "modified": "2018-08-20T16:45:50.186Z", "visibility": "PUBLIC", "start_on": "2016-08-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-08-01", "slug": "hand-development-kit"}, {"website": "", "description": "<p>KickSoul is a wearable device that maps natural foot movements into inputs for digital devices. It consists of an insole with embedded sensors that track movements and trigger actions in devices that surround us. We present a novel approach to use our feet as input devices in mobile situations when our hands are busy. We analyze the foot's natural movements and their meaning before activating an action.</p>", "people": ["changzj@media.mit.edu", "joep@media.mit.edu", "pattie@media.mit.edu"], "title": "KickSoul: A Wearable System for Foot Interactions with Digital Devices", "modified": "2017-08-07T14:36:03.410Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["responsive-environments", "fluid-interfaces"], "published": true, "active": false, "end_on": "2017-08-01", "slug": "kicksoul-a-wearable-system-for-foot-interactions-with-digital-devices"}, {"website": "", "description": "<p>Scientific managers and administrators need to understand the impact of the research they support. Yet, most of the online tools available to explore scholarly publication data (e.g. Google Scholar and Citeseerx) present atomized views focused on single scholars and papers, failing to put scholars in a social, institutional, and national context; and failing to provide aggregate views for universities and countries. Here, we introduce<span style=\"font-size: 18px; font-weight: 400;\">&nbsp;Opus,</span><span style=\"font-size: 18px; font-weight: 400;\">&nbsp;an interactive online platform that integrates, aggregates, and visualizes scholarly data from Google Scholar and Microsoft Academic graph. Opus present users with scientific data at five different scales: scholars, countries, organizations, journals, and papers; and at each scale, it provides properly benchmarked visualizations that facilitate the comparison among scholars (e.g. from the same age), organizations, and nations. We build Opus using React as a front-end framework and Replot a new React visualization library developed in the course of this project as its central visualization library. Finally, we will have users compare Opus with similar tools such as Google Scholar, Microsoft Academic, and Citeseerx.</span></p>", "people": ["arista@media.mit.edu", "almaha@media.mit.edu", "hidalgo@media.mit.edu"], "title": "Opus: Exploring Scientific Data through Visualizations", "modified": "2017-10-16T23:04:37.291Z", "visibility": "LAB-INSIDERS", "start_on": "2017-01-10", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2018-07-04", "slug": "opus-exploring-scientific-data-through-visualizations"}, {"website": "", "description": "<p><b>Can tattoos embrace technology in order to make the skin interactive?</b></p><p>The DermalAbyss project is the result of a collaboration between MIT researchers Katia Vega, Xin Liu, Viirj Kan and Nick Barry and Harvard Medical School researchers Ali Yetisen and Nan Jiang.&nbsp;<br></p><p>DermalAbyss is a proof-of-concept that presents a novel approach to bio-interfaces in which the body surface is rendered an interactive display. Traditional tattoo inks are replaced with biosensors whose colors change in response to variations in the interstitial fluid. It blends advances in biotechnology with traditional methods in tattoo artistry.&nbsp;</p><p>This is a research project, and there are currently no plans to develop Dermal Abyss as a product or to pursue clinical trials.<br></p>", "people": ["xxxxxxin@media.mit.edu", "joep@media.mit.edu", "nbarry@media.mit.edu", "viirj@media.mit.edu", "katiav@media.mit.edu", "pattie@media.mit.edu"], "title": "DermalAbyss: Possibilities of Biosensors as a Tattooed Interface", "modified": "2018-04-27T17:45:10.726Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["responsive-environments", "fluid-interfaces"], "published": true, "active": false, "end_on": "2017-05-31", "slug": "d-Abyss"}, {"website": "", "description": "<p>To help prevent the mindless sharing of content and promote the sharing of thoughtful content, a socially acceptable \u201cstamp of approval\u201d was created, backed by distributed ledger technology (DLT) that would integrate itself into a post. These stamps of approval&nbsp;are social indicators to one's network that the person sharing the content (and receiving the stamp of approval)&nbsp;has reviewed the content before sharing.</p><p>DLTs, specifically blockchains, have the architectural benefit of providing a public ledger platform where all recordings/transactions are immutable and verifiable, thus being an excellent platform for audits and verifying provenance\u2014even though this isn\u2019t what we care about. In traditional blockchain architecture, miners perform a computational intensive process called PoW (proof-of-work) in order to prove to the complete blocks that maintain the network. Using \u201cpatience\u201d as a variable to allow a user-defined block completion algorithm for every assertion made, we created proof-of-patience. A user defines the level of patience\u2014time and computational resources they are willing to give up, from a scale of one to three and then they mine their block.</p><p>When the user is done mining for their block, we created PoPBot (\"Proof of Patience\"-bot)to tweet at the user the content they want to share with a badge that is representative of the work/patience they put in and the user can then retweet it to their network.&nbsp;</p>", "people": ["oceane@media.mit.edu"], "title": "Proof of Patience", "modified": "2019-04-17T13:55:21.434Z", "visibility": "PUBLIC", "start_on": "2018-11-11", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2019-01-01", "slug": "proof-of-patience"}, {"website": "", "description": "<p>Mouseless is an invisible computer mouse that provides the familiarity of interaction of a physical mouse without actually needing a real hardware mouse. Despite the advances in computing hardware technologies, the two-button computer mouse has remained the predominant means to interact with a computer. Mouseless removes the requirement of having a physical mouse altogether, but still provides the intuitive interaction of a physical mouse with which users are familiar.&nbsp;<span style=\"font-size: 18px;\">Mouseless consists of an Infrared (IR) laser beam and an Infrared camera. Both IR laser and IR camera are embedded in the computer. The laser beam module is modified with a line cap and placed such that it creates a plane of IR laser just above the surface the computer sits on. The user cups their hand, as if a physical mouse was present underneath, and the laser beam lights up the hand which is in contact with the surface. The IR camera detects those bright IR blobs using computer vision. The change in the position and arrangements of these blobs are interpreted as mouse cursor movement and mouse clicks.</span></p>", "people": ["pranav@media.mit.edu", "pattie@media.mit.edu"], "title": "Mouseless", "modified": "2018-10-12T16:32:37.681Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "mouseless"}, {"website": "", "description": "<p>PoCoMo is an implementation of a vision in future-projected social interfaces. In this project we try to capture the playfulness of collaborative gaming and apply it to projected interfaces. The maturing of handheld micro-projector technology, in conjunction with advanced mobile environments, enable this novel type of interaction. Our system is made of a micro-projector mobile device with a specially designed case that turns it into a first-of-a-kind handheld mini-projector-camera system. Computer Vision algorithms support collaborative interaction between multiple users of the system. Through PoCoMo, we wish to explore the social nature of projected interfaces. To accommodate this we designed the projection to be of human cartoon-like characters that play out a personal interaction. Following their human controllers, they recognize each other, wave hello, shake hands, and exchange presents. </p>", "people": ["pattie@media.mit.edu", "roys@media.mit.edu"], "title": "PoCoMo", "modified": "2018-10-12T16:40:54.851Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "pocomo"}, {"website": "", "description": "<p>Rotational  musical  instrument  interfaces  are  a  very  intuitive representation of the fundamentals of music itself, as they  are  inherently  simple  oscillators  that  can  run  from low control frequencies up to audio. The downside of many rotating instruments is that they are bounded by the rotating mechanisms themselves, that being a fixed axle where the rotation is driven by a hand or a motor.  This leads to a reduction of the performer\u2019s expressive ability to influence the generated sound by constraining them to fixed locations and/or positions. In this work, we avoid these drawbacks,  but maintain the affordance of the rotational interface by incorporating non-contact sensing and wireless data transfer.</p><p>We designed an untethered digital synthesizer that can be held and manipulated while broadcasting audio data to a receiving off-the-shelf Bluetooth receiver.  The synthesizer allows the user to freely rotate and reorient the instrument while exploiting non-contact light sensing for a truly expressive performance. The system consists of a suite of sensors that convert rotation, orientation, touch, and user proximity into various audio filters and effects operated on preset wave tables, while offering a persistence of vision display for input visualization.  </p>", "people": ["pa26123@media.mit.edu"], "title": "CD-Synth", "modified": "2019-04-16T14:49:26.340Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2019-01-31", "slug": "cd-synth"}, {"website": "", "description": "<p>Because of rising energy costs, of considerable interest in many parts of the world is the smart energy meter: a tool to let consumers know how much electricity they are using in their homes or offices at any given time. Instead of guessing at how much energy they are using and waiting to see their bill at the end of the month, users can directly observe the costs of, for example, leaving a computer on overnight, or how much they save by lowering the heat by one degree. The current project studies how consumers react to energy meters in their homes: how much and in what way they save energy, if these changes are lasting, and how to encourage further energy savings.</p>", "people": [], "title": "Smart Energy Meters and Consumer Behavior", "modified": "2016-12-05T00:17:03.693Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-320A", "groups": ["erationality"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "smart-energy-meters-and-consumer-behavior"}, {"website": "", "description": "<p>ShowMe is an immersive mobile collaboration system that allows remote users to communicate with peers using video, audio, and gestures. With this research, we explore the use of head-mounted displays and depth sensor cameras to create a system that (1) enables remote users to be immersed in another person's view, and (2) offers a new way of sending and receiving the guidance of an expert through 3D hand gestures. With our system, both users are surrounded in the same physical environment and can perceive real-time inputs from each other. </p>", "people": ["amores@media.mit.edu", "pattie@media.mit.edu", "xavib@media.mit.edu"], "title": "ShowMe: Immersive Remote Collaboration System with 3D Hand Gestures", "modified": "2018-10-12T16:45:04.639Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["terrestrial-sensing", "fluid-interfaces"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "showme-immersive-remote-collaboration-system-with-3d-hand-gestures"}, {"website": "", "description": "<p>The way we represent ourselves in social media is intangible. What we choose to wear is public to the world and we are aware of it. In contrast, what we post online about ourselves reaches thousands of people and generates social consequences, but it doesn\u2019t feel that way. Is the current form of social media really making our relationships better? Current technologies are good at connecting people at a distance, but less so at connecting them within the same environment.</p><p>Social textiles embodies who you are and dynamically reflects your shared interests with people nearby. It enables you to gain access to communities of people in the physical world and enhances social affordances and icebreaking interactions through wearable social messaging.</p><p>Social Textiles embody who you are and dynamically reflect your shared interests with people nearby. They enable you to gain access to communities of people in the physical world and enhance social affordances and icebreaking interactions through wearable social messaging. Social Textiles can connect community members with niche interests, philosophical beliefs, personalities, emotional statuses, and ethical views. They have the potential to enable members to bypass superficial or generic interests through \"filtering\" individuals, in order to tune social experiences toward people who are more compatible.</p>", "people": ["katsuyaf@media.mit.edu", "changzj@media.mit.edu", "ishii@media.mit.edu", "amores@media.mit.edu", "viirj@media.mit.edu", "pattie@media.mit.edu"], "title": "Social Textiles", "modified": "2018-10-12T16:50:34.930Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["fluid-interfaces", "tangible-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "social-textiles"}, {"website": "http://cs.wellesley.edu/~asimonso/nostalgiabox", "description": "<h1>Nostalgia Box</h1><h2>A deep learning visualization of your own memories</h2><h2>By <a href=\"http://cs.wellesley.edu/~asimonso/nostalgiabox/\">Aubrey Simonson</a>&nbsp;<br>Commissioned by Manuel Cebrian and Iyad Rahwan</h2><p>Nostalgia Box is a continually shifting soup of memories. Images, curated for their nostalgic emotional impact, are dreamed over one another using a neural style machine learning algorithm, based on the paper \"A Neural Algorithm of Artistic Style\" by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. The resulting images are then overlaid into a video which never fully solidifies into any one image, but instead is always several at once. To someone who is familiar with the content of the images, they are still recognizable. However, a stranger should see only a haze of vague shapes which occasionally contains the suggestion of a face. This is machine learning<i>, </i>stripped of the elements of spam. It is an amalgamation of its creator's digital history, but which is being used as a tool for reflection, rather than as a means to more effectively market products.&nbsp;&nbsp;</p>", "people": ["cebrian@media.mit.edu", "irahwan@media.mit.edu"], "title": "Nostalgia Box", "modified": "2018-05-07T22:25:44.225Z", "visibility": "PUBLIC", "start_on": "2018-02-01", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "nostalgia-box"}, {"website": "", "description": "<p>TeleTouch lets you touch and control from far. See thru your smartphone's camera and control EVERYTHING you see on the screen by touching it. Now you can interact with your appliances from far, in a fun way. Here it goes. You look at a scene thru the camera of your smartphone and control what you see - home appliances (TV, alarm, music player, ...), Open the door; or turn on that light. Just by touching it on screen. It's simple. TeleTouch lets you interact with your world with touch, but now from far.</p>", "people": ["pranav@media.mit.edu", "pattie@media.mit.edu"], "title": "TeleTouch", "modified": "2018-10-12T16:52:12.814Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "teletouch"}, {"website": "", "description": "<p>Fluxa is a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a transient wearable display to foster richer self-expression and communication in daily life . It can be used to enhance existing social gestures such as handwaving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a decoration device that generates images around dancing bodies.</p>", "people": ["xxxxxxin@media.mit.edu", "mingrui@media.mit.edu", "joep@media.mit.edu", "katiav@media.mit.edu", "pattie@media.mit.edu"], "title": "Fluxa", "modified": "2018-10-12T16:57:48.806Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["responsive-environments", "fluid-interfaces"], "published": true, "active": false, "end_on": "2017-11-30", "slug": "fluxa"}, {"website": "", "description": "<p>Physiological arousal is an important part of occupational therapy for children with autism and ADHD, but therapists do not have a way to objectively measure how therapy affects arousal. We hypothesize that when children participate in guided activities within an occupational therapy setting, informative changes in electrodermal activity (EDA) can be detected using iCalm. iCalm is a small, wireless sensor that measures EDA and motion, worn on the wrist or above the ankle. Statistical analysis describing how equipment affects EDA was inconclusive, suggesting that many factors play a role in how a child's EDA changes. Case studies provided examples of how occupational therapy affected children's EDA. This is the first study of the effects of occupational therapy's in situ activities using continuous physiologic measures. The results suggest that careful case study analyses of the relation between therapeutic activities and physiological arousal may inform clinical practice.</p>", "people": ["picard@media.mit.edu", "hedman@media.mit.edu"], "title": "Measuring arousal during therapy for children with autism and ADHD", "modified": "2019-04-19T17:17:14.754Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2016-05-17", "slug": "measuring-arousal-during-therapy-for-children-with-autism-and-adhd"}, {"website": "", "description": "<p>Recording your reaction to a short video is becoming the new gossip; famous watchers get as many as 750,000 views. We attempt to transform this utterly useless and talentless event into a socially constructive alternative to simultaneous, synchronized, group viewing. Any user can opt in to be recorded and added to the shared, collective viewing experience. No talent or skills required.</p>", "people": ["suzwang@media.mit.edu", "weller@media.mit.edu", "lip@media.mit.edu"], "title": "Watch People Watch", "modified": "2018-10-09T18:03:44.000Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2017-06-30", "slug": "watch-people-watch"}, {"website": "http://www.mallcong.com", "description": "<h2>Translating ambient sounds of moment into tangible and shareable memories through animated paper</h2><p>We present a tangible memory notebook\u2014reMi\u2014that records ambient sounds and translates them into a tangible and shareable memory using animated paper. The paper replays the recorded sounds and deforms its shape to generate synchronized motions with the sounds. Computer-mediated communication interfaces have allowed us to share, record, and recall memories easily through visual records. However, those digital visual-cues that are trapped behind the device\u2019s 2D screen are not the only means to recall a memory we experienced with more than the sense of vision. To develop a new way to store, recall, and share a memory, we investigate how tangible motion of a paper that represents sound can enhance \"reminiscence.\"</p>", "people": ["yun_choi@media.mit.edu"], "title": "reMi", "modified": "2019-04-10T14:41:04.282Z", "visibility": "PUBLIC", "start_on": "2017-10-30", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2019-05-31", "slug": "remi"}, {"website": "", "description": "<p>PalimPost is a converged system for storing, searching, and sharing digital and physical world information using sticky notes and mobile devices.  PalimPost extracts contextual cues from a user's physical environment and activities, connects them to the user's digital world research, and subsequently presents to the user systematically categorized, relevant, and JIT information.  Whether a user is writing down a shopping list on a sticky note after surfing the internet at home, or checking out hundreds of products at hand in a physical store, whether a user is preparing a list of dinner ingredients in the kitchen or buying food outside in the market, PalimPost integrates information from different time and location to form a seamlessly connected experiences for the user. </p><p><br></p><p><br></p><p>Wayback Archive Description:</p><p>PalimPost is taking sticky notes to the cloud era. So far sticky notes could only be used for saving a very short message, as a pointer to a bigger piece of information in our memory. However nothing in our new computing environment behaves this way. Every object now has an internal memory of Giga-Bytes, and our human memory is used for more emotional, social or inspirational information. But the affordances of plain sticky-paper notes are compelling: they are writable and also sticky! So how can we take them to our new ditized world? Enter: The Cloud, allowing digital information to be stored and retrieved swiftly. However, the experience of using the sticky note is about recording a moment, a fleeting thought. This is the goal of PalimPost, to help us capture moments in time, digitize them, and store them on the cloud.</p>", "people": ["holtzman@media.mit.edu", "pattie@media.mit.edu", "roys@media.mit.edu"], "title": "PalimPost", "modified": "2018-10-12T17:59:51.275Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["fluid-interfaces", "information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "palimpost"}, {"website": "", "description": "<p>The Sound of Touch is a new instrument for real-time capture and sensitive physical stimulation of sound samples using digital convolution. The hand-held wand can be used to (1) record sound, then (2) brush, scrape, strike or otherwise physically manipulate this sound against physical objects. These actions produce sound in a manner that leverages peoples existing intuitions about sonic properties of physical materials. The Sound of Touch permits real-time exploitation of the sonic properties of a physical environment, to achieve a rich and expressive control of digital sound that is not typically possible in electronic sound synthesis and control systems. </p>", "people": ["pattie@media.mit.edu"], "title": "The Sound of Touch", "modified": "2016-12-05T00:17:03.736Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-320", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "the-sound-of-touch"}, {"website": "", "description": "<p>Oasis won <a href=\"http://www.edisonawards.com/winners2017.php\">Silver at the Edison Awards 2017</a>.</p><p>Oasis received the <a href=\"https://www.vrst2016.lrz.de/keynotes-award/\">Best Paper Award</a> at VRST 2016.<br></p><p>Oasis is a novel system for automatically generating immersive and interactive virtual reality environments using the real world as a template. The system captures indoor scenes in 3D, detects obstacles like furniture and walls, and maps walkable areas to enable real-walking in the generated virtual environment. Depth data is additionally used for recognizing and tracking objects during the VR experience. The detected objects are paired with virtual counterparts to leverage the physicality of the real world for a tactile experience. Our system allows a casual user to easily create and experience VR in any indoor space of arbitrary size and shape without requiring specialized equipment or training.</p><p>Oasis can be used, for example, to create storyspaces where friends and family can remotely participate in a session of storytelling around the campfire. The freedom to move around and interact with the virtual world allows for a new form of storytelling when combined with &nbsp;traditional &nbsp;narration techniques like vocalization, movement, and gestures. We call this <b>human-in-the-loop storytelling</b>,&nbsp;distinguishing it from current VR storytelling experiences where the software system is the storyteller.</p>", "people": ["sra@media.mit.edu", "pattie@media.mit.edu"], "title": "Oasis", "modified": "2018-08-20T16:24:31.868Z", "visibility": "PUBLIC", "start_on": "2016-02-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "oasis"}, {"website": "", "description": "<p>The setup involves adding one controller/tracker per foot and one at the base of the back along with two hand-held controllers and the HMD. I'm using the FinalIK asset from the Unity Asset Store.</p>", "people": ["sra@media.mit.edu"], "title": "VR Full Body Tracking", "modified": "2018-08-20T16:21:46.064Z", "visibility": "PUBLIC", "start_on": "2017-01-09", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-07-31", "slug": "full-body-tracking"}, {"website": "", "description": "<p>Skrin is an exploration project on digitalized body skin surface using embedded electronics and prosthetics. Human skin is a means for protection, a mediator of our senses, and a presentation of our selves. Through several projects, we expand the expression capacity of the body's surface and emphasize the dynamic aesthetics of body texture by technological means.&nbsp;<span style=\"font-size: 18px;\">Working with conventional special effect makeup artists, we \u201chide\u201d electronics into silicone which is applied onto skin and covered by cosmetics. The digitalized skin surface is connected with the affective experience, while the illuminated body is a representation of internal state.</span></p><p>Working with bionic pop artist Viktoria Modesta, we deployed the project in Music Tech Festival Berlin 2016 and transformed her body as a canvas along with the performance.</p>", "people": ["xxxxxxin@media.mit.edu", "joep@media.mit.edu", "katiav@media.mit.edu", "pattie@media.mit.edu"], "title": "Skrin", "modified": "2018-12-05T14:35:37.656Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["responsive-environments", "fluid-interfaces"], "published": true, "active": false, "end_on": "2017-06-30", "slug": "skrin"}, {"website": "", "description": "<p>Open Hybrid is an open source augmented reality platform for physical computing and Internet of Things. It is based on the web and Arduino.</p><p>This platform allows you to:</p><ul><li>Create augmented reality content with HTML tools</li><li>Create augmented reality without any knowledge of 3D programming</li><li>Connect the functionality of objects with a simple drag and drop paradigm</li><li>Program your physical hybrid objects and connect them to the AR-UI using Arduino</li></ul><p>Learn more about this project at: <a href=\"https://web.archive.org/web/20180314110710/http://www.openhybrid.org/\">http://www.openhybrid.org</a></p>", "people": ["pattie@media.mit.edu", "heun@media.mit.edu"], "title": "Open Hybrid", "modified": "2018-10-20T23:06:14.400Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2017-06-30", "slug": "open-hybrid"}, {"website": "http://magiclab.nyc", "description": "<h1>\n                    Can a magician and a robot collaborate on stage to create a believable, evocative performance?</h1><p><span style=\"font-size: 18px; font-weight: normal;\">We are studying how the quality of robot movement, perceived robot agency, and blended static/dynamic interactivity between a robot and human performer might influence an audience\u2019s emotional state and belief in the validity of a robot character during a performance.&nbsp;</span></p><h2>Agency and Believability</h2><p>Robotic animation techniques for live performance typically rely on backstage human puppeteering or playback of pre-rendered animation sequences. However, these methods are insufficient for high-speed, close human-robot proximity and coordination, especially when the human performer\u2019s position and timing are unpredictable (ex. rapid passing of objects between human-hands and robot-grippers). Furthermore, simple playback of animation can detract from the believability of the performance if an audience is not convinced that the robot has agency (i.e. its ability to act on its own).</p><h2>Static / Dynamic Interaction</h2><p>We are developing tools that allow us to compose a human-robot performance that blends pre-rendered choreography with key moments of dynamic interactivity to enhance the realism of the character. If the performance successfully modulates the degree to which the robot responds to the human in a pre-defined manner versus behavior that is completely reactive to the dynamic performer, then the audience might still perceive the robot as having complete agency. For example, as the robot is playing back a choreographed series of poses, it might also track the face of the performer to maintain eye contact. By blurring lines in this interaction, the audience might be more willing to believe the robot is animate.</p><h2><b>Additional Research Questions</b></h2><ul><li>\u2022What is the affect space for a human-magician performance?</li><li class=\"\">Can we improve on current robot animation techniques by including computational choreography and aesthetics-influenced motion planning in ways that lead to desired emotional reactions in observed human-robot collaboration?</li><li class=\"\">What are challenges and opportunities when designing human-robot performances? \u2022 Can we generate a new class of tools and approaches that facilitate artistic and functional robot programming by non-experts?</li></ul><p class=\"\"><br></p>", "people": ["tempest@media.mit.edu", "dnunez@media.mit.edu"], "title": "Magic Robot Interaction", "modified": "2017-11-25T16:09:32.395Z", "visibility": "PUBLIC", "start_on": "2015-03-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2015-12-31", "slug": "magicrobot"}, {"website": "", "description": "<p>STARRING&nbsp;<a href=\"https://mxjadephoenix.com/\">JADE PHOENIX</a>&nbsp;&amp; JADE RENEGADE<br>IN COLLABORATION WITH ORGASMIC CREATIVE</p>", "people": [], "title": "Housewives Making Drugs", "modified": "2018-01-08T19:30:24.940Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2017-06-01", "slug": "housewives-making-drugs"}, {"website": "", "description": "<p><i>Tree</i> is a virtual experience that transforms you into a rainforest tree. With your arms as branches and body as the trunk, you experience the tree\u2019s growth from a seedling into its fullest form and witness its fate firsthand.&nbsp;Collaborating with director Milica Zec and Winslow Porter, we designed and constructed the entire tactile experience throughout the film. With precisely controlled physical elements including vibration, heat, fan and body haptics, the team created a fully immersive virtual reality storytelling to, where the audience no longer watches but is transformed into a new identity, a giant tree in the peruvian rainforest.</p><p><i>Tree</i> debuted at Sundance Film Festival 2017 New Frontier and also had its presentation in Tribeca Film Festival 2017. &nbsp;</p><p>The project is part of our research about body ownership illusion in virtual reality (early project: <a href=\"https://www.media.mit.edu/projects/treesense/overview/\">TreeSense</a>).&nbsp;The tactile experience is crucial for establishing a body ownership illusion instead of restricting the experience to the visual world. We aim to have the audience not just see, but feel and believe \"being\" a tree.&nbsp;</p>", "people": ["xxxxxxin@media.mit.edu", "yedan@media.mit.edu", "pattie@media.mit.edu"], "title": "Tree", "modified": "2018-10-20T22:01:11.764Z", "visibility": "PUBLIC", "start_on": "2017-01-18", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2017-06-30", "slug": "tree"}, {"website": "", "description": "<p>Light, color, texture, geometry and other architectural design elements have been shown to produce predictable and measurable effects on our minds, brains, and bodies. This suggests spaces that can mirror or transform feelings or serve specific purposes like improving learning or enhancing wellbeing can be designed. With Auris, we take a first step towards the design of such spaces in virtual reality by attempting to automatically generate affective virtual environments that can affect our emotions. The input to Auris is a song (audio and lyrics) and the output is a VR world that encapsulates the mood and content of the song.</p>", "people": ["sra@media.mit.edu", "pralav@media.mit.edu", "pattie@media.mit.edu"], "title": "Auris: Creating Affective Virtual Spaces from Music", "modified": "2018-08-20T16:22:53.032Z", "visibility": "PUBLIC", "start_on": "2017-02-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-07-31", "slug": "auris-creating-affective-virtual-spaces-from-music"}, {"website": "", "description": "<p>Memento is a home-based reminder system that allows users to associate customized voice reminders with specific activities in their daily lives. Memento consists of three major components: (1) a handheld computing interface for initiating and receiving reminders; (2) a sensor subsystem that detects activity-related changes in the home environment; and (3) a central server that acquires sensor readings and attempts to infer user activity from the data in real time. These three components function together to provide a new and viable approach to augmenting human memory.</p>", "people": ["kll@media.mit.edu", "intille@media.mit.edu"], "title": "Memento", "modified": "2018-05-04T18:21:03.600Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "memento"}, {"website": "", "description": "<h2>Exploring contextual multimodal cues as memory aids&nbsp;</h2><p><b>Objective&nbsp;<br></b></p><p>We are exploring the potential of proximity-triggered contextual audio and visual cues to help early-stage Alzheimer\u2019s patients&nbsp;recall familiar people and places. In particular, we are using proximity beacons to determine when the user is physically close to another person, such as a loved one. The beacons will then trigger cues in the form of:</p><ol><li>audio conveying contextual information such as name, relationship, time/place/details of last interaction;</li><li>images and video (using AR) showing previous interactions along with text displaying contextual information; and</li><li>music in the form of specific songs associated with specific individuals.</li></ol><p><b>Research Questions</b></p><p>We\u2019re interested in tackling the following questions:</p><ul><li>Which cue modalities are the most effective in improving recognition in early-stage Alzheimer\u2019s patients?</li><li>What advantages and challenges are afforded by each of the different modalities?</li></ul>", "people": ["tomasero@media.mit.edu", "davidsu@media.mit.edu", "pattie@media.mit.edu", "vparth@media.mit.edu"], "title": "Cue", "modified": "2018-12-14T17:56:32.014Z", "visibility": "LAB-INSIDERS", "start_on": "2017-09-22", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2017-12-20", "slug": "cue"}, {"website": "", "description": "<p>A conference table that subtly encourages mindfulness</p>", "people": [], "title": "Balance Table", "modified": "2017-01-02T21:07:54.902Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2017-01-31", "slug": "balance-table"}, {"website": "", "description": "<p>This project is currently promoting a survey of data from the East Side (Zona Leste) of the city of Sao Paulo, Brazil. The aim is to detect the landscape dynamics: infrastructure and urban planning, critical landscapes, housing, productive territory, recycling, and public space. The material will be made available on a digital platform, accessible by computers and mobile devices: a tool specially developed to enable local communities to disseminate productive and creative practices that occur in the area, as well as to enable a greater participation in the formulation of public policies. ZL Vortice is an instrument that will serve to strengthen social, productive, and cultural networks of the region.</p>", "people": ["ethanz@media.mit.edu", "adelineg@media.mit.edu"], "title": "ZL Vortice", "modified": "2017-01-08T20:05:40.680Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2016-01-01", "slug": "zl-vortice"}, {"website": "", "description": "<p>A tapestry where each pixel represents a pledge by an individual to vote. Anyone can participate and watch the growing and changing image that emerges as others agree to vote. The tapestry evolves and will encourage repeated attention. This tests local reinforcement to support actions.</p><p>Follow this <a href=\"https://votomosaic.media.mit.edu/\">link</a> for more information.</p>", "people": ["kalli@media.mit.edu", "lip@media.mit.edu"], "title": "votoMosaic", "modified": "2019-04-18T15:05:48.332Z", "visibility": "PUBLIC", "start_on": "2018-08-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "votomosaic"}, {"website": "", "description": "<p>&nbsp;<span style=\"font-size: 18px;\">Boxlab Visualizer is a tool for replaying and annotating multi-modal datasets synchronized to audio/video recordings for ground truth labeling. Many common data formats are supported, and visualizations are automatically populated with the most appropriate representation, such as time series, histogram, heat map, or cartesian space. BoxLab Visualizer is intended to facilitate free sharing of datasets and collaborative annotation.</span></p>", "people": [], "title": "BoxLab Visualizer", "modified": "2017-10-26T15:53:15.273Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2011-12-31", "slug": "boxlab-visualizer"}, {"website": "", "description": "<p>We developed a smartphone application that detects users\u2019 affect and provides personalized positive psychology interventions in order to enhance users\u2019 psychological wellbeing. Users\u2019 emotional states were measured by analyzing facial expressions and the sentiment of SMS messages. A virtual character in the application prompted users to verbally journal about their day by providing three positive psychology interventions. The system used a Markov Decision Process (MDP) model and a State-Action-Reward-State-Action (SARSA) algorithm to learn users\u2019 preferences about the positive psychology interventions. Nine participants were recruited for an experimental study to test the application. They used it daily for three weeks. The interactive journaling activity increased participants\u2019 arousal and valence levels immediately following each interaction, and we saw a trend toward improved self-acceptance levels over the three week period. The interaction duration increased significantly throughout the study as well. The qualitative analysis on journal entries showed that the application users explored and reflected on various aspects of themselves by looking at daily events, and found novel appreciation for and meanings in their daily routine.</p>", "people": ["cynthiab@media.mit.edu", "sooyeon6@media.mit.edu"], "title": "Interactive Journaling", "modified": "2017-06-05T16:12:47.957Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["personal-robots", "advancing-wellbeing"], "published": true, "active": false, "end_on": "2016-12-31", "slug": "interactive-journaling"}, {"website": "", "description": "<p>To gain insights into how digital technologies can make local governments more responsive and deepen citizen engagement, we are studying the Spanish town of Jun (population 3,500). For the last four years, Jun has been using Twitter as its principal medium for citizen-government communication. We are mapping the resulting social networks and analyzing the dynamics of the Twitter interactions, in order to better understand the initiative's impact on the town. Our long-term goal is to determine whether the system can be replicated at scale in larger communities, perhaps even major cities.</p>", "people": ["dkroy@media.mit.edu", "billp@media.mit.edu", "msaveski@media.mit.edu"], "title": "Responsive Communities: Pilot Project in Jun, Spain", "modified": "2017-02-16T16:31:17.794Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2016-12-31", "slug": "responsive-communities-pilot-project-in-jun-spain"}, {"website": "", "description": "<p>Lower-limb exoskeletons are of great interest in the robotics community because of their various applications in enhancement and rehabilitation. We design an autonomous exoskeleton for ankle plantar\ufb02exion assistance. The exoskeleton has a high ef\ufb01ciency transmission system, with reduction ratio up to 28.8 : 1 without the need of a gear box. This allows relocating the actuator to the hip to reduce device inertia. A simple feed-forward torque controller based on \ufb01eld-oriented voltage control for controlling brushless DC motors is used to control the system. Through various performance tests, the exoskeleton was shown to provide a torque control bandwidth of 17.5Hz and can effectively track biological torque pro\ufb01les. This exoskeleton establishes an autonomous platform for experiments involving ankle assistance, including testing control algorithms for metabolic cost reduction.<br></p>", "people": ["xingbang@media.mit.edu"], "title": "An Autonomous Running Exoskeleton for Ankle Plantar\ufb02exion Assistance", "modified": "2018-10-22T19:33:13.961Z", "visibility": "LAB-INSIDERS", "start_on": "2017-12-01", "location": "", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "an-autonomous-running-exoskeleton-for-ankle-plantar-exion-assistance"}, {"website": "http://robotshelpingkids.yale.edu/", "description": "<p>Our mission is to develop the computational techniques that will enable the design, implementation, and evaluation of \"relational\" robots, in order to encourage social, emotional, and cognitive growth in children, including those with social or cognitive deficits. Funding for the project comes from the NSF Expeditions in Computing program. This expedition has the potential to substantially impact the effectiveness of education and healthcare, and to enhance the lives of children and other groups that require specialized support and intervention. In particular, the MIT effort is focusing on developing second-language learning companions for pre-school aged children, ultimately for ESL (English as a Second Language).</p>", "people": ["cynthiab@media.mit.edu", "edith@media.mit.edu", "havasi@media.mit.edu", "jakory@media.mit.edu", "michalg@media.mit.edu", "jinjoo@media.mit.edu", "samuelsp@media.mit.edu", "lukulele@media.mit.edu", "sooyeon6@media.mit.edu", "ggordon@media.mit.edu"], "title": "Socially Assistive Robotics: An NSF Expedition in Computing", "modified": "2017-06-07T16:38:24.487Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2017-04-30", "slug": "socially-assistive-robotics-an-nsf-expedition-in-computing"}, {"website": "", "description": "<p>We investigated the spread of all of the <i>verified</i> news stories\u2013verified as either true or false\u2013distributed on Twitter from 2006 to 2017.</p><p>With a data set of roughly 126K stories tweeted by around 3M people over 4.5M times, we classified news as true or false using information from six independent fact-checking organizations that exhibited 95-98% agreement on the classifications. False information &nbsp;spread significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political information than for false information about terrorism, natural disasters, science, urban legends, or financial information. We found that false information was more novel than true information, which suggests that people were more likely to share novel information. While false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust.<br></p><p>Contrary to conventional wisdom, robots accelerated the spread of both true and false news at the same rate. This implies that false news spreads more than the truth because humans\u2013not robots\u2013are more likely to spread it.<br></p>", "people": ["dkroy@media.mit.edu", "soroush@media.mit.edu"], "title": "The Spread of True and False Information Online", "modified": "2018-06-08T00:37:43.310Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2018-05-31", "slug": "the-spread-of-false-and-true-info-online"}, {"website": "", "description": "<h2><b>&nbsp;Technical Summary</b></h2><p>Histopathology tissue samples are widely available in two states: paraffin-embedded unstained and non-paraffin- embedded stained whole slide RGB images (WSRI). Hema- toxylin and eosin stain (H&amp;E) is one of the principal stains in histology but suffers from several shortcomings related to tissue preparation, staining protocols, slowness and human error. We report two novel approaches for training machine learning models for the computational H&amp;E staining and destaining of prostate core biopsy RGB images. The staining model uses a conditional generative adversarial network that learns hierarchical non-linear mappings between whole slide RGB image (WSRI) pairs of prostate core biopsy before and after H&amp;E staining. The trained staining model can then generate computationally H&amp;E-stained prostate core WSRIs using previously unseen non-stained biopsy images as input. The destaining model, by learning mappings between an H&amp;E stained WSRI and a non-stained WSRI of the same biopsy, can computationally destain previously unseen H&amp;E-stained images. Structural and anatomical details of prostate tissue and colors, shapes, geometries, locations of nuclei, stroma, vessels, glands and other cellular components were generated by both models with structural similarity indices of 0.68 (staining) and 0.84 (destaining). The proposed staining and destaining models can engender computational H&amp;E staining and destaining of WSRI biopsies without additional equipment and devices.</p>", "people": [], "title": "Computational Histological Staining and Destaining of Prostate Core Biopsy RGB Images with Generative Adversarial Neural Networks", "modified": "2018-10-21T23:28:59.172Z", "visibility": "LAB", "start_on": "2017-11-01", "location": "", "groups": ["health-0-0"], "published": false, "active": false, "end_on": "2018-10-17", "slug": "computational-histological-staining-and-destaining-of-prostate-core-biopsy-rgb-images-with-generative-adversarial-neural-networks"}, {"website": "", "description": "<p>Inspired by the Surrealists' Exquisite Corpse art game, the NeverEnding Drawing project is one of several applications developed on a scalable architecture and platform for collaborative creativity. Users co-create and edit each other's augmented sketchbooks in real time. By tracking individual pages of each live sketchbook, the system loads the appropriate background audiovisual content and enables users to add to it using a variety of real materials and means of mark-making. Users take pictures and record sounds to be sent back and forth between collaborators on the network. Additionally, the live sketchbooks facilitate non-linear, asynchronous  access to the evolving, co-created content through their physical editing interface. By using crayons, colored pens, and various tactile and light-diffusing materials, the analog/digital hybrid model of content creation requires no expertise and creates a safe environment for sharing unfinished work with others.</p>", "people": ["cynthiab@media.mit.edu", "vmb@media.mit.edu", "gid@media.mit.edu"], "title": "NeverEnding Drawing Machine", "modified": "2017-03-24T14:50:40.999Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["personal-robots", "tangible-media", "object-based-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "neverending-drawing-machine"}, {"website": "", "description": "<p>The CityCar electric automobile, developed and prototyped by Smart Cities, is designed to meet the demand for enclosed personal mobility \u2013 with weather protection, climate control and comfort, secure storage, and crash protection \u2013 in the cleanest and most economical way possible. It weighs less than a thousand pounds, parks in much less space than a Smart Car, and is expected to get the equivalent of 150 to 200 miles per gallon of gasoline. Since it is battery-electric, it produces no tailpipe emissions.</p><p>The architecture of the CityCar is radical. It does not have a central engine and traditional power train, but is powered by four in-wheel electric motors. Each wheel unit contains drive motor (which also enables regenerative braking), steering, and suspension, and is independently digitally controlled. This enables maneuvers like spinning on its own axis (an O-turn instead of a U-turn), moving sideways into parallel parking spaces, and lane changes while facing straight ahead.</p>", "people": ["kll@media.mit.edu", "rchin@media.mit.edu"], "title": "CityCar", "modified": "2017-08-24T12:02:37.347Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-001", "groups": ["city-science"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "citycar"}, {"website": "", "description": "<p>This 2018 AI and Governance Assembly project, <a href=\"http://equalais.media.mit.edu/\">EqualAIs</a>, was a multifaceted investigation into the technical, policy and societal questions raised by the increasing capabilities of facial recognition software and the unprecedented  capability for automated, real time identification and tracking of  individuals.</p><p>Our team demonstrated the technical feasibility of facial recognition adversarial attacks through the creation of a working prototype, successfully attacking the major APIs' facial recognition classifiers. We filed a FOIA with partners including the ACLU for information about US Customs' use of facial recognition algorithms and we sought to encourage public dialogue about facial recognition policy choices through a series of public talks and the creation of open source resources.<br></p>", "people": ["ggreene@media.mit.edu"], "title": "EqualAIs: Facial Recognition, Adversarial Attacks and Policy Choice", "modified": "2018-10-18T01:27:03.683Z", "visibility": "PUBLIC", "start_on": "2018-02-01", "location": "", "groups": ["ethics-and-governance"], "published": true, "active": false, "end_on": "2018-04-20", "slug": "equalais-adversarial-attacks-for-facial-recognition"}, {"website": "", "description": "<p>Let's play: Waste at MIT is a game about trash; it\u2019s also a game about understanding civic infrastructure. This project explores the use of games to engage citizens, impacting real-world actions in complex civic systems. Working with waste management and sustainability efforts on campus, this project uses MIT as a living lab to study the link between interaction and action in civic media. More generally, this project looks at ways in which participatory, critical and exploratory games can give people agency over the complex systems that surround them.</p><p>Participate in the project by playing! Visit <a href=\"http://wasteatmit.media.mit.edu\">wasteatmit.media.mit.edu</a> to learn more. The code for this project is open-sourced at:&nbsp;<a href=\"http://github.com/agnescameron/trashgame\">http://github.com/agnescameron/trashgame</a></p>", "people": ["agnescam@media.mit.edu"], "title": "Let's play: Waste at MIT", "modified": "2019-06-04T19:37:53.019Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2019-06-07", "slug": "waste-at-mit"}, {"website": "", "description": "", "people": [], "title": "Networked Playscapes: Andamio", "modified": "2017-03-24T16:41:42.608Z", "visibility": "PUBLIC", "start_on": "2015-07-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2016-10-31", "slug": "networked-playscapes-andamio"}, {"website": "http://rnd.studio/project/amoeba-wall", "description": "<p>Amoeba Wall: A context aware wall system:&nbsp;Amoetecture is a set of amoeba-like dynamic spatial elements, including transformable floors, ceilings, tables, chairs, and workstations. We focus on designing architecture robotics and platforms that enable a hyper-efficient and dynamically reconfigurable coworking space that accommodates a wide range of activities in a small area.</p><p><strong>&nbsp;Award</strong></p><p><strong>-&nbsp;</strong><a href=\"https://www.designboom.com/design/2019-a-design-award-and-competition-call-for-entries-02-07-2019/\"><strong>A' Design Award&nbsp;</strong><strong>2017 -&nbsp;</strong><strong>Gold Prize</strong></a></p><p><strong>-&nbsp;</strong>Honorable Mention - Tomorrow Workplace Competition by METROPOLIS</p><p><b>Publication</b></p><p><b>-</b><a href=\"http://www.academia.edu/35684876/Amoeba_Wall\">H Deng, H Ho, L Alonso, X Li, J Angulo, K Larson Amoeba Wall - PASAJES - archquitectura NO.143, pp8-9</a></p>", "people": ["alonsolp@media.mit.edu", "honghaod@media.mit.edu", "oi7@media.mit.edu"], "title": "Amoeba Wall: A context-aware wall system", "modified": "2019-02-11T03:26:16.506Z", "visibility": "PUBLIC", "start_on": "2016-01-04", "location": "", "groups": [], "published": true, "active": false, "end_on": "2016-12-31", "slug": "amoeba-wall"}, {"website": "", "description": "<p>SandScape is a tangible interface for designing and understanding landscapes through a variety of computational simulations using sand. The simulations are projected on the surface of a sand model representing the terrain; users can choose from a variety of different simulations highlighting height, slope, contours, shadows, drainage, or aspect of the landscape model, and alter its form by manipulating sand while seeing the resulting effects of computational analysis generated and projected on the surface of sand in real time. SandScape demonstrates an alternative form of computer interface (tangible user interface) that takes advantage of our natural abilities to understand and manipulate physical forms while still harnessing the power of computational simulation to help in our understanding of a model representation.</p>", "people": ["ishii@media.mit.edu"], "title": "SandScape", "modified": "2016-12-05T00:17:03.872Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "sandscape"}, {"website": "", "description": "<p>Imagine a physical object that floats through the air, unconstrained by gravity, which can move freely to display the motion of a planet in orbit or the flight path of an airplane. We introduce ZeroN, a new tangible interface element based on a levitated object that can be moved freely in a three-dimensional space to represent and control the dynamic status of computation.  Users can see, feel, and control computation through interaction with ZeroN. One can place and move the ZeroN in the air to animate characters' movements or simulate physics in a tangible form.</p>", "people": ["ishii@media.mit.edu", "rehmi@media.mit.edu"], "title": "ZeroN: Levitated Tangible Interface", "modified": "2019-04-09T13:41:09.022Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "zeron-levitated-tangible-interface"}, {"website": "", "description": "<p><a href=\"https://twitter.com/beeme_mit\">BeeMe</a> a massive immersive social game that directly draws inspiration from<a href=\"https://owa.exchange.mit.edu/owa/redir.aspx?C=ZxOiBK_RTx9yjsDV33RV7Hlj5grGY-DdQlJ9osuamIzqlCueCD7WCA..&amp;URL=https%3a%2f%2fwww.imdb.com%2ftitle%2ftt3531824%2f\"> popular culture</a>,<a href=\"https://owa.exchange.mit.edu/owa/redir.aspx?C=QFKraQ8VaK9kjKe7_S9IhtxaoVSC9OSCF9QKfrMOtGDqlCueCD7WCA..&amp;URL=https%3a%2f%2fwww.amazon.com%2fCircle-Dave-Eggers%2fdp%2f159413961X\"> literature</a>,<a href=\"https://owa.exchange.mit.edu/owa/redir.aspx?C=4UOPlp7_QKNzFMGuulv4ZaXzzgukEG_B2e8c1iqa8_XqlCueCD7WCA..&amp;URL=https%3a%2f%2fwww.theguardian.com%2fartanddesign%2f2014%2fmay%2f12%2fmarina-abramovic-ready-to-die-serpentine-gallery-512-hours\"> performing arts</a>, gaming, and YouTube streaming culture.</p><p>On Halloween night (10/31/2018) at 11pm ET, an actor will give up their free will and let Internet users control their every action. The event will follow the story of an evil AI by the name of <i>Zookd, </i>who has accidentally been released online. Internet users will have to coordinate at scale and collectively help the actor (also a character in the story) to defeat <i>Zookd</i>. If they fail, the consequences could be disastrous.</p><p>To play, simply connect to<a href=\"http://beeme.online\"> beeme.online</a> on the night of Halloween at 11pm ET. We believe BeeMe will provide insight into ways we can fundamentally change the nature of online interactions and entertainment.&nbsp;</p>", "people": ["niccolop@media.mit.edu"], "title": "BeeMe", "modified": "2018-10-31T13:08:43.651Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": "2018-11-01", "slug": "beeme"}, {"website": "", "description": "<p>The spread of malicious or accidental misinformation in social media, especially in time-sensitive situations, such as real-world emergencies, can have harmful effects on individuals and society. In this work, we developed models for automated verification of rumors (unverified information) that propagate through Twitter. To predict the veracity of rumors, we identified salient features of rumors by examining three aspects of information spread: linguistic style used to express rumors, characteristics of people involved in propagating information, and network propagation dynamics. The predicted veracity of a time series of these features extracted from a rumor (a collection of tweets) is generated using Hidden Markov Models. The verification algorithm was trained and tested on 209 rumors representing 938,806 tweets collected from real-world events, including the 2013 Boston Marathon bombings, the 2014 Ferguson unrest, and the 2014 Ebola epidemic, and many other rumors about various real-world events reported on popular websites that document public rumors. The algorithm was able to correctly predict the veracity of 75% of the rumors faster than any other public source, including journalists and law enforcement officials. The ability to track rumors and predict their outcomes may have practical applications for news consumers, financial markets, journalists,&nbsp;and emergency services, and more generally to help minimize the impact of false information on Twitter.</p>", "people": ["dkroy@media.mit.edu", "soroush@media.mit.edu"], "title": "Rumor Gauge: Automatic Detection and Verification of Rumors on Twitter", "modified": "2018-06-08T00:36:14.072Z", "visibility": "PUBLIC", "start_on": "2013-04-15", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2015-06-01", "slug": "rumor-gauge-automatic-detection-and-verification-of-rumors-in-twitter"}, {"website": "", "description": "<p>HyperCubes is an augmented reality platform to help children understand computational concepts drawn from their physical surroundings, from their most immediate and tangible reality. Children become creators and learn while tinkering with commands such as transformations in space that control little characters or geometry shapes.&nbsp;</p>", "people": ["afuste@media.mit.edu"], "title": "HyperCubes: Learning computational concepts in Augmented Reality", "modified": "2018-10-18T01:45:06.375Z", "visibility": "PUBLIC", "start_on": "2018-05-07", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "hypercubes"}, {"website": "", "description": "<p>8K Time into Space is a user interface for a video exploration system with an 8K display. 8K is an ultra high-definition video system and it can present a huge amount of visual content on one display. In our system, video thumbnails with shifted playback time in chronological order are spaced out like tiles. The time range of a scene that a viewer wants to check can be adjusted with a touch interface, and resolution of the thumbnails is changed depending of the range. 8K Time into Space aims to provide responsive and intuitive experiences for video consumption.</p>", "people": ["ohmata@media.mit.edu", "lip@media.mit.edu"], "title": "8K Time into Space", "modified": "2019-04-17T22:34:57.493Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "8k-time-into-space"}, {"website": "", "description": "<p>Only 40% of the eligible population votes in the typical US midterm election, and among young people turnout is even lower. In this experiment, we develop a game that encourages people to influence their friends to physically go to the polls. The system is reminiscent of <a href=\"https://www.media.mit.edu/projects/fiftynifty/overview/\">Fifty Nifty</a>, where people competed to amass points by both calling representatives and spreading the message to others. In addition to awarding points, vote.lol aims to motivate players by allowing them influence over the outcome of a shared narrative that develops in real-time before (and during) the election. Interactive stories with real-world game mechanics are characteristic of alternate reality games (ARGs), which have received scholarly attention for their potential to instigate viral communications among players who self-organize to solve complex problems. The purpose of this study is to test whether ARG techniques can motivate gamers to solve the intractable problem of getting their peers to vote.</p>", "people": ["mhjiang@media.mit.edu", "agnescam@media.mit.edu", "smpsnr@media.mit.edu", "lip@media.mit.edu"], "title": "Election ARG", "modified": "2019-04-18T00:52:49.802Z", "visibility": "PUBLIC", "start_on": "2018-10-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "election-arg"}, {"website": "", "description": "<p>Test description</p>", "people": [], "title": "TMG test project", "modified": "2017-04-03T17:44:50.744Z", "visibility": "PUBLIC", "start_on": "2017-04-03", "location": "", "groups": ["tangible-media"], "published": false, "active": false, "end_on": "2017-04-04", "slug": "tmg-test-project"}, {"website": "http://www.realityeditor.org", "description": "<p>The Reality Editor is a new kind of tool for empowering you to connect and manipulate the functionality of physical objects. Just point the camera of your smartphone at an object and its invisible capabilities will become visible for you to edit. Drag a virtual line from one object to another and create a new relationship between these objects. With this simplicity, you are able to master the entire scope of connected objects.</p>", "people": ["benolds@media.mit.edu", "hobinjk@media.mit.edu", "pattie@media.mit.edu", "heun@media.mit.edu"], "title": "Reality Editor", "modified": "2018-04-30T14:19:37.322Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2017-06-30", "slug": "reality-editor"}, {"website": "", "description": "<p><span style=\"font-size: 18px; font-weight: 400;\">Robots today can be made of various materials; we are no longer&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">limited to heavy metal parts and assemblies. This project explores \"informal\" materials for movement of&nbsp; structures created from a single sheet of paper. By relying on the flexibility or rigidity of various materials, we use origami folding techniques to achieve a precise movement.&nbsp; Kinematics and mechanical principles seen in pop-up and flat folding structures are implemented using a laminate assembly technique.&nbsp;</span><br></p>", "people": ["lizbethb@media.mit.edu", "elenack@media.mit.edu"], "title": "Origami Robotics", "modified": "2019-04-23T13:35:59.036Z", "visibility": "PUBLIC", "start_on": "2019-01-01", "location": "", "groups": ["responsive-environments", "space-enabled"], "published": true, "active": false, "end_on": "2019-06-01", "slug": "origami-robotics"}, {"website": "https://blackrockatlas.mit.edu", "description": "<p>Burning Man is a magical place that gets the best of human creativity and collaboration to flourish. To further understand what makes this magic happen, we are creating the first ever Black Rock Atlas\u2013a map of the social patterns and networks that exist on the playa. </p><p>To do this, we are tracking the decentralized journey of a multitude of vessels through the gift economy of Burning Man with GPS technology and generative photography. The Atlas will explore new ways of community interaction, storytelling, and data visualization.</p>", "people": ["cebrian@media.mit.edu", "niccolop@media.mit.edu", "zive@media.mit.edu", "groh@media.mit.edu", "emoro@media.mit.edu", "irahwan@media.mit.edu", "nobradov@media.mit.edu"], "title": "Black Rock Atlas", "modified": "2018-10-15T20:44:54.352Z", "visibility": "PUBLIC", "start_on": "2018-08-15", "location": "", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": "2018-12-11", "slug": "black-rock-atlas"}, {"website": "", "description": "<p>The galvactivator is a glove-like wearable device that senses the wearer's skin conductivity and maps its values to a bright LED display. Increases in skin conductivity across the palm tend to be good indicators of physiological arousal, causing the galvactivator display to glow brightly. The galvactivator has many potentially useful purposes, ranging from self-feedback for stress management, to facilitation of conversation between two people, to new ways of visualizing mass excitement levels in performance situations or visualizing aspects of arousal and attention in learning situations. One of the findings in mass-communication settings was that people tended to \"glow\" when a new speaker came onstage, and during live demos, laughter, and live audience interaction. They tended to \"go dim\" during powerpoint presentations.  In smaller educational settings, students have commented on how they tend to glow when they are more engaged with learning.</p>", "people": ["picard@media.mit.edu"], "title": "Galvactivator", "modified": "2017-01-09T17:48:33.951Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "galvactivator"}, {"website": "", "description": "<p>This project is the first of two projects in collaboration with GSK. We are developing a computational simulation that allows a human user (or AI) to test drug manufacturing investment scenarios for an entire portfolio over multiple years. We aspire to help decision-makers understand the possible impact of new techniques such as CBM on selected key performance metrics. This game like simulation allows various stakeholders to come together and make collaborative decisions regarding the entire supply chain. The software works dynamically with a Tactile Matrix, which is an interactive decision support system that allows users to instantly and collaboratively explore the models in an approachable, tangible way. </p><p>Screenshots courtesy of Ira Winder. Photos by Nina Lutz.</p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu", "nlutz@media.mit.edu"], "title": "GSK Manufacturing Initiative", "modified": "2018-08-20T20:42:11.340Z", "visibility": "PUBLIC", "start_on": "2017-01-09", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": "2018-06-01", "slug": "gsk-manufacturing-initiative"}, {"website": "", "description": "<p>This project is part of a parallel research endeavor with GSK Manufacturing. By simulating how scientists at the Upper Providence site interact with one another and the space around them, we hope to help assist future renovations in a range of GSK locations. This was motivated by GSK\u2019s drive to improve spatial configuration within their organization, including various open office environments and even smart labs.&nbsp;</p><p>We hope this project will serve as both a packaged decision support system and a framework for GSK scientists and stakeholders to reconfigure with their own spatial inquiries and case studies. Our goals can be enumerated as below.&nbsp;</p><ol>\n<li>Design a decision support tool that allows R&amp;D to understand how changes to physical environments and adjacencies may have an impact on key workplace indicators. Encourage data-driven demonstration and discussion of decisions related to spatial changes.&nbsp;</li>\n<li>Employ spatial mathematical models to calculate key workplace indicators, including</li>\n<ol>\n\n<li>Space utilization</li>\n<li>Time accessibility between amenities</li>\n<li>Synergy, defined as potential for interaction among researchers</li>\n\n</ol>\n<li>Deploy the decision support tool as an evolving platform that exists in two forms:&nbsp;</li>\n<ol>\n\n<li>Tangible user interface \u201cTactile Matrix\u201d</li>\n<li>Traditional executable application that can be used on personal machines with mouse and keyboard interface.&nbsp;</li>\n<li>Provide transparent source code and open source, off the shelf technologies that allows for adaptation and customization for various case studies and applications.&nbsp;</li>\n\n</ol>\n</ol><br><p>Screenshots courtesy of Nina Lutz.&nbsp;<br></p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu", "nlutz@media.mit.edu"], "title": "GSK Places Initiative", "modified": "2018-08-20T20:43:34.774Z", "visibility": "PUBLIC", "start_on": "2017-01-09", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": "2018-06-01", "slug": "gsk-places-initiative"}, {"website": "", "description": "<p>This is an open source geospatial exploration tool. Using various public APIs including Open Street Map and the United States Census, we can make dynamic, flexible models of how people are moving through the city. These models include accessibility in cities, multimodal transportation networks, and diversity. Overall this allows anyone with or without an urban planning background to build strong models with geospatial and urban data. This system works dynamically with a Tactile Matrix, which is an interactive decision support system that allows users to instantly and collaboratively explore the models in a tangible way.</p><p>Photos by Nina Lutz.&nbsp;</p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu", "nlutz@media.mit.edu"], "title": "GeoBits", "modified": "2018-09-04T19:47:57.544Z", "visibility": "PUBLIC", "start_on": "2016-07-11", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": "2018-06-01", "slug": "geobits"}, {"website": "", "description": "<p>We present coded focal stack photography as a computational photography paradigm that combines a focal sweep and a coded sensor readout with novel computational algorithms. We demonstrate various applications of coded focal stacks, including photography with programmable non-planar focal surfaces and multiplexed focal stack acquisition. By leveraging sparse coding techniques, coded focal stacks can also be used to recover a full-resolution depth and all-in-focus (AIF) image from a single photograph. Coded focal stack photography is a significant step towards a computational camera architecture that facilitates high-resolution post-capture refocusing, flexible depth of field, and 3D imaging.</p>", "people": ["raskar@media.mit.edu", "gordonw@media.mit.edu", "naik@media.mit.edu"], "title": "Coded Focal Stack Photography", "modified": "2016-12-05T00:17:04.011Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2015-09-15", "slug": "coded-focal-stack-photography"}, {"website": "http://maggic.ooo/Open-Source-Estrogen-2015", "description": "<p>Biomolecules to biopolitics: hormones with institutional biopower! Open Source Estrogen combines do-it-yourself science, body and gender politics, and ethics of hormonal manipulation. The goal of the project is to create an open source protocol for estrogen biosynthesis. The kitchen is a politically charged space, prescribed to women as their proper dwelling, making it the appropriate place to prepare an estrogen synthesis recipe. With recent developments in the field of synthetic biology, the customized kitchen laboratory may be a ubiquitous possibility in the near future. Open-access estrogen would allow women and transgender females to exercise greater control over their bodies by circumventing governments and institutions. We want to ask: What are the biopolitics governing our bodies? More importantly, is it ethical to self-administer self-synthesized hormones?</p>", "people": ["sputniko@media.mit.edu", "maggic@media.mit.edu"], "title": "Open Source Estrogen", "modified": "2017-10-11T20:29:44.233Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["design-fiction"], "published": true, "active": false, "end_on": "2017-10-31", "slug": "open-source-estrogen"}, {"website": "", "description": "<p>This project focused on pedestrian accessibility in collaboration with Singapore Centre for Liveable Cities. Researchers and planners came together to design an interface that would allow both citizens and planners to interact with a model regarding pedestrian accessibility. The tangible interface allows users to come together to have conversations and make interventions to make the case study area more accessible for pedestrians.&nbsp;</p>", "people": ["jiw@media.mit.edu", "kll@media.mit.edu", "nlutz@media.mit.edu"], "title": "Singapore Pedestrian Accessibility", "modified": "2018-08-22T03:18:47.497Z", "visibility": "PUBLIC", "start_on": "2016-06-06", "location": "", "groups": ["city-science"], "published": true, "active": false, "end_on": "2018-06-01", "slug": "singapore-pedestrian-accessibility"}, {"website": "", "description": "<p>With over a billion people carrying camera-phones worldwide, we have a new opportunity to upgrade the classic bar code to encourage a flexible interface between the machine world and the human world. Current bar codes must be read within a short range, and the codes occupy valuable space on products. We present a new, low-cost, passive optical design so that bar codes can be shrunk to fewer than 3mm and can be read by unmodified ordinary cameras several meters away.</p>", "people": ["raskar@media.mit.edu", "naik@media.mit.edu"], "title": "Bokode: Imperceptible Visual Tags for Camera-Based Interaction from a Distance", "modified": "2017-04-03T17:47:50.500Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-320", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "bokode-imperceptible-visual-tags-for-camera-based-interaction-from-a-distance"}, {"website": "", "description": "<p>Embodied theories of language propose that the way we communicate verbally is grounded in our bodies. Nevertheless, the way a second language is conventionally taught does not capitalize on kinesthetic modalities. The tracking capabilities of room-scale virtual reality systems afford a way to incorporate kinesthetic learning in language education.&nbsp; Words in Motion is a virtual reality language learning system that reinforces associations between word-action pairs by recognizing a student\u2019s movements and presenting the corresponding name of the performed action in the target language. Experiments with Words in Motion suggest that the kinesthetic approach in virtual reality has less immediate learning gain in comparison to a text-only condition. However, virtual kinesthetic learners showed significantly higher retention rates after a week of exposure. Positive correlation between the times a word-action pair was executed and the times a word was remembered by the subjects, supports the premise that virtual reality can impact language learning by leveraging kinesthetic elements.</p>", "people": ["cdvm@media.mit.edu", "pattie@media.mit.edu"], "title": "Words in Motion", "modified": "2018-08-20T16:26:03.600Z", "visibility": "PUBLIC", "start_on": "2017-02-13", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-05-31", "slug": "words-in-motion"}, {"website": "", "description": "<p>We are performing long-term measurements of autonomic nervous system (ANS) activity on patients with epilepsy. In certain cases, autonomic symptoms are known to precede seizures. Usually in our data, the autonomic changes start when the seizure shows in the EEG, and can be measured with a wristband (much easier to wear every day than wearing an EEG). We found that the larger the signal we measure on the wrist, the longer the duration of cortical brain-wave suppression following the seizure.  The duration of the latter is a strong candidate for a biomarker for SUDEP (Sudden Unexpected Death in Epilepsy), and we are working with scientists and doctors to better understand this. In addition, bilateral changes in ANS activity may provide valuable information regarding seizure focus localization and semiology.</p>", "people": ["picard@media.mit.edu", "zher@media.mit.edu"], "title": "Autonomic Nervous System Activity in Epilepsy", "modified": "2019-04-18T03:27:11.639Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2014-04-14", "slug": "autonomic-nervous-system-activity-in-epilepsy"}, {"website": "", "description": "<p>The Social Robot Toolkit aims to provide a platform for children to learn through playful interaction. The social robot (Soro) toolkit allows preschool children to experiment with computational concepts while teaching a social robot new rules. The toolkit also provides a platform for learning interpersonal skills through the use of storytelling that integrates interpersonal and computational concepts. This harnesses preschoolers' natural interest in social interaction to familiarize them with new concepts.</p>", "people": ["cynthiab@media.mit.edu", "edith@media.mit.edu", "michalg@media.mit.edu", "randiw12@media.mit.edu", "haewon@media.mit.edu"], "title": "Social Robot Toolkit", "modified": "2017-06-05T16:13:22.057Z", "visibility": "PUBLIC", "start_on": "2014-08-23", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2017-04-30", "slug": "soro"}, {"website": "", "description": "<p>Body Quest is a room-scale virtual reality playground for learning about biology and chemistry. Learning about how complex microscopic 3D structures interact is hard on paper, and only slightly easier with videos or passive 3D simulations. Interactive, room-scale VR environments open up new possibilities for building an intuitive and visual understanding of these subjects\u2014and it can even be fun!</p><p>Our video was submitted to the DOE EdSim Challenge. It is a fully functional prototype\u2014the mixed reality video representing the physical and virtual experiences were composited in real-time by calibrating physical and virtual cameras, while filming in front of a green screen. The prototype is built around one particular biochemical interaction, whereby a viral protein cleaves sugar off the end of a mucus chain. We hope to develop future learning interactions around a general simulation backend, which will host both structured and unstructured learning experiences, including games.</p><p><br></p>", "people": ["swgreen@media.mit.edu"], "title": "Body Quest: A Room-Scale VR Playground for Biology and Chemistry", "modified": "2018-08-20T16:43:43.257Z", "visibility": "PUBLIC", "start_on": "2016-12-15", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "body-quest"}, {"website": "", "description": "<p>We are using nanowires to build structures for synthetic photosynthesis for the solar generation of liquid fuels.</p>", "people": ["jacobson@media.mit.edu"], "title": "Synthetic Photosynthesis", "modified": "2016-12-05T00:17:04.139Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "synthetic-photosynthesis"}, {"website": "", "description": "<p>Motion sensing is of fundamental importance for user interfaces and input devices. In applications where optical sensing is preferred, traditional camera-based approaches can be prohibitive due to limited resolution, low frame rates, and the required computational power for image processing. We introduce a novel set of motion-sensing configurations based on laser speckle sensing that are particularly suitable for human-computer interaction. The underlying principles allow these configurations to be fast, precise, extremely compact, and low cost.</p>", "people": ["raskar@media.mit.edu", "olwal@media.mit.edu", "naik@media.mit.edu"], "title": "SpeckleSense", "modified": "2017-04-03T18:24:45.671Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "specklesense"}, {"website": "", "description": "<p>Surface and object recognition is of significant importance in ubiquitous and wearable computing. While various techniques exist to infer context from material properties and appearance, they are typically neither designed for real-time applications nor for optically complex surfaces that may be specular, textureless, and even transparent. These materials are, however, becoming increasingly relevant in HCI for transparent displays, interactive surfaces, and ubiquitous computing. We present SpecTrans, a new sensing technology for surface classification of exotic materials, such as glass, transparent plastic, and metal. The proposed technique extracts optical features by employing laser and multi-directional, multi-spectral LED illumination that leverages the material's optical properties. The sensor hardware is small in size, and the proposed classification method requires significantly lower computational cost than conventional image-based methods, which use texture features or reflectance analysis, thereby providing real-time performance for ubiquitous computing.</p>", "people": ["raskar@media.mit.edu", "shigeoy@media.mit.edu", "olwal@media.mit.edu", "naik@media.mit.edu", "shiboxin@media.mit.edu"], "title": "SpecTrans: Classification of Transparent Materials and Interactions", "modified": "2017-04-03T18:25:33.571Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2016-01-01", "slug": "spectrans-classification-of-transparent-materials-and-interactions"}, {"website": "", "description": "<p>Much of the recent interest in virtual worlds has focused on using the immersive properties of virtual worlds to recreate an experience like that of interacting face to face with other participants. This work instead focuses on how we can use the distinctive properties of virtual spaces to create experiences native to virtual worlds. I present two projects that have different perspectives on this concept. The first project, Information Spaces,  demonstrates how visualization of behavior in a 3-D meeting space can augment the meeting process and provide participants new behavioral ways to communicate. The second project, *space, is an abstract, 2-D virtual platform for prototyping and experimenting with virtual world experiences that provides a structure for changing properties of the virtual space to influence people's behavior in that space.</p>", "people": ["judith@media.mit.edu"], "title": "starspace: Algorithmic Architecture in Virtual Spaces", "modified": "2016-12-05T00:17:04.269Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "starspace-algorithmic-architecture-in-virtual-spaces"}, {"website": "", "description": "<p>Buildings consume more than a third of the energy used in the United States, but most people have no sense of how much their actions can affect a building's energy use. We are testing the hypothesis that if people have a convenient way to record their energy use and learn ways to improve it, they will change their habits. We have created visualizations of HVAC use throughout the Media Lab to test this hypothesis in a large-scale space. The system uses touch-screen networked displays strategically placed throughout the building to convey real-time and historical temperature and thermostat settings, and ultimately electric usage. Not only can people see a heat map of their lab area, they can also observe trends and compare their energy usage to those in other areas.</p>", "people": ["lip@media.mit.edu"], "title": "Social Energy", "modified": "2016-12-05T00:17:04.413Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "social-energy"}, {"website": "", "description": "<p>SmileSeeker is a novel, machine-vision system that captures and provides quantified information about nonverbal communication where social interactions naturally happen. For example, in banking services, tellers observe facial expressions, head gestures, and eye gaze of customers, but this tool lets them both observe their own expressions and analyze how these interact with those of the customer to influence their mutual experience.  The tool allows either real-time or offline feedback to help people reflect on what these interactions mean and determine how to elicit better experiences, such as true customer delight.  The first deployment of this project focuses on eliciting and capturing smiles, and doing so in a way that is respectful of both customer and employee feelings. This project will also explore ways to share this information and link it to outcomes such as banking fee reductions or donations to charity.</p>", "people": ["picard@media.mit.edu"], "title": "SmileSeeker: Customer and Employee Affect Tagging System", "modified": "2016-12-05T00:17:04.322Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-448", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "smileseeker-customer-and-employee-affect-tagging-system"}, {"website": "", "description": "<p>Social Network Fragments is interested in exploring the structure of an individual's social network. The relationship between actors in one's network says a great deal about the individual, revealing how he or she segments his or her social circles to reflect different facets of his or her life and social identity. In our research, we use visualization techniques to reveal the network structure that people develop during their email correspondences, thereby creating the tools to help understand aspects of social interaction. Additionally, this research reveals how much information can be derived about a large group of people by analyzing one person's habits. This is particularly interesting given our desire to understand communities as a whole, not just individuals. Such research raises serious questions about privacy, representation, and analysis that must be addressed as we move forward.</p>", "people": ["judith@media.mit.edu"], "title": "Social Network Fragments", "modified": "2016-12-05T00:17:04.441Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-383", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "social-network-fragments"}, {"website": "", "description": "<p>Online public discussion spaces such as Usenet news groups are rich social environments. The social dynamics within the community are not obvious by looking at the strings of text-based content. Only a careful reading of the threads allows the viewer to discern complexities and nuances of social interactions. Expressive visualization, however, is an alternative medium for effectively conveying such information. In order to animate the dynamic social qualities found within the static data of a Usenet interface, motion is used as the communicative agent for this visual translation. The goal of the research is to isolate those elements that comprise visual motion, such as position, direction, speed, and time, in order to develop a visual language through which the social complexities of online communities can be communicated. A series of studies exploring this problem are being carried out using a theoretical framework inspired by cognitive and artistic precedents. We hope these investigations result in an understanding of how motion can be successfully employed as a visual language for social expression.</p>", "people": ["judith@media.mit.edu"], "title": "Socio-Kinetics", "modified": "2016-12-05T00:17:04.538Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-450", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "socio-kinetics"}, {"website": "", "description": "<p>In a modern urban setting, the organization of architectural space structures spatial proximity in the same fashion as peculiarities of natural landscape defined it for our nomadic primogenitors. Patterns of face-to-face interactions in a building are, to a large extent, defined when architects differentiate spaces in it for distinct functions and organize circulation between the spaces. Buildings of the future could be instrumented to inform communication devices carried by their inhabitants and visitors about the intercommunication configuration of spaces within and outside them, thus defining proximity in software. The goal of this project is to develop and evaluate a prototype wearable system for proximity-based synchronous voice messaging in a space configured for intercommunication. Potential applications range from wearable computers promoting ad hoc collaboration within organizations to communication systems for firefighters.</p>", "people": [], "title": "Spatial Aspects of Mobile Ad Hoc Collaboration", "modified": "2016-12-05T00:17:04.813Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "--Choose Location", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "spatial-aspects-of-mobile-ad-hoc-collaboration"}, {"website": "", "description": "<p>When humans collaborate face-to-face, they use a wide range of nonverbal signals that support the conversation process. Many of these signals are not available when people have conversations online by exchanging text messages, or are distorted when they attempt conversation through direct transmission, such as video. This project presents a new approach to computer-mediated conversation where the missing signals are added based on a textual analysis of the message and the dynamic discourse context. Spark is the software architecture that supports the automated analysis of communicative function of text messages, the transformation of those functions into nonverbal cues and the performance of those cues by animated avatars.</p>", "people": [], "title": "Spark: Avatar Augmented Online Collaboration", "modified": "2016-12-05T00:17:04.721Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "spark-avatar-augmented-online-collaboration"}, {"website": "", "description": "<p>A thing cannot be in two places at once. It cannot move through a solid object. If it goes from point A to point B, it must move through a series of intermediate, adjacent points. These are examples of common sense knowledge belonging to every young child, but heretofore absent in computers. This project involves designing a cognitive architecture for building systems with this kind of spatial and physical common sense.</p>", "people": ["minsky@media.mit.edu"], "title": "Spatial and Physical Common Sense", "modified": "2016-12-05T00:17:04.747Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-383", "groups": ["society-of-mind"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "spatial-and-physical-common-sense"}, {"website": "", "description": "<p>This project uses spatialized audio and presentation of audio recordings from different points in space around the listener's head to enable more efficient listening. Examples include: browsing multiple streams of audio simultaneously, with computer-enhanced, selective attention modeling and automatic notification of salient events on secondary channels; spatial representation of a single audio recording to map time into position for more effective dynamic browsing; and auditory \"information landscapes\" which allow a listener to move in a virtual space among recordings.</p>", "people": ["geek@media.mit.edu"], "title": "Spatial Listening for Auditory Presentation", "modified": "2016-12-05T00:17:04.771Z", "visibility": "PUBLIC", "start_on": "1993-12-31", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "spatial-listening-for-auditory-presentation"}, {"website": "", "description": "<p>SoundSieve is an exciting new way to look at the structure of music, designed to create a visual \ufffdfingerprint that can provide the listener with instant information about any given piece. SoundSieve aims to counter the trend towards overly simplistic, flashy music visualization programs by creating a real-time visualization that is not only informative, but enlightening as to the underlying musical structure of the piece. From any MP3 file, SoundSieve uses simple, intuitive mapping to create a picture and video playback that highlight key audio features. In addition, SoundSieve allows the user to actively examine the piece: to zoom in on portions of interest, adjust playback, and add informative overlays. SoundSieve has recently been used to create customized animations that highlight particular features in a piece (e.g., Tod Machover\ufffds \"The System\" for the opera \"Death and the Powers\").</p>", "people": ["tod@media.mit.edu"], "title": "SoundSieve", "modified": "2016-12-05T00:17:04.793Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-443", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "soundsieve"}, {"website": "", "description": "<p>Speaker identification techniques (such as a Gaussian Mixture Model (GMM) and a Hidden Markov Model, using cepstral coefficients as features) have been applied to determine the question of whether individual marine mammals can be identified by their vocalizations alone. With a dataset of four killer whales uttering sounds previously classified as belonging to call type n2, and over 10 sounds from each individual, we have found a very high success rate of 80 to 100% correct for the six pairwise comparisons and around 78% correct for identification among all four individuals. The ability to identify marine mammals from their vocalizations alone, in addition to the theoretical interest for production mechanisms, is extremely valuable in the ability to track these mammals from remote locations where visual information is not present.</p>", "people": ["brown@media.mit.edu", "bv@media.mit.edu"], "title": "Speaker Identification of Marine Mammals", "modified": "2016-12-05T00:17:04.849Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-310", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "speaker-identification-of-marine-mammals"}, {"website": "", "description": "<p>The news is probably one of the first things people check in the morning, but how much does what you know and understand about the world depend on your news source? Will you view the world differently if you head over to CNN instead of BBC?</p><p>\u201cPerspectives\u201d presents top news stories from many points of view. The viewer can easily see the different perspectives and get the whole story.</p>", "people": ["lip@media.mit.edu", "jasrub@media.mit.edu"], "title": "Perspectives", "modified": "2018-10-17T23:15:04.211Z", "visibility": "PUBLIC", "start_on": "2016-08-01", "location": "", "groups": ["ultimate-media", "viral-communications"], "published": true, "active": false, "end_on": "2018-06-06", "slug": "perspectives"}, {"website": "", "description": "<p>Teaching multiple AI agents to coordinate their behavior represents a challenging task, that can be difficult to achieve without training all agents with a centralised controller, or allowing agents to view each others' reward functions. We present a new approach to multi-agent reinforcement learning (MARL), in which agents are given an incentive for being able to causally influence each others' actions. Causal influence is assessed using counterfactual reasoning. We show that this social influence reward gives rise to more coordinated behavior, better collective outcomes, and even emergent communication. In fact, the influence reward can be learn to train agents to use an explicit communication protocol in a meaningful way, when they cannot learn to do this under normal circumstances. Finally, we show that this reward can be computed by training each agent to model the actions of other agents. An agent can then \"imagine\" counterfactual actions it could have taken, and predict how this would have affected other agents behavior, thus computing its own influence reward. This mechanism allows each agent to be trained independently, representing a significant improvement over prior MARL work.&nbsp;</p>", "people": ["jaquesn@media.mit.edu"], "title": "Causal Influence Intrinsic Social Motivation for Multi-Agent Reinforcement Learning", "modified": "2019-04-18T15:25:07.087Z", "visibility": "PUBLIC", "start_on": "2018-05-14", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2018-10-31", "slug": "causal-influence-intrinsic-social-motivation-for-multi-agent-reinforcement-learning"}, {"website": "", "description": "<p>We are exploring new modalities of creative photography through robotics and long-exposure photography. Using a robotic arm, a light source is carried through precise movements in front of a camera. Photographic compositions are recorded as images of volumetric light. Robotic light 'painting' can also be inverted: the camera is moved via the arm to create an image 'painted' with environmental light. Finally, adding real-time sensor input to the moving arm and programming it to explore the physical space around objects can reveal immaterial fields like radio waves, magnetic fields, and heat flows.</p>", "people": ["stevenk@media.mit.edu", "neri@media.mit.edu"], "title": "Robotic Light Expressions", "modified": "2016-12-05T00:17:05.179Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "robotic-light-expressions"}, {"website": "", "description": "<p>The Beatbugs are handheld rhythm instruments that allow creation, manipulation, and sharing of rhythmic motives through a simple interface. When multiple Beatbugs are connected in a network, players can share and develop each others' motives to form larger-scale compositions. In \"Nerve\" (a composition written for the Beatbugs in the framework of the Toy Symphony), players can choose between manipulating their peers' motives and entering their own, creating an evolving musical system driven by the interaction between the players' choices and the system's stochastic algorithm.</p>", "people": ["tod@media.mit.edu"], "title": "Beatbugs", "modified": "2016-12-13T19:41:59.045Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "beatbugs"}, {"website": "", "description": "<p>Plazza brings group formation and dynamic content distribution capabilities to broadband wireless devices, together with the illusion of infinite bandwidth and  storage. Formerly restricted to PDA functionality, these devices dynamically discover content available in the wireless local network and other user's access patterns to it. Users can discover like-minded individuals or leverage implicit collaborative filtering to select the content they may be interested in, share the content with other stationary or mobile nodes, and view it in their devices or arrange for it to be transferred to their home node, where they can enjoy without battery or screen size constraints. </p>", "people": ["lip@media.mit.edu"], "title": "Plazza: Group Formation and Dynamic Content Distribution for Wireless and Mobile Systems", "modified": "2016-12-13T19:42:10.892Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "plazza-group-formation-and-dynamic-content-distribution-for-wireless-and-mobile-systems"}, {"website": "", "description": "<p>I am currently creating perfumes that capture the smell of individuals who have emotional significance to me.  An exploration in the use of science for emotional ends, I have successfully bottled the scent profiles of three people.  In the obsessively hygienic and reason-driven laboratory where I distill these smells, I often reflect on the constant negotiation between the animal and the cultured human within ourselves.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">This project is currently conducted in tandem with research in neuroscience, correlating olfactory stimuli with behavioral responses. &nbsp;</span></p>", "people": ["wonder@media.mit.edu"], "title": "Human Perfume", "modified": "2017-10-11T20:28:56.930Z", "visibility": "PUBLIC", "start_on": "2016-09-01", "location": "", "groups": ["design-fiction"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "human-perfume"}, {"website": "http://oi7.me", "description": "<p>OpenScope is an open source project that combines three components for anyone to explore the micro world anytime, anywhere. The 3D-printable open hardware turns your smartphone into a 200x microscope, the image processing application helps you recognize specific objects, and the online community allows you to share and contribute your findings from the microscope. OpenScope is expanding microscopy technologies beyond research laboratories and transforming the way we interact with the micro world.</p>", "people": ["emreiser@media.mit.edu", "ethanz@media.mit.edu", "oi7@media.mit.edu", "nsavidis@media.mit.edu"], "title": "OpenScope", "modified": "2018-05-04T10:52:12.271Z", "visibility": "PUBLIC", "start_on": "2016-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2017-01-01", "slug": "openscope"}, {"website": "", "description": "<p>Children can be better at learning than adults. Socio-psychological factors often inhibit adults in learning environments; they are afraid to make mistakes or look silly around their peers and as a result don\u2019t engage with the material as a child would. In language learning, this becomes even more prominent, as children are less afraid of making weird sounds or mispronouncing new words, whereas older students hesitate to speak out loud.&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">Not only are children less constrained by socio-psychological pressures, but they tend to be naturally more curious and open to new ideas when learning. They approach the task of learning, in many ways, differently than an adult would.&nbsp;</span></p><p><span style=\"font-size: 18px; font-weight: 400;\">Inner Child intends to tap into the potential benefits of changing the way we perceive ourselves by altering our body and environment in virtual reality.&nbsp;Inner Child is a virtual reality experience developed on the HTC Vive that embodies users in the virtual avatar of a child. The platform leverages the tracking capabilities of room-scale virtual reality to create the illusion of feeling younger, so that learners can approach learning with a more childlike mindset.&nbsp;</span></p><p>Results from our user study suggest that simply having a virtual body impacts the acquisition of vocabulary in virtual reality in a positive way. Furthermore, the illusion of being younger in VR improved participants\u2019 performance in subsequent recall tests both immediately and one week after exposure to the system, which was dependent on the type of body a participant inhabited in the virtual scenario. These results can radically change the way we learn in the classrooms of the future, suggesting that we can alter our notion of self to enhance the way we approach learning on a subconscious level.&nbsp;</p>", "people": ["cdvm@media.mit.edu", "pattie@media.mit.edu", "taikawa@media.mit.edu"], "title": "Inner Child", "modified": "2018-08-20T16:27:41.575Z", "visibility": "LAB-INSIDERS", "start_on": "2018-01-01", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-05-31", "slug": "inner-child"}, {"website": "", "description": "<p>Breathing actions are used to augment controller-based input by giving superpowers to players in two VR games. Blowing out long and strong turns you into a fire-breathing dragon, while holding your breath sends you into stealth mode.&nbsp; By using&nbsp; breathing as a directly controlled physiological signal, BreathVR can facilitate unique and engaging play experiences through natural interaction in single and multiplayer virtual reality games. Paper available here:&nbsp;<a href=\"http://web.media.mit.edu/~sra/breathvr.html\">http://web.media.mit.edu/~sra/breathvr.html</a></p>", "people": ["xuhaixu@media.mit.edu", "sra@media.mit.edu", "pattie@media.mit.edu"], "title": "BreathVR", "modified": "2018-08-20T16:27:18.375Z", "visibility": "PUBLIC", "start_on": "2017-08-16", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-07-31", "slug": "breathvr"}, {"website": "", "description": "<p>GVS or galvanic vestibular stimulation is a technology that directly affects a user's vestibular system by altering their sense of balance and direction. It works through electrical stimulation via electrodes placed on the mastoid bones behind each ear. In standing users, GVS evokes a prolonged \"galvanic body sway.\" In walking users, it affects balance and causes users to stagger in the anodal direction. However, in walking users, with their head pitched forward, it causes them to turn smoothly from their planned trajectory in the anodal direction. Dark Room is a cooperative asymmetrical \"escape the room\" style game played by a VR and a PC user, inspired by the single-player mobile game Dark Echo. The PC user controls the walking direction of the VR user to guide them around virtual or physical obstacles. The VR player uses echolocation to detect obstacles. Video and paper available here:&nbsp;<a href=\"http://web.media.mit.edu/~sra/gvs.html\">http://web.media.mit.edu/~sra/gvs.html</a></p>", "people": ["xuhaixu@media.mit.edu", "sra@media.mit.edu", "pattie@media.mit.edu"], "title": "A VR Collaboration Interface Using Galvanic Vestibular Stimulation", "modified": "2019-04-19T17:08:16.864Z", "visibility": "PUBLIC", "start_on": "2017-08-15", "location": "", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2018-07-31", "slug": "galvr-a-novel-collaboration-interface-using-gvs"}, {"website": "", "description": "<p>Hydrogels are crosslink polymers that are capable of absorbing great amounts of water. They have been studied during the last 50 years, largely due to their hydrophilic character at ambient temperatures, which make them biocompatible and attractive for various biological applications. Nevertheless, in our project, we are interested in their hydrophilic-hydrophobic phase-transition, occurring slightly above room temperature. We investigate the mechanical and optical transformations at this phase transition\u2013namely, their swelling, permeability, and optical transmission modification\u2013as enabling responsive or passive dynamics for future product design.</p>", "people": ["neri@media.mit.edu"], "title": "Responsive Glass", "modified": "2016-12-14T01:46:36.641Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "responsive-glass"}, {"website": "http://robotic.media.mit.edu/portfolio/robot-vocal-expressivity", "description": "<p>Prior research with preschool children has established that book reading, especially when children are encouraged to actively process the story materials through dialogic reading, is an effective method for expanding young children\u2019s vocabulary.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">A growing body of research also suggests that social robots have potential as learning companions and tutors for young children\u2019s early language education. Social robots are new technologies that combine the adaptability, customizability, and scalability of technology with the embodied, situated world in which we operate.</span></p><p>In this project, we asked whether a social robot can effectively engage preschoolers in dialogic reading. Given that past work has shown that children can and do learn new words from social robots, we investigate what factors modulate their learning. In particular, we looked at whether the verbal expressiveness of the robot impacted children\u2019s learning and engagement during a dialogic reading activity.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">This project was funded by an NSF Cyberlearning grant.</span></p>", "people": ["cynthiab@media.mit.edu", "jakory@media.mit.edu", "sooyeon6@media.mit.edu", "haewon@media.mit.edu"], "title": "Robot Expressiveness Affects Children's Learning", "modified": "2019-03-02T01:42:38.931Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "", "groups": ["ml-learning", "personal-robots"], "published": true, "active": false, "end_on": "2017-09-01", "slug": "robot-vocal-expressiveness"}, {"website": "", "description": "<p>Granular materials can be put into a jammed state through the application of pressure to achieve a pseudo-solid material with controllable rigidity and geometry. While jamming principles have been long known, large-scale applications of jammed structures have not been significantly explored. The possibilities for shape-changing machines and structures are vast and jamming provides a plausible mechanism to achieve this effect. In this work, jamming prototypes are constructed to gain a better understanding of this effect. As well, potential specific applications are highlighted and demoed. Such applications range from a morphable chair, to a floor which dynamically changes its softness in response to a user falling down to reduce injury, to artistic free-form sculpting.</p>", "people": ["stevenk@media.mit.edu", "neri@media.mit.edu"], "title": "Morphable Structures", "modified": "2016-12-14T01:46:49.791Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "morphable-structures"}, {"website": "", "description": "<p>The reproductive organs of the female body have long been a site of contention, where opposing ideologies in religion, politics, and cultural differences often play out. Of all the questions, that of reproductive rights strikes a particularly sensitive nerve.</p><p>US President Donald Trump recently signed an executive order which cut off all US funding to international NGOs whose work includes abortion services or advocacy. Images of this executive order being signed by Donald Trump flanked by a cabinet of men have circulated widely, begging the question: why do these men feel they have a right to determine women's reproductive choices?</p>", "people": ["wonder@media.mit.edu"], "title": "Brain-Controlled Interface for  the Motile Control of Spermatozoa", "modified": "2017-10-11T20:28:30.946Z", "visibility": "PUBLIC", "start_on": "2017-01-09", "location": "", "groups": ["design-fiction"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "woman-of-STEAM-grabs-back"}, {"website": "http://robotic.media.mit.edu/portfolio/storytelling-companion", "description": "<p>Children\u2019s oral language skills in preschool can predict their academic success later in life. Helping children improve their language and vocabulary skills early on could help them succeed later, in middle and high school. Learning language is also a very social, interactive activity. Learning language also takes time.&nbsp;Social robots could have great impact in this area, since they can leverage the same kinds of social cues and presence that people use.&nbsp;</p><p>In this work, we asked whether a sociable robotic learning/teaching companion could supplement children\u2019s early long-term language education. Children played with the robot for two months.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">The robot was designed as a social character, engaging children as a peer, not as a teacher, within a relational, dialogic context. The robot targeted the social, interactive nature of language learning through a storytelling game that the robot and child played together. The game was on a tablet: the tablet showed a couple of characters that the robot or child could move around while telling their story. During the game, the robot introduced new vocabulary words and modeled good story narration skills.</span></p><p>Furthermore, because children may learn better when appropriately challenged, we asked whether a robot that matched the \u201clevel\u201d or complexity of the language it used to the general language ability of the child might help children improve more. The robot told easier or harder stories based on an assessment of the child\u2019s general ability.&nbsp;<span style=\"font-size: 18px; font-weight: normal;\">This work is supported by the NSF Expeditions in Computing award in Socially Assistive Robots.</span></p>", "people": ["cynthiab@media.mit.edu", "jakory@media.mit.edu"], "title": "Storytelling Companion", "modified": "2017-06-07T16:37:09.873Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2014-09-01", "slug": "storytelling-companion"}, {"website": "http://robotic.media.mit.edu/portfolio/children-use-nonverbal-cues-learn-robots/", "description": "<p>When learning from human partners, infants and young children will pay attention to nonverbal signals, such as gaze and bodily orientation, to figure out what a person is looking at and why. They may follow gaze to determine what object or event triggered another's emotion, or to learn about the goal of another's ongoing action. They also follow gaze in language learning, using the speaker's gaze to figure out what new objects are being referred to or named.</p><p>In this project, we examine whether young children will attend to the same social cues from a robot as from a human partner during a word learning task, specifically gaze and bodily orientation.</p>", "people": ["cynthiab@media.mit.edu", "jakory@media.mit.edu", "sooyeon6@media.mit.edu"], "title": "Children Use Nonverbal Cues to Learn from Robots", "modified": "2017-06-07T16:38:44.917Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2015-12-31", "slug": "children"}, {"website": "http://media.mit.edu/~jacobsj", "description": "<p>Codeable Objects is a library for processing that allows people to design and build objects using geometry and programming. Geometric computation offers a host of powerful design techniques, but its use is limited to individuals with a significant amount of programming experience or access to complex design software. In contrast, Codeable Objects allows a range of people, including novice coders, designers, and artists to rapidly design, customize, and construct an artifact using geometric computation and digital fabrication. The programming methods provided by the library allow the user to program a wide range of structures and designs with simple code and geometry. When users compile their code, the software outputs tool paths based on their specifications, which can be used in conjunction with digital fabrication tools to build their objects. </p>", "people": ["leah@media.mit.edu", "jacobsj@media.mit.edu", "mres@media.mit.edu"], "title": "Codeable Objects", "modified": "2017-04-04T15:19:21.631Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "codeable-objects"}, {"website": "http://media.mit.edu/~jacobsj", "description": "<p>DressCode is a computer-aided design and fabrication tool that combines programming with graphic drawing and manipulation, allowing novice programmers to create computationally-generated, physical artifacts. The software consists of a programming environment and a graphic-user interface design tool, as well as a custom programming language. The GUI tools allow for a unique combination of graphic drawing and computational manipulation, because the software automatically generates editable code in the programming environment that reflects the designer's drawing actions. DressCode exports designs that are compatible with digital fabrication machines, allowing for the creation of physical artifacts. We have introduced DressCode to amateur programmers with a series of craft activities that allow them to produce functional, beautiful, and unique objects including t-shirts, jewelry, and personal accessories.</p>", "people": ["leah@media.mit.edu", "jacobsj@media.mit.edu", "mres@media.mit.edu"], "title": "DressCode", "modified": "2017-04-04T14:49:37.761Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "dresscode"}, {"website": "http://media.mit.edu/~jacobsj", "description": "<p>Fashion Design is a domain that offers exciting possibilities when combined with programming and digital fabrication. Soft Objects is an extension of the Codeable Objects software that allows novice programmers to create and build personal clothing and garments through computational design. The programming methods provided by the software allow the user to program a wide range of structures and designs with simple code and geometry. The software includes a graphical user interface that allows users to preview their designs once they have compiled their code.  Users can also import existing patterns and forms to incorporate pre-drawn elements into their shapes and patterns.  Soft Objects supports a variety of digital fabrication output devices by including ink-jet printers, vinyl cutters, laser cutters, and computationally controlled embroidery machines. </p>", "people": ["leah@media.mit.edu", "jacobsj@media.mit.edu"], "title": "Computational Clothing Design", "modified": "2017-04-04T15:04:14.224Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2013-09-01", "slug": "computational-clothing-design"}, {"website": "http://media.mit.edu/~jacobs", "description": "<p>Most digital fabrication workflows impose a strong separation between design and fabrication. Designs are first modeled in computer aided design software, and when completed, converted to tool-paths which are uploaded to control software and autonomously executed by the fabrication machine. While there are advantages to this highly structured workflow, it restricts the ability for improvisation and revision. In addition, it eliminates the opportunity for embodied forms of expression, and direct engagement with the material during the fabrication process.</p><p>I explored ways of supporting exploratory, intuitive, and immediate design practices in digital fabrication by creating a system for interactive control of a three-axis Computer Numerical Control (CNC) machine. I circumvented the traditional control interface of a large-format ShopBot machine to enable direct control by a human operator. I developed a tablet-based interface where people could draw designs with a pressure sensitive stylus. Each stroke a person drew was executed by the machine as it was completed. I also developed a custom drawing tool that fit into the ShopBot spindle and enabled the designer to switch between two different colors of acrylic paint as they drew. The tool mechanism was wireless, and was driven by two servomotors controlled through a bluetooth-enabled microcontroller.</p><p>The complete system was made available to the general public during a four-day installation which enabled people to interact with the machine. In the process, I observed how the drawing-based interface lowered barriers to entry for digital fabrication and enabled people to execute organic and gestural forms and patterns with the machine. This work is part of ongoing research to explore ways of modifying existing digital fabrication machines to support embodied and intuitive forms of design and making.</p>", "people": [], "title": "Sketch-based CNC", "modified": "2018-01-03T21:38:05.112Z", "visibility": "PUBLIC", "start_on": "2016-08-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "sketch-based-cnc"}, {"website": "", "description": "<p>It is well established that countries, regions and institutions tend to develop towards related activities. This implies that, for instance, countries are more likely to enter a new activity that is closer/related with the activities it has already developed. An empirical fact that results from the overlapping of the necessary knowledge of each activity. In this context, the product space\u2014a network relating countries economic activities\u2014has been instrumental in capturing the role of relatedness in the economic development of countries. But, although relatedness seems to be a major driver for the diversification of countries exports and research activities, there are many instances when countries deviate from this norm, but to what extent do they benefit from such actions? Is it possible to pinpoint a particular stage of development of a country in which these exceptions are more likely to occur or are they purely at random?</p><p>Using 50 years of trade date we have analyzed how countries diversify their products portfolios in the context of the Economic Complexity and Product Space10. We have shown that 1) there is an intermediate and non-trivial stage of economic development at which countries are more likely to develop towards unrelated activities; that 2) countries that do so achieve a faster economic growth; and 3) that low and high developed economies are the ones that are more likely to diversify towards related varieties.</p><p>These results have significant implications in the literature of regional development. For instance, recently the European Union presented a regional plan of development, coined as Smart Specialization, which advocates for a one rule that fits all: regions should develop the most related and highest reward activities. Our results suggest more caution. Indeed, our findings point out that the development stage of a country, or a region, plays a determinant role in devising a development strategy. For instance, while low and highly developed regions should look forward to developing related activities, regions at an intermediate level of development should be incentivized to pursue the development of unrelated activities and diversify. These results build up to the conclusions of the previous project (2.1), in the sense that economies should adopt dynamical diversification strategies in which the big challenge is to identify the narrow window for unrelated diversification.</p>", "people": ["flaviopp@media.mit.edu", "aamena@media.mit.edu", "hidalgo@media.mit.edu", "hartmado@media.mit.edu"], "title": "Do countries benefit from jumping into unrelated varieties?", "modified": "2017-10-11T00:23:45.415Z", "visibility": "PUBLIC", "start_on": "2016-06-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2017-12-31", "slug": "a-new-project"}, {"website": "", "description": "<p>Planning a city is a complex task requiring collaboration between multiple stakeholders with different, and often conflicting, goals and objectives. Researchers have studied the role of technology in group collaboration for many years. It has been noted that when the task between collaborators increases in complexity, such as in a decision-making process, the use of computer technology could either enhance, or disturb, the collaboration process. City Game evaluates the impact of computer interfaces on a multi-objective negotiation problem. Using a tangible user interface (TUI) is more effective for multi-objective group decision-making than a graphical or multitouch user interface; this project will focus on designing and developing a TUI and a serious game for an urban planning scenario. We will test the game on different computer interfaces to evaluate the decision-making process between different collaborators with conflicting objectives.</p>", "people": ["kll@media.mit.edu", "mkh@media.mit.edu"], "title": "City Game", "modified": "2017-10-11T13:06:14.048Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "city-game"}, {"website": "", "description": "<p>Connected Coral integrates physical and digital elements in a visualization of the environmental impacts on reefs. This complex projection mapping uses multiple projectors, angled mirrors, and a motion sensor to create an interactive digital skin on a complex three-dimensional surface.<br></p><p>To integrate the projected content with the physical design, the students fabricated the physical coral model based on photogrammetry scans of real coral, warped and blended the projected areas, and factored in hardware specifications. These modifications minimize visual distortion on the uneven surface and allow for an uninhibited interactive experience.</p><p>This project was created through the Open Ocean Initiative and will be on display at the MIT Museum through Spring 2019.</p>", "people": ["katybell@media.mit.edu", "emilysa@media.mit.edu", "nlutz@media.mit.edu"], "title": "Connected Coral", "modified": "2018-11-08T19:07:36.290Z", "visibility": "PUBLIC", "start_on": "2018-03-29", "location": "", "groups": ["open-ocean", "object-based-media"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "connected-coral"}, {"website": "http://web.media.mit.edu/", "description": "<p>Computation is a powerful artistic medium. The introduction of computers as a tool for making art has established new forms of art which are dynamic: able to actively change in response to an artist\u2019s actions. Tools for dynamic art, like programming languages, offer artists new creative capabilities, but can often be difficult to learn and use in expressive ways.  In my dissertation work I'm developing two systems for supporting Active Dynamic Drawing:  Para and Dynamic Brushes. </p><p>Para is a direct-manipulation parametric tool that supports accessible but expressive procedural graphic art through a direct-manipulation interface. Dynamic Brushes is a system for enabling artists to create their own dynamic drawing tools. Dynamic Brushes builds on lessons gained through evaluating Para to support the combination of drawing by hand with procedural manipulation and augmentation. </p><p>The development of Para and Dynamic Brushes is informed through in-depth interviews with professional artists, and evaluated through a series of open-ended studies where professional artists create their own artwork with the tools.  These studies demonstrate  how dynamic mediums can extend manual art practice by supporting exploration, enabling gradual learning, and allowing manual artists to leverage existing skills.</p>", "people": ["jacobsj@media.mit.edu"], "title": "Active Drawing", "modified": "2017-10-10T22:34:46.711Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2017-08-31", "slug": "active-drawing"}, {"website": "", "description": "<p>After meeting someone for the first time, we come away with an intuitive sense of how much we can trust that person. Nonverbal behaviors, such as gaze patterns, body language, and facial expressions, have been explored as honest or \ufffdleaky\ufffd signals that are salient cues in gaining trust insights. We are working to create a computational model for recognizing interpersonal trust in social interactions. By observing the trust-related nonverbal cues expressed in the social interaction, we aim to design a machine-learning algorithm that is capable of differentiating whether a person finds his or her socially assistive robot to be a trustworthy or untrustworthy partner. We aim to enable robots to understand our nonverbal signals. With so much of our communication being passed in these nonverbal streams, we hope that by enabling robots to understand these signals, we can design robots that can communicate with us more effectively.</p>", "people": ["cynthiab@media.mit.edu", "jinjoo@media.mit.edu"], "title": "Computationally Modeling Interpersonal Trust Using Nonverbal Behavior for Human-Robot Interactions", "modified": "2017-10-15T14:37:39.275Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "computationally-modeling-interpersonal-trust-using-nonverbal-behavior-for-human-robot-interactions"}, {"website": "", "description": "", "people": [], "title": "ASAP-A Spatial Auditory Platform for prototyping User experiences", "modified": "2018-05-04T15:35:14.440Z", "visibility": "LAB-INSIDERS", "start_on": "2018-03-01", "location": "", "groups": ["tangible-media"], "published": false, "active": false, "end_on": "2018-08-01", "slug": "asap-a-spatial-auditory-platform-for-prototyping-user-experiaces"}, {"website": "", "description": "<p><b>ASAP&nbsp;&nbsp;</b>is a natural extension of the <a href=\"https://www.media.mit.edu/projects/leakyphones/overview/\">LeakyPhones project</a>.&nbsp;&nbsp;It is a rapid and <b>functional</b> \u201ccardboard prototyping\u201d platform for auditory augmented reality (AAR), location-based sound (LBS), and object&nbsp; sonification.&nbsp;The ASAP system is simple and versatile. It could be used by designers, doctors, educators and others with minimal prototyping and computer skills as a simple tool to rapidly <b>ideate</b>, <b>prototype,</b> and <b>test</b> ideas with real users.</p>", "people": ["amosg@media.mit.edu"], "title": "ASAP: A Spatial Auditory Platform for rapid prototyping of sonic user experiences", "modified": "2018-05-07T13:44:21.752Z", "visibility": "LAB-INSIDERS", "start_on": "2018-03-01", "location": "", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2018-08-01", "slug": "ASAP"}, {"website": "", "description": "<p>Mapping the urban perception<br></p>", "people": ["hidalgo@media.mit.edu"], "title": "Place Pulse", "modified": "2018-05-07T18:02:16.452Z", "visibility": "PUBLIC", "start_on": "2013-10-01", "location": "", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2014-08-01", "slug": "place-pulse-1"}, {"website": "", "description": "<p>Designed as a platform to enable rich contextual data collection in real homes, BoxLab uses a broad array of wireless sensing devices to study responsive applications situated in natural home settings. BoxLab has been deployed in homes around the Boston area, and has generated a dataset containing over 10,000 hours of sensor data to be used as training libraries for computational activity recognition and other applications of artificial intelligence. BoxLab also enables rapid deployment of context-triggered applications that allow systems to respond to occupant activities in real time.</p>", "people": ["kll@media.mit.edu"], "title": "BoxLab", "modified": "2018-05-04T18:19:26.740Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "1CC-4th Floor", "groups": ["city-science"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "boxlab"}, {"website": "", "description": "<p>RePlace is a 3D data visualization platform that takes the normally invisible activity of sensors and data loggers and overlays that information as 3D sprites in the environment. RePlace uses an augmented reality viewfinder as a window into the data environment, and enables interaction with real-time and historic data feeds through the viewfinder. </p>", "people": ["kll@media.mit.edu", "csmuts@media.mit.edu"], "title": "RePlace", "modified": "2018-05-04T18:23:12.980Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["city-science", "changing-places"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "OLD_replace"}, {"website": "", "description": "<p>Users have high-level goals when they browse the Web or perform searches.  However, the two primary user interfaces positioned between users and the Web\ufffdWeb browsers and search engines\ufffdhave very little interest in users\ufffd goals.  Present-day Web browsers provide only a thin interface between users and the Web, and present-day search engines rely solely on keyword matching. This project leverages large knowledge bases of semantic information to provide users with a goal-oriented Web browsing experience. By understanding the meaning of Web pages and search queries, this project demonstrates how Web browsers and search engines can proactively suggest content and services to users that are both contextually relevant and personalized. This project presents (1) Creo, a Programming by Example system that allows users to teach their computers how to automate interactions with their favorite Web sites by providing a single demonstration; (2) Miro, a data detector that matches the content of a Web page to high-level user goals, and allows users to perform semantic searches; and (3) Adeo, an application that streamlines browsing the Web on mobile devices, allowing users to complete actions with a minimal amount of input and output.</p>", "people": ["lieber@media.mit.edu"], "title": "A Goal-Oriented User Interface for Personalized Semantic Search", "modified": "2016-12-05T00:17:05.587Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "a-goal-oriented-user-interface-for-personalized-semantic-search"}, {"website": "", "description": "<p>Many of our online interactions take place in community spaces. We keep track of friends and share pictures on Facebook, chatter with friends on Twitter, and participate in discussions on online forums. But how do the design choices we make impact the kinds of social spaces that develop? To better understand this relationship, we conducted a study of a discussion forum with a particularly unusual design: 4chan.org. Perhaps best known for its role in driving Internet culture and its involvement with the \"Anonymous\" group, we believe 4chan's design plays a large role in its success, despite its counter-intuitiveness. In our first paper exploring this area, we quantify 4chan's ephemerality (there are no archives; most posts are deleted in a matter of minutes) and anonymity (there are no traditional user accounts, and most posts are fully anonymous) and discuss how the community adapts to these unusual design strategies.</p>", "people": ["geek@media.mit.edu"], "title": "4chan and /b/: Anonymity and Ephemerality", "modified": "2016-12-05T00:17:05.443Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["lifelong-kindergarten", "living-mobile"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "4chan-and-b-anonymity-and-ephemerality"}, {"website": "", "description": "<p>The Kit-of-No-Parts is an approach to crafting electronics rather than designing discrete components. The collection of recipes and ingredients on the Kit-of-No-Parts website describes how to build electronics from a wide variety of conductive and non-conductive materials using a range of traditional and contemporary craft techniques.</p>", "people": ["leah@media.mit.edu"], "title": "A Kit-of-No-Parts", "modified": "2016-12-05T00:17:05.472Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "a-kit-of-no-parts"}, {"website": "", "description": "<p>We have developed tightly coupled sensor network that tries to mimic some of the sensory characteristics of a natural skin. Developed in a flexible substrate (Kapton Polyimide), this array forms a flexible surface of small, networked sensor elements. Each node is capable of sensing four channels of pressure, two axes of strain, sound, temperature, ambient light, and two channels of whisker mechanoreception. By seamlessly blending sensing, computation, and local and global networking, our \"skin\" is ultimately self-configuring and scalable\ufffdit can be built as large as desired and signals can be extracted from the entire array at any node.</p>", "people": ["joep@media.mit.edu"], "title": "A Scalable Networked Smart Sensate Skin", "modified": "2016-12-05T00:17:05.675Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-353", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "a-scalable-networked-smart-sensate-skin"}, {"website": "", "description": "<p>AAGO is a mobile app for Apple devices focusing on media creation, organization and sharing. Targeted to teens, the app aims to help them document group or individual creative projects (filmmaking, tech or art projects, citizen journalism) by creating \"stories\" made up of mobile photos, videos, and audio clips, which can then be arranged and exported to the web.</p>", "people": ["ethanz@media.mit.edu", "nitin@media.mit.edu"], "title": "AAGO: Mobile Media Diaries for Youth Citizens Journalists", "modified": "2016-12-05T00:17:05.703Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "aago-mobile-media-diaries-for-youth-citizens-journalists"}, {"website": "", "description": "<p>This work seeks to apply acoustical analysis to a hierarchical model of discourse structure so as to allow for the division of an audio recording into layers of detail. This \"outline\" structure is then used in interactive presentations, which allow a listener to browse at various levels of detail.</p>", "people": ["geek@media.mit.edu"], "title": "Acoustical Cues to Discourse Structure", "modified": "2016-12-05T00:17:05.735Z", "visibility": "PUBLIC", "start_on": "1992-12-31", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "acoustical-cues-to-discourse-structure"}, {"website": "", "description": "<p>Smart agents represent users by creating models of particular users, and the agents' recommendations are based on these models. But models are rarely complete; this problem is particularly acute when there are changes either in the tastes and preferences of the individual, or in the marketplace (e.g., new products). Our agent attempts to solve this by actively trying to learn a user's preferences. The active agent balances two goals: to be immediately useful and to make high-quality recommendations (\"selling\"); and to learn more about the user. These recommendations have a lower quality, but also a higher \"up-side\" potential (\"learning\").</p>", "people": [], "title": "Active-Learning Smart Agents", "modified": "2016-12-05T00:17:05.775Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-448", "groups": ["erationality"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "active-learning-smart-agents"}, {"website": "", "description": "<p>The activeShelves are a networked system of surfaces, servers, and displays that give real-time data about a collection of physical objects. Shelf surfaces are embedded with radiofrequency (RF) readers to track tagged containers. The system is connected to a server, and can be used as a relational database for physical objects. The user gets information directly about usage, location, and status of objects on activeShelves, or can query remotely via the web. The activeShelves can display simple text and graphics on LED displays. Based on our work with TouchCounters, activeShelves continues the exploration of enhancing typical organizational systems. The new system senses using RFID tags instead of mechanical magnetic contacts. The system's data-entry components and o utput display LEDs are located on the shelves instead of just the physical containers. The activeShelves system acts as a relational database instead of just tracking usages of individual containers.</p>", "people": ["ishii@media.mit.edu"], "title": "activeShelves: Interactive Storage Shelf System", "modified": "2016-12-05T00:17:05.806Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2001-09-01", "slug": "activeshelves-interactive-storage-shelf-system"}, {"website": "", "description": "<p>In this project, we propose a new Bayesian receiver for signal detection in flat-fading channels. First, the detection problem is formulated as an inference problem in a hybrid dynamic system that has both continuous and discrete variables. Then, an expectation propagation algorithm is proposed to address the inference problem. As an extension of belief propagation, expectation propagation efficiently approximates a Bayesian estimation by iteratively propagating information between different nodes in the dynamic system and projecting exact messages into the exponential family. Compared to sequential Monte Carlo filters and smoothers, the new method has much lower computational complexity since it makes analytically deterministic approximations instead of Monte Carlo approximations. Our simulations demonstrate that the new receiver achieves accurate detection without the aid of any training symbols or decision feedbacks. Future work involves joint decoding and channel estimation, where convolutional codes are used to protect signals from noise corruption. Initial results are promising.</p>", "people": ["picard@media.mit.edu"], "title": "Adaptive, Wireless, Signal Detection and Decoding", "modified": "2016-12-05T00:17:05.831Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "adaptive-wireless-signal-detection-and-decoding"}, {"website": "", "description": "<p>\"All Cows Eat Grass\" is a mnemonic used in music instruction for A C E G\ufffdthe notes on the white spaces of the bass clef. The All Cows Eat Grass project is an online platform that provides cost-effective, real-time private music lessons. The system connects music instructors and students using a low-latency audio and video link, provides motivational support to practice between lessons, and lowers the barrier to learn music with self-instruction material and self-evaluation musical games. </p>", "people": ["bv@media.mit.edu"], "title": "All Cows Eat Grass", "modified": "2016-12-05T00:17:05.919Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "all-cows-eat-grass"}, {"website": "", "description": "<p>The Actuated Workbench is a device that uses magnetic forces to move objects on a table in two dimensions. It is intended for use with existing tabletop tangible interfaces, such as Sensetable, providing an additional feedback loop for computer output and helping to resolve inconsistencies that otherwise arise from the computer\ufffds inability to move objects on the table. Current developments include the physical synchronizing of two distributed tables for remote collaboration, and the use of actuation to program physical constraints into a simulation, allowing users to quickly set up a physical system that determines possible parameters for the underlying computational simulation.</p>", "people": ["ishii@media.mit.edu", "jpatten@media.mit.edu"], "title": "Actuated Workbench", "modified": "2016-12-05T00:17:05.950Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "actuated-workbench"}, {"website": "", "description": "<p>ASQ investigates ways to teach social-emotion skills to children interactively with toys. One of the  first goals is to help autistic children recognize expressions of emotion in social situations. The system  uses four \"dwarfs\" expressing sad, happy, surprise, and angry, and each  communicates wirelessly to  the system and detects which plush doll was selected by the child.  The computer plays short  entertaining video clips displaying examples of the four emotions and cues the child to pick a dwarf  that closely matches that emotion.  Future work includes improving the ability of the system to  recognize direct displays of emotion by the child. \n</p>", "people": ["picard@media.mit.edu"], "title": "Affective Social Quest", "modified": "2016-12-05T00:17:06.104Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["affective-computing", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "affective-social-quest"}, {"website": "", "description": "<p>Computer modeling and simulation are changing the nature of scientific investigation, by enabling researchers to pose new kinds of questions and explore phenomena in ways that were not possible just a short time ago. Just as the technological revolution continues to influence the practice of scientific research, it also presents opportunities to change the way that science is taught in the classroom. The Adventures in Modeling Project introduced students and teachers to the process of designing, creating, and analyzing their own models of complex, dynamic systems using StarLogo. Our aims were to educate and motivate teachers to transform the way that they teach science and to engage students in authentic science practice by giving them the tools and the ability to pose, investigate, and answer their own questions. Through model creation, and participation in a variety of off-computer activities, teachers and students learned the science of modeling to develop a deeper understanding of patterns and processes in the world.</p>", "people": ["mres@media.mit.edu"], "title": "Adventures in Modeling", "modified": "2016-12-05T00:17:05.997Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2000-09-01", "slug": "adventures-in-modeling"}, {"website": "", "description": "<p>The Affective Mirror is an attempt to build a fully automated system that intelligently responds to a person's affective state in real time. Current work is focused on building an agent that realistically mirrors a person's facial expression and posture. The agent detects affective cues through a facial-feature tracker and a posture-recognition system developed in the Affective Computing group; based on what affect a person is displaying, such as interest, boredom, frustration, or confusion, the system responds with matching facial affect and/or posture. This project is designed to be integrated into the Learning Companion Project, as part of an early phase of showing rapport-building behaviors between the computer agent and the human learner.</p>", "people": ["picard@media.mit.edu"], "title": "Affective Mirror", "modified": "2016-12-05T00:17:06.018Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "affective-mirror"}, {"website": "", "description": "<p>Companies would like more new products to be successful in the marketplace, but current evaluation methods such as focus groups do not accurately predict customer decisions. We are developing new technology-assisted methods to try to improve the customer-evaluation process and better predict customer decisions. The new methods involve multi-modal affective measures (such as facial expression and skin conductance) together with behavioral measures, anticipatory-motivational measures, and self-report cognitive measures. These measures are combined into a novel computational model, the form of which is motivated by findings in affective neuroscience and human behavior. The model is being trained and tested with customer product evaluations and marketplace outcomes from real product launches.</p>", "people": ["picard@media.mit.edu"], "title": "Affective-Cognitive Product Evaluation and Prediction of Customer Decisions", "modified": "2016-12-05T00:17:06.058Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "affective-cognitive-product-evaluation-and-prediction-of-customer-decisions"}, {"website": "", "description": "<p>The aim of this project is to build a database of natural speech showing a range of affective variability. It is an extension of our ongoing research focused on building models for automatic detection of affect in speech. At a very basic level, training such systems requires a large corpus of speech containing a range of emotional vocal variation. A traditional approach to this research has been to assemble databases where actors have provided the affective variation on demand. However, this method often results in unnatural sounding speech and/or exaggerated expressions. \n\nWe have developed a prototype of an interactive system that guides a user through a question and answer session. Without any rehearsals or scripts, the user navigates through touch and spoken language an interface guided by embodied conversational agents which prompt the user to speak about an emotional experience. Some of the issues we are addressing include the design of the text and character behavior (including speech and gesture) so as to obtain a convincing and disclosing interaction with the user.</p>", "people": ["picard@media.mit.edu"], "title": "Affect in Speech:  Assembling a Database", "modified": "2016-12-05T00:17:06.077Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-001", "groups": ["affective-computing", "cc"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "affect-in-speech-assembling-a-database"}, {"website": "", "description": "<p>Effective image annotation and retrieval is bound up with image use. In this project, annotation, retrieval, and use are integrated, facilitating the finding and using images. A proactive user-interface agent seeks chances for image annotation and retrieval in the context of the user's everyday work, using an agent that sit in the user's text editor or other application and continuously monitors typing. Searches are automatically performed from an image library, and images relevant to the current text can be inserted in a single operation. Descriptions of images for storytelling can be seamlessly employed as raw material for annotation. Common-sense knowledge about situations in which pictures are taken, described, or used can help provide semi-automatic annotation and indirect inference for retrieval. Our approach does not completely automate the annotation/retrieval process, but it does reduce user-interface overhead, leading to better-annotated image libraries and fewer missed opportunities for image use.</p>", "people": ["lieber@media.mit.edu"], "title": "Agents for Integrated Annotation and Retrieval of Images", "modified": "2016-12-05T00:17:06.177Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "agents-for-integrated-annotation-and-retrieval-of-images"}, {"website": "", "description": "<p>The first of this project's two goals is to enable those with internalized, psychologically distressing states to speak their own stories out loud to others without fear. The second goal involves understanding the \ufffdarchitecture\ufffd of current public spaces, both physical and online, and designing the structure and interaction of a physically located audiospace project that enables and encourages citizens to respond easily, and to be responsible towards each other in urban public places. This project revolves around the design and building of a public installation to let people call from any touch-tone phone into a public space. At the site of this public space will be a sculpture of sorts\ufffda physical tele-presence for the person calling. The person calling will actually speak out of the physical sculpture. A full duplex audio link will be opened up between the caller and the tele-presence sculpture via existing telephone infrastructures. People in the public space can talk with the remote caller by talking to the sculpture. This project brings the privacy and anonymity of online communication into a physical public space to facilitate emotional processing by individuals. Public disclosure of private information may open dialogues around topics otherwise private or dangerous. Individuals may take steps towards dealing with personal situations while simultaneously offering public citizens voices articulating broadly pertinent issues. People present at the site of the tele-presence sculpture will be called upon to be responsive towards others while simultaneously feeling less alone in facing issues they may also be dealing with personally. Public spaces will potentially become more vital, and private issues less alienating.</p>", "people": ["judith@media.mit.edu", "monster@media.mit.edu"], "title": "AgoraPhone", "modified": "2016-12-05T00:17:06.229Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "agoraphone"}, {"website": "", "description": "<p>We design and evaluate an ambient blood glucose level visualization and feedback system that uses an Ambient Orb for diabetes self-care and social support.  The social support is provided by a friend or family member of an individual with diabetes. This research study was carried out with adult patients at Joslin Diabetes Center.</p>", "people": ["amohan@media.mit.edu", "picard@media.mit.edu"], "title": "Ambient Displays for Social Support and Diabetes Management", "modified": "2016-12-05T00:17:06.396Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "--Choose Location", "groups": ["affective-computing", "gray-matters"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "ambient-displays-for-social-support-and-diabetes-management"}, {"website": "", "description": "<p>Furniture is the infrastructure for human activity. Every day we open cabinets and drawers, pull up to desks, recline in recliners, and fall into bed. How can technology augment these everyday rituals in elegant and useful ways? The Ambient Furniture project mixes apps with the IKEA catalog to make couches more relaxing, tables more conversational, desks more productive, lamps more enlightening, and beds more restful. With input from Vitra and Steelcase, we are prototyping a line of furniture to explore ideas about peripheral awareness (Google Latitude doorbell), incidental gestures (Amazon restocking trash can and the Pandora lounge chair), pre-attentive processing (energy clock), and eavesdropping interfaces (Facebook photo coffee table).</p>", "people": ["drose@media.mit.edu", "ishii@media.mit.edu"], "title": "Ambient Furniture", "modified": "2016-12-05T00:17:06.426Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "ambient-furniture"}, {"website": "", "description": "<p>Amino is a design-driven mini-lab that allows users to carry out a bacterial transformation and enables the subsequent care and feeding of the cells that are grown. Inspired by Tamagotchis, the genetic transformation of an organism's DNA is performed by the user through guided interactions, resulting in a synthetic organism that can be cared for like a pet. Amino is developed using low-cost ways of carrying out lab-like procedures in the home, and is packaged in a suitcase-sized continuous bioreactor for cells.</p>", "people": ["slavin@media.mit.edu"], "title": "Amino: A Tamagotchi for Synthetic Biology", "modified": "2016-12-05T00:17:06.454Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["playful-systems"], "published": true, "active": false, "end_on": "2015-07-01", "slug": "amino-a-tamagotchi-for-synthetic-biology"}, {"website": "", "description": "<p>Cell phones are great for communication in a virtual manner, but lack expressiveness in personal surroundings. Many people try to give their phones a personal touch by customizing them. Android Meets Arduino is a toolkit to connect Android-driven mobile devices with Arduino microcontrollers via Bluetooth. The toolkit provides easy access to internal phone events which can be further processed on the Arduino open-source prototyping platform. This toolkit seeks to empower people to externalize their phone events to creatively demonstrate them on wearables, living spaces, or other tangibles.</p>", "people": ["leah@media.mit.edu"], "title": "Android Meets Arduino", "modified": "2016-12-05T00:17:06.696Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "E15-368", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "android-meets-arduino"}, {"website": "", "description": "<p>Despite many studies of marine mammal vocalizations, there is no generally accepted theory of sound production by whales or dolphins, and only qualitative descriptions of their spectra. Of particular interest are the unusual sounds made by killer whales, which appear to arise from two independent sources and are called \"biphonic.\" We have analyzed a number of these sounds from Northern Resident Whales and find the resulting spectrum is due to convolution of the spectra of two sources. Since convolution in the frequency domain translates to multiplication in the time domain, it means that the sound of one source is amplitude modulating the sound from the second source.  This is a non-linear interaction rather than a simple linear superposition of two sources.  We are attempting to find a physical model which would explain this behavior and have a Matlab program that simulates the original sounds.</p>", "people": ["brown@media.mit.edu", "bv@media.mit.edu"], "title": "Analysis and Synthesis of Pulsed Vocalizations by Killer Whales", "modified": "2016-12-05T00:17:06.524Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "analysis-and-synthesis-of-pulsed-vocalizations-by-killer-whales"}, {"website": "", "description": "<p>Transients occur in human walking during a transition to, from, and between steady state walking and acts as an impulse destabilizing a gait cycle. Turns, rapid stops, and accelerated starts are all common transients encountered and managed intelligently by humans everyday. Humanoid bipeds are rapidly becoming a more common part of our everyday life. Therefore, they must also be able to navigate our environments adroitly if they are to assist us in our daily living. This project takes biomechanical principals of angular momentum and applies them to design of controllers for bipeds using angular momentum primitives. These primitives are basic units that simplify the control problem and reduce the dimensionality of the state-space and the objective task. The task in this project is to accomplish transient behaviors and steady state walking together. Through this we are able to realize a more efficient and effective control for humanoid robots.</p>", "people": ["hherr@media.mit.edu"], "title": "Angular Momentum Primitives: Modeling of Transients in Human Walking", "modified": "2016-12-05T00:17:06.585Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-054", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "angular-momentum-primitives-modeling-of-transients-in-human-walking"}, {"website": "", "description": "<p>The Physical Language Workshop designs tools for creating digital content in a networked environment, and the means by which the content can be leveraged as creative capital within an experimental, online micro-economy that we call OPENSTUDIO. Our primary impact targets are in the areas of general digital media service architectures, global e-commerce, distance education, and visual information display systems.</p>", "people": ["holtzman@media.mit.edu"], "title": "OPENSTUDIO", "modified": "2016-12-05T00:17:06.609Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "openstudio"}, {"website": "", "description": "<p>This project explores performing statistical signal processing with analog circuits. The first instantiation is a low-complexity circuit for phase acquisition and tracking of a pseudo-random sequence generated by an LFSR. Eventually we hope to scale the complexity of this kind of cicuitry to be able to perform any high-speed signal processing desired for a digital wireless front-end. In many application domains, this approach could be 10 to 100 times more efficient in terms of power consumption and square millimeters of silicon than the equivalent digital signal processor (DSP).</p>", "people": ["neilg@media.mit.edu"], "title": "Analog VLSI Graphical Models", "modified": "2016-12-05T00:17:06.637Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "analog-vlsi-graphical-models"}, {"website": "", "description": "<p>Economic theories of valuation generally assume that prices of commodities and assets are derived from underlying \"fundamental\" values. For example, consumer microeconomics assumes that demand curves for consumer products can ultimately be traced to the valuation of pleasures that consumers anticipate receiving from these products. Current work suggests that preferences are initially malleable, but become \"imprinted\" (precisely defined and largely invariant) after the individual makes an initial decision. Prior to imprinting, preferences are \"arbitrary,\" (highly responsive both to normative and non-normative influences). Following imprinting, preferences are \"coherent\" (more precisely defined and largely fixed in subsequent decisions). The model predicts that consumers will respond to changes in conditions in a coherent fashion, as if supported by demand curves derived from fundamental preference, even when their initial valuations are arbitrary.</p>", "people": [], "title": "Arbitrary Coherence in Behavioral Economics", "modified": "2016-12-05T00:17:06.719Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "--Choose Location", "groups": ["erationality"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "arbitrary-coherence-in-behavioral-economics"}, {"website": "", "description": "<p>Like the visible layers of a canyon, the layers in Artifacts of the Presence Era tell a story of past events. The images and sounds produced in a gallery at Boston's Institute for Contemporary Arts were captured and then visualized as a growing, organic landscape that served as an historical record. Like its natural counterpart, this process revealed long-term patterns (the rhythm of night and day, periods of great activity or empty silence), while retaining occasionally serendipitous, but often mundane, samples of the passage of life. The project visualized the accumulating layers of data and allowed visitors to navigate the captured images and ambient sounds, peeking back into the history of the gallery.</p>", "people": ["judith@media.mit.edu"], "title": "Artifacts of the Presence Era", "modified": "2016-12-05T00:17:06.756Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "artifacts-of-the-presence-era"}, {"website": "", "description": "<p>We are developing a system of wireless medical devices which can be used to monitor the condition of patients with chronic diseases such as diabetes or heart failure. Examples of medical paramaters which will be monitored are blood pressure, heart rate, temperature, respiration rate, and body weight.</p>", "people": ["joep@media.mit.edu"], "title": "At Home Chronic Care Monitoring", "modified": "2016-12-05T00:17:06.776Z", "visibility": "LAB", "start_on": "2001-01-01", "location": "E15-441", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "at-home-chronic-care-monitoring"}, {"website": "", "description": "<p>We are augmenting Computer Numeric Controlled (CNC) machines used in fabrication, prototyping, and construction. This project aims to develop processes that enable additive prototyping and construction at a large, architectural scale. One implementation combines robot arms with 3D weaving technology to create a new, high-accuracy prototyping machine for on-site fabrication in industries such as architecture, aerospace, and automotive. It would also be suitable for environments that are difficult for humans to inhabit\ufffdremote mountain or desert regions, deep sea or even outer space! Currently, industrial robot arms are not only used for repetitive assembly line tasks, but also for composite lay-up, bricklaying, milling and routing, welding, applying adhesives, but these automated CNC systems are mostly stationary and depend on molds to form the final shapes. We are investigating the potential for on-site construction machines that would cut down on overhead in management, coordination, fabrication, and transportation.</p>", "people": [], "title": "Architectural Machines", "modified": "2016-12-05T00:17:06.793Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "architectural-machines"}, {"website": "", "description": "<p>By designing relationships between a large body of pictorial information and a viewer's attention at single, or multiple viewpoints, it is possible to prepare rich, large-grain, temporal texturing of information landscapes in a manner that is uniquely relevant to the viewer. A variety of scenarios are being investigated in the construction of a soft immersive experience into very large pools of visual information.</p>", "people": [], "title": "Attentive Reaction", "modified": "2016-12-05T00:17:06.848Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "attentive-reaction"}, {"website": "", "description": "<p>As online auctions become more popular, it is important to understand better the process by which people set their bids and the factors that influence them. In this project, we use a survey methodology to survey real bidders in real, online auctions and to try to understand the factors that influence both their willingness to bid, and the magnitude of their bids.</p>", "people": [], "title": "Auction Behavior", "modified": "2016-12-05T00:17:06.865Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-313", "groups": ["erationality"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "auction-behavior"}, {"website": "", "description": "<p>Standard loudspeakers transmit sound which necessarily spreads very quickly, and the control of sound projection and position is only about as flexible as where you can hang a loudspeaker. The Audio Spotlight is a device that will project sound much like a spotlight projects light: shining it at a listener allows only that person to hear it, while shining it at a surface causes the sound to appear to originate from there, creating something of a \"virtual loudspeaker.\" Beamsteering will allow the sound to move, enabling the user dynamically to place sound\ufffdexactly, and only, where it is desired.</p>", "people": ["bv@media.mit.edu"], "title": "Audio Spotlight", "modified": "2016-12-05T00:17:06.901Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "audio-spotlight"}, {"website": "", "description": "<p>AudioVortex is an interactive audio spatialization installation that explores the difference between sound propagation in an open space versus sound propagation in a closed space. A circle of eight loudspeakers is divided in two by a physical barrier, while a microphone on either side of the barrier samples sounds from its respective environment. The two modes of operation include a spatial inversion, where sounds from outside are heard inside while sounds from inside are heard outside. The second mode blurs the boundary between inside and out both by moving the sound of the two spaces in a circular motion and by distorting the natural sounds with effects such as flange, delay, and reverberation. AudioVortex attempts to overcome physical boundaries through the use of sound, thus giving people the ability to aurally interact and communicate with each other in what would otherwise be non-communicable spaces.</p>", "people": ["bv@media.mit.edu"], "title": "AudioVortex", "modified": "2016-12-05T00:17:06.934Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-485A", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "audiovortex"}, {"website": "", "description": "<p>Persons on the autism spectrum often report hypersensitivity to sound. Efforts have been made to manage this condition, but there is wide room for improvement. One approach\ufffdexposure therapy\ufffdhas promise, and a recent study showed that it helped several individuals diagnosed with autism overcome their sound sensitivities. In this project, we borrow principles from exposure therapy, and use fun, engaging games to help individuals gradually get used to sounds that they might ordinarily find frightening or painful.</p>", "people": ["rmorris@media.mit.edu", "mgoodwin@media.mit.edu", "picard@media.mit.edu"], "title": "Auditory Desensitization Games", "modified": "2016-12-05T00:17:07.009Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-450", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "auditory-desensitization-games"}, {"website": "", "description": "<p>Recognizing non-verbal cues, which constitute a large percentage of our communication, is a prime facet of building emotionally intelligent systems. Facial expressions and movements such as a smile or a nod are used either to fulfill a semantic function, to communicate emotions, or as conversational cues. We are developing an automatic tool using computer vision and various machine-learning techniques, which can detect the different facial movements and head gestures of people while they are interacting naturally with the computer. Past work on this project determined techniques to track upper facial features (eyes and eyebrows) and detect facial actions corresponding to those features (eyes squintint or widening, eyebrows raised). The ongoing project is expanding its scope to track and detect facial actions corresponding to the lower features. Further, we hope to integrate the facial expression analysis module with other sensors developed by the Affective Computing group to reliably detect and recognize different emotions.</p>", "people": ["picard@media.mit.edu"], "title": "Automatic Facial Expression Analysis", "modified": "2016-12-05T00:17:07.149Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "automatic-facial-expression-analysis"}, {"website": "", "description": "<p>We are characterizing changes in autonomic nervous system (ANS) during sleep. This can potentially provide insight into circadian rhythms as well as identification of various sleep stages. Furthermore, we are investigating differences between ANS activity in neurotypicals and people with sleep disorders or electrical status epilepticus of sleep (ESES).</p>", "people": ["picard@media.mit.edu", "zher@media.mit.edu", "akanes@media.mit.edu"], "title": "Autonomic Nervous System Activity in Sleep", "modified": "2016-12-05T00:17:07.216Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "autonomic-nervous-system-activity-in-sleep"}, {"website": "", "description": "<p>This project examines the psycho-acoustic bases of the perception of musical structure by human listeners. Computational models will be built to mimic basic musical perception, such as parsing music into phrases or sections (i.e., recurrent structural analysis), identifying the main themes or hooks of a musical piece (i.e., music summarization), and detecting the most \ufffdinformative\ufffd parts of music for making certain judgments (i.e., detection of musical salience), upon taking complex acoustic signals as input. It will inquire scientifically into the nature of the music-listening process, and offer a practical solution to difficult problems in computer-based multimedia.</p>", "people": ["bv@media.mit.edu"], "title": "Automated Analysis of Musical Structure for Music Segmentation", "modified": "2016-12-05T00:17:07.337Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "automated-analysis-of-musical-structure-for-music-segmentation"}, {"website": "", "description": "<p>The Behavior Expression Animation Toolkit (BEAT) allows animators to input typed text that they wish to be spoken by an animated human figure, and to obtain as output appropriate and synchronized nonverbal behaviors and synthesized speech in a form that can be sent to a number of different animation systems. The nonverbal behaviors are assigned on the basis of actual linguistic and contextual analysis of the typed text, relying on rules derived from extensive research into human conversational behavior.</p>", "people": ["bickmore@media.mit.edu"], "title": "BEAT: the Behavior Expression Animation Toolkit", "modified": "2016-12-05T00:17:07.494Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "beat-the-behavior-expression-animation-toolkit"}, {"website": "", "description": "<p>In this project, we construct a virtual desktop centered around the smartphone display with the surface around the display opportunistically used for input. We use a 3-pixel optical time-of-flight sensor, Mime, to capture hand motion. The sensor on the phone allows the table surface next to the phone to be mapped to conventional desktop windows, and the phone's display is a small viewport onto this desktop. Moving the hand is like moving the mouse, and as the user shifts into another part of the desktop, the phone viewport display moves with it. We demonstrate that instead of writing new applications to use smart surfaces, existing applications can be readily controlled with the hands.</p>", "people": ["geek@media.mit.edu"], "title": "Back to the Desktop", "modified": "2016-12-05T00:17:07.460Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "back-to-the-desktop"}, {"website": "", "description": "<p>Integrating information processing and networking technologies into kinetic sculpture creates new opportunities for exploring the aesthetics of movement and interaction. These technologies enable the creation of sculpture that can receive, store, modify, and transmit information and make possible a new type of work: computational abstractions of living biological systems.</p>", "people": [], "title": "Behavioral Kinetic Sculpture", "modified": "2016-12-05T00:17:07.520Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-443", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "behavioral-kinetic-sculpture"}, {"website": "", "description": "<p>Bin-ary is a self-contained gas detector that analyzes organic trash odor compounds and releases a subtle burst of scent when bad odor is detected. The prototype is meant to be used as a plugin to make trash bins and dumpsters smarter and prevent insalubrity in cities and villages. In order to detect the state of organic trash we primarily focused on the chemical compounds that are produced when food starts to rot or ripen: Hydrogen Sulfide (H2S), Ethylene (C2H4) and Carbon Dioxide (CO2).</p>", "people": ["amores@media.mit.edu", "pattie@media.mit.edu"], "title": "Bin-ary: Detecting the State of Organic Trash to Prevent Insalubrity", "modified": "2016-12-05T00:17:07.567Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "bin-ary-detecting-the-state-of-organic-trash-to-prevent-insalubrity"}, {"website": "", "description": "<p>We are developing a control architecture for bipedal locomotion devices such as robots and powered orthotics. Advanced nonlinear control techniques, including feedback linearization, sliding control, and multivariable optimization, are utilized in this control architecture, yielding a highly stable and tunable controller for a highly unstable and nonlinear plant. Tests\ufffdusing a 3-D, 12-degree-of-freedom humanoid model\ufffdinclude a variety of disturbed initial states and of control goals for the center of mass, swing foot, and other points being controlled. An interesting property of this controller is the emergence of appropriate non-contact limb behavior in response to disturbances. Also, due to its large range of operation, this control architecture can reject significant disturbances more easily than simpler controllers, and requires a less-detailed reference trajectory than simpler controllers. This has the additional benefit of reducing the computational workload of a motion planner in an integrated motion planning and control system. Such control architectures will find use in assistive devices for the elderly and handicapped.</p>", "people": ["hherr@media.mit.edu"], "title": "Bipedal Balancing Using Integrated Movement of Non-Contact Limbs", "modified": "2016-12-05T00:17:07.741Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-054", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "bipedal-balancing-using-integrated-movement-of-non-contact-limbs"}, {"website": "", "description": "<p>The Bishop project explores the subtleties of human language when talking about spatial scenes. In particular, we investigate the various descriptive strategies human speakers employ in talking about objects in relation to other objects. These strategies include ordering objects, visually grouping them, describing their spatial relation or even referring back to objects that used to be in the scene. Furthermore, human subjects frequently perform combinations of these strategies, for example \"the green one to the left of the three purple ones.\" We are building a computational system that replicates both the individual phenomena and their  compositional behavior. As a result, this system understands relatively complex expressions referring to a scene of objects and can indicate the object being described. This work has direct applications for understanding for natural-language user interfaces, especially in augmenting direct-manipulation interfaces with intelligent speech control. A good example is speech interfaces for GPS map devices in cars where users speak about objects on the map.</p>", "people": ["dkroy@media.mit.edu"], "title": "Bishop: Understanding Complex Spatial Language", "modified": "2016-12-05T00:17:07.666Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "bishop-understanding-complex-spatial-language"}, {"website": "", "description": "<p>BlitzScribe is a new approach to speech transcription driven by the demands of today's massive multimedia corpora. High-quality annotations are essential for indexing and analyzing many multimedia datasets; in particular, our study of language development for the Human Speechome Project depends on speech transcripts. Unfortunately, automatic speech transcription is inadequate for many natural speech recordings, and traditional approaches to manual transcription are extremely labor intensive and expensive. BlitzScribe uses a semi-automatic approach, combining human and machine efforts to dramatically improve transcription speed. Automatic methods identify and segment speech in dense, multitrack audio recordings, allowing us to build streamlined user interfaces maximizing human productivity. The first version of BlitzScribe is already about 4-6 times faster than existing systems. We are exploring user-interface design, machine-learning and pattern-recognition techniques to build a human-machine collaborative system that will make massive transcription tasks feasible and affordable.</p>", "people": ["dkroy@media.mit.edu"], "title": "BlitzScribe: Speech Transcription for the Human Speechome Project", "modified": "2016-12-05T00:17:07.803Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-441", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "blitzscribe-speech-transcription-for-the-human-speechome-project"}, {"website": "", "description": "<p>How do we leverage people to make systems more intelligent, efficient, and successful? Is it worthwhile to involve people heavily in the goals of a system? How does a system most effectively coax stories out of people? To investigate these questions, a robot was built that facilitated interaction and documentary gathering within an ubiquitous media framework. We then let the robot roam freely, with the goal of capturing stories about its environment. This was done by leveraging human mobility and intelligence, as the robot relied upon people to move long distances and achieve its goals. The end products were a study of how people related to a robot asking for assistance and interaction in various ways, and a set of movies showing the robot navigating the resulting \"thread\" of a narrative.</p>", "people": ["joep@media.mit.edu"], "title": "Boxie the Robot: Interactive Physical Agents for Story Gathering", "modified": "2016-12-05T00:17:07.879Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "boxie-the-robot-interactive-physical-agents-for-story-gathering"}, {"website": "", "description": "<p>We are developing a multimodal interface for hand rehabilitation following stroke. EMG forearm sensors read attempted finger presses in disordered limbs, and serve as an input to an expressive feedback interface.  Auditory, visual, and tactile cues are presented to support rehabilitation of the representation of finger movements across sensory domains. The multisensory feedback is embedded in a rich task, situated between piano learning and expressive music performance.  A user of this system will rehabilitate finger movement while developing an expressive music performance. Imagine a complete shift in the form and function of rehabilitation, towards something empowering, where individuals strive in tandem with tailored interfaces, mapped to push them forward at each step, and as part of fundamentally enriching expressive tasks. Our rehabilitative health care environments can sculpt our minds, while changing our lives, if we invent the right tools.</p>", "people": ["tod@media.mit.edu"], "title": "Brain Instrument Interfaces", "modified": "2016-12-05T00:17:07.897Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "brain-instrument-interfaces"}, {"website": "", "description": "<p>People form dynamic groups focused on topics that emerge serendipitously during everyday life. They can be long-lived or of short duration. Examples include people interested in buying the same product, those with similar expertise, those in the same location, or any collection of such attributes. We call this the Human Discovery Protocol (HDP). Similar to how computers follow well-established protocols like DNS in order to find other computers that carry desired information, HDP presents an open protocol for people to announce bits of information about themselves, and have them aggregated and returned back in the form of a group of people that match against the user\ufffds specified criteria. We are experimenting with a web-based implementation (brin.gy) that allows users to join and communicate with groups of people based on their location, profile information, and items they may want to buy or sell.</p>", "people": ["ypod@media.mit.edu", "holtzman@media.mit.edu", "lip@media.mit.edu"], "title": "Brin.gy: What Brings Us Together", "modified": "2016-12-05T00:17:08.000Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["viral-communications", "information-ecology"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "bringy-what-brings-us-together"}, {"website": "", "description": "<p>We are building a set of free, open-source web and mobile tools to support informal face-to-face meetings and organizing.  The first of these tools we've developed provides a lightweight solution for group brainstorming and decision making.</p>", "people": ["ethanz@media.mit.edu", "borovoy@media.mit.edu"], "title": "BrownBag Toolkit", "modified": "2016-12-05T00:17:08.022Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "brownbag-toolkit"}, {"website": "", "description": "<p>The Buddy Compass, a watch-like device that points in the direction of the wearer's friend, explores an alternative user interface for location-specific information display. Developed on top of the Location Linked Information infrastructure, the Buddy Compass is a lightweight example that combines physical presence with virtual information.</p>", "people": [], "title": "Buddy Compass", "modified": "2016-12-05T00:17:08.050Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "NE18-4FL", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "buddy-compass"}, {"website": "", "description": "<p>This project helps a community tell its collective story through collaborative hypertext. Members of the community contribute many short anecdotes, and the system helps identify relationships of times, people, places, etc. The relationships are used to present the scattered memories of many people in a logical fashion and to elicit contributions of further memories. Our first prototype, Building 20 Memories, is specialized for the people who worked in the famous MIT Building 20.</p>", "people": ["lieber@media.mit.edu"], "title": "Building 20 Memories: Agent-Assisted Hypertext for Capturing Community History", "modified": "2016-12-05T00:17:08.071Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "building-20-memories-agent-assisted-hypertext-for-capturing-community-history"}, {"website": "", "description": "<p>GPS is transforming how we live globally, but not locally. To bring the  same capability for three-dimensional positioning indoors, we are  developing RF systems that can make picosecond timing measurements on  signals with a wavelength on the order of the size of a building, thereby  providing coupling without being perturbed by the internal structure.</p>", "people": ["neilg@media.mit.edu"], "title": "Building Positioning System", "modified": "2016-12-05T00:17:08.104Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "building-positioning-system"}, {"website": "", "description": "<p>C5 is our fourth-generation behavior architecture. It has been re-architected and fully documented. Our intention is to use C5 as a toolkit to continue to explore learning and training for interactive animated characters, as well as computational models of development for interactive characters. As always, the design of the toolkit (written almost entirely in Java, and designed to run on Mac OS X) is heavily inspired by what we know about animal behavior, learning, and development. C5 is being developed in conjunction with the Robotic Life group.</p>", "people": [], "title": "C5: Development and Learning", "modified": "2016-12-05T00:17:08.211Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-489", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "c5-development-and-learning"}, {"website": "", "description": "<p>We are building  a system that can watch for certain signs of  stress in drivers, specifically stress related to talking on the car phone, as may be caused by increased mental workload. To gather data for training and testing our system, subjects were asked to 'drive' in a simulator past several curves while keeping their speed close to a predetermined desired constant value. In some cases they were simultaneously asked to listen to random  numbers from a speech-synthesis software and to perform simple mathematical tasks over a telephone headset. Several measures drawn from the subjects' driving behavior were examined as possible indicators of  the subjects' performance and of their mental  workload. When subjects were instructed (by a visible sign) to brake, most braked within 0.7-1.4 seconds after the sign came into view.  However, in a  significant number of incidents, subjects never braked or braked 1.5-3.5 seconds after the message; almost all of these incidents were when subjects were on the phone.  On average, we found that drivers on the phone braked 10% slower than when not on the phone; additionally, the variance in their braking time was four times higher -- suggesting that although delayed driver reactions were infrequent, when delays happened they could be large and potentially dangerous.  Furthermore, their infrequency could create a false sense of security.   In future experiments, subjects' physiological data will be analyzed jointly with measures of workload, stress and performance.   \n</p>", "people": ["picard@media.mit.edu"], "title": "Car Phone Stress", "modified": "2016-12-05T00:17:08.156Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["affective-computing", "cc"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "car-phone-stress"}, {"website": "", "description": "<p>BUZZwatch distills and tracks trends, themes, and topics within collections of texts across time (e.g. Internet discussions, newspaper archives, and Web pages). This system works by combining new natural language processing techniques with text analysis and retrieval methods, and the novel application of statistical time series analysis. Applications for the system include data mining and visualization of topic dynamics within texts and Internet chatrooms, as well as improved retrieval for Web search engines.</p>", "people": [], "title": "BUZZwatch: Tracking Themes Within Text", "modified": "2016-12-05T00:17:08.188Z", "visibility": "PUBLIC", "start_on": "1995-12-31", "location": "E15-391", "groups": ["edevelopment", "software-agents"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "buzzwatch-tracking-themes-within-text"}, {"website": "", "description": "<p>We introduce a novel interactive method to assess cataracts in the human eye by crafting an optical solution that measures the perceptual impact of forward scattering on the foveal region. Current solutions rely on highly trained clinicians to check the back scattering in the crystallin lens and test their predictions on visual acuity tests. Close-range parallax barriers create collimated beams of light to scan through sub-apertures, scattering light as it strikes a cataract. User feedback generates maps for opacity, attenuation, contrast, and local point-spread functions. The goal is to allow a general audience to operate a portable, high-contrast, light-field display to gain a meaningful understanding of their own visual conditions. The compiled maps are used to reconstruct the cataract-affected view of an individual, offering a unique approach for capturing information for screening, diagnostic, and clinical analysis.</p>", "people": ["raskar@media.mit.edu", "elawson@media.mit.edu", "naik@media.mit.edu"], "title": "CATRA: Mapping of Cataract Opacities Through an Interactive Approach", "modified": "2016-12-05T00:17:08.240Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2015-09-15", "slug": "catra-mapping-of-cataract-opacities-through-an-interactive-approach"}, {"website": "", "description": "<p>Based on the work of Michael Schrage, this research argues\nthat tomorrow's innovations will be increasingly the by-product of how companies and their customers behave--and misbehave--around the new generation of models, prototypes, and simulations. The distinction between serious play and serious work dissolves as technology gives innovators ever-increasing opportunities to simulate and prototype their ideas. As the media for modeling radically change, so will the organizations that use them. The future of prototyping is the future of innovation.</p>", "people": [], "title": "CC++: Serious Play", "modified": "2016-12-05T00:17:08.263Z", "visibility": "LAB", "start_on": "2001-01-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2001-01-01", "slug": "cc-serious-play"}, {"website": "", "description": "<p>Based on the work of Michael Schrage, this research argues that tomorrow's innovations will be increasingly the by-product of how companies and their customers behave--and misbehave--around the new generation of models, prototypes, and simulations. The distinction between serious play and serious work dissolves as technology gives innovators ever-increasing opportunities to simulate and prototype their ideas. As the media for modeling radically change, so will the organizations that use them. The future of prototyping is the future of innovation.</p>", "people": ["rchin@media.mit.edu"], "title": "CC++: Serious Play: Simulation and Modeling in the Automotive Industry", "modified": "2016-12-05T00:17:08.284Z", "visibility": "LAB", "start_on": "1999-01-01", "location": "Cube", "groups": [], "published": true, "active": false, "end_on": "2002-09-01", "slug": "cc-serious-play-simulation-and-modeling-in-the-automotive-industry"}, {"website": "", "description": "<p>Expressive encoding of spatial ambience allows us to embed technological narratives in objects so as to provoke a re-thinking of our relationship to objects and spaces. The nature of this project addresses how objects may be expressive of attitude; here we explore the space above us. Hanging from the ceiling, organically shaped pods interface as a portal to another imaginative space. Passersby or visitors engage in an interaction with an encountered entity. This entity imparts a day-to-day dependent narrative, expressive and reactive to an accumulation of past interactions. This research explores both the dichotomy of tension and the engagement of affective states and historical data collected from interactions between the real-world visitor and the encountered imaginative character.</p>", "people": ["monster@media.mit.edu", "gid@media.mit.edu"], "title": "Ceiling with an Attitude", "modified": "2016-12-05T00:17:08.328Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-368", "groups": ["media-fabrics"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "ceiling-with-an-attitude"}, {"website": "", "description": "<p>Why do our advanced cell phones still ring at completely inappropriate times, such as at the theatre, at the movies, or during family dinners? How would a person contact me in an emergency if I have turned off my cell phone because I am in a meeting? In this work, we explore ways to make mobile communication devices socially intelligent, both in their internal reasoning and in how they interact with people. We propose the concept of an Autonomous Interactive Intermediary that helps the user manage her mobile communication devices in a socially appropriate way.</p>", "people": ["geek@media.mit.edu", "stefanm@media.mit.edu"], "title": "Cellular Squirrel: Autonomous Interactive Intermediaries", "modified": "2016-12-05T00:17:08.352Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "cellular-squirrel-autonomous-interactive-intermediaries"}, {"website": "", "description": "<p>How can traditional values be embedded into a digital object? We explore this concept by implementing a special guitar that combines physical acoustic properties with virtual capabilities. The acoustical values will be embodied by a wooden heart\ufffda unique, replaceable piece of wood that will give the guitar a unique sound. The acoustic signal created by this wooden heart will be digitally processed in order to create flexible sound design.</p>", "people": ["joep@media.mit.edu"], "title": "Chameleon Guitar: Physical Heart in a Virtual Body", "modified": "2016-12-05T00:17:08.375Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-320", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "chameleon-guitar-physical-heart-in-a-virtual-body"}, {"website": "", "description": "<p>ChatScape is a chat environment in which user-programmable autonomous behaviors are part of the communication repertoire.  We are developing tools that will allow users to combine a number of simple actions into a complex set of behaviors that respond to events in the environment, user commands, and the activity of others. Our goal with this project is to develop a more expressive and engaging environment for online conversation.</p>", "people": ["judith@media.mit.edu"], "title": "Chatscape", "modified": "2016-12-05T00:17:08.396Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-449", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "chatscape"}, {"website": "", "description": "<p>The goal of the Cinema Fabriqu\ufffd system is to create a software environment, complementary wearable devices, and a usage methodology for producing engaging cinematic experiences in real time for live audiences through natural-language control. Current multimedia performance packages suffer from input bandwith bottlenecks that restrict the scope of user control and audience engagement. Our proposed alternative aims to couple a high degree of user control through gestural and speech input with intelligent software to create rich audiovisual output.</p>", "people": [], "title": "Cinema Fabriqu\ufffd", "modified": "2016-12-05T00:17:08.480Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-301", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "cinema-fabriqu"}, {"website": "", "description": "<p>We demonstrate how the CityHome, which has a very small footprint, can function as an apartment two to three times that size. This is achieved through a transformable wall system which integrates furniture, storage, exercise equipment, lighting, office equipment, and entertainment systems. One potential scenario for the CityHome is where the bedroom transforms to a home gym, the living room to a dinner party space for 14 people, a suite for four guests, two separate office spaces plus a meeting space, or an a open loft space for a large party. Finally, the kitchen can either be open to the living space, or closed off to be used as a catering kitchen. Each occupant engages in a process to personalize the precise design of the wall units according to his or her unique activities and requirements.</p>", "people": ["hlarrea@media.mit.edu", "kll@media.mit.edu"], "title": "CityHome", "modified": "2017-10-18T01:06:53.526Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["city-science"], "published": true, "active": false, "end_on": "2016-12-01", "slug": "OLD_cityhome2"}, {"website": "", "description": "<p>The Civic Crowdfunding project is an initiative to collect data and advance social research into the emerging field of civic crowdfunding, the use of online crowdfunding platforms to provide services to communities. The project aims to bring together folks from across disciplines and professions\u2014from research and government to the tech sector and community organizations\u2014to talk about civic crowdfunding and its benefits, challenges, and opportunities. It combines qualitative and quantitative research methods, from analysis of the theory and history of crowdfunding to fieldwork-based case studies and geographic analysis of the field.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "Civic Crowdfunding Research Project", "modified": "2016-12-05T00:17:08.672Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "civic-crowdfunding-research-project"}, {"website": "", "description": "<p>The Clubhouse Village is an online community that connects people at The Clubhouse Network after-school centers around the world. Through the Village, Clubhouse members and staff at more than 100 Clubhouses in 19 countries can share ideas with one another, get feedback and advice on their projects, and work together on collaborative design activities.</p>", "people": ["chrisg@media.mit.edu", "sylvan@media.mit.edu", "mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "Clubhouse Village", "modified": "2016-12-05T00:17:08.759Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2016-08-31", "slug": "clubhouse-village"}, {"website": "", "description": "<p>Coda is a collaborative musical-knowledge database in which articles are accessed by selecting graphic entities in a musical score. Coda is meant to serve as the center for a community of learners sharing music-theory knowledge and musical ideas through musical pieces with personal meaning.</p>", "people": ["tod@media.mit.edu"], "title": "Coda", "modified": "2016-12-05T00:17:08.816Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "coda"}, {"website": "", "description": "<p>To foster and better understand collaboration in the Scratch Online Community, we created Collab Camp, a month-long event in which Scratch community members form teams (\ufffdcollabs\ufffd) to work together on Scratch projects. Our goals include: analyzing how different organizational structures support collaboration in different ways; examining how design decisions influence the diversity of participation in collaborative activities; and studying the role of constructive feedback in creative, collaborative processes. (Collab Camp was a collaboration with Yasmin Kafai's research group at University of Pennsylvania.)</p>", "people": ["ria@media.mit.edu", "mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "Collab Camp", "modified": "2016-12-05T00:17:08.935Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "collab-camp"}, {"website": "", "description": "<p>Common sense enables us to build innovative recommendation systems that are more interactive and user-friendly than traditional collaborative filtering systems. By applying ideas from blending and PerspectiveSpace to recommendations, we discover the characteristics of products that drive user ratings. We can use these characteristics to build intelligent recommendation agents and effective product exploration tools.</p>", "people": ["lieber@media.mit.edu"], "title": "Common-Sense Recommendations", "modified": "2016-12-05T00:17:08.986Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "--Choose Location", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "common-sense-recommendations"}, {"website": "", "description": "<p>A CAPTCHA is a \"Completely Automated Public Turing Test to tell Computers and Humans Apart.\" CAPTCHAs are tools used frequently to stop spammers from using computer programs to register for Web accounts or to deface wiki pages with advertisements. CAPTCHAs are most familiar as a picture of garbled or distorted text that humans are asked to decode on a Website. This project uses a new type of CAPTCHA that asks a series of common-sense questions; this new CAPTCHA is accessible to vision-impaired users and seems capable of distinguishing between humans and computers effectively. Most importantly, it allows individuals, in the process of their normal actions, to make easy and constructive contributions to AI research by helping to verify shared common-sense databases.</p>", "people": ["walter@media.mit.edu"], "title": "Commonsense CAPTCHA", "modified": "2016-12-05T00:17:09.049Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-020C", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "commonsense-captcha"}, {"website": "", "description": "<p>comMotion is a location-aware, mobile communication system that delivers timely, travel condition-related information to the mobile user. How many times have you gone to the grocery store but left the shopping list on the fridge door? Wouldn't it make more sense to have the list delivered to you when you were about to drive by the store?</p>", "people": ["geek@media.mit.edu"], "title": "comMotion", "modified": "2016-12-05T00:17:09.074Z", "visibility": "PUBLIC", "start_on": "1997-01-01", "location": "E15-344", "groups": ["living-mobile", "cc", "e-markets"], "published": true, "active": false, "end_on": "1999-01-01", "slug": "commotion"}, {"website": "", "description": "<p>Quilts have served as a cultural symbol for centuries and represent the diversity of people, places, ideas, and events from every area of a community. The Clubhouse Quilt was a vehicle for kids in the Computer Clubhouse Network to share their diverse projects (artwork, music, movies, poetry, and 3-D animation) with the rest of the Clubhouse community. \nThe Quilt consisted of patches, each of which represented a single project with links to information about the project designer(s), their clubhouse(s) and local community. This project provided an outlet for Clubhouse members to share the uniqueness of their Clubhouse and community and learn about others.</p>", "people": ["mres@media.mit.edu", "leob@media.mit.edu"], "title": "Computer Clubhouse Quilt", "modified": "2016-12-05T00:17:09.362Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2001-09-01", "slug": "computer-clubhouse-quilt"}, {"website": "", "description": "<p>Imagine that a full computer (with touch screen, sensors, keyboard, and everything else) was simply just another craft on your craft table. How would you use it? Computer crafting weaves together full computers with regular paper and markers, textiles, and everyday objects. By using computers as just another craft, the everyday world can be programmed and combined with computers.</p>", "people": ["kbrennan@media.mit.edu", "silver@media.mit.edu", "mres@media.mit.edu"], "title": "Computer Crafting", "modified": "2016-12-05T00:17:09.382Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "LEGO Learning Lab", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "computer-crafting"}, {"website": "", "description": "<p>Design and modeling processes in architecture and the automotive industry have evolved along completely separate paths. This research project compares these processes in order to discover areas of potential translation or cross-fertilization. Areas such as aesthetics, software, tools, manufacturing, and fabrication have the highest likelihood of translatability.</p>", "people": ["rchin@media.mit.edu"], "title": "Comparative Design Processes: Architecture vs. Automotive", "modified": "2016-12-05T00:17:09.120Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "comparative-design-processes-architecture-vs-automotive"}, {"website": "", "description": "<p>Consumer photography is undergoing a paradigm shift with the development of light field cameras. Commercial products such as those by Lytro and Raytrix have begun to appear in the marketplace with features such as post-capture refocus, 3D capture, and viewpoint changes. These cameras suffer from two major drawbacks: major drop in resolution (converting a 20 MP sensor to a 1 MP image) and large form factor. We have developed a new light-field camera that circumvents traditional resolution losses (a 20 MP sensor turns into a full-sensor resolution refocused image) in a thin form factor that can fit into traditional DSLRs and mobile phones.</p>", "people": ["raskar@media.mit.edu", "bandy@media.mit.edu", "gordonw@media.mit.edu"], "title": "Compressive Light-Field Camera: Next Generation in 3D Photography", "modified": "2016-12-05T00:17:09.150Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2015-09-15", "slug": "compressive-light-field-camera-next-generation-in-3d-photography"}, {"website": "", "description": "<p>Our hearing mechanism is very good in ignoring redundant sounds and parsing complex auditory scenes. This is a subject that has been extensively studied, but most of the work is in the heuristic level thus impractical for machine implementation. By redefining listening theories in a more rigorous and mathematical framework we can come closer to constructing machines capable of auditory consciousness.  When successful, we can apply these principles to engineering problems to obtain significant results, one such application has been source separation.</p>", "people": ["bv@media.mit.edu"], "title": "Computational Auditory Scene Analysis", "modified": "2016-12-05T00:17:09.177Z", "visibility": "PUBLIC", "start_on": "1994-01-01", "location": "--Choose Location", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "computational-auditory-scene-analysis"}, {"website": "", "description": "<p>This research continues to explore technology and computation as expressive elements on the body, while also addressing the larger issue of enabling design in software, firmware, and hardware simultaneously. By blurring the distinction between screen-based applications and behavioral physical devices, we hope to establish a contiguous space for creating computational objects. We are developing an all-purpose system for building small hardware modules that can act as interfaces, output devices, or independent computational units, specifically geared towards quick implementation of custom designed projects.</p>", "people": [], "title": "Computational Garments / Jetset", "modified": "2016-12-05T00:17:09.213Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-443", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "computational-garments-jetset"}, {"website": "", "description": "<p>How are people's interests related? By exploring people's interests from LinkedIn profiles, we have created an interest graph to understand these relationships. The graph nodes are interests (hiking, painting, design), color-coded by category. Links between nodes appear when interests co-occur in multiple profiles. By looking at the graph, we can see which interests are more commonly shared among people, and whether these interests are in the same category (that is, similar to each other). This allows us to form hypotheses about how and why certain interests co-occur more often than others, and measure the diversity of a person's interests.</p>", "people": ["lip@media.mit.edu"], "title": "Compound Interest", "modified": "2016-12-05T00:17:09.323Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "compound-interest"}, {"website": "", "description": "<p>The ComTouch project explores interpersonal communication by use of haptic technology. Touch as an augmentation to existing communication may provide and enhance existing audio and video media. ComTouch is a handheld device that allows the squeeze under each finger to be represented as vibration. This haptic device will enable users to transmit thoughts, feelings, and concepts to each other remotely. The form factor is designed to fit on the back of a cellular phone; as users talk on the cell phone, they can squeeze and transmit a representation of their touch. Through this research, we aim to describe more accurately the language of touch-based communication. We also hope to devise a set of guidelines for designing touch-based communication devices.</p>", "people": ["ishii@media.mit.edu"], "title": "ComTouch: Haptic Communication Medium", "modified": "2016-12-05T00:17:09.458Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "comtouch-haptic-communication-medium"}, {"website": "", "description": "<p>Americans have a negative personal savings rate; this indicates budgeting and saving can be very hard to do\ufffdmoney is abstract, fungible, and difficult to think about, and current software for planning budgeting and savings does not remove this difficulty. However, research from psychology and behavioral economics suggests ways to make personal budgeting easier, more natural, and more successful, by making trade-offs and the effect of risk more concrete.  This project aims to use these principles to create a more effective tool for personal money management.</p>", "people": [], "title": "Concrete Budgeting for Personal Savings", "modified": "2016-12-05T00:17:09.552Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-320A", "groups": ["erationality"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "concrete-budgeting-for-personal-savings"}, {"website": "", "description": "<p>This new version of the original online Confectionary project is open and extensible. Designed for constructing and exchanging mediated personal stories, the software invites participants to create story compositions by assembling video, text, image, and audio content within a two-dimensional collage interface. Participants can make use of flexible privacy controls. They publish their stories to a collaborative library and can create a sequence of multiple Confectionaries using the make-path feature. New work involves adaptations for multicultural and international use in schools.</p>", "people": ["gid@media.mit.edu"], "title": "Confectionary 2.0", "modified": "2016-12-05T00:17:09.588Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-368", "groups": [], "published": true, "active": false, "end_on": "2008-09-01", "slug": "confectionary-20"}, {"website": "", "description": "<p>Starting with a simple rectangular box, we are exploring possibilities for \"extracting\" virtual form with the help of minimal constructs that are, in a way, hidden within its volume. The process is not unlike a (subtractive) mode of sculpting, although the notion of interactivity poses new challenges to an otherwise fundamental concept in visualization. Our current goal is to explore form-making at the basic level in order to provide a comprehensive framework for more complex analyses in the future.</p>", "people": [], "title": "Constructive Geometries", "modified": "2016-12-05T00:17:09.703Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-443", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "constructive-geometries"}, {"website": "", "description": "<p>When browsing the Web, what quality information does the user have about the information underlying the links? The only hint behind links is that they change their color when visited. But a great deal of relevant knowledge possessed by colleagues is not exploited. The aim of this project is to develop a system for enhancing links in Web pages, based on the experience of the community. Using a mediator based on IBM WBI intermediary [1] as a platform, we developed a system that behaves like a proxy with some changes: first, it logs user requests, and builds a repository of access statistics and page metadata; second, it keeps user profiles based on their homepages; third, it ranks pages based on these access statistics and a matches with the expert profiles (based on the first two points); fourth, it uses this information to augment browsing via a unique user interface.</p>", "people": ["lieber@media.mit.edu"], "title": "Context-Aware Annotation Proxy Based System (CAPS)", "modified": "2016-12-05T00:17:09.754Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-320", "groups": ["context-aware-computing", "software-agents"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "context-aware-annotation-proxy-based-system-caps"}, {"website": "", "description": "<p>Current biological research workflows make use of disparate, poorly integrated systems that impose a large mental burden on the scientist, leading to mistakes, often on long, complex, and costly experimental procedures. The lack of open tools to assist in the collection of distributed experimental conditions and data is largely responsible for making protocols difficult to debug, and laboratory practice hard to learn. In this work, we describe an open Protocol Descriptor Language (PDL) and system to enable a context-rich, quantitative approach to biological research. We detail the development of a closed-loop pipetting technology and a wireless sample-temperature sensor that integrate with our Protocol Description platform, enabling novel, real-time experimental feedback to the researcher, thereby reducing mistakes and increasing overall scientific reproducibility.</p>", "people": ["jacobson@media.mit.edu", "fracchia@media.mit.edu"], "title": "Context-Aware Biology", "modified": "2016-12-05T00:17:09.781Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2015-12-01", "slug": "context-aware-biology"}, {"website": "", "description": "<p>These tables are aware of their height and of neighboring tables and respond accordingly. When they are lowered, the high-resolution surface presents pictures you would find in a coffee-table book. When raised, they become bar menus. Adjusting the height also allows drums to be tuned from bongo to kettle drums to a cowbell. When connected to a neighboring table, the two can be used for collaboration and table-to-table interactions. A low-cost, capacitive sensor integrated into the table's piston enables these transformations. The capacitive piston-sensor consists of an evaporative, deposited, metallic layer on a thin dielectric that is wrapped around the piston shaft of a pneumatic cylinder. The capacitor is held in place and pressed against the moving shaft with a plastic retaining clip. These capacitors can tell the height of a chair or table.</p>", "people": [], "title": "Context-Aware Tables", "modified": "2016-12-05T00:17:09.828Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-320", "groups": ["context-aware-computing", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-09-01", "slug": "context-aware-tables"}, {"website": "", "description": "<p>Cooperative communication represents a paradigm shift from current point-to-point, client-server interaction to a fully decentralized, symmetric model of communication. In order to realize the gains of this model of communication, a new class of protocols is required. In this project, we design protocols and architectures for multipoint-to-multipoint communication, ranging from media distribution to real-time communication.</p>", "people": ["lip@media.mit.edu"], "title": "Cooperative Transport Protocols", "modified": "2016-12-05T00:17:09.945Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-483", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "cooperative-transport-protocols"}, {"website": "", "description": "<p>Cord UIs are sensorial augmented cords that allow for simple, metaphor-rich interactions to interface with their connected devices. Cords offer a large, under-explored space for interactions, as well as unique properties and a diverse set of metaphors that make them potentially interesting tangible interfaces. We use cords as input devices and explore different interactions like tying knots, stretching, pinching, and kinking to control the flow of data and/or power. We also look at ways to use objects in combination with augmented cords to manipulate data or properties of a device. For instance, placing a clamp on a cable can obstruct the audio signal to the headphones. Using special materials such as piezo copolymer cables and stretchable cords, we built five working prototypes to showcase these interactions.</p>", "people": ["sangwon@media.mit.edu", "ishii@media.mit.edu", "pattie@media.mit.edu"], "title": "Cord UIs: Controlling Devices with Augmented Cables", "modified": "2016-12-05T00:17:09.974Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces", "tangible-media"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "cord-uis-controlling-devices-with-augmented-cables"}, {"website": "", "description": "<p>We present the design of a cost-effective, wearable sensor to detect and indicate the strength and other characteristics of the electric field emanating from a laptop display. Our bracelet can provide an immediate awareness of electric fields radiated from a frequently used object, thus supporting awareness of ambient background emanation beyond human perception. We discuss how detection of such radiation might help to \"fingerprint\" devices and aid in applications that require determination of indoor location.</p>", "people": ["ishii@media.mit.edu"], "title": "Cost-Effective Wearable Sensor to Detect EMF", "modified": "2016-12-05T00:17:09.996Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-350", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "cost-effective-wearable-sensor-to-detect-emf"}, {"website": "", "description": "<p>Critical Cartography develops mapping tools for urban activists. The current prototype is a data-collection and route-planning application for wireless devices. This system is currently in use by privacy-rights advocates in New York and Amsterdam to monitor the proliferation of closed-circuit television (CCTV) cameras surveilling public spaces.</p>", "people": [], "title": "Critical Cartography", "modified": "2016-12-05T00:17:10.161Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "critical-cartography"}, {"website": "", "description": "<p>Cronicas de Heroes documents acts of extraordinary and everyday heroism in communities that sometimes seem devoid of heroes\ufffdcities on the US-Mexico border, where drug violence has destroyed communities and silenced many forms of community dialog. Cronicas de Heroes invites people to share their (signed or anonymous) reports of heroism and visualizes them on maps of the city. Currently operating in four Mexican cities, Cronicas de Heroes is now expanding on both sides of the border, and in other communities around the world.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "Cronicas de Heroes (Hero Reports) ", "modified": "2016-12-05T00:17:10.196Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "cronicas-de-heroes-hero-reports"}, {"website": "", "description": "<p>It takes us years to learn our own musical tradition, and it is rare to find people who have become musically multilingual. However, by learning to compose in different cultural styles, we can expand our compositional palette and communicate more effectively across cultural boundaries. We are designing a computer-assisted compositional tool that can help composers to begin composing melodies in other cultural styles by dynamically analyzing the musical context and presenting as musical analogies melodic materials from various cultures. These melodic patterns address questions such as: In a target cultural style, how does one develop a musical idea? What are the idiomatic melodic progressions? How does one establish a structural pitch? What are the possible continuations to an unfinished melody and how does one cadence? This tool makes it more accessible for composers to transform and render their musical ideas in other musical languages.  </p>", "people": ["bv@media.mit.edu"], "title": "Cross-Cultural Melodic Transformation", "modified": "2016-12-05T00:17:10.060Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "cross-cultural-melodic-transformation"}, {"website": "", "description": "<p>We can use sensor networks as a way of building a comprehensive picture of a physical space to transmit a sense of that space into a virtual environment. In this project, we use the Spinner sensor nodes to capture video, audio, and environmental information. This information is used to build an interactive three-dimensional space in Second Life that makes it easy to explore the history of any sensor node, as well as interact live using text and audio with people at any of the nodes. Avatars\ufffd virtual presence near nodes is also shown on the physical nodes, so visitors to the physical space know when there are virtual visitors nearby.</p>", "people": ["joep@media.mit.edu"], "title": "Cross-Reality Demonstration", "modified": "2016-12-05T00:17:10.080Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "cross-reality-demonstration"}, {"website": "", "description": "<p>In 2009, DARPA launched the Network Challenge to explore the roles the Internet and social networking play in the timely communication, wide-area team-building, and urgent mobilization required to solve broad-scope, time-critical problems. The challenge was to be the first to locate 10 moored, 8-foot, red weather balloons at 10 random locations in the continental United States. A team from MIT won by locating all balloons in under 9 hours. We helped analyze the factors behind the team's success. We then quantified the limits of this kind of mobilization, and introduced techniques for improving information verification in mass collaboration.</p>", "people": ["sandy@media.mit.edu", "dsouza@media.mit.edu", "irahwan@media.mit.edu"], "title": "Crowdsourcing Search: The Red Balloon Challenge", "modified": "2016-12-05T00:17:10.099Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["scalable-cooperation"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "crowdsourcing-search-the-red-balloon-challenge"}, {"website": "", "description": "<p>We worked with the residents at Camfield Estates, a housing development in Roxbury, MA, to investigate the role of community technology for the purpose of community building in low- to moderate-income and minority communities. In particular, examined how new technology-supported activities can help to increase social capital and to activate cultural capital within the housing development. Our larger goal was to develop new approaches for bridging the digital divide.</p>", "people": ["mres@media.mit.edu"], "title": "Creating Community Connections", "modified": "2016-12-05T00:17:10.135Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2001-09-01", "slug": "creating-community-connections"}, {"website": "", "description": "<p>Our glass building lets us see across spaces\ufffdthrough the walls that enclose us and beyond. Yet invisibly, networks of sensors inside and out capture the often imperceivable dimensions of the built and natural environment. Our project uses multi-channel spatial sound to bring that data into the utilitarian experience of riding the glass elevator. In the past, we've mixed live sound from microphones throughout the building with sonification of sensor data, using a pressure sensor to provide fine-grained altitude for control. In its present form, the elevator is displaying data from the Living Observatory, a wetland restoration site 60 miles away. Each string pluck represents a new data point streaming in; its pitch corresponds to the temperature at the sensor and its timbre reflects the humidity. Live and recorded sound reflect the real ambience of the remote wetland.</p>", "people": ["gershon@media.mit.edu", "bmayton@media.mit.edu", "joep@media.mit.edu", "sfr@media.mit.edu"], "title": "Data-Driven Elevator Music", "modified": "2016-12-05T00:17:10.401Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "data-driven-elevator-music"}, {"website": "", "description": "<p>We all use systems for organizing our cluttered schedules, from the day planner, to to-do lists, to methods such as Getting Things Done. Daydar is a framework that makes this process social: Can you learn from the working styles of others? Can you collaboratively create an environment of healthy competition by being aware of your friends' daily accomplishments? Can this help you to find a better balance between work and play? Within this framework, we are experimenting with physical and digital artifacts that enable you to reflect on your own goals through your peers' work habits, get motivated, and externalize your tasks in order to improve the process of accomplishing projects.</p>", "people": ["jkestner@media.mit.edu", "dsmall@media.mit.edu", "holtzman@media.mit.edu"], "title": "Daydar: Framework for Socially Motivated Goal Fulfillment", "modified": "2016-12-05T00:17:10.425Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "daydar-framework-for-socially-motivated-goal-fulfillment"}, {"website": "", "description": "<p>How would you translate the word \"samosa\" for someone who has never experienced this food item? One way would be to map the word onto names of similar foods that you think the person does know, and then explain ways in which the foods are similar and different. We are developing a system called TLC that will learn to translate food terms in this way. An understanding of food will be created in two ways. First, the system will acquire a set of data structures that capture the similarities between basic food ingredients such as sugar, eggs, or chicken. This level of representation will model aspects of the human taste and olfactory systems. Second, TLC will compile a massive collection of recipes from the Internet into a structured database. With these two sources of knowledge, the system will be able to compare foods with potentially complex underlying structures and act as a language translator. Although TLC is focused on the domain of food, the underlying data representations and algorithms can be applied to numerous other terminology translation problems.</p>", "people": ["dkroy@media.mit.edu"], "title": "Deep Semantic Representations for Language Translation", "modified": "2016-12-05T00:17:10.480Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "deep-semantic-representations-for-language-translation"}, {"website": "", "description": "<p>The Digital Cigarette is an exploration in motivation. It addresses the design of a device that can operate in multiple modalities, on behalf of its user and society.  It is a small electronic device with a microchip that senses when it has been lit, when its user has inhaled or exhaled, when it is tapped, and when it is in the proximity of other digital cigarettes.  Responding with sound and light it engages its user. For the individual user it is a device that strives to provide a break, a reminder of an agenda, procrastination control, meditation, stress relief, and focusing.  For a group it strives to facilitate social interaction through gift giving and has some of the attributes of  pocket pets, cell phones, text messaging, candy, cards, and actual cigarettes.</p>", "people": [], "title": "Digital Cigarette", "modified": "2016-12-05T00:17:10.659Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "digital-cigarette"}, {"website": "", "description": "<p>What do customers really think about your company or brand? Using skin conductance sensors, we measure what excites and frustrates customers when discussing topics relevant to your brand. For example, with the National Campaign to Prevent Teenage Pregnancy, we saw conversations about empowerment and abortion upset conservative families. However, talking about the importance of strong families excited and engaged them. Rather than rely on self-reports, physiological measurements allow us to pinpoint what words and concepts affect your customers. We hope work like this will help companies better reflect on how their actions and messaging affect their customer's opinion in more detailed and accurate ways.</p>", "people": ["picard@media.mit.edu", "hedman@media.mit.edu"], "title": "Digging into Brand Perception with Psychophysiology", "modified": "2016-12-05T00:17:10.573Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "digging-into-brand-perception-with-psychophysiology"}, {"website": "", "description": "<p>We are adding digital capabilities to the traditional toys of childhood, and, in the process, redefining how and what children learn. When children play with traditional toys (such as beads, blocks, and balls), they can gain an understanding of concepts such as number, size, and shape. With our new digital versions of these toys, children can learn concepts (such as process, probability, and emergence) that were previously seen as too complex for children.</p>", "people": ["borovoy@media.mit.edu", "mres@media.mit.edu", "bss@media.mit.edu"], "title": "Digital Manipulatives", "modified": "2016-12-05T00:17:10.596Z", "visibility": "PUBLIC", "start_on": "1995-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "digital-manipulatives"}, {"website": "", "description": "<p>We are developing technologies and community methodologies in electronic commerce for social and economic development. We study e-commerce technologies and systems software that are appropriate and appropriable by the majority of the world. Research and development activities integrate with activist field projects that are community-centered. We are keenly interested in first-class technologies that are cost-appropriate for people who make less than two dollars a day.</p>", "people": [], "title": "Development E-commerce", "modified": "2016-12-05T00:17:10.617Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-391", "groups": ["edevelopment", "software-agents", "e-markets"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "development-e-commerce"}, {"website": "", "description": "<p>For centuries, luthiers have failed to duplicate a Stradivarius. We're now passing a remarkable threshold when sensors and computers can exceed the performance of nature at this level of description, so that a mathematical description of an instrument can become a playable instrument. In this project we are creating the instrumentation and algorithms to let computers develop real-time models to emulate the observed performance of complex driven physical systems.</p>", "people": ["neilg@media.mit.edu"], "title": "Digital Stradivarius", "modified": "2016-12-05T00:17:10.700Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-023", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "digital-stradivarius"}, {"website": "", "description": "<p>Digito is a gesturally controlled virtual musical instrument, controlled through a number of intricate hand gestures which provide both discrete and continuous control of its sound engine. The hand gestures are captured using a 3D depth sensor and recognized using computer vision and machine learning algorithms. Digito is currently being used to evaluate the possible strengths and limitations of gesturally controlled virtual musical instruments and to assist in uncovering new questions regarding the design of gestural musical interfaces.</p>", "people": ["joep@media.mit.edu"], "title": "Digito: A Fine-Grained, Gesturally Controlled Virtual Musical Instrument", "modified": "2016-12-05T00:17:10.768Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "digito-a-fine-grained-gesturally-controlled-virtual-musical-instrument"}, {"website": "", "description": "<p>Photorefractive polymer has many attractive properties for dynamic holographic displays; however, the current display systems based around its use involve generating holograms by optical interference methods that complicate the optical and computational architectures of the systems, and limit the kinds of holograms that can be displayed. We are developing a system to write computer-generated diffraction fringes directly from spatial light modulators to photorefractive polymers, resulting in displays with reduced footprint and cost, and potentially higher perceptual quality.</p>", "people": ["sjolly@media.mit.edu", "vmb@media.mit.edu"], "title": "Direct Fringe Writing of Computer-Generated Holograms", "modified": "2016-12-05T00:17:10.795Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "direct-fringe-writing-of-computer-generated-holograms"}, {"website": "", "description": "<p>We are creating thirty-dollar, sunlight-readable, low-power, and high-resolution displays for laptop computers. We are focusing on the replacement of color filters with LED back- and front-lights, and, additionally, on <$100 projectors for the classroom.</p>", "people": ["mlj@media.mit.edu"], "title": "Displays for the $100 Laptop", "modified": "2016-12-05T00:17:10.839Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2008-01-01", "slug": "displays-for-the-100-laptop"}, {"website": "", "description": "<p>We can direct users through a building to the least crowded destination using cues that indicate the next step in a path; providing next-step directions to users is a well-known technique. Often, the path planning is static, occurring before users start off to their destinations. This project will direct users through the Media Lab to specific destinations using real-time status information in an ad hoc network. We can infer the crowd size at locations such as demo areas using sensors. The sensors forward on that crowd data to nodes in hallways and doorways whose displays direct users to the least-crowded destination. As crowd sizes change, the nodes adapt and redirect users to different areas without any centralized control.</p>", "people": ["lip@media.mit.edu"], "title": "Distributed-In, Distributed-Out Sensor Networks", "modified": "2016-12-05T00:17:10.885Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-495", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "distributed-in-distributed-out-sensor-networks"}, {"website": "", "description": "<p>We consider the problem of compressing an image into packets sets that are mutually refinable. This problem is applicable to distributed storage of data: each user owns a different  compressed version of the same original picture. Each version by itself is a coarse approximation of the original picture. Then, people can decide to share their versions. The more versions people contribute, the better the resulting approximation becomes, independently of which people are actually involved: only the number of users that collaborate matters. We will address this problem by using multiple description techniques combined with sub-band coders.</p>", "people": ["lip@media.mit.edu"], "title": "Distributed Storage of Data Using Multiple Description Techniques", "modified": "2016-12-05T00:17:10.927Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-483", "groups": ["media-and-networks"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "distributed-storage-of-data-using-multiple-description-techniques"}, {"website": "", "description": "<p>e-MotionInfo enables users to explore the harmonization of their movements, digital information, and responsive objects. e-MotionInfo creates links between motions, digital content, and associated objects to improve upon expressive and natural user interactions.</p>", "people": ["holtzman@media.mit.edu"], "title": "E-MotionInfo", "modified": "2016-12-05T00:17:11.236Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "e-motioninfo"}, {"website": "", "description": "<p>Electronic Popables is an interactive pop-up book that sparkles, sings, and moves. The book integrates traditional pop-up mechanisms with thin, flexible, paper-based electronics; the result is an artifact that looks and functions much like an ordinary pop-up book, but has added elements of dynamic interactivity. </p>", "people": ["leah@media.mit.edu", "jieqi@media.mit.edu"], "title": "Electronic Popables: An Interactive Pop-Up Book", "modified": "2016-12-05T00:17:11.292Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-368", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "electronic-popables-an-interactive-pop-up-book"}, {"website": "", "description": "<p>We are designing an \ufffdelectronic lens\ufffd that provides pedestrians with immediate, on-the-spot, geographically and temporally contextualized information about the attractions and resources that a city offers. The Electronic Lens explores and creates new paradigms of civic ubiquitous networking with mobile technologies. The eLens matches electronic information with the physical environment in an innovative way; for example, eLens users can post lasting messages in physical locations, tag buildings and places, or create social networks based on interest and social affinities. eLens interactions combine the physical environment with formal and institutional information and the annotations from users\ufffd personal experiences.</p>", "people": ["sajid@media.mit.edu", "federico@media.mit.edu"], "title": "eLens: The Electronic Lens", "modified": "2016-12-05T00:17:11.324Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "elens-the-electronic-lens"}, {"website": "", "description": "<p>Books with printed pages are unique in that they embody the simultaneous, high-resolution display of hundreds of pages of information. The representation of information on a large number of physical pages, which may be physically turned and written on, constitutes a highly preferred means of information interaction. An obvious disadvantage of the printed page, however, is its immutability once typeset. We are currently developing electronically addressable paper-page displays that use real paper substrates. This effort includes the development of novel, electronically addressable contrast media, microencapsulation chemistry, and desktop printing technologies to print functional circuits, logic, display elements, and actuators on paper or paper-like substrates.</p>", "people": ["jacobson@media.mit.edu"], "title": "Electronic Ink and Electronic Paper", "modified": "2016-12-05T00:17:11.359Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "--Choose Location", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "electronic-ink-and-electronic-paper"}, {"website": "", "description": "<p>We have an unconscious understanding of the meaning of different physical objects through our extensive interactions with them. Designers can extend and adapt the existing symbolic meanings through the design of these objects, adding a layer of emotive expression by manipulating their forms. Tactile Allegory explores the physical design language encoded into objects and asks: how can objects be computationally designed to communicate specific information through their very forms? This research explores the underlying design \"grammar\" of the form of objects, particularly how objects can communicate information to us through their form. This framework is used to create a computational design tool to help people design expressively shaped objects that can express higher-level sentiments of their ideas via aesthetic forms.</p>", "people": ["pip@media.mit.edu", "vmb@media.mit.edu"], "title": "EmotiveModeler: Emotive Form Design Taxonomy", "modified": "2016-12-05T00:17:11.379Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "emotivemodeler-tactile-allegory-design-framework"}, {"website": "", "description": "<p>The Emotion Bottles are tangibly enticing objects that embody three emotions:  angry, happy, and sad. When a bottle is opened, a vocal output is generated as if the emotion that was stored within the bottle is released. The bottles are placed near each other and represent a person in three possible emotional states. Varying degrees of these emotions are \"bottled up\" inside. The three bottles were chosen to maintain the simplicity of exploring the combination of distinct emotional states (eight possibilities). While not completely representative of the possible emotional state of a person, the bottles explore the interface in accessing emotions, the interaction between conflicting emotions, and the meaning of transition between clear emotional states as a person empathizes with or projects their feelings onto the bottles.</p>", "people": ["picard@media.mit.edu"], "title": "Emotion Bottles", "modified": "2016-12-05T00:17:11.415Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "emotion-bottles"}, {"website": "", "description": "<p>Using machine learning, computer vision, and wrist-worn, smaller, time-of-flight cameras, we can recover hand pose and micro-gesture (small movements of the fingers and thumb). It is clear that ubiquitous wearables will need a similar eyes-free user interface\ufffdbut how should this interface be designed? We are examining interaction through user tests\ufffdwhat gesture set designs work well for text entry or focus selection? How can we predict user experience and the usability of such systems? We hope to answer such questions through the EMGRIE system and experimental application design.</p>", "people": ["joep@media.mit.edu"], "title": "Ergonomic Micro-Gesture Recognition and Interaction Evaluation", "modified": "2016-12-05T00:17:11.526Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "ergonomic-micro-gesture-recognition-and-interaction-evaluation"}, {"website": "", "description": "<p>Drowsy drivers are a danger to themselves and others on the road. Additionally, for many commuters it is hard to get ample time to exercise during the week as commuting time eats away at personal time. The ExcerCar project is an effort to enhance both alertness and physical activity. We are developing an exercise interface to the vehicle that counteracts the effects of fatigue and inactivity while driving. The hypothesis is that exercise will mitigate the effects of fatigue and increase a drowsy driver's alertness. Put another way, can Fred Flintstone's car make him a more alert driver?</p>", "people": ["win@media.mit.edu"], "title": "ExcerCar: A Solution to Drowsy Driving?", "modified": "2016-12-05T00:17:11.625Z", "visibility": "PUBLIC", "start_on": "1999-09-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "excercar-a-solution-to-drowsy-driving"}, {"website": "", "description": "<p>To change radically the physical shape and feeling of technology, designers must also be able to change the materials from which it is made. Fabric Interfaces are just one example of what can happen when design materials, like fabric and thread, become electronic materials. The musical ball, a continuous-control, plush musical instrument, uses embroidered conductive thread as pressure sensors. These embroidered pressure sensors allow the ball to be soft and plush, rather than covered with hard buttons or keys. They also make manufacturing cheap and easy, and allow designers to shape their sensors anyway they want. The whimsical electronic table cloths and coaster ID tags are part of a digital game of cocktail Jeopardy. They use appliqued conductive fabric and embroidered threads to create, in the table cloths, a tag reader, and decorative key pad.</p>", "people": ["tod@media.mit.edu", "rehmi@media.mit.edu"], "title": "Fabric Interfaces", "modified": "2016-12-05T00:17:11.788Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-493", "groups": ["opera-of-the-future", "toys-of-tomorrow", "counter-intelligence"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "fabric-interfaces"}, {"website": "", "description": "<p>Facilitorials are tutorials that try to act more like a facilitator. By mimicking a real facilitator, facilitorials end up prioritizing a healthy learning environment over maximal transfer of information. Facilitorials wait to be pulled rather than pushing themselves, and when they're pulled they try to communicate by \"doing\" more often than by \"saying.\" We are focusing especially on video facilitorials for the Scratch  programming language.</p>", "people": ["silver@media.mit.edu", "mres@media.mit.edu"], "title": "Facilitorials", "modified": "2016-12-05T00:17:11.942Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "Cube", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "facilitorials"}, {"website": "", "description": "<p>\"Progress and catastrophe are the opposite faces of the same coin.\ufffd (Hannah Arendt, 1965) This research focuses on our perception of technology as both the source of our vulnerabilities and the answer to all of our problems. Large-scale, real-time failure has become a prominent feature of the late 20th century, and fear of it has inaugurated the 21st century. Technology can only be as perfect, precise, and efficient as the weakest people who design, maintain, and run a given system. New, improved techniques are often discovered by accident, or upon the failure of an intended process. This project will conduct an autopsy on an historic technological disaster case, examining the build-up to the accident. The object of the experimental artwork is to examine the inevitability of accidents and highlight to the viewer that risk is intrinsic to our world and that technological disaster has become integral in our lives in the 21st century.</p>", "people": ["csik@media.mit.edu"], "title": "Failsafe", "modified": "2016-12-05T00:17:11.968Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-020C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "failsafe"}, {"website": "", "description": "<p>FakeID is an OpenID server that gives you control over your online identity. With your existing OpenID account, you can use FakeID to create unique online identities.</p>", "people": [], "title": "FakeID", "modified": "2016-12-05T00:17:11.999Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "fakeid"}, {"website": "", "description": "<p>Figments is a theatrical performance that tells a story inspired by a variety of source texts, including Dante Alighieri's prosimetrum La Vita Nuova. Framed by a woman's accidental discovery of the compelling journals of the Dante-archetype, three inner vignettes reveal the timeless tribulations of the memoir's author(s). Figments was created using Media Scores, a framework in development to facilitate the composition of Gesamtkunstwerk using parametric score-like visual notation. The Media Score for Figments is realized in this production through the performance of actors, light, visuals, and the generation of musical accompaniment in response to the expressive qualities represented in the score. The score served as a reference during the creation and design of the piece, a guide during rehearsals, and as show control for the final production.</p>", "people": ["patorpey@media.mit.edu", "tod@media.mit.edu"], "title": "Figments", "modified": "2016-12-05T00:17:12.141Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "figments"}, {"website": "", "description": "<p>At present, luminous efficacy and cost remain the greatest barriers to broad adoption of LED lighting. However, it is anticipated that within several years, these challenges will be overcome. While we may think our basic lighting needs have been met, this technology offers many more opportunities than just energy efficiency: this research attempts to alter our expectations for lighting and cast aside our assumptions about control and performance. We will introduce new, low-cost sensing modalities that are attuned to human factors such as user context, circadian rhythms, or productivity, and integrate these data with atypical environmental factors to move beyond traditional lux measurements. To research and study these themes, we are focusing on the development of superior color-rendering systems, new power topologies for LED control, and low-cost multimodal sensor networks to monitor the lighting network as well as the environment.</p>", "people": ["joep@media.mit.edu", "maldrich@media.mit.edu", "nanzhao@media.mit.edu"], "title": "Feedback Controlled Solid State Lighting", "modified": "2016-12-05T00:17:12.176Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "feedback-controlled-solid-state-lighting"}, {"website": "", "description": "<p>The dream of connecting everything to everything else presupposes a network infrastructure up to the task. Traditionally, connecting a device to such a network has required a whole computer, often costing orders of magnitude more than the devices actually needed. Connecting lightbulbs to toasters to stereos to telephones requires a small and inexpensive network interface. Filament is such a device: it is a serial-to-ethernet interface on a single board three inches long and an inch and a half wide. With Filament, a wide range of devices can be easily connected to a local network, and even to the Internet.</p>", "people": ["neilg@media.mit.edu"], "title": "Filament", "modified": "2016-12-05T00:17:12.209Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-022", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "filament"}, {"website": "", "description": "<p>The Musical Fireflies are digital rhythm toys that introduce mathematical concepts in music such as beat, accent, mono- and polyrhythm without requiring users to have any prior knowledge of music theory or instruction. Through simple controls, the Fireflies allow users to input rhythmical patterns and embellish them in real time. The toys' wireless communication system allows players to synchronize these patterns and trade instrument sounds with other players. Since this interaction increases the richness and complexity of the game, the Fireflies also motivate collaboration and social play.</p>", "people": ["tod@media.mit.edu"], "title": "Fireflies", "modified": "2016-12-05T00:17:12.296Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-491", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "fireflies"}, {"website": "", "description": "<p>We developed a robotics programming language, with the goal of empowering children to construct more sophisticated behaviors for their robots, and to learn more by doing so. After initial experiments with Flogo I, a visual language using icons, boxes, and wires to represent real-time dataflow relationships, we developed Flogo II. In Flogo II, programs were written as text, but with two key innovations: (i) the text was \"live:\" it revealed its activity as it runs, and it could be edited while running; (ii) Flogo II's programming paradigm integrated two approaches to programming: a program as a sequence of instructions, and a program as a collection of dynamic rules and relationships. In current piloting work with children, we refined the design of Flogo II, explored the learning challenges for children posed by robotics programming, and began to design higher-level behavior construction tools that could be embedded within the language.</p>", "people": ["mres@media.mit.edu"], "title": "Flogo: Robotics Programming for Children", "modified": "2016-12-05T00:17:12.451Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "flogo-robotics-programming-for-children"}, {"website": "", "description": "<p>\"Folk Computing\" supported face-to-face communication and community, modeled on the communicative process of folklore. Although an abundance of research on technologies that support community existed, very little of it focused on co-present communities whose dominant means of interacting is face-to-face. Folk Computing research included earlier work on computationally augmented name tags for conferences, and work on toys that let children create, trade, and track their own digital playthings.</p>", "people": ["borovoy@media.mit.edu", "mres@media.mit.edu", "bss@media.mit.edu"], "title": "Folk Computing", "modified": "2016-12-05T00:17:12.499Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2001-09-01", "slug": "folk-computing"}, {"website": "", "description": "<p>Sometimes learners have to focus while experiencing strong emotions (e.g., family problems).\ufffdThey may also face challenges in perservering when encountering repeated failures in problem solving. The ability to know what one is feeling (e.g., worried, frustrated) and rise above it and handle the situation productively involves meta-affective skills. With such skills, a learner feeling \"I can't do this; I want to quit,\" might instead think, \"I am frustrated, but this is OK\ufffdit happens to experts. I should look for a different way to solve this.\"\ufffdThis research develops theory and technology to help learners develop meta-affective skills. Two recent achievements are development of (1) a technology with machine \"common-sense\" emotion\ufffdreasoning for enabling teenage girls to reflect on emotions in stories that they've constructed and improve their affect awareness; and (2) a technology to help students become stronger learners even when they feel like quitting.</p>", "people": ["picard@media.mit.edu", "win@media.mit.edu"], "title": "Fostering Affect Awareness and Regulation in Learning", "modified": "2016-12-05T00:17:12.588Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "fostering-affect-awareness-and-regulation-in-learning"}, {"website": "", "description": "<p>This checklist is designed to help projects that include an element of data collection to develop appropriate consent policies and practices. The checklist can be especially useful for projects that use digital or mobile tools to collect, store, or publish data, yet understand the importance of seeking the informed consent of individuals involved (the data subjects). This checklist does not address the additional considerations necessary when obtaining the consent of groups or communities, nor how to approach consent in situations where there is no connection to the data subject. This checklist is intended for use by project coordinators, and can ground conversations with management and project staff in order to identify risks and mitigation strategies during project design or implementation. It should ideally be used with the input of data subjects.</p>", "people": ["ethanz@media.mit.edu", "bl00@media.mit.edu"], "title": "Framework for Consent Policies", "modified": "2016-12-05T00:17:12.642Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "framework-for-consent-policies"}, {"website": "", "description": "<p>Freedom Flies is an inexpensive, open-sourced Unmanned Aerial Vehicle for free press and human rights applications. Made from off-the-shelf parts (bicycle rims, weed whacker, water bottle) it can carry a payload of 30 pounds for over three hours, surveying locations off-limits or too dangerous for journalists.</p>", "people": ["csik@media.mit.edu"], "title": "Freedom Flies", "modified": "2016-12-05T00:17:12.667Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-001", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "freedom-flies"}, {"website": "", "description": "<p>Light field camera have inherent trade offs between spatial and angular resolution. We mathematically model the image capture in light field cameras in spatial domain and prove that full resolution light field capture is possible using the heterodyne camera. We demonstrate refocusing a d full resolution imaging results using heterodyne light field camera.</p>", "people": ["raskar@media.mit.edu"], "title": "Full Resolution Lightfields using Heterodyne Camera", "modified": "2016-12-05T00:17:12.758Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "full-resolution-lightfields-using-heterodyne-camera"}, {"website": "", "description": "<p>Funk2 is a novel process-description language that keeps track of everything that it does. Remembering these causal execution traces allows parallel threads to reflect, recognize, and react to the history and status of other threads. Novel forms of complex, adaptive, nonlinear control algorithms can be written in the Funk2 programming language. Currently, Funk2 is implemented to take advantage of distributed grid processors consisting of a heterogeneous network of computers, so that hundreds of thousands of parallel threads can be run concurrently, each using many gigabytes of memory. Funk2 is inspired by Marvin Minsky's Critic-Selector theory of human cognitive reflection.</p>", "people": ["joep@media.mit.edu"], "title": "Funk2: Causal Reflective Programming", "modified": "2016-12-05T00:17:12.802Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-351", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "funk2-causal-reflective-programming"}, {"website": "", "description": "<p>The Future Music Blender (FMB) is a totally new part of the Brain Opera, which was permanently installed in Vienna in summer 2000. The FMB is the culminating experience of the Brain Opera. Visitors will enter a second room after exploring the Brain Opera's Mind Forest. In this FMB room, they will listen to multiple music and sound samples, culled from the Mind Forest, the Internet, and a pre-processor database. Selected sounds represented as ID-tagged tokens will be submitted to a central \"blender\" sculpture, which will read the ID and immediately incorporate the selected sound into an active performance database. A specially adapted Sensor Chair, with a Multi-Modal Mixer (MMM) for extra left-hand control, will allow one user at a time to select sounds from the active database (organized spatially according to perceptually salient characteristics), to create musical collages by waving a hand in the air, and to mix or \"blend\" these sounds with MMM gestures and commands. A generative algorithm will analyze the sound collage playing at any given moment, and supplement it with an appropriate musical accompaniment. Thus, found sounds will be turned into beautiful music.</p>", "people": ["tod@media.mit.edu"], "title": "Future Music Blender", "modified": "2016-12-05T00:17:12.821Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "future-music-blender"}, {"website": "", "description": "<p>GalvaPhone is the expansion of EMotoPhone, an earlier project in the Speech Interfaces group. EMotoPhone allows cell-phone users to choose a face manually, representing their current emotion, and send it over a cell phone to another user. The recipient of the EMotoPhone face can then decide whether to accept a phone call from the sender. With GalvaPhone, information about a user's electrodermal activity (also known as the galvanic skin response) is also sent, in addition to the user's self-selected emotion. The electrodermal activity measure is determined by a galvactivator-a glove that measures skin conductivity.</p>", "people": ["geek@media.mit.edu"], "title": "GalvaPhone", "modified": "2016-12-05T00:17:12.896Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-384C", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "galvaphone"}, {"website": "", "description": "<p>Today's digital typography is more or less based on the format defined by movable type, with only a few parametric innovations. It has gone a long way, and surmounted many obstacles, to have similar visual features as printed type. At the same time, people are writing less by hand, and the ancient art of handwriting\ufffdwhich was used and necessary to express many personal attributes\ufffdis dying. We are exploring ways to keep written, personal expression a part of our digital life, as well as methods for our personal communication to continue to inherently express our personality, status, and current emotions. For example, in our digital lives, what will a signature look like, or a quickly scribbled love note? We are working with a new typographic format and experimenting with input devices inherent to human expression, such as gesture, voice, and body motion, to address these questions.</p>", "people": ["dsmall@media.mit.edu"], "title": "Giving Character to Characters", "modified": "2016-12-05T00:17:13.090Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-301", "groups": ["design-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "giving-character-to-characters"}, {"website": "", "description": "<p>This research is about finding ways to understand dynamic sources of information. Network usage patterns and computational processes are being explored through representations that focus on extracting the qualitative features from these very large data sets. The current focus of this work is in genomics and bioinformatics. In addition to the standard computer screen, alternative display devices are being studied, including handheld computers and large format printing.</p>", "people": [], "title": "Genomic Cartography", "modified": "2016-12-05T00:17:12.944Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-301", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "genomic-cartography"}, {"website": "", "description": "<p>We are developing gestural interaction techniques and applications using the g-speak platform from Oblong Industries. This work seeks to blend the value of physical, tangible interfaces with the power of gestural interaction. Our goal is to create seamless interfaces that scale and adapt between ambulatory and manipulatory, physical and graphical, ambient and direct, and representational and abstract\ufffdinterfaces that can break the boundaries of existing paradigms.</p>", "people": ["daniell@media.mit.edu", "ishii@media.mit.edu", "labrune@media.mit.edu"], "title": "Gestural Interaction", "modified": "2016-12-05T00:17:12.969Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-368", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "gestural-interaction"}, {"website": "", "description": "<p>Emotions are often conveyed through gesture. Instruments that respond to gestures offer musicians new, exciting modes of musical expression. This project gives musicians wireless, gestural-based control over guitar effects parameters. </p>", "people": ["rmorris@media.mit.edu", "picard@media.mit.edu", "tod@media.mit.edu"], "title": "Gesture Guitar", "modified": "2016-12-05T00:17:12.999Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-450", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "gesture-guitar"}, {"website": "", "description": "<p>In this research, a proactive emotional health system, geared toward supporting emotional self-awareness and empathy, was built as a part of a long-term research plan for understanding the role digital technology can play in helping people to reflect on their beliefs, attitudes, and values. The system, G.I.R.L.S. (Girls Involved in Real-Life Sharing), allows users to reflect actively upon the emotions related to their situations through the construction of pictorial narratives. The system employs common-sense reasoning to infer affective content from the users' stories and support emotional reflection. Users of this new system were able to gain new knowledge and understanding about themselves and others through the exploration of authentic and personal experiences. Currently, the project is being turned into an online system for use by school counselors.</p>", "people": ["picard@media.mit.edu"], "title": "Girls Involved in Real-Life Sharing", "modified": "2016-12-05T00:17:13.033Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "girls-involved-in-real-life-sharing"}, {"website": "", "description": "<p> This project is an effort to develop computer vision methodologies to perform behavioral evaluation on dyadic interactions grounded in longitudinal video recordings. The proposed system can be applied on any surveillance output, to analyze or recover event dynamics hidden on multi-agent interactions. Applications include analyzing word-learning dynamics in young children, new kinds of video retail analysis aimed at customer-associate interactions, determining dominant agents in a group, and establishing longitudinal diagnostic means for child developmental disorders.</p>", "people": ["tsourk@media.mit.edu", "dkroy@media.mit.edu"], "title": "Gestalt Video Analyzer", "modified": "2016-12-05T00:17:13.059Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "gestalt-video-analyzer"}, {"website": "", "description": "<p>The GoGo board provides a simple interface for computers to interact with their surrounding environments. Projects using the GoGo board include various sensing and control applications, games, and environmental sensing. It allows students to be engaged in the design and implementation of solutions to a problem by augmenting locally found, low-cost materials with digital technology. The GoGo board has been designed and used in schools in S\ufffdo Paulo, and a network of teachers, students, and other users of the GoGo board is being created. The simplicity of the GoGo board design opens up possibilities for learners to create or modify the board for their particular needs. The research goal is to study the factors, both technical and social, needed to support communities of not-so-technical people to participate in the process of designing the tools that they want.</p>", "people": ["cavallo@media.mit.edu"], "title": "GoGo Board: A Personalized Technology Toolset for Learning", "modified": "2016-12-05T00:17:13.299Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-443C", "groups": [], "published": true, "active": false, "end_on": "2001-01-01", "slug": "gogo-board-a-personalized-technology-toolset-for-learning"}, {"website": "", "description": "<p>A novice search-engine user may find searching the Web for information difficult and frustrating because he may naturally express search goals rather than the topic keywords search engines need. GOOSE (Goal-Oriented Search Engine Interface) is an adaptive search-engine interface that uses natural-language processing to parse a user\ufffds search goal, and common-sense reasoning to interpret this goal and reason from it an effective query. For a source of common-sense knowledge, we use the Open Mind Common Sense Project. While we cannot be assured of the robustness of the common-sense inference, in a substantial number of cases GOOSE is more likely to satisfy the user's original search goals than simple keywords or conventional query.</p>", "people": ["lieber@media.mit.edu"], "title": "Goal-Oriented Web Search User Interfaces", "modified": "2016-12-05T00:17:13.244Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "goal-oriented-web-search-user-interfaces"}, {"website": "", "description": "<p>GoodApp is a cloud environment for the community development of socially responsible and transparent Web applications. It provides a shared infrastructure and pedagogy for the development of civic or \"good\" apps. This infrastructure includes educational components, developmental tools (editing, collaboration, versioning, and visualization), and simple mechanisms for deployment. Conceptually, we seek to understand the technical similarities of, and the sustainability challenges for, civic applications.</p>", "people": ["csik@media.mit.edu"], "title": "GoodApp", "modified": "2016-12-05T00:17:13.323Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "E15-001", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "goodapp"}, {"website": "", "description": "<p>GrandChair elicits, records, and plays back grandparents' stories within an interaction model based on face-to-face conversation. Its environment is designed to be comfortable and story-evoking: tellers sit in a comfortable rocking chair and tell stories with the assistance of an attentive child conversational agent.</p>", "people": [], "title": "GrandChair", "modified": "2016-12-05T00:17:13.365Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-320", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "grandchair"}, {"website": "", "description": "<p>We are working on learning agents that can automate semi-repetitive graphical procedures by watching a user perform the procedures on concrete visual examples. The agent is embedded in a graphical editor that records user actions, and uses machine learning techniques to generalize the procedure. The generated procedure can be used in new situations that are similar to, but not necessarily exactly the same as, the original. The user can give advice to the system by drawing graphical annotations that express information like part/whole relations, or can use speech recognition to explain actions verbally.</p>", "people": ["lieber@media.mit.edu"], "title": "Graphics by Example", "modified": "2016-12-05T00:17:13.406Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2003-12-30", "slug": "graphics-by-example"}, {"website": "", "description": "<p>Language is grounded in experience. Unlike dictionaries which define words in terms of other words, humans understand many basic words in terms of associations with sensory-motor experiences. People must interact physically with their world to grasp the essence of words like \"red,\" \"heavy,\" and \"above.\" Abstract words are acquired only in relation to more concretely grounded terms. Grounding is thus a fundamental aspect of spoken language, which enables humans to acquire and to use words and sentences in context. We are developing an interactive robot which learns and understands spoken langauge via multisensory grounding and robotic embodiment. The robot is designed with six degrees of freedom and has auditory, visual, proprioceptive, tactile, and balance sensors. This system will serve as a test bed for experiments in acquiring and understanding elementary semantics and syntax of spoken language. Our goals are two-fold. First, we are interested in using computational models to gain insights into how humans process language. By building and testing models with realistic data, we are able to test theories which are difficult to assess using traditional methods based on observation and analysis. Second, we hope to build a new generation of spoken language interfaces with richer semantic representations leading to more intelligent machine behavior.</p>", "people": ["dkroy@media.mit.edu"], "title": "Grounded Language Learning and Understanding", "modified": "2016-12-05T00:17:13.505Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-384", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "grounded-language-learning-and-understanding"}, {"website": "", "description": "<p>The tongue has extremely dense sensing resolution, as well as an extraordinary degree of neuroplasticity\ufffdthe ability to adapt to and internalize new input. Research has shown that electro-tactile tongue displays paired with cameras can be used as vision prosthetics for the blind or visually impaired; users quickly learn to read and navigate through natural environments, and many describe the signals as an innate sense. However, existing displays are expensive and difficult to adapt. Tongueduino is an inexpensive, vinyl-cut tongue display designed to interface with many types of sensors besides cameras. Connected to a magnetometer, for example, the system provides a user with an internal sense of direction, like a migratory bird. Plugged into weighted piezo whiskers, a user can sense orientation, wind, and the lightest touch. Through Tongueduino, we hope to bring electro-tactile sensory substitution beyond vision replacement, towards open-ended sensory augmentation.</p>", "people": ["gershon@media.mit.edu", "joep@media.mit.edu"], "title": "Hackable, High-Bandwidth Sensory Augmentation", "modified": "2016-12-05T00:17:13.691Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "hackable-high-bandwidth-sensory-augmentation"}, {"website": "https://feedback.davidbramsay.com", "description": "<p>GroupLoop is a browser-based, collaborative audio feedback control system for musical performance. Upon logging in, GroupLoop users send their microphone stream to other participants while simultaneously controlling the mix of other users' streams played through their speakers. Complex feedback loops involving several participants are possible by working together\u2014in some cases, multiple feedback paths may overlap and interact. Users are able to shape the feedback sounds in real time by adjusting delay, EQ, and gain, as well as by manipulating the acoustics of their portion of the audio feedback path.&nbsp;</p><p>GroupLoop is capable of diverse and unexpected sounds, immeasurable reconfigurability, and in some cases, unrepeatable complexity. It creates new topologies for collaboration in performance, and invites thoughtful reflection on future topologies for real-time music collaboration over distance.</p><p>Try it at https://feedback.davidbramsay.com<span style=\"font-size: 18px; font-weight: normal;\">.</span></p>", "people": ["dramsay@media.mit.edu", "joep@media.mit.edu"], "title": "GroupLoop: A Collaborative, Network-Enabled Audio Feedback Instrument", "modified": "2016-12-05T00:17:13.639Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "grouploop-a-collaborative-network-enabled-audio-feedback-instrument"}, {"website": "", "description": "<p>This project experiments with combining spatially and metrically registered holographic and force images. A user may either inspect or carve an image, using a handheld device, while observing and feeling resulting changes in the underlying model geometry.</p>", "people": ["wjp@media.mit.edu", "vmb@media.mit.edu"], "title": "Haptic Holography", "modified": "2016-12-05T00:17:13.730Z", "visibility": "LAB", "start_on": "2000-01-01", "location": "E15-044A", "groups": [], "published": true, "active": false, "end_on": "2004-09-01", "slug": "haptic-holography"}, {"website": "", "description": "<p>\n                    We have developed a set of \"optogenetic\" reagents: fully genetically encoded reagents that, when targeted to specific cells, enable their physiology to be controlled via light. To confront the three-dimensional complexity of the living brain, enabling the analysis of the circuits that causally drive or support specific neural computations and behaviors, our lab and our collaborators have developed hardware for delivery of light into the brain, enabling control of complexly shaped neural circuits, as well as the ability to combinatorially activate and silence neural activity in distributed neural circuits. We anticipate that these tools will enable the systematic analysis of the brain circuits that mechanistically and causally contribute to specific behaviors and pathologies. We distribute these tools as freely as possible, and host visitors regularly to learn how to use them .\n                </p>", "people": ["esb@media.mit.edu", "azorzos@media.mit.edu"], "title": "Optogenetics: Hardware Enabling Neural Control by Light", "modified": "2016-12-05T00:17:13.769Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-435", "groups": ["synthetic-neurobiology"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "hardware-and-systems-for-control-of-neural-circuits-with-light"}, {"website": "", "description": "<p>In 2002, The New York Metropolitan Transit Authority understood that an alert public is an important asset in protecting the peace. The city was given tools of agency with the security campaign, If You See Something Say Something. Inspired by the 1,944 New Yorkers who saw something and said something, Hero Reports asks citizens to report not just the suspicious activity, but also the civic courage that also protects the peace.  These everyday acts of heroism serve are organized and mapped to provide economy to the difference we make to others, and the difference others make to us.</p>", "people": ["pattie@media.mit.edu"], "title": "Hero Reports: Mapping Civic Courage", "modified": "2016-12-05T00:17:13.799Z", "visibility": "LAB", "start_on": "2008-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2008-01-01", "slug": "hero-reports-mapping-civic-courage"}, {"website": "", "description": "<p>This effort has resulted in a compact, wireless, low-power sensor card that transmits 8 analog sensor channels (4 are conditioned for pressure measurements), plus bi-directional bend, 2-axis low-G acceleration (tilt), 3-axis high-G acceleration (shock), 3-axis DC magnetic field strength (orientation), angular rate about vertical (spin), height above an active floor, and translational position as derived from a sonar pickup. The current system runs at 20 kbits/sec, and is able to provide full 8-bit state updates across the wireless link (up to 100 meters away) at 50 Hz. We have instrumented a dance sneaker with this device to measure many parameters of foot, sole, and toe expression, continuously broadcasting them to a base-station and PC over a wireless link. Our current demonstrations allow dancers and athletes to directly produce musical streams from their performances. Ongoing work explores the identification of gait characteristics from the sensor data in applications that reach beyond interactive performance, encompassing digital athletic coaching, podiatric therapy, and context extraction for wearable computers.</p>", "people": ["joep@media.mit.edu"], "title": "High-Density Wireless Sensing for Expressive Footwear", "modified": "2016-12-05T00:17:13.828Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-357", "groups": ["responsive-environments", "health", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "high-density-wireless-sensing-for-expressive-footwear"}, {"website": "", "description": "<p>Honey I'm Home is the simplest smell output device possible: a one-bit system that can be used for any purpose. We present it as a presence-awareness device: a quick spray of a favorite scent or perfume lets you know when a loved one is thinking of you. However, we also present the device as a simple toolkit for introducing smell into other applications.</p>", "people": ["mike@media.mit.edu"], "title": "Honey I'm Home", "modified": "2016-12-05T00:17:13.955Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-068", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "honey-im-home"}, {"website": "", "description": "<p>In a series of psychometric experiments, we tested subjects' perception of lighting in a virtual environment to assess the possibility of describing and subsequently controlling lighting in a dimension other than brightness. Our findings suggest that human perception of lighting is also explained by variables other than brightness. These data are used to design a lighting control system that simultaneously maps the spatial and visual characteristics of the room into a more natural and intuitive form of control. </p>", "people": ["joep@media.mit.edu", "maldrich@media.mit.edu", "nanzhao@media.mit.edu"], "title": "Human Factors and Lighting", "modified": "2016-12-05T00:17:14.129Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2014-09-01", "slug": "human-factors-and-lighting"}, {"website": "", "description": "<p>In the Renaissance, kings rarely traveled to find a bride: instead, an artist was sent to paint her likeness. These artisans were often not the official court painter but lesser-known painters who also illuminated manuscripts as their highly refined trade. One particular Netherlandish family established a workshop serving British monarch Henry VIII, producing miniature portraits called \"Limnings\" and setting a trend that would continue in England and on the Continent for centuries. For today's celebrated personages, \"Digital Limnings\" are also miniature memories wherein the past is stored in the present.</p>", "people": ["judith@media.mit.edu", "monster@media.mit.edu"], "title": "ID/entity: Digital Limnings", "modified": "2016-12-05T00:17:14.408Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "identity-digital-limnings"}, {"website": "", "description": "<p>This project creates a synthesis engine that predicts the timbre of arbitrary acoustic instruments. Musically meaningful, it is controlled by the perceptual features extracted from the audio stream of an acoustic or electric violin in real-time. The timbre models are built from the analysis of pitch, loudness, brightness, and the instantaneous power spectrum of real recordings. Although highly compressed, the sound quality is fully preserved. Additionally, the playability remains smooth, expressive, predictable, and adequate for sound morphing.</p>", "people": ["tristan@media.mit.edu", "tod@media.mit.edu"], "title": "Hyperviolin: Real-Time Timbre Analysis/Synthesis", "modified": "2016-12-05T00:17:14.299Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "hyperviolin-real-time-timbre-analysissynthesis"}, {"website": "", "description": "<p>When it comes to designing wearable computers creators must play within the constraints of what is socially acceptable, or risk limiting the technology to a small audience.  With this in mind we endeavour to incorporate sensor networks into ordinary headphones, and use the information garnered from this network to reason about a user's state.  We are also developing software that will support the explicit and implicit interactions supported by I/O Earphones.</p>", "people": [], "title": "I/O Earphones", "modified": "2016-12-05T00:17:14.318Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "--Choose Location", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "io-earphones"}, {"website": "", "description": "<p>Alternative Autobiographies explores Richard Kostelanetz's long-time interest in describing a life (in this case, his own) in books and electronic media in ways other than chronological, continuous narrative. By bringing these elements together as never before, to be read, projected, heard, and seen, a truth and completeness is (perhaps) realized that was impossible in continuous prose.</p>", "people": ["judith@media.mit.edu"], "title": "ID/entity: Alternative Autobiographies", "modified": "2016-12-05T00:17:14.340Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "identity-alternative-autobiographies"}, {"website": "", "description": "<p>The makings of identities: personal identity is guided by an invisible apparatus. This apparatus is ever-changing. Surveillance data is continuously being gathered about each of us as we move about in the world. From grocery tabs and credit card transactions to cameras in highway tollbooths and face recognition systems on telephone poles in major cities, we leave ever-more coherent sets of traces. Unknowingly. Perceptions and presentations of selves are assemblages continuously updated as one interacts with people and things. There is not such a thing as a being; there are only projections, interpretations, idealized memories, idolized constructions. A person exists through autorecollections, and knowingly surrounds herself with self-defining tokens. Autotopographies. A pulsed laser hologram takes hours to set up but only six nanoseconds to shoot\ufffda portrait capturing self-conscious poses while revealing details more minute than could ever be planned. Bacteria on the skin and the location of every tiny hair. Digital voyeurism. How do we respond to these unsolicited incursions?</p>", "people": ["judith@media.mit.edu", "monster@media.mit.edu"], "title": "ID/entity: Strangers to Ourselves", "modified": "2016-12-05T00:17:14.438Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "identity-strangers-to-ourselves"}, {"website": "", "description": "<p>Memory is elusive and the need to hold onto its origins is always fleeting. This installation is designed as a stage in which the viewer can become an actor. Triggered by the viewer's approach, the mirrored reflection of the painter transforms into the turning of his back. Enacting the metaphor of an uneasy marriage, Van Eyck's Mirror becomes a signifier of the divorce we, in the present, experience with the distant past.</p>", "people": ["judith@media.mit.edu", "stefan@media.mit.edu"], "title": "ID/entity: Van Eyck's Mirror", "modified": "2016-12-05T00:17:14.462Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-468", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "identity-van-eycks-mirror"}, {"website": "", "description": "<p>The IDEAS (Innovative Design Experiences After School) Institute is an innovative professional-development program for people working at after-school centers in low-income communities. Our goal is to create a network of people committed to deepening the learning experiences of youth through creative uses of new technologies.</p>", "people": ["mres@media.mit.edu", "nrusk@media.mit.edu"], "title": "IDEAS Institute", "modified": "2016-12-05T00:17:14.525Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "Cube", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "ideas-institute"}, {"website": "", "description": "<p>Imagine holding an animated pixel in the palm of your hand. What could you create with physical pixels that fit together like children's blocks? Envision a video display that is no longer rigid and flat, but any shape and size you wish to build. The Physical Pixel Project is developing a system of tangible, interactive pixels that enables a user to build dynamic color in a sculptural form. The goal is to move computer graphics off of the screen and into the physical world.</p>", "people": ["mike@media.mit.edu"], "title": "Imaging in 3P: The Physical Pixel Project", "modified": "2016-12-05T00:17:14.573Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "E15-468", "groups": ["personal-information-architecture", "toys-of-tomorrow"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "imaging-in-3p-the-physical-pixel-project"}, {"website": "", "description": "<p>Immersion is a visual data experiment that delivers a fresh perspective of your email inbox. Focusing on a people-centric approach rather than the content of the emails, Immersion brings into view an important personal insight\ufffdthe network of people you are connected to via email, and how it evolves over the course of many years. Given that this experiment deals with data that is extremely private, it is worthwhile to note that when given secure access to your Gmail inbox (which you can revoke any time), Immersion only uses data from email headers and not a single word of any email's subject or body content.</p>", "people": ["hidalgo@media.mit.edu"], "title": "Immersion", "modified": "2016-12-05T00:17:14.613Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2016-10-22", "slug": "immersion"}, {"website": "", "description": "<p>We are identifying people's activities while browsing the Web solely by analyzing their mouse movement behavior. We use machine-learning algorithms to develop systems that predict user activities and user interest using algorithms that correlate mouse tracks and implicit metrics for activity and interest. Clustering low-level mouse data into a relatively small set of features is vital in tracking people\ufffds behaviors as they occur. Our systems offer a trade-off between model, feature, and computational complexity to be suitable for online usage.</p>", "people": [], "title": "Implicit Metric of Interest and Attention", "modified": "2016-12-05T00:17:14.649Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "implicit-metric-of-interest-and-attention"}, {"website": "", "description": "<p>One of the defining characteristics of online auctions is the information bidders can get from other bidders during the process. In the current project, we are trying to understand how bidders make inferences, what the inferences are based upon, and how accurate their inferences are.</p>", "people": [], "title": "Inferring Values from Others in Online Auctions", "modified": "2016-12-05T00:17:14.830Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "--Choose Location", "groups": ["erationality"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "inferring-values-from-others-in-online-auctions"}, {"website": "", "description": "<p>The wonder that occurs while watching a good magic trick or admiring a gorgeous natural vista is a strong emotion that has not been well studied. Educators, media producers, entertainers, scientists and magicians could all benefit from a more robust understanding of wonder. A new model was developed, and an experiment was conducted to investigate how several variables affect how magic tricks are enjoyed. The experiment showed 70 subjects 10 videos of magic while recording their responses and reactions to the tricks. Some individuals were shown the explanations to the magic tricks to gauge their impact on enjoyment. The style of the presentation was varied between two groups to compare the effect of magic presented as a story to magic presented as a puzzle. Presentation style has an effect on magic enthusiasts' enjoyment and a story-oriented presentation is associated with individuals being more generous towards a charity. </p>", "people": ["picard@media.mit.edu"], "title": "In Search of Wonder: Measuring Our Response to the Miraculous", "modified": "2016-12-05T00:17:14.706Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-448", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "in-search-of-wonder-measuring-our-response-to-the-miraculous"}, {"website": "", "description": "<p>This project addresses the question of how we can build a new style of telephone-like communications system that uses data shared among all of the users to make an architecture for new services. The system is made up of an infrastructure that facilitates the acquisition and exchange of user context information called Sensorama, and an IP-based telephony application based on this infrastructure.</p>", "people": ["lip@media.mit.edu"], "title": "inCall: Exploring the Role of Context in Communication", "modified": "2016-12-05T00:17:14.737Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-483", "groups": ["media-and-networks"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "incall-exploring-the-role-of-context-in-communication"}, {"website": "", "description": "<p>We have been developing comfortable, safe, attractive physiological sensors that infants can wear around the clock to wirelessly communicate their internal physiological state changes. The sensors capture sympathetic nervous system arousal, temperature, physical activity, and other physiological indications that can be processed to signal changes in sleep, arousal, discomfort or distress, all of which are important for helping parents better understand the internal state of their child and what things stress or soothe their baby. The technology can also be used to collect physiological and circadian patterns of data in infants at risk for developmental disabilities.</p>", "people": ["fletcher@media.mit.edu", "mgoodwin@media.mit.edu", "picard@media.mit.edu"], "title": "Infant Monitoring and Communication", "modified": "2016-12-05T00:17:14.754Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "infant-monitoring-and-communication"}, {"website": "", "description": "<p>We are developing a set of very low-cost, wireless, wearable sensors that enable a large group of people (e.g., hundreds or thousands) to participate in an interactive musical performance. The sensors themselves are simple piezoelectric accelerometers that detect extremes of limb motion, upon which they transmit a narrow RF pulse. Although one can distinguish between sensors on the upper and lower body by using a different carrier frequency, and zone the locations of activity roughly through carrier strength, we do not plan to independently ID each performer, but instead to measure and react to the characteristics of ensemble behavior. We have built this system and have developed algorithms that use this data to explore techniques of mapping large-group, real-time musical interaction.</p>", "people": ["geppetto@media.mit.edu", "joep@media.mit.edu"], "title": "Inexpensive Wearable Sensors for Large-Crowd Interaction", "modified": "2016-12-05T00:17:14.811Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-441", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "inexpensive-wearable-sensors-for-large-crowd-interaction"}, {"website": "", "description": "<p>Much of information visualization is done, as the name would imply, visually. While research has looked into haptic feedback to help humans \ufffdfeel\ufffd their way through information or interfaces, very little research has looked at the ways that smell can provide us with information or lead to user actions, outside of a replication of familiar smells. InfoSmell looks at how we can use our sense of smell to notify, indicate, or even persuade users, introducing a limited language of unique smells associated with specific information such as email, blogs, or news.</p>", "people": ["holtzman@media.mit.edu"], "title": "InfoSmell: Smell Your Data", "modified": "2016-12-05T00:17:14.854Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "infosmell-smell-your-data"}, {"website": "", "description": "<p>The purpose of the INNER-active Journal system is to provide a way for users to reconstruct their emotions around events in their lives, and to see how recall of these events affects their physiology. Expressive writing, a task in which the participant is asked to write about extremely emotional events, is presented as a means towards story construction. Previous use of expressive writing has shown profound benefits for both psychological and physical health. In this system, measures of skin conductivity, instantaneous heart rate, and heart stress entropy are used as indicators of activities occurring in the body. Users have the ability to view these signals after taking part in an expressive writing task.</p>", "people": ["picard@media.mit.edu"], "title": "INNER-active Journal", "modified": "2016-12-05T00:17:14.956Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "inner-active-journal"}, {"website": "", "description": "<p>Using the 300M IT Edition as a research platform, this project aims to develop \"aerospace systems\" that are applicable to cars. In cooperation with Professor R. John Hansman from the MIT Department of Aeronautics/Astronautics, there will be a comparative analysis of the impact of information technology on aircraft and automobile cockpits, including the identification of key human factors. After defining possible aerospace systems applicable to automobiles, this project will implement two new devices, based on preliminary studies, into the 300M. The Standby device includes an investigation of the driver's stress level. The Warning device assists the driver with warning and maintenance messages obtained from the car's network bus. Depending on the state of the Standby device, some non-essential warnings could be suppressed, so as not to distract the driver.</p>", "people": [], "title": "Insights from Aerospace: Transitions into Automobile Cockpits", "modified": "2016-12-05T00:17:15.004Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-441", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "insights-from-aerospace-transitions-into-automobile-cockpits"}, {"website": "", "description": "<p>Developing biologically inspired robotic prostheses necessitates precise understanding of the dynamic interaction between amputee, prosthesis, and the environment they act on. In our research, we are instrumenting a biomimetic ankle-foot powered prosthesis prototype with a series of sensory units to estimate the ground reaction forces (GRF) and zero moment point (ZMP) trajectory. The incorporation of this sensory information with a morphologically realistic human model and basic feedback methods will contribute to the development of balance-control strategies in the mentioned device. These strategies will enhance amputees\ufffd perception and control of their dynamic stability. With this new generation of robotic ankle-foot prostheses, we are addressing some of the main difficulties that amputees encounter with current passive devices, including non-symmetric gait, increased walking energy cost, and appropriate maintenance of balance during standing and walking.</p>", "people": ["hherr@media.mit.edu"], "title": "Instrumentation of a Biomimetic Ankle-Foot Prosthesis", "modified": "2016-12-05T00:17:15.039Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-054", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "instrumentation-of-a-biomimetic-ankle-foot-prosthesis"}, {"website": "", "description": "<p>Technical documentation for hardware and software is expensive to produce, and often inaccurate and inadequate. We are exploring a new approach to producing technical documentation in which an expert interacts with a simulation of a device, and the system automatically produces both written English descriptions and visual illustrations.</p>", "people": ["lieber@media.mit.edu"], "title": "Intelligent Technical Documentation", "modified": "2016-12-05T00:17:15.069Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "intelligent-technical-documentation"}, {"website": "", "description": "<p>Modern music education program for children emphasizes the importance of\ncreating a rich music environment in which children become involved as creative and active musicians. However, seldom do we see a music machine learn in such an interactive way. This project aims to take hints from the developmental process of children's music ability and build a machine which learns to listen, perform, and create music in an interactive and developmental fashion. Several machines will receive specific music curriculum and thus the difference between their behavior in response to music can be observed and analyzed. In addition, these music machines will be natural interactive performers in concert with human performers.</p>", "people": ["bv@media.mit.edu"], "title": "Interactive Music-Learning Machine", "modified": "2016-12-05T00:17:15.149Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "interactive-music-learning-machine"}, {"website": "", "description": "<p>Plants are very common in our world and and contain a vast amount of information.  Although there are open debates about the intelligence of plants, it is undeniable that plants have a great ablity to do low-cost sensing and rudimentary communication. The electrophysiology of plants has sparked interest since the late 1800s, but this topic has not been explored recently in the context of modern information technology. We are building novel electronic sensors that \"take a peek\" inside the biological activity of living plants, and explore their use as sensors and thought-provoking educational tools for children and museum exhibits.</p>", "people": ["fletcher@media.mit.edu", "ishii@media.mit.edu"], "title": "Interfacing Electronics to Living Plants", "modified": "2016-12-05T00:17:15.172Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "interfacing-electronics-to-living-plants"}, {"website": "", "description": "<p>Radio news programs are most valuable when hands and eyes are busy, such as during the morning commute or while working in the kitchen. But radio news is presented in small segments and only at scheduled times. This project seeks to compile audio news from a number of sources, segment stories based on acoustic and possibly semantic cues, and present these on demand in an interactive environment, such as over the telephone or on a computer workstation.</p>", "people": ["geek@media.mit.edu"], "title": "Interactive Radio", "modified": "2016-12-05T00:17:15.223Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "interactive-radio"}, {"website": "", "description": "<p>How do high-level cognitive functions emerge from primitive neural computations to mediate complex human behavior?  We are developing precise, focal ways of investigating phenomena such as trust and risk-taking, in order to understand how they play roles in purchasing, decision-making, social interaction, and other real-world scenarios.</p>", "people": ["dkroy@media.mit.edu", "esb@media.mit.edu", "barbara@media.mit.edu", "aithpao@media.mit.edu"], "title": "Internomics", "modified": "2016-12-05T00:17:15.278Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-435", "groups": ["synthetic-neurobiology", "social-machines"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "internomics"}, {"website": "", "description": "<p>While it is increasingly easy to send data to geosynchronous orbit, getting  the data the last few feet to where it is needed remains cumbersome at best. By perturbing the average voltage of a body by a tiny amount, it is possible to send data through a person, creating a Personal Area Network. This lets familiar gestures be associated with logical meaning, such as exchanging business cards through a handshake, authenticating an identity by touching a doorknob, or downloading messages by picking up a telephone.</p>", "people": ["neilg@media.mit.edu"], "title": "Intrabody Signaling", "modified": "2016-12-05T00:17:15.343Z", "visibility": "PUBLIC", "start_on": "1995-12-31", "location": "", "groups": ["physics-and-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "intrabody-signaling"}, {"website": "", "description": "<p>Malleable user interfaces have the potential to enable radically new forms of interactions and expressiveness through flexible, free-form, and computationally controlled shapes and displays. This work specifically focuses on particle jamming as a simple, effective method for flexible, shape-changing user interfaces where programmatic control of material stiffness enables haptic feedback, deformation, tunable affordances, and control gain. We explore the design space of malleable and organic user interfaces enabled by jamming through four motivational prototypes that highlight jamming\ufffds potential in HCI, including applications for tabletops, tablets, and for portable shape-changing mobile devices.</p>", "people": ["daniell@media.mit.edu", "olwal@media.mit.edu", "ishii@media.mit.edu"], "title": "Jamming User Interfaces", "modified": "2016-12-05T00:17:15.508Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "jamming-user-interfaces"}, {"website": "", "description": "<p>Over the last two decades, digital technologies have flattened old hierarchies in the news business and opened the conversation to a multitude of new voices. To help comprehend this promising but chaotic new public sphere, we're building a \"social news machine\" that will provide a structured view of the place where journalism meets social media. The basis of our project is a two-headed data ingest. On one side, all the news published online 24/7 by a sample group of influential US media outlets. On the other, all Twitter comments of the journalists who produced the stories. The two streams will be joined through network analysis and algorithmic inference. In future work we plan to expand the analysis to include all the journalism produced by major news outlets and the overall public response on Twitter, shedding new light on such issues as bias, originality, credibility, and impact.</p>", "people": ["dkroy@media.mit.edu", "pernghwa@media.mit.edu", "mmv@media.mit.edu", "soph@media.mit.edu"], "title": "Journalism Mapping and Analytics Project (JMAP)", "modified": "2016-12-05T00:17:15.736Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2016-05-31", "slug": "journalism-mapping-and-analytics-project-jmap"}, {"website": "", "description": "<p>Language is inextricably linked to the activities and events that make up our daily lives. For a child learning language, everyday activities provide an important context for learning first words. This work builds on the corpus collected for the Human Speechome Project, the largest multimodal corpus of one child's early life, to explore how experience with language ties to space, time, and daily activity to support word learning. We use manual and fully automatic methods, ranging from direct annotation to computer vision and unsupervised latent variable approaches, to identify the abstract \"stuff of life\" that makes up early experience. We show how a word's contextual grounding predicts when it will be learned.</p>", "people": ["dkroy@media.mit.edu"], "title": "Language, Word Learning, and the Activity Substrate of Everyday Life", "modified": "2016-12-05T00:17:15.801Z", "visibility": "PUBLIC", "start_on": "2012-09-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "language-word-learning-and-the-activity-substrate-of-everyday-life"}, {"website": "", "description": "<p>LaserWho is large-scale gesture-based installation of the Visual Who project that uses the LaserWall for display and input. Users place anchors, each representing a different topic relevant to that community, on the screen. The names of people who are particularly drawn to a topic flow towards the associated anchor, and the resulting animation reveals the structure of the community in terms of shared interests and affiliations. Although LaserWho uses data visualization algorithms and techniques, the feel of this installation is very different than the typical analytic application. The image is shown on a large, rear-projection screen and its input is via the natural, intuitive gestures of picking up and placing objects. Music accompanies the visualization: each anchor has its own theme and the changing state of the system shapes and modulates the composition. LaserWho has been shown at Opera Totale in Venice and in New Orleans as part of the SIGGRAPH 2000 Emerging Technologies exhibition.</p>", "people": ["judith@media.mit.edu", "joep@media.mit.edu"], "title": "LaserWho", "modified": "2016-12-05T00:17:15.848Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "BATCAVE", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "laserwho"}, {"website": "", "description": "<p>L2T:T2L is a multi-year, three-phase program enabling urban youth to learn science, technology, engineering, and mathematics. First, the students study in a variety of technical areas, including programming, robotics, personal fabrication, digital video, alternative energy, and Web design and tools. Next, they develop their own projects over an extended period using computational tools. Finally, they will work with younger children in several community centers to provide the same type of learning experiences. The idea is that the youth will not only learn by teaching and thereby solidify their knowledge, they also will gain the satisfaction of contributing to the development of their community. The project also serves to test new content, tools, and learning materials for subsequent use in schools, and scales through consistent participation.</p>", "people": ["cavallo@media.mit.edu", "papert@media.mit.edu"], "title": "Learn to Teach: Teach to Learn", "modified": "2016-12-05T00:17:15.866Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-368", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "learn-to-teach-teach-to-learn"}, {"website": "", "description": "<p>This project explores how learning must be the basis of sustainable development and how constructionist approaches to learning can be applied to assist environmentally benign sustainable development. If solutions come from outside local contexts, the situation is inherently unsustainable. We are developing new computational tools for agriculture, environment, and collaborative problem-solving, including tools for those who are not textually literate. We are doing fieldwork in Senegal with the United Nations Food and Agriculture Organization (UN-FAO) and local partners to test and refine the tools and methodologies.</p>", "people": ["cavallo@media.mit.edu"], "title": "Learning as a Basis for Sustainable Development", "modified": "2016-12-05T00:17:15.882Z", "visibility": "PUBLIC", "start_on": "2001-09-01", "location": "E15-489", "groups": ["future-of-learning-2"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "learning-as-a-basis-for-sustainable-development"}, {"website": "", "description": "<p>Developing nations typically settle for technologies that were designed elsewhere, with other purposes in mind. The Learning Independence Networks project aims to change this, working with networks of organizations in developing nations (including universities, foundations, companies, and NGOs) to help them build the capacity to develop their own technologies, appropriate for local needs. We are working most closely with universities, helping them to develop new courses, research programs, technological infrastructures, and strategies for collaborating with other members of the network on the design of new technologies. The first Learning Independence Network, called Prometheus, is currently under development in Costa Rica, aiming to produce a new generation of technologies for use in community development and education.</p>", "people": [], "title": "Learning Independence Networks", "modified": "2016-12-05T00:17:15.948Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["personal-fabrication"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "learning-independence-networks"}, {"website": "", "description": "<p>LifeNet is a first-person model of a single person's knowledge. We are using multiple LifeNets to model multiple people and attempt to predict typical social knowledge-sharing patterns, given the different knowledge structures of different people. Possible applications include more efficient social communication protocols, project team suggestions, social user model comparisons, and story understanding.</p>", "people": ["minsky@media.mit.edu"], "title": "LifeNet: Social Common Sense", "modified": "2016-12-05T00:17:16.246Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-309", "groups": ["society-of-mind"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "lifenet-social-common-sense"}, {"website": "", "description": "<p>LightSwarm is a platform for interaction between humans and swarm robots. Swarm robots are implemented as augmented-reality agents that communicate and interact through movements. While each robot is simple, the aggregate shows complex behavior. In this way, LightSwarm invites one to think of the mind as a loose consensus of a society of agents, which allows for more nuanced interactions.</p>", "people": ["cynthiab@media.mit.edu", "palash@media.mit.edu"], "title": "LightSwarm", "modified": "2016-12-05T00:17:16.350Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "lightswarm"}, {"website": "", "description": "<p>Light Piping, an installation of solar technology on the roof of the new Media Lab extension, will bring natural light into the old building by focusing the light from an array of small lenses that focus the light into fiber optic cable. The light is transmitted as full-spectrum sunlight throughout the day, lowering energy costs, increasing psychological well being, and lighting an indoor garden symbolic of the connection between inside and outside in a more permeable building.</p>", "people": ["labrune@media.mit.edu"], "title": "Light Piping Solar Panels", "modified": "2016-12-05T00:17:16.410Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "Lower Atrium", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "light-piping-solar-panels"}, {"website": "", "description": "<p>Line of Sound shows how data can be used to deliver sound information only in the direction in which one looks. The demonstration is done using two 55-inch screens which transmit both human and machine relevant information. Each screen is used to show a video that flashes a single bit indicator, which transmits to a camera mounted on headphones. This is used to distinguish between the two screens, and to correlate an audio track to the video track.</p>", "people": ["borovoy@media.mit.edu", "lip@media.mit.edu"], "title": "Line of Sound", "modified": "2016-12-05T00:17:16.435Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "line-of-sound"}, {"website": "", "description": "<p>We leverage the ubiquity of Bluetooth-enabled devices and propose a decentralized, Web-based architecture that allows users to share their location by following each other in the style of Twitter. We demonstrate a prototype that operates in a large building and which generates a dataset of detected Bluetooth devices at a rate of ~30 new devices per day, including the respective location where they were last detected. Users then query the dataset using their unique Bluetooth ID and share their current locations with their followers by means of unique Universal Resource Identifiers (URIs) that they control. Our separation between producers (the building) and consumers (the users) of Bluetooth device location data allows us to create socially aware applications that respect user privacy while limiting the software necessary to run on mobile devices to just a Web browser.</p>", "people": ["ypod@media.mit.edu", "lip@media.mit.edu"], "title": "Location Sharing in Large Indoor Environments", "modified": "2016-12-05T00:17:16.635Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "location-sharing-in-large-indoor-environments"}, {"website": "", "description": "<p>Pushpin Computing is a research platform for exploring sensing techniques and algorithms on a dense sensor network of one hundred nodes distributed across the area of two square meters. The goal of this project is to develop distributed estimation applications for the Pushpin network, principally an acoustic-phased array and an artificial retina, which capitalize on the high node-density in the Pushpin system. These algorithms have two challenging requirements which will form the foundation of this research: first, each sensor node must know its position in a globally agreed upon, ad hoc coordinate system; and second, time synchronization must be achieved and maintained throughout the entire network. At the moment, we are developing a novel localization approach based on creating points of correspondence which are shared by all nodes by using ultrasonic ranging techniques.</p>", "people": ["joep@media.mit.edu"], "title": "Localization and Sensing Applications in the Pushpin Computing Network", "modified": "2016-12-05T00:17:16.613Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "localization-and-sensing-applications-in-the-pushpin-computing-network"}, {"website": "", "description": "<p>This project is developing a very novel noncontact microwave sensor system that detects the presence and activity of participants in front of interactive surfaces or structures. We are taking two approaches. In one, we have embedded a real-time software feature detector onto a small, micropatch Doppler motion-detection radar\ufffdthis device produces a continuous data stream that specifies the amount of motion detected, the dominant speed, and the dominant direction to and from the radar antenna. We are also adapting a low-power ranging radar originally designed by one of our sponsor collaborators to detect the level of fluid in large fuel tanks into a much smaller package able to determine the distance to users as they approach interactive surfaces or devices. Unlike IR, vision, or sonar systems, these techniques aren't affected by lighting, clothing, or visual clutter. Unlike capacitive sensing, we can design antennae to direct the sensing region into a forward-looking beam.These devices can also sense through opaque, nonconductive material, such as plastic, wood, or wallboard.</p>", "people": ["joep@media.mit.edu"], "title": "Low-Power Radar for Interactive Environments", "modified": "2016-12-05T00:17:16.721Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-441", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "low-power-radar-for-interactive-environments"}, {"website": "", "description": "<p>This project explores the application of new commercial RF chipsets and research devices being developed at the MIT Microelectronics Laboratory to short-range, moderate-to-high bit-rate, ultra-low-power, minimally complex, channel-shared wireless communication for ubiquitously embedded smart sensor modules and lightweight, reconfigurable communication networks. A software radio base station is now being built to coordinate these sensor modules. This base station will subsample and digitize the received signals at the first IF. Such an early digitization allows the base station to have a flexible, reconfigurable architecture. Since most of the signal processing will be done in the digital domain, the same base station will work for different modulation schemes. Another part of this project is to capture currently evolving commercial technology (e.g., Bluetooth, minimal 802.11, and miscellaneous chip-based or hybrid RF transcievers), introducing them into our sensor packages as appropriate.</p>", "people": ["joep@media.mit.edu"], "title": "Low-Power RF Chipsets for Wireless, Embedded Sensing", "modified": "2016-12-05T00:17:16.746Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "low-power-rf-chipsets-for-wireless-embedded-sensing"}, {"website": "", "description": "<p>Light is an important factor in the regulation of all kinds of circadian rhythms in our body, such as heart rate, blood pressure, digestion, and mood. This project aims to measure the light environment around users, and provide real-time feedback about appropriate light intensity and color depending on the time of day.</p>", "people": ["picard@media.mit.edu", "akanes@media.mit.edu"], "title": "Lux Meter: Real-Time Feedback in Ambient Light Environment", "modified": "2016-12-05T00:17:16.815Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "lux-meter-real-time-feedback-in-ambient-light-environment"}, {"website": "", "description": "<p>This project develops new theory and algorithms to enable computers to make rapid and accurate inferences from multiple modes of data, such as determining a person's affective state from multiple sensors\ufffdvideo, mouse behavior, chair pressure patterns, typed selections, or physiology. Recent efforts focus on understanding the level of a person's attention, useful for things such as determining when to interrupt. Our approach is Bayesian: formulating probabilistic models on the basis of domain knowledge and training data, and then performing inference according to the rules of probability theory. This type of sensor fusion work is especially challenging due to problems of sensor channel drop-out, different kinds of noise in different channels, dependence between channels, scarce and sometimes inaccurate labels, and patterns to detect that are inherently time-varying. We have constructed a variety of new algorithms for solving these problems and demonstrated their performance gains over other state-of-the-art methods.</p>", "people": ["picard@media.mit.edu"], "title": "Machine Learning and Pattern Recognition with Multiple Modalities", "modified": "2016-12-05T00:17:16.849Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-443", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "machine-learning-and-pattern-recognition-with-multiple-modalities"}, {"website": "", "description": "<p>Expert or fraud, the powerful person in front of an orchestra or choir attracts both hate and admiration. But what is the actual influence a conductor has on the musician and the sounding result? To throw light on the fundamental principles of this special gestural language, we try to prove a direct correlation between the conductor's gestures, muscle tension, and the physically measurable reactions of musicians in onset-precision, muscle tension, and sound quality. We also measure whether the mere form of these gestures causes different levels of stress or arousal. With this research we aim not only to contribute to the development of a theoretical framework on conducting, but also to enable a precise mapping of gestural parameters in order to develop and demonstrate a new system to the optional enhancement of musical learning, performance, and expression.</p>", "people": ["tod@media.mit.edu", "platte@media.mit.edu"], "title": "Maestro Myth: Exploring the Impact of Conducting Gestures on Musician's Body and Sounding Result", "modified": "2016-12-05T00:17:16.936Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2016-06-30", "slug": "maestro-myth-exploring-the-impact-of-conducting-gestures-on-musicians-body-and-sounding-result"}, {"website": "", "description": "<p>Mapping the Globe is an interactive tool and map that helps us understand where the <i>Boston Globe</i> directs its attention. Media attention matters\u2014in quantity and quality. It helps determine what we talk about as a public and how we talk about it. Mapping the Globe tracks where the paper's attention goes and what that attention looks like across different regional geographies in combination with diverse data sets like population and income. Produced in collaboration with the <i>Boston Glob</i>e.</p>", "people": ["dignazio@media.mit.edu", "ethanz@media.mit.edu", "rahulb@media.mit.edu", "a_hashmi@media.mit.edu"], "title": "Mapping the Globe", "modified": "2016-12-05T00:17:16.960Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2015-09-01", "slug": "mapping-the-globe"}, {"website": "", "description": "<p>Recognition-based technology has made substantial advances in the past few years because of enhanced algorithms and faster processing speeds. However, current recognition systems are still not reliable enough to be integrated into user interface designs. A possible solution to this problem is to combine results from existing recognition systems and mutually disambiguate the unreliable sections. Piecing together partial results obtained from each mode of recognition can derive more reliable results. In addition, the results of one recognition system can be used to prepare the other recognition system. We are experimenting with an approach that uses a software agent to integrate off-the-shelf recognition applications via scripting languages. We use a software agent called MARCO (Multimodal Agent for Route Construction) that utilizes multiple recognition systems to assist users in giving directions for urban navigation.</p>", "people": ["lieber@media.mit.edu"], "title": "MARCO: Mutual Disambiguation of Recognition Errors in a Multimodal Navigational Agent", "modified": "2016-12-05T00:17:17.018Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "marco-mutual-disambiguation-of-recognition-errors-in-a-multimodal-navigational-agent"}, {"website": "", "description": "<p>Many new applications for robots require them to work alongside people as capable members of human-robot teams.  We have developed Mars Escape, a two-player online game designed to study how humans engage in teamwork, coordination, and interaction. Data gathered from hundreds of online games is being used to develop computational models of human collaborative behavior in order to create an autonomous robot capable of acting as a reliable human teammate.  In the summer of 2010, we will recreate the Mars Escape game in real life at the Boston Museum of Science and invite museum visitors to perform collaborative tasks together with the autonomous MDS robot Nexi.</p>", "people": ["cynthiab@media.mit.edu"], "title": "MDS: Crowdsourcing Human-Robot Interaction: Online Game to Study  Collaborative Human Behavior", "modified": "2016-12-05T00:17:17.153Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2011-09-01", "slug": "mds-crowdsourcing-human-robot-interaction-online-game-to-study-collaborative-human-behavior"}, {"website": "", "description": "<p>As robots become more and more capable, we will begin to invite them into our daily lives. There have been few examples of mobile robots able to carry out everyday tasks alongside humans. Though research on this topic is becoming more and more prevalent, we are just now beginning to understand what it means to collaborate. This project aims to unravel the dynamics involved in taking on leadership roles in collaborative tasks as well as balancing and maintaining the expectations of each member of the group (whether it be robot or human). This challenge involves aspects of inferring internal human state, role support and planning, as well as optimizing and keeping synchrony amongst team members \"tight\" in their collaboration.</p>", "people": ["cynthiab@media.mit.edu", "robbel@media.mit.edu", "jinjoo@media.mit.edu", "ndepalma@media.mit.edu"], "title": "MDS: Exploring the Dynamics of Human-Robot Collaboration", "modified": "2016-12-05T00:17:17.171Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "mds-exploring-the-dynamics-of-human-robot-collaboration"}, {"website": "", "description": "<p>That people have emotional responses to music is a truism. However, we have little understanding of the ways in which music brings about these emotions.  Indeed, we lack decent ways to measure these responses in a quantitative way. As an early step in this area, we devised a listening experiment with a novel response paradigm. Listeners chose from a set of around twenty emotional descriptors, selecting a strength value for each chosen word. Importantly, we did not prevent the listener from selecting conflicting words, or limit her to only one choice. We then used unsupervised machine learning techniques to explore the space of responses. Early results show good agreement with prior studies, but with the potential for more nuanced understanding. We plan to extend this work into considering a broader space of influencing factors on emotional response.</p>", "people": ["tod@media.mit.edu"], "title": "Measuring Emotional Responses to Music", "modified": "2016-12-05T00:17:17.295Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "measuring-emotional-responses-to-music"}, {"website": "", "description": "<p>In this project we examine the social media and traditional media's response to the Boston Marathon bombings from the moment of the explosion to two weeks after the events, including the search, hunt, and capture of the suspects. We use big data analytics, natural language processing, and complex system and network analysis techniques. We focus specifically on information flow, engagement and attention of the audience, emergence of broadcasters, source and spread of rumors, and interplay of various media. We hope to develop a better understanding of the nature of information generation and flow from broadcasters and audiences across different media. Using this event as a case study, we can find out what went wrong or right, and come up with recommendations for different actors (news sources, social media participants, police departments) to better facilitate information flow and minimize misunderstanding and the spread of false information.</p>", "people": ["dkroy@media.mit.edu", "soroush@media.mit.edu"], "title": "Media Ecosystem Analysis: Lessons from the Boston Marathon Bombings", "modified": "2016-12-05T00:17:17.386Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "media-ecosystem-analysis-lessons-from-the-boston-marathon-bombings"}, {"website": "", "description": "<p>We present two scalable ways to explore and distribute media in all forms: video, text, and graphics; published and conversational. The first presentation has been demonstrated as an interactive, dynamic time/source array where one can see the pulse of publication and suggest media for friends. A revision organizes content as 3D \ufffdstacks\ufffd that correspond to people and topics. The Matrix dissolves media silos and types and assembles it in a data- and socially driven way. \ufffdGlue\ufffd is the engine that drives assembly.</p>", "people": ["vdiep@media.mit.edu", "lip@media.mit.edu"], "title": "Media Matrix", "modified": "2016-12-05T00:17:17.472Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2014-09-01", "slug": "media-matrix"}, {"website": "", "description": "<p>Merry Miser is a mobile application which persuades people to spend less money, and think more about their spending. By combining users' real financial transaction information, their location, and personal assessments of spending, the application presents deeply personalized and compelling interventions at the time and place when they are near an opportunity to shop. The interventions help to reinforce choices that are in the users' better long-term self interest, against short-term impulses.</p>", "people": ["geek@media.mit.edu"], "title": "Merry Miser", "modified": "2016-12-05T00:17:17.546Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "merry-miser"}, {"website": "", "description": "<p>A major challenge for the development of novel biosensors is packaging the sensor for a specific application or experiment. The use of microfluidic systems in conjunction with microfabricated sensors promises advantages over traditional benchtop biology, including small-volume analyte consumption and higher throughput. Using rapid prototyping processes in this lab and in the microfabrication facility on campus, we develop microfluidic systems in silicon, glass, cast polymers, and laser-cut plastics. These include parallel channel arrays that allow rapid typing of many analytes, small volume fluidic cells that incorporate electrical contacts, and microcapillaries for functionalizing microfabricated biosensors.</p>", "people": ["scottm@media.mit.edu"], "title": "Microfluidics", "modified": "2016-12-05T00:17:17.576Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-420", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "microfluidics"}, {"website": "", "description": "<p>We are synthesizing artificial materials for manipulating electromagnetic waves. These materials have a negative index of refraction, allowing novel imaging and fabrication geometries which will lead to improved lithography and measurement techniques.</p>", "people": ["jacobson@media.mit.edu", "ike@media.mit.edu"], "title": "MetaMaterials", "modified": "2016-12-05T00:17:17.651Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-427", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "metamaterials"}, {"website": "", "description": "<p>This project focuses on fabricating precision micro-machine systems in one, two, and three dimensions. The typical systems are smaller than 1mm x 1mm in size, and incorporate sensing and actuation in a single, micro-fabricated device.  We are also interested in novel mechanical mechanisms and how they translate to micro-scale devices.</p>", "people": ["jacobson@media.mit.edu"], "title": "Micro-Electro Mechanical Devices (MEMS)", "modified": "2016-12-05T00:17:17.689Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-015", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "micro-electro-mechanical-devices-mems"}, {"website": "", "description": "<p>Linear arrays of surface-emitting semiconductor lasers provide the basis for very small, low-power video projectors requiring minimal optics. Such projectors are suitable for handheld devices like phones.</p>", "people": ["vmb@media.mit.edu"], "title": "Miniature Video Projector (Personal Projection)", "modified": "2016-12-05T00:17:17.795Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "Garden Conference Room", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "miniature-video-projector-personal-projection"}, {"website": "", "description": "<p>A fast moving workplace, calls for... a fast moving workstation!  The mobile office is a prototype robotic office fitted with a remote controlled, motorized base, onboard AC power storage for 6-8 hours, and 4 axis robotic arm.  The mobile office is great for taking your work down into the machine shop or to lengthy collaboration meetings.</p>", "people": ["neri@media.mit.edu"], "title": "Mobile Office", "modified": "2016-12-05T00:17:17.848Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "mobile-office"}, {"website": "", "description": "<p>As part of the Living Observatory ecological sensing initiative, we've been developing new approaches to mobile, wearable sensor data visualization. The Tidmarsh app for Google Glass visualizes real-time sensor network data based on the wearer's location and gaze. A user can approach a sensor node to see 2D plots of its real-time data stream, and look across an expanse to see 3D plots encompassing multiple devices. On the back-end, the app showcases our Chain API, crawling linked data resources to build a dynamic picture of the sensor network. Besides development of new visualizations, we are building in support for voice queries, and exploring ways to encourage distributed data collection by users.</p>", "people": ["gershon@media.mit.edu", "bmayton@media.mit.edu", "joep@media.mit.edu", "ddh@media.mit.edu", "sfr@media.mit.edu"], "title": "Mobile, Wearable Sensor Data Visualization", "modified": "2016-12-05T00:17:17.896Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "mobile-wearable-sensor-data-visualization"}, {"website": "", "description": "<p>Synthesizing music from non-Western cultures is a problem for current music technology, which is based on constant pitch and note-oriented concepts such as MIDI. This is an issue for Indian music because time-varying pitch inflections, called gamakas, are an essential part of its construct. We are analyzing songs and instrumental pieces from the South Indian tradition, and developing software that enables musicians to synthesize Indian music with the required ornamentations. Such innovations will allow the music industry to cross cultural boundaries and provide appropriate representations for the expressive artifacts of non-Western music, in particular from Asia and the Middle-East.</p>", "people": ["bv@media.mit.edu"], "title": "Modeling Indian Music", "modified": "2016-12-05T00:17:18.013Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "modeling-indian-music"}, {"website": "", "description": "<p>How do people perceive the way music can convey a story or a feeling, tension and resolution in music, and musical structures? In order to answer these questions, we are collecting data from all around the world to help illuminate these issues and reveal universal aspects of music perception. An online study has been designed with examples that take into account a number of musical parameters: pitch contour, rhythmic regularity, tempo, loudness, and harmony. These examples both isolate and combine these parameters, allowing us to gather empirical data. The model is trained on this data in order to mirror human perception as closely as possible. As of September 2005, more than 2,000 people have taken part in our study.</p>", "people": ["tod@media.mit.edu"], "title": "Modeling Tension and Resolution in Music", "modified": "2016-12-05T00:17:18.037Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "modeling-tension-and-resolution-in-music"}, {"website": "", "description": "<p>Modular Sound Blocks offer a way to create, design, and explore sound and music. Essentially a programming environment for music with a tangible interface, Modular Sound Blocks allow children or adults to experiment with and construct original creations. Using simple mathematics, exploring relationships, or simply \ufffdplaying,\ufffd people can create complex structures of sound by networking the blocks together. Young children can create and explore simple relationships with only a few blocks, while older children and adults will construct enormously intricate, complicated networks.</p>", "people": ["bv@media.mit.edu"], "title": "Modular Sound Blocks", "modified": "2016-12-05T00:17:18.085Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "modular-sound-blocks"}, {"website": "", "description": "<p>VLSI will hit fundamental physical limits within two decades. Quantum mechanics is the one resource in our universe that is exponentially large, but has been untapped for computation. We've found that, paradoxically, it is possible to preserve coherence in a quantum computer while still providing external access by using the orientation of the nuclei of atoms in molecules in a liquid to store quantum information, and programming them with radio-frequency pulses.</p>", "people": ["neilg@media.mit.edu"], "title": "Molecular Quantum Computation", "modified": "2016-12-05T00:17:18.136Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-023", "groups": ["physics-and-media", "silicon-biology", "personal-fabrication"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "molecular-quantum-computation"}, {"website": "", "description": "<p>Nuclear magnetic resonance can be used to manipulate and to control simple molecules and compex biomolecules. We have demonstrated the use of one molecule as a seven quantum bit quantum computer. In an additional joint project with Joseph Jacobson's Molecular Machines group, we demonstrate control over biomolecules to cause them to selectively heat up and undergo chemical changes by building antennas into them.</p>", "people": ["ike@media.mit.edu"], "title": "Molecular Control with NMR", "modified": "2016-12-05T00:17:18.238Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "--Choose Location", "groups": ["quanta"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "molecular-control-with-nmr"}, {"website": "", "description": "<p>Mr. Java is an intelligent coffee machine: it recognizes you, learns your preferences, and feeds you both the bits and the atoms you desire.  A cornerstone of the Kitchen Sync project, Mr. Java demonstrates a new concept of appliances that work with and not against you.  Mr. Java also logs cumulative coffee consumption for the entire Lab over time, a concept easily extensible to a wide domain of applications.  We are currently looking at other applications for the existing Mr. Java technology.</p>", "people": ["mike@media.mit.edu"], "title": "Mr. Java", "modified": "2016-12-05T00:17:18.313Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "mr-java"}, {"website": "", "description": "<p>MouseTrack is a web logging system that tracks mouse movements on websites. The system includes a visualization tool that displays the mouse cursor path followed by website visitors. It helps web site administrators run usability tests and analyze the collected data. Practitioners can track any existing webpage by simply entering its URL. \n\nThe visualization tool allowed inexperienced observers to concretely learn important user interface principles by observation. Future work includes fully validating the approach by correlating eye gaze with mouse tracks in multiple web environments. Our goal is to introduce this approach to web usability practitioners and obtain universally identifiable mouse patterns that could be automatically labeled to reveal useful information about site design and speed up the iterative design process.</p>", "people": [], "title": "MouseTrack: A Web-Based Usability Tool", "modified": "2016-12-05T00:17:18.339Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "mousetrack-a-web-based-usability-tool"}, {"website": "", "description": "<p>This work is an application of a synthetic character metaphor to the production of music. Presented here is a small colony of creatures whose virtual bodies produce sound. Played out in a spatial representation, passages of music are exchanged between the creatures and consequently distorted; retrograde variations and repeated subsegments are swapped back and forth in an accidental, recombinant evolution. It draws parallels between musical problems and the problems of motor control and learning in synthetic characters. This work also points towards a system in which characters' movements are shaped (by training) in real time; consequently, by creating music with the resulting system, we are able to perform similar training and shaping within a musical domain.</p>", "people": ["tod@media.mit.edu"], "title": "Music Creatures", "modified": "2016-12-05T00:17:18.400Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-450", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "music-creatures"}, {"website": "", "description": "<p>Our work in Music, Mind, and Health has culminated in a recent PhD thesis, showing the technologies and perspectives required to build on the transformative nature of music to drive specific neurological, physical, and psychological change. A radically new \"Personal Instrument\" is currently being used by Dan Ellsey, a quadraplegic individual, who controls this interface to sculpt an expressive performance of music in real time. A three-month study of Ellsey's expressive behavior\ufffdits potential as well as its limits\ufffdresulted in an interface tailored just for him, enabling him to access expressive performance despite his physical disability. This new line of work highlights principles for future instruments and applications, where the impact is in the marriage of the interface and uniqueness of the person. In this way, we are pursuing new design philosophies, technologies, and collaborations within the scientific community, public performance, and clinical research.</p>", "people": ["tod@media.mit.edu"], "title": "Music, Mind, and Health", "modified": "2016-12-05T00:17:18.455Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-445", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "music-mind-and-health"}, {"website": "", "description": "<p>musicBottles introduces a tangible interface that deploys bottles as containers and controls for digital information. The system consists of a specially designed table and three corked bottles that \"contain\" the sounds of the violin, the cello, and the piano in Edouard Lalo's \"Piano Trio in C Minor, Op. 7.\" Custom-designed electromagnetic tags embedded in the bottles enable each one to be identified wirelessly. The opening and closing of a bottle is also detected. When a bottle is placed onto the stage area of the table and the cork is removed, the corresponding instrument becomes audible. A pattern of colored light is rear-projected onto the table's translucent surface to reflect changes in pitch and volume. The interface allows users to structure the experience of the musical composition by physically manipulating the different sound tracks.</p>", "people": ["fletcher@media.mit.edu", "joep@media.mit.edu", "ishii@media.mit.edu"], "title": "musicBottles", "modified": "2016-12-05T00:17:18.480Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "musicbottles"}, {"website": "", "description": "<p>This project explores the new learning possibilities created through interaction between physical and virtual objects. We hope to show how the tangibility of physical objects and the programmability of computers can be combined to encourage and enable deep understanding of mechanical, structural, and interactive design. Two case studies are being developed: the first involves interactive sculpture design through the use of wire and found objects as construction materials, combined with active sensors/actuators to provide interaction; the other study involves self-balancing robots through which learners develop strategies of balance based on the analysis of their own bodies.</p>", "people": ["cavallo@media.mit.edu"], "title": "New Content, Materials, and Environments for the Digital Age", "modified": "2016-12-05T00:17:18.816Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-385", "groups": [], "published": true, "active": false, "end_on": "2005-09-01", "slug": "new-content-materials-and-environments-for-the-digital-age"}, {"website": "", "description": "<p>Building on the Media Lab hyperstring project of 1990-1993 (Machover, Chung, Gershenfeld, Paradiso, et al.), we are improving and redefining the technology and the functionality of this enhanced expert performance instrument. Sensor improvements are being made, and new techniques for pitch, timbre, and \"expressivity\" measurement are being developed. A new model of hyperstring performance is also being explored, which uses the virtuosic instrument as an intermediary for training young performers, as well as for shaping and manipulating complicated musical events (such as the sonic output from a roomful of Simple Thing players). In addition, the new hyperstring instrument will be interfaced with a communication and modulation environment defined by Big Thing construction, allowing children to define the meaning of a hyperstring gesture and performance.</p>", "people": ["tristan@media.mit.edu", "tod@media.mit.edu"], "title": "New Generation Hyperstrings", "modified": "2016-12-05T00:17:18.837Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-483", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "new-generation-hyperstrings"}, {"website": "", "description": "<p>This project addresses human cognitive models of reflective problem solving in terms of psychology, neuroscience, and artificial intelligence. A programming language describing reflective human thought processes is being developed for the purpose of understanding the biological process of thought. This description language allows distributed reflective monitoring and control of parallel threads.  In addition to being a novel method for the robust control of distributed computer programs, this technology is directed toward consumer HCI and medical cures for neuropsychological problems, and has applications for neural-interface computer gaming peripherals, aging population cognitive evaluation, and training.</p>", "people": ["joep@media.mit.edu", "minsky@media.mit.edu"], "title": "Neural Models of Mind", "modified": "2016-12-05T00:17:18.669Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-351", "groups": ["responsive-environments", "society-of-mind"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "neural-models-of-mind"}, {"website": "", "description": "<p>New Century Cities is a joint research initiative between MIT\ufffds Center for Real Estate, City Design and Development (Urban Studies and Planning), and the Media Lab\ufffds Smart Cities group. It focuses on a new generation of development projects. These ambitious initiatives are emerging at the intersection of social policy, technology, urban design, and real estate development, and are located in what we call New Century Cities (NCCs). You will find them today in New York City; Cambridge, Massachusetts; Belfast; Helsinki; Copenhagen; Seoul; and Singapore. In addition, NCC projects are on the drawing boards in places such as Florianopolis, Brazil and Zaragoza, Spain. These projects vary in size and in how their development is organized and led. The research initiative focuses on developing a profile of these projects, creating development guidelines, and collecting case studies of technologies that fulfill these visions for future urban spaces.</p>", "people": ["susanne@media.mit.edu"], "title": "New Century Cities: Real Estate Value in a Digital World", "modified": "2016-12-05T00:17:18.715Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-309", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "new-century-cities-real-estate-value-in-a-digital-world"}, {"website": "", "description": "<p>We have designed an online algorithm to approximate maximum welfare in selling adwords when click-through rates are unknown. This algorithm will aid advertisers as well as ad-placement agencies (such as search-engine companies) in achieving increased revenue through advertising placement.</p>", "people": [], "title": "New Methods for Selling Adwords", "modified": "2016-12-05T00:17:18.746Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-313", "groups": ["erationality"], "published": true, "active": false, "end_on": "2007-01-01", "slug": "new-methods-for-selling-adwords"}, {"website": "", "description": "<p>OPEN I/O provides a hardware base and a suite of network services for the development of I/O devices. With OPEN I/O, we aspire to make the creation of Ethernet-enabled devices accessible to artists and software engineers.  OPEN I/O devices are automatically discovered by the OPEN I/O service and assigned to users who control the devices. Using the OPEN I/O router, devices can communicate with each other, or with network-based applications and computation, such as provided by OPENSTUDIO and OPENCODE. Users can configure and program their hardware remotely, and interact with similar hardware devices around the world. OPEN I/O provides common device libraries to help bridge the gap between software and hardware development.</p>", "people": ["holtzman@media.mit.edu"], "title": "OPEN I/O", "modified": "2016-12-05T00:17:18.916Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "open-io"}, {"website": "", "description": "<p>This project is aimed at building a system to recognize emotional expression given four physiological signals.  Data was gathered from a graduate student with  acting experience as she intentionally tried to experience eight different emotional states daily over a period of several weeks. Several features are extracted from each of her physiological signals. The first classifiers gave a classification result of 88% success when discriminating among 3 emotions (pure chance would be 33.3%), and of 51% when discriminating among 8 emotions (pure chance 12.5%). New, improved classifiers reach an 81% success rate when discriminating among all 8 emotions. Furthermore, an online classifier has now been built using the old method, which gives a success rate only 8% less than its old offline counterpart (i.e. 43%). We expect this percentage to sharply increase when the new methods are adapted to run online. \n</p>", "people": ["picard@media.mit.edu"], "title": "Online Emotion Recognition", "modified": "2016-12-05T00:17:19.028Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "online-emotion-recognition"}, {"website": "", "description": "<p>Open Gender Tracker is a suite of open-source tools and APIs that make it easy for newsrooms and media monitors to collect metrics and gain a better understanding of gender diversity in their publications and audiences. This project has been created in partnership with Irene Ros of Bocoup, with funding from the Knight Foundation.</p>", "people": ["ethanz@media.mit.edu", "jnmatias@media.mit.edu"], "title": "Open Gender Tracker", "modified": "2016-12-05T00:17:19.067Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2016-10-01", "slug": "open-gender-tracker"}, {"website": "", "description": "<p>When an environmental crisis strikes, the most important element to saving lives is information. Information regarding water depths, spread of oil, fault lines, burn scars, and elevation are all crucial in the face of disaster. Much of this information is publicly available as infrared satellite data. However, with today\ufffds technology, this data is difficult to obtain, and even more difficult to interpret. Open Infrared, or OpenIR, is an ICT (information communication technology) offering geo-located infrared satellite data as on-demand map layers and translating the data so that anyone can understand it easily. OpenIR will be pilot tested in Indonesia, where ecological and economic vulnerability is apparent from frequent seismic activity and limited supporting infrastructure. The OpenIR team will explore how increased accessibility to environmental information can help infrastructure-challenged regions to deal with environmental crises of many kinds.</p>", "people": ["holtzman@media.mit.edu"], "title": "OpenIR: Data Viewer", "modified": "2016-12-05T00:17:19.270Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "openir-data-viewer"}, {"website": "", "description": "<p>We are studying and implementing a swarm of 36 holonomic-drive robots for use in an upcoming robotic opera. Each robot will eventually comprise roughly eight degrees of freedom, and will follow a centralized control, allowing swarm behaviors as well as pre-scripted paths. In May, a test platform of three 3-DOF robots will be dancing, controlled by an animation with music composed by Tod Machover.</p>", "people": ["cynthiab@media.mit.edu", "tod@media.mit.edu"], "title": "Operobots: A Robotic Swarm for Artistic Expression", "modified": "2016-12-05T00:17:19.356Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-468", "groups": ["personal-robots", "opera-of-the-future"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "operobots-a-robotic-swarm-for-artistic-expression"}, {"website": "", "description": "<p>Distributed sensor networks offer many new capabilities for monitoring environments with applicability to medical, industrial, military, environmental, and experiential fields. By making the system mobile, we increase the application-space for the distributed sensor network mainly by providing context-aware deployment, continual relocatabililty, automatic node recovery, and a larger area of coverage. In existing models, the addition of actuation to sensor network nodes has exacerbated three of the main problems with these types of systems: power usage, node size, and node complexity. This work proposes a solution to these problems in the form of parasitically actuated nodes that gain their mobility and local navigational intelligence by selectively engaging and disengaging from mobile hosts in their environment. This work aims to design, implement, evaluate, and demonstrate a parasitically actuated wireless sensor network as a solution to these problems and to explore new applications and features of a system with this type of mobility.</p>", "people": ["joep@media.mit.edu"], "title": "Parasitic Mobility for Sensor Networks", "modified": "2016-12-05T00:17:19.694Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "parasitic-mobility-for-sensor-networks"}, {"website": "", "description": "<p>As the power requirements for microelectronics continue decreasing, environmental energy sources can begin to replace batteries in certain wearable subsystems. In this spirit, this effort has examined devices that can be built into a shoe (where excess energy is readily harvested), and used for generating electrical power \"parasitically\" while walking. Two of these are piezoelectric in nature: a unimorph strip made from piezoceramic composite material and a stave made from a multilayer laminate of PVDF foil. The third is a shoe-mounted rotary magnetic generator. As a self-powered application example, a system had been built around the piezoelectric shoes that periodically broadcasts a 12-bit digital RFID as the wearer walks.  Nate Shenk's research explored better matching to the piezo source with a switching regulator. Ongoing work investigates exciting the piezo sources at higher frequency, and more optimal integration of the magnetic generators.</p>", "people": ["geppetto@media.mit.edu", "joep@media.mit.edu"], "title": "Parasitic Power Harvesting", "modified": "2016-12-05T00:17:19.725Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-357", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "parasitic-power-harvesting"}, {"website": "", "description": "<p>Air and water pollution are well-known concerns in cities throughout the world. However, communities often lack practical tools to measure and record pollution levels, and thus are often powerless to motivate policy change or government action. Current government-funded pollution monitors are sparsely located, and many large national and local governments fail to disclose this environmental data in areas where pollution is most prevalent. We have been developing very low-cost, ultra low-power environmental sensors for air, soil, and water, that enable communities to easily sample their environment and upload data to their mobile phone and an online map. This not only empowers communities to enact new policies, but also serves as a public resource for city health services, traffic control, and general urban design.</p>", "people": ["fletcher@media.mit.edu", "kll@media.mit.edu"], "title": "Participatory Environmental Sensing for Communities ", "modified": "2016-12-05T00:17:19.758Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["changing-places"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "participatory-environmental-sensing-for-communities"}, {"website": "", "description": "<p>We have developed a low-cost device that can wirelessly detect a beating heart over a short distance (1m) and does not require any sensor placed on the person's body. This device can be used for wireless medical/health applications as well as security and safety applications, such as automobile/truck drivers as well as ATM machines. We have also created a small battery-powered version of this sensor that can be worn on a person's clothing but does not require touching the person's skin.</p>", "people": ["fletcher@media.mit.edu", "picard@media.mit.edu"], "title": "Passive Wireless Heart-Rate Sensor", "modified": "2016-12-05T00:17:19.783Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-448", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "passive-wireless-heart-rate-sensor"}, {"website": "", "description": "<p>During the nineties, software and computer graphics developers came head-to-head with artists and graphic designers, facilitating a need for their respective fields to understand and merge new technologies and academic concepts. However, a silence currently remains about the aesthetic qualities of garment design, as new \ufffdwearable\ufffd computing mechanisms develop. This project explores how the two disciplines redefine each other as they merge. Built with an emphasis on qualitative elements such as style, comfort, and functionality, Peppermint is a handbag adorned with a dynamic display surface. Each morning, the bag can be presented with the important events of the current day (such as class, a demo, or crew practice) via a tactile interface. Over the course of the day, the dynamic surface gradually modulates in behavior. This bag is an exploration of kinetic design in relation to the aesthetics of fashion.</p>", "people": [], "title": "Peppermint", "modified": "2016-12-05T00:17:19.916Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-305A", "groups": ["aesthetics-computation"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "peppermint"}, {"website": "", "description": "<p>The ability to display and perceive emotion is a key component in intelligent behavior. Emotion helps to guide both human and animal behavior, and is a useful method of conveying information between species. This project is focused on building tools that help synthetic characters perceive emotion in vocalizations from their human interactors. The project also focuses on creating creatures that use these tools to react appropriately to the emotion they perceive based on their own personalities and experiences.</p>", "people": [], "title": "Perceiving Emotional Affect in Vocalizations", "modified": "2016-12-05T00:17:19.945Z", "visibility": "PUBLIC", "start_on": "2003-09-01", "location": "E15-441", "groups": ["synthetic-characters"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "perceiving-emotional-affect-in-vocalizations"}, {"website": "", "description": "<p>The saying, \"if you can't measure it, you can't manage it\" may be appropriate for stress. Many people are unaware of their stress level, and of what is good or bad for it. The issue is complicated by the fact that while too much stress is unhealthy, a certain amount of stress can be healthy as it motivates and energizes. The \"right\" level varies with temperment, task, and other factors, many of which are unknown. There seems to be no data analyzing how stress levels vary for the average healthy individual, over day-to-day activities. We would like to build a device that helps to gather and present data for improving an individual's understanding of both healthy and unhealthy stress in his or her life. The device itself should be comfortable and should not increase the user's stress. (It is noteworthy that stress monitoring is also important in human-computer interaction for testing new designs.) Currently, we are building a new, wireless, stress-mornitoring system by integrating Fitsense's heart-rate sensors and Motorola's iDen cell phone with our heart-rate-variability estimation algorithm.</p>", "people": ["picard@media.mit.edu"], "title": "Personal Heart-Stress Monitor", "modified": "2016-12-05T00:17:19.969Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-001", "groups": ["affective-computing", "gray-matters"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "personal-heart-stress-monitor"}, {"website": "", "description": "<p>We are developing an opt-in camera network, in which users carrying wearable tags are visible to the network and everyone else is invisible. Existing systems for configurable dynamic privacy in this context are opt-out and catch-all; users desiring privacy carry pre-registered tags that disable sensing and networked media services for everyone in the room. To address these issues, we separate video into layers of flexible sprites representing each person in the field of view, and transmit video of only those who opt-in. Our system can also define groups of users who can be dialed in and out of the video stream dynamically. For cross-reality applications, these dynamic layers achieve a new level of video granularity, allowing users and groups to uncover correspondences between their activities across spaces.</p>", "people": ["gershon@media.mit.edu", "joep@media.mit.edu"], "title": "Personal Video Layers for Privacy", "modified": "2016-12-05T00:17:19.988Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "personal-video-layers-for-privacy"}, {"website": "", "description": "<p>PictureXS is a browser-constrained data repository that builds a semantic network of images and keywords. It explores issues related to user interfaces, organization and visualization of large amounts of digital content, and alternative ways of regulating social systems on the World Wide Web by avoiding the user account convention, and implementing a simple open censorship model.</p>", "people": ["holtzman@media.mit.edu"], "title": "PictureXS", "modified": "2016-12-05T00:17:20.117Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-301", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "picturexs"}, {"website": "", "description": "<p>Place Pulse is a website that allows anybody to quickly run a crowdsourced study and interactively visualize the results. It works by taking a complex question, such as \"Which place in Boston looks the safest?\" and breaking it down into easier-to-answer binary pairs. Internet participants are given two images and asked \"Which place looks safer?\" From the responses, directed graphs are generated and can be mined, allowing the experimenter to identify interesting patterns in the data and form new hypotheses based on their observations. It works with any city or question and is highly scalable. With an increased understanding of human perception, it should be possible for calculated policy decisions to have a disproportionate impact on public opinion.</p>", "people": ["hidalgo@media.mit.edu"], "title": "Place Pulse", "modified": "2016-12-05T00:17:20.139Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "--Choose Location", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2016-10-22", "slug": "place-pulse"}, {"website": "", "description": "<p>Liquid crystals modulate the polarization state of light, and thus a significant amount of light (and money) is lost converting light into a polarized state.  We explore as an alternative liquid crystal modes that do not require the use of a polarizer to allow liquid crystal displays to be readable in bright sunlight.</p>", "people": ["mlj@media.mit.edu"], "title": "Polarization-Independent Liquid Crystals Display", "modified": "2016-12-05T00:17:20.299Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2008-01-01", "slug": "polarization-independent-liquid-crystals-display"}, {"website": "", "description": "<p>This project focuses on the development of a low-cost system to track the precise 3-D position of large numbers of objects tagged with passive microwave RF transponders at short-range (3-100m) and in real time. The proposed system will allow selection, identification, tracking, and encoding of data from connected sensors to be relayed by the passive tags. These tags can address a multitude of applications, including new human-computer interfaces, robot control, animation and virtual reality, gaming, low-cost tracking of machinery and animals, search and rescue, and warehouse monitoring.</p>", "people": ["joep@media.mit.edu"], "title": "Precision, Wide-Area Tracking of Small, Passive RF Tags", "modified": "2016-12-05T00:17:20.687Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "precision-wide-area-tracking-of-small-passive-rf-tags"}, {"website": "", "description": "<p>The printing of nanoparticle-based inks has been extended to produce three-dimensional structures consisting of hundreds of layers. Linear drive motors and thermal actuators have been demonstrated.</p>", "people": ["jacobson@media.mit.edu", "hamad@media.mit.edu"], "title": "Printed Machines", "modified": "2016-12-05T00:17:20.840Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-015", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "printed-machines"}, {"website": "", "description": "<p>We are extending the child's construction kit, building computational power directly into LEGO bricks. Children are using these Programmable Bricks to build everything from robotic creatures to interactive kinetic sculptures, and, in the process, learning about engineering and design. We are currently developing a family of Programmable Bricks, known as Crickets, that designed especially for integrating art and engineering. The LEGO company now sells a product (called MindStorms) based on our Programmable Brick research.</p>", "people": ["borovoy@media.mit.edu", "mres@media.mit.edu", "bss@media.mit.edu"], "title": "Programmable Bricks", "modified": "2016-12-05T00:17:20.932Z", "visibility": "PUBLIC", "start_on": "1992-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "programmable-bricks"}, {"website": "", "description": "<p>The quality of communication, in all its various forms, is heavily dependent on the appropriate medium, timing, and level of trust between parties. Protocol is a tool that helps you, and society at large, understand and share preferences\ufffdpersonal or professional protocol\ufffdto communicate more effectively. Protocol is a dynamic and syndicated contact sheet, seen most often as an email signature or status update; it also functions as an independent site. More specifically, we see it as a preference page to share how and where you want to be contacted. Soon you will be able to embed your protocol in your site of choice (hey about.me, flavours.me, linkedin.com and wordpress.com).</p>", "people": ["holtzman@media.mit.edu"], "title": "Protocol", "modified": "2016-12-05T00:17:21.094Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["information-ecology"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "protocol"}, {"website": "", "description": "<p>An assortment of everyday objects is given the ability to understand multitouch gestures of the sort used in mobile-device user interfaces, enabling people to use such increasingly familiar gestures to control a variety of objects, and to \"copy\" and \"paste\" configurations and other information among them.</p>", "people": ["vmb@media.mit.edu"], "title": "ProtoTouch: Multitouch Interfaces to  Everyday Objects", "modified": "2016-12-05T00:17:21.144Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "prototouch-multitouch-interfaces-to-everyday-objects"}, {"website": "", "description": "<p>The standard model of labor in economics assumes that work is aversive and that the employer, therefore, has to pay the employees money to compensate them for the disutility they incur when producing labor. While this basic model approximates the provision of some types of jobs (smelling shoes to diagnose odor problems, collecting garbage, lifting bricks), it seems to mis-characterize the motivations people have in taking jobs requiring more intellectual abilities, thinking, or creativity. Professors, physicians, teachers, and engineers are but a few examples of professions that fulfill aspects of one\ufffds life that are not captured by the standard model. We aim to develop new frameworks, based on the psychology of labor, from which to understand the complex and important problem of compensation and its relationship to motivation, effort, and loyalty.</p>", "people": [], "title": "Psychology of Labor", "modified": "2016-12-05T00:17:21.172Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-314", "groups": ["erationality"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "psychology-of-labor"}, {"website": "", "description": "<p>The Pushpin Computing project demonstrates several concepts in distributed, embedded, and ubiquitous computing and dense sensor networks. Small pushpin-sized computers, each with two pins resembling thumbtacks, draw their power from a layered polyurethane/aluminum composite into which they are pushed. Communication between pushpins is local and neighbor-to-neighbor via IR. Current work with the Pushpin testbed includes distributed algorithm prototyping, building middleware to allow users to interface with the network, and integrating the Pushpins into a wider, more heterogeneous sensor network.</p>", "people": ["joep@media.mit.edu"], "title": "Pushpin Computing", "modified": "2016-12-05T00:17:21.246Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "pushpin-computing"}, {"website": "", "description": "<p>A single atom provides a superb single quantum bit. We are now building traps to contain and move single atoms using electrodynamic fields, so that computation can be done using narrow atomic resonances addressed by laser excitation.</p>", "people": ["ike@media.mit.edu"], "title": "Quantum Computation with Trapped Atoms", "modified": "2016-12-05T00:17:21.340Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-427", "groups": [], "published": true, "active": false, "end_on": "2008-01-01", "slug": "quantum-computation-with-trapped-atoms"}, {"website": "", "description": "<p>Bringing in knowledge from human discourse analysis and social cognition, we are developing autonomous agents that are capable of having a real-time face-to-face conversation with a human. These agents are human in form and communicate using both verbal and non-verbal modalities. Our current efforts focus on the integration of social and task dialogue, the ways in which people nonverbally signal topic shifts in their conversation, and studying the effectiveness of these agents on PDAs and compared to speech-only dialog systems.</p>", "people": ["bickmore@media.mit.edu"], "title": "Rea: The Conversational Humanoid", "modified": "2016-12-05T00:17:21.480Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-320", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "rea-the-conversational-humanoid"}, {"website": "", "description": "<p>ReachBand is a wireless wristband with an RFID reader that can detect user interactions with everyday objects, such as holding a book or opening a door. ReachBand enables the delivery of personalized services and just-in-time information to the user. It is used by various projects at the Lab, such as ReachMedia and Ambient Semantics. ReachBand v2 is being further developed, and will include updated electronics and communcation capabilities, as well as reduced dependence on a PC.</p>", "people": ["sajid@media.mit.edu", "pattie@media.mit.edu"], "title": "ReachBand: An RFID Wristband", "modified": "2016-12-05T00:17:21.497Z", "visibility": "PUBLIC", "start_on": "2005-09-01", "location": "Pond", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "reachband-an-rfid-wristband"}, {"website": "", "description": "<p>Read Out Loud is an application that empowers adults learning English to turn almost any reading material into an experience to help them learn. Learners can take a picture of a page of text; the app then scans in the page and presents the learner with a host of additional tools to facilitate reading. They can read the text aloud, which helps learners who are more comfortable with spoken English understand what is written. They can also select words to translate them into their native language. With this prototype, we want to give adult learners more agency to learn from material that focuses on subjects they care about, as well as increase access to English language learning material. Any book from the public library could become learning material with support in their native language. Read Out Loud is a prototype within the larger Making Learning Work project.</p>", "people": ["ps1@media.mit.edu", "jnazare@media.mit.edu"], "title": "Read Out Loud", "modified": "2016-12-05T00:17:21.549Z", "visibility": "PUBLIC", "start_on": "2014-09-01", "location": "--Choose Location", "groups": ["ml-learning"], "published": true, "active": false, "end_on": "2016-06-01", "slug": "read-out-loud"}, {"website": "", "description": "<p>People are surprisingly predictable. We use real-time video analysis to extract patterns of behavior from crowds browsing demos in our lab space.  We can discover meaningful locations and sequences just from observing how people interact in the space.  We can even begin to predict what people might do next.</p>", "people": ["dkroy@media.mit.edu"], "title": "Real-Time Behavior Analysis", "modified": "2016-12-05T00:17:21.612Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "real-time-behavior-analysis"}, {"website": "", "description": "<p>Cell-phone contact lists represent a very large peer-to-peer network. We are creating a cell-phone-based application that allows users to perform real-time searches on their local social network, against pieces of information that their contacts have provided about themselves. Our matchmaking agent uses Open Mind Common Sense to understand users' goals, and to expand logically on their queries.</p>", "people": ["lieber@media.mit.edu"], "title": "Real-Time Searches on a Local Social Network", "modified": "2016-12-05T00:17:21.642Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "real-time-searches-on-a-local-social-network"}, {"website": "", "description": "<p>The Reality Editor system supports editing the behavior and interfaces of so-called \"smart objects\": objects or devices that have an embedded processor and communication capability. Using augmented reality techniques, the Reality Editor maps graphical elements directly on top of the tangible interfaces found on physical objects, such as push buttons or knobs. The Reality Editor allows flexible reprogramming of the interfaces and behavior of the objects, as well as defining relationships between smart objects in order to easily create new functionalities. </p>", "people": ["hobinjk@media.mit.edu", "pattie@media.mit.edu", "heun@media.mit.edu"], "title": "Reality Editor: Programming Smarter Objects", "modified": "2016-12-05T00:17:21.670Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "reality-editor-programming-smarter-objects"}, {"website": "", "description": "<p>This research project is concerned with building computational models for the automatic recognition of affective expression in speech. We are in the process of completing an investigation of how acoustic parameters extracted from the speech waveform (related to voice quality, intonation, loudness and rhythm) can help disambiguate the affect of the speaker without knowledge of the textual component of the linguistic message. We have carried out a multi-corpus investigation, which includes data from actors and spontaneous speech in English, and evaluated the model's performance. In particular, we have shown that the model exhibits a speaker-dependent performance which reflects human evaluation of these particular data sets, and, held against human recognition benchmarks, the model begins to perform competitively.</p>", "people": ["picard@media.mit.edu"], "title": "Recognizing Affect in Speech", "modified": "2016-12-05T00:17:21.713Z", "visibility": "PUBLIC", "start_on": "1998-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2003-09-01", "slug": "recognizing-affect-in-speech"}, {"website": "", "description": "<p>Most of us go through the chore of capturing our knowledge in a very perfunctory manner through some form of documentation. This is usually our knowledge-in-action and is restricted to certain learned theories, principles, and rules for resolving problems. However, reflection-in-action, a much deeper phenomena, and one from which we can accentuate our knowing, remains relatively unexplored. This work proposes the creation of an application that aids in the process of reflection-in-action. It will facilitate the process of documentation by utilizing the rich computational tools to which many people now have access. It provides an organization structure and taxonomy around which to compile tacit knowledge and its representation, allowing for exploration of such knowledge in a richer fashion. This will aid in bringing forth the intrinsic \"ifs\" and \"thens\" as well as generating the potential for serendipitous learning experiences. All of this is very useful in bringing some form of rigor into the practice of reflective inquiry.</p>", "people": ["walter@media.mit.edu"], "title": "Reflective Practice Documentation", "modified": "2016-12-05T00:17:21.891Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-320", "groups": ["electronic-publishing"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "reflective-practice-documentation"}, {"website": "", "description": "<p>The device that is proposed is a low cost simplification on the industrial sinking electro-discharge machine.   What is proposed is to remove the controlling actuators (usually stepping motors or servomotors) altogether and instead use the electromotive force of the spark discharge itself to drive a simple solenoid coil connected to the spark discharge in series, pulling the sparking electrode out of contact with the machined material when a discharge occurs.  This cycle is then biased by gravity or another constant force (such as a spring), making the process self-sustaining and requiring very little external control circuitry to operate.  This device would be useful for imprecise uses of EDM (such as burning out broken taps) and due to the simplicity of design and fabrication, priced affordably.</p>", "people": ["neilg@media.mit.edu", "neri@media.mit.edu"], "title": "Reflex Electrical Discharge Machining", "modified": "2016-12-05T00:17:21.912Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2012-09-01", "slug": "reflex-electrical-discharge-machining"}, {"website": "", "description": "<p>Renga is a Web-based system that allows children around the world to  collaboratively tell a story in real time. Users with only e-mail access to  the Internet can also play through the Renga e-mail interface.\n</p>", "people": [], "title": "Renga: A Web-Based Collaborative Storytelling System", "modified": "2016-12-05T00:17:22.015Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "renga-a-web-based-collaborative-storytelling-system"}, {"website": "", "description": "<p>An intelligent robot must build an understanding of its environment with minimal human mediation. We investigate the characterization of objects in terms of their affordances\ufffdthe ways in which an agent can functionally interact with objects. To effect this goal, we represent objects as expectations with respect to sensorimotor schema\ufffdcollections of data that describe how an agent expects its interaction with the world to affect its perception of the world. We are constructing novel robotic manipulators that have enhanced abilities to actively touch their environment to develop these ideas. This work will contribute to our larger effort of grounding natural language in the physical environment.</p>", "people": ["dkroy@media.mit.edu"], "title": "Representing Affordances of Objects Through Active Touch", "modified": "2016-12-05T00:17:22.039Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-441", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "representing-affordances-of-objects-through-active-touch"}, {"website": "", "description": "<p>The members of electronic communities are often unrelated to each other; they may have never met and have no information on each other's reputation.  This kind of information is vital in electronic commerce interactions, where the potential counterpart's reputation can be a significant factor in the negotiation strategy. In this project we are inventing reputation mechanisms that rely on collaborative rating and personalized evaluation of the various ratings assigned to each user.  While these reputation mechanisms are developed in the context of electronic commerce, we believe that they may have applicability in other types of electronic communities such as chatrooms, newsgroups, mailing lists, etc.</p>", "people": ["pattie@media.mit.edu"], "title": "Reputation Mechanisms", "modified": "2016-12-05T00:17:22.066Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "", "groups": ["software-agents"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "reputation-mechanisms"}, {"website": "", "description": "<p>A prime goal of ubiquitous computing is the embedding of sensing, communication, and computation into everyday objects. One consequence of this paradigm is that perhaps nothing will be lost again, allowing us to gain rapid access to the objects we want. This is particularly important in the age of digital storage media, where information content is often not obviously expressed by the appearance of packaging. The mechanical constraints of CDs, DVDs, and MiniDVs, for example, means that their content is abstracted into or scrawled onto anonymous, homogeneous containers. This is even true of books. How many times have we stared at a shelf full of books in the library only to find what we are looking for half an hour later? We have designed an active tagging system that responds to a coded optical beam from several meters away. The tags contain a minimalist microprocessor that ambiently operates in shutdown mode and, upon detecting particular frequency components in the AM-modulated interrogation beam, awakens to decode the incident digital message and produce an appropriate response. The lack of linear amplifiers means that these tags draw under 0.5 \ufffdA when sleeping, hence can operate up to 10 years on a lithium coin cell. The interrogator is a \ufffdflashlight,\ufffd with which one scans an area; when the light beam hits a tag programmed with a code that matches that sent by the interrogator, an on-tag LED flashes, indicating that the desired object is \ufffdfound.\"</p>", "people": ["joep@media.mit.edu"], "title": "Responsive Tagging Based on Optically Triggered Microprocessor Wakeup", "modified": "2016-12-05T00:17:22.092Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-441", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "responsive-tagging-based-on-optically-triggered-microprocessor-wakeup"}, {"website": "", "description": "<p>In a given organization, thousands of pages of material are printed only to be read once and discarded. We are developing a desktop printer and reusable paper substrate capable of producing a high-quality, high-contrast image, which may subsequently be erased and rewritten multiple times without any expendable materials, such as toner or chemicals. Current related investigations include the development of color substrates and the development of electronically addressable, extremely high-resolution color wall-coverings, public message boards, whiteboards, and art canvases.</p>", "people": ["jacobson@media.mit.edu", "hamad@media.mit.edu"], "title": "Rewritable Substrates for Printing and Large-Scale Reflective Displays", "modified": "2016-12-05T00:17:22.120Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-015", "groups": ["molecular-machines"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "rewritable-substrates-for-printing-and-large-scale-reflective-displays"}, {"website": "", "description": "<p>Robocrop combines sensors, networking, and plants to create a modular desktop gardening system, in which gardeners can have as much information and control as they wish. We are developing a kit of parts to enable a deeper understanding of the variables that effect plant growth in an accessible way\ufffda MindStorms for nature.</p>", "people": ["mike@media.mit.edu"], "title": "Robocrop", "modified": "2016-12-05T00:17:22.178Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-068", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "robocrop"}, {"website": "", "description": "<p>Using a material expressively requires knowledge and practice. Digital media can be more demanding in this respect; unlike clay or paint, they often lack entry points with which artists may immediately begin creating. Expanding on the Cricket toolkit previously developed at the Media Lab for robotic constructions, we created a suite of tools and activities to introduce artists to robotic/electronic media, allowing them to design and create early in the learning process yet fully supporting further explorations in the future.</p>", "people": ["mres@media.mit.edu"], "title": "Robotic Art Studio", "modified": "2016-12-05T00:17:22.268Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-001", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2002-09-01", "slug": "robotic-art-studio"}, {"website": "", "description": "<p>The Robotic F.A.C.E. project explores the use of a physical object in the form of a face as a means of user interaction, taking advantage of socially intuitive facial expressions. As a testbed, we use an expressive robotic head, based on the mechanics of a commercial robotic toy that can convey socially strong, non-verbal facial cues to alert and notify. The head, which can be controlled via a powerful serial protocol, is capable of expressing most basic emotions not only statically, but also dynamically, as animation loops that vary some parameter, such as activity, over time. </p>", "people": ["geek@media.mit.edu", "stefanm@media.mit.edu"], "title": "Robotic F.A.C.E.: Facial Alerting in a Communication Environment", "modified": "2016-12-05T00:17:22.315Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-368", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "robotic-face-facial-alerting-in-a-communication-environment"}, {"website": "http://robotic.media.mit.edu/portfolio/cyberlearning/", "description": "<p>The language and literacy skills of young children entering school are highly predictive of their long-term academic success. Children from low-income families are particularly at risk. Parents often work multiple jobs, giving them less time to talk to and read with their children. Parents might be illiterate or not speak the language taught in local schools, and they may not have been read to as children, providing less experience of good co-reading practice to draw upon. We are currently developing a robotic reading companion for young children, trained by interactive demonstrations from parents and/or educational experts. We intend for this robot to complement parental interaction and emulate some of their best practices in co-reading, building language and literacy through asking comprehension questions, prompting exploration, and simply being emotionally involved in the child's reading experience.</p>", "people": ["cynthiab@media.mit.edu", "jakory@media.mit.edu", "samuelsp@media.mit.edu", "dnunez@media.mit.edu"], "title": "Robotic Learning Companions", "modified": "2017-06-07T16:36:47.771Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2016-06-01", "slug": "robotic-learning-companions"}, {"website": "", "description": "<p>We are investigating e-textiles and fiber-electronics to develop unique soft-architecture robotic components. We have been developing large area force sensors utilizing quantum tunneling composites integrated into textiles creating fabrics that can cover the body/surface of the robot and sense touch. By using e-textiles we shift from the metaphor of a sensing skin, often used in robotics, to one of sensing clothing. We incorporated apparel design and construction techniques to develop modular e-textile surfaces that can be easily attached to a robot and integrated into a robotic system. Adding new abilities to a robot system can become as simple as changing its clothes. Our goal is to study social touch interaction and communication between people and robots while exploring the benefits of textiles and the textile aesthetic.</p>", "people": ["cynthiab@media.mit.edu"], "title": "Robotic Textiles", "modified": "2016-12-05T00:17:22.383Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "robotic-textiles"}, {"website": "", "description": "<p>Roboverse is a physical artificial intelligence simulation. The environment supports rigid body physical simulation of wheeled and legged robots with default algorithms for handling the basic reactive control layers to move from one location to another by straight line, to observe objects in the local neighborhood of the robot's 2D position, to pick up, translate and rotate objects by using a servo interface. Higher level cognitive functions, such as speech and social problem solving were developed as part of Push Singh's cognitive architecture design, the Emotion Machine v1.0 (EM-1).</p>", "people": ["joep@media.mit.edu", "minsky@media.mit.edu"], "title": "Roboverse: Physical Artificial Intelligence Simulation", "modified": "2016-12-05T00:17:22.436Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-353", "groups": ["responsive-environments", "society-of-mind"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "roboverse-physical-artificial-intelligence-simulation"}, {"website": "", "description": "<p>SAGE is an authoring environment that allows children to create interactive  storytellers, embodied in stuffed animals, with which they can talk and listen.</p>", "people": [], "title": "SAGE Storyteller", "modified": "2016-12-05T00:17:22.539Z", "visibility": "PUBLIC", "start_on": "1994-12-31", "location": "", "groups": ["gesture-and-narrative-language"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "sage-storyteller"}, {"website": "", "description": "<p>The Sustainable Access in Rural India project (SARI) seeks to show that viable markets exist for information and communication services in poor rural areas by inventing and deploying innovative technologies, assessments, and business models. The ultimate goal is to link these activities to sustainable human development objectives. SARI is part of the Digital Nations consortium of the MIT Media Lab and Harvard's Center for International Development. Other key partners include IIT-Madras and the I-Gyan Foundation.</p>", "people": [], "title": "SARI: Sustainable Access in Rural India", "modified": "2016-12-05T00:17:22.640Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-391", "groups": ["edevelopment"], "published": true, "active": false, "end_on": "2002-12-30", "slug": "sari-sustainable-access-in-rural-india"}, {"website": "", "description": "<p>We demonstrate the design and implementation of a new versatile, scalable, and cost-effective sensate surface. The system is based on a new conductive inkjet technology, which allows capacitive sensor electrodes and different types of RF antennas to be cheaply printed onto a roll of flexible substrate that may be many meters long. By deploying this surface on (or under) a floor, it is possible to detect the presence and whereabouts of users through both passive and active capacitive coupling schemes. We have also incorporated GSM and NFC electromagnetic radiation sensing and piezoelectric pressure and vibration detection. We believe that this technology has the potential to change the way we think about covering large areas with sensors and associated electronic circuitry\ufffdnot just floors, but potentially desktops, walls, and beyond.</p>", "people": ["nanwei@media.mit.edu", "joep@media.mit.edu"], "title": "Scalable and Versatile Surface for Ubiquitous Sensing", "modified": "2016-12-05T00:17:22.665Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "scalable-and-versatile-surface-for-ubiquitous-sensing"}, {"website": "", "description": "<p>Current datebook programs such as Palm Desktop or Microsoft Outlook will give you a single audiovisual reminder a set period of time before your appointment is due. We are exploring the use of smell to convey information about upcoming events. Scent Reminder is an addition to Microsoft Outlook's Calendar program. When you make the appointment, it releases your chosen scent to help connect the stimulus with the event, and then releases the scent at the appropriate time to remind you. We have also started to explore gradually releasing more of a scent, or combinations of scents, over time.</p>", "people": ["mike@media.mit.edu"], "title": "Scent Reminder", "modified": "2016-12-05T00:17:22.709Z", "visibility": "PUBLIC", "start_on": "2000-12-31", "location": "E15-068", "groups": ["personal-information-architecture", "counter-intelligence"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "scent-reminder"}, {"website": "", "description": "<p>Should students be prosecuted for innovative projects? In December 2014, four undergraduates associated with the Media Lab were subpoenaed by the New Jersey Attorney General after winning a programming competition with a bitcoin-related proof of concept. We worked with MIT administration and the Electronic Frontier Foundation to support the students and establish legal support for informal innovation. In September 2015, MIT announced the creation of a new clinic for business and cyberlaw.</p>", "people": ["ethanz@media.mit.edu", "jnmatias@media.mit.edu"], "title": "Student Legal Services for Innovation", "modified": "2016-12-05T00:17:22.864Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2016-10-01", "slug": "student-legal-services-for-innovation"}, {"website": "", "description": "<p>Recent research in 3D user interfaces has pushed in two directions: immersive graphics and actuated tangible shape displays. We seek their hybrid by thinking about physical material density as a parameter in 3D rendering. We want to explore how digital models, handles, and controls can be rendered either as virtual 3D graphics or dynamic physical shapes, and move fluidly and quickly between these states, allowing physical affordances to be rendered only when needed. We were inspired by the different states of water: solid, gas, and liquid. We view digital computation and models as liquid, which can be vaporized into mid-air graphics, or solidified into dynamic physical shape. We also investigate transitions between solid and gas: sublimation and vaporization. To explore this, we have implemented a system which combines an actuated shape display and a spatial augmented reality display. This system can render physical shapes and volumetric graphics, co-located in the same space. We explore interaction techniques and motivating demonstration applications to explore 3D interaction between these boundaries. We also present results of a user study showing that freehand interaction with a physical shape display with co-located graphics outperforms direct interaction with only 3D graphics through a wand.</p>", "people": ["daniell@media.mit.edu", "olwal@media.mit.edu", "ishii@media.mit.edu"], "title": "Sublimate", "modified": "2016-12-05T00:17:22.885Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "sublimate"}, {"website": "", "description": "<p>When synthesizing ceramic powders for use in high-temperature superconductors, the bulk fraction of the synthesized powder that is actually superconductive is often low. In the specific case of YBaCuO 1-2-3 synthesis, the oxygen content of the sintered material is delicate (often destroyed by moisture) and critical to the observation of superconductivity above 77K (N2 boiling point). An apparatus is proposed that will preferentially filter out superconductive particles from non-superconductive particles from a finely ground powder (~100 um).  Filtered, superconductive material will then be sintered together (or drawn into a copper/brass carrying wire as is common with BSCCO) to yield a ceramic with higher bulk fraction superconductivity.  This apparatus would allow inexpensive superconductors to be fabricated with loose tolerances/purities on starter chemicals and firing apparatus.</p>", "people": ["neri@media.mit.edu"], "title": "Superconductive Powder Purification Device", "modified": "2016-12-05T00:17:22.902Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "", "groups": ["mediated-matter"], "published": true, "active": false, "end_on": "2012-09-01", "slug": "superconductive-powder-purification-device"}, {"website": "", "description": "<p>We present Tattio, a fabrication process that draws from current body decoration processes (i.e., jewelry such as metallic temporary tattoos) for the creation of on-skin technology. The fabrication process generates functional components such as Near Field Communication (NFC) tags, while maintaining the aesthetics and user experience of existing metallic temporary tattoos. The fabrication process is low-cost, accessible, and customizable; we seek to enable individuals to design, make, and wear their own skin technology creations.</p>", "people": ["cindykao@media.mit.edu", "geek@media.mit.edu"], "title": "Tattio", "modified": "2016-12-05T00:17:23.072Z", "visibility": "PUBLIC", "start_on": "2015-09-01", "location": "--Choose Location", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2016-03-01", "slug": "tattio"}, {"website": "", "description": "<p>Tangible CityScape is a platform for users to explore the real 3D cityscape by changing parameters such as population, building capacity, traffic, energy consumption, and shadow simulation for collaborative review of urban planning. By integrating a 2.5D actuated shape display, immersive 2D displays, 3D projection mapping, and handheld AR, CityScape combines the strength of bits (pixels) and atoms (tangibles) to represent city models at different scales and translate the tangible view onto a larger, underlying data set.</p>", "people": ["heibeck@media.mit.edu", "daniell@media.mit.edu", "ishii@media.mit.edu"], "title": "Tangible CityScape", "modified": "2016-12-05T00:17:23.096Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "tangible-cityscape"}, {"website": "", "description": "<p>This project is a physical realization of the self-organizing algorithms work started by Rob Poor with Hyphos. TephraNet is a self-organizing network of 100 nodes deployed in the Hawaii Volcanoes National Park to monitor endangered species of plants and animals. The project is a collaboration with the University of Hawaii, which is using the data collected for microclimatology research.</p>", "people": ["mike@media.mit.edu"], "title": "TephraNet: Self-Organizing Wireless Sensor Network", "modified": "2016-12-05T00:17:23.128Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "E15-469", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "2001-12-30", "slug": "tephranet-self-organizing-wireless-sensor-network"}, {"website": "", "description": "<p>Terra Incognita is a global news game and recommendation system. Terra Incognita helps you discover interesting news and personal connections to cities that you haven't read about. Whereas many recommendation systems connect you on the basis of \"similarity,\" Terra Incognita connects you to information on the basis of \"serendipity.\" Each time you open the application, Terra Incognita shows you a city that you have not yet read about and gives you options for reading about it. Chelyabinsk (Russia), Hiroshima (Japan), and Dhaka (Bangladesh) are a few of the places where you might end up.</p>", "people": ["dignazio@media.mit.edu", "ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "Terra Incognita: 1000 Cities of the World", "modified": "2016-12-05T00:17:23.202Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2014-06-01", "slug": "terra-incognita-1000-cities-of-the-world"}, {"website": "", "description": "<p>The Conductor's Jacket is a unique wearable device that measures physiological and gestural signals.  Together with  the Gesture Construction, a musical software system, it interprets these signals and applies them expressively in a  musical context.  Sixteen sensors have been incorporated into the Conductor's Jacket in such a way as to not  encumber or interfere with the gestures of a working orchestra conductor.  The Conductor's Jacket system gathers up to sixteen data channels reliably at rates of 3 kHz per channel, and also provides real-time graphical feedback.  Unlike many gesture-sensing systems it not only gathers positional and accelerational data but also senses muscle tension from several locations on each arm.  We will demonstrate the Gesture Construction, a musical software system that analyzes and performs music in real-time based  on the performer's gestures and breathing signals.  A bank of software filters extract several of the features that were found in the conductor study, including beat intensities and the alternation between arms.  These features are then used to generate real-time expressive effects by shaping the beats, tempos, articulations,  dynamics, and note lengths in a musical score.</p>", "people": ["picard@media.mit.edu", "tod@media.mit.edu"], "title": "The Conductor's Jacket", "modified": "2016-12-05T00:17:23.332Z", "visibility": "PUBLIC", "start_on": "1996-12-31", "location": "E15-384", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "the-conductors-jacket"}, {"website": "", "description": "<p>With more than six billion people and 15 billion products, the world economy is anything but simple. The Economic Complexity Observatory is an online tool that helps people explore this complexity by providing tools that can allow decision makers to understand the connections that exist between countries and the myriad of products they produce and/or export. The Economic Complexity Observatory puts at everyone's fingertips the latest analytical tools developed to visualize and quantify the productive structure of countries and their evolution.</p>", "people": ["hidalgo@media.mit.edu"], "title": "The Economic Complexity Observatory", "modified": "2016-12-05T00:17:23.398Z", "visibility": "PUBLIC", "start_on": "2010-09-01", "location": "--Choose Location", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2016-10-22", "slug": "the-economic-complexity-observatory"}, {"website": "", "description": "<p>This project compares automobile cockpits with aircraft cockpits. It also looks at some of the issues involved in drive-by-wire vs. fly-by-wire. Professor Hansman conducts research on information technologies applied to air and ground transportation, including vehicle operations, Air Traffic Control and safety. He has over 4800 hours of pilot in-command time in airplanes, helicopters and sailplanes, including meteorological, production, and engineering flight test experience.</p>", "people": [], "title": "The Coming Transition in Automobile Cockpits", "modified": "2016-12-05T00:17:23.589Z", "visibility": "LAB", "start_on": "2001-01-01", "location": "E15-383", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "the-coming-transition-in-automobile-cockpits"}, {"website": "", "description": "<p>In the constant cycling between centralized and distributed processing in which the centralized VAX gave way to the distributed PC now giving way to the virtually centralized YouTube, the notion of tiny networked processors is still a new idea. Imagine an orchestra of virtual instruments that are sufficiently sensitive to each other as to induce the excitement we get from seeing a live orchestra trying to play together. Now suppose the audience is also free to jump up on stage and get involved. The susceptible processors might be $100 laptops or acoustically enhanced mobiles, or they may be greeting cards that have brought you all to a party in which all the voices of \"Happy Birthday\" are heard only if everyone comes.</p>", "people": ["bv@media.mit.edu"], "title": "The Future of Collaborative Distributed Processing", "modified": "2016-12-05T00:17:23.613Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-484", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "the-future-of-collaborative-distributed-processing"}, {"website": "", "description": "<p>Most interactions between cultures require overcoming a language barrier, which is why multilingual speakers play an important role in facilitating such interactions. In addition, certain languages (not necessarily the most spoken ones) are more likely than others to serve as intermediary languages. We present the Language Group Network, a new approach for studying global networks using data generated by tens of millions of speakers from all over the world: a billion tweets, Wikipedia edits in all languages, and translations of two million printed books. Our network spans over eighty languages, and can be used to identify the most connected languages and the potential paths through which information diffuses from one culture to another. Applications include promotion of cultural interactions, prediction of trends, and marketing.</p>", "people": ["hidalgo@media.mit.edu"], "title": "The Language Group Network", "modified": "2016-12-05T00:17:23.662Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2016-10-22", "slug": "the-language-group-network"}, {"website": "", "description": "<p>This project explores how agency affects decision-making, spending, and risk-taking. We are working to determine if agents behave differently when acting on behalf of others.</p>", "people": [], "title": "The Money of Others: Agency, Risk-Taking, and Spending", "modified": "2016-12-05T00:17:23.683Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "--Choose Location", "groups": [], "published": true, "active": false, "end_on": "2009-09-01", "slug": "the-money-of-others-agency-risk-taking-and-spending"}, {"website": "", "description": "<p>Measuring development is difficult, and most of the widely accepted measures of progress, such as GDP per capita, or the U.N.\ufffds Human Development Index, are aggregates that fail to consider people\ufffds activities and achievements. Our civilization, however, has attained much more than providing services and purchasing power to an increasing population. Our history is punctuated by achievements, from the works of Leonardo da Vinci to the footsteps of Neil Armstrong. At the Observatory of Global Culture we are working on the creation of data, visualizations, and measures of cultural development with the goal of improving our understanding of the impact of culture. By producing these measures, we hope to highlight what our society has achieved and focus our social goals on concrete accomplishments that advance the development of human capacities, rather than more amorphous forms of aggregate growth.</p>", "people": ["hidalgo@media.mit.edu", "kzh@media.mit.edu", "amy_yu@media.mit.edu"], "title": "The Observatory of Global Culture", "modified": "2016-12-05T00:17:23.721Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["collective-learning"], "published": true, "active": false, "end_on": "2014-01-01", "slug": "the-observatory-of-global-culture"}, {"website": "", "description": "<p>PlaceMap constructs a new model for maps that increases our awareness of the world around us by exploring new theoretical frameworks for spatial applications called user centered mapping. Grounded in our understanding of real-world models of spatial decision-making and the failings of traditional mapping applications, user-centered mapping provides the tools for a new approach to mapping that mirrors the way we think about space. By establishing a computational model of place we can begin formulate understanding of how to design the next generation of spatial applications.</p>", "people": [], "title": "The PlaceMap Project and the Design of Spatial Applications", "modified": "2016-12-05T00:17:23.799Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-320", "groups": ["context-aware-computing"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "the-placemap-project-and-the-design-of-spatial-applications"}, {"website": "", "description": "<p>MRI and CAT-scan cameras gather 3-D data, but holography offers the only way of examining those images in fully 3-D form. This project explores the new image-processing, editing, and rendering tools that are needed to make these complex 3-D images quickly and accurately interpretable by physicians.</p>", "people": [], "title": "Three-Dimensional Capture and Virtual Cinematographer", "modified": "2016-12-05T00:17:24.032Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-441", "groups": [], "published": true, "active": false, "end_on": "2004-01-01", "slug": "three-dimensional-capture-and-virtual-cinematographer"}, {"website": "", "description": "<p>Throwing Light is an exploration into computational display systems that surpass the traditional rectangular screen. Using conventional, off-the-shelf video projectors and surveillance cameras, combined with robotic mounts and CG generated images, our system will throw a parallax-corrected projected image anywhere in an architectural space. The system will allow for a tight interaction between space, mobile viewers, and the output. While our initial applications are directed towards generating narratives that are linked to a specific site, there are many potential implications for human-computer interactions, such as augmented reality applications, mobile work-screens, and real-world agents.</p>", "people": ["csik@media.mit.edu"], "title": "Throwing Light", "modified": "2016-12-05T00:17:24.059Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-20C", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "throwing-light"}, {"website": "", "description": "<p>The Tower is a modular development system for designing and prototyping computational devices. Physically, the Tower consists of a primary foundation layer with a central processor. At this point, two foundations are available, equipped with either the PIC16F877 microcontroller from Microchip Inc.\ufffd, or the Rabbit\ufffd 2300 processor from Rabbit Semiconductor\ufffd. Additional modules can be added to the stack as needed by specific applications, and a special prototyping layer allows for simplified design of new modules for the system. Currently, layers are being produced that allow for sensor readings, servo-motor control, MIDI music playback, data collection, network connectivity, and RF and infrared communication. In addition to the currently available layers, we are actively designing a set of hardware and software development tools to make it easy for anyone to add their own layers to the system as an application's needs require.</p>", "people": [], "title": "The Tower: A Modular Electronics Design Environment", "modified": "2016-12-05T00:17:23.839Z", "visibility": "PUBLIC", "start_on": "2001-01-01", "location": "E15-344", "groups": ["personal-fabrication"], "published": true, "active": false, "end_on": "2004-01-01", "slug": "the-tower-a-modular-electronics-design-environment"}, {"website": "", "description": "<p>How can we make archives more accessible and our learning activities more salient, organic, and memorable? Storytelling is the key. In this project, we explore ways to encourage user annotation of archival video and common sense methods that use  these annotations to generate a \"storied\" search through the archival corpus. </p>", "people": ["gid@media.mit.edu"], "title": "Thinking Cultures", "modified": "2016-12-05T00:17:23.961Z", "visibility": "LAB", "start_on": "2006-09-01", "location": "E15-368", "groups": [], "published": true, "active": false, "end_on": "2006-09-01", "slug": "thinking-cultures"}, {"website": "", "description": "<p>This shape-sensing quilt consists of a communicating grid of textile tilt sensors.  It showcases the wide variety of techniques we have developed for constructing fabric-based sensors and circuits.</p>", "people": ["leah@media.mit.edu"], "title": "Tilt-Sensing Quilt", "modified": "2016-12-05T00:17:24.110Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "E15-368", "groups": ["high-low-tech"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "tilt-sensing-quilt"}, {"website": "", "description": "<p>Urban spaces have different uses and different histories for the people who move through and use them. Those layered histories can enrich our understanding of spaces and provide common ground for collaboration and understanding for people... but only if we can make them visible. Timenesia allows people to annotate the spaces they live in, recording their stories and memories and attaching them to locations in a neighborhood. When you encounter a Timenesia mark, you can explore it via calling a toll-free phone number, by scanning a QR code, or by taking a tour using Google Maps or a custom smart phone application. After exploring memories and meanings of a specific place, you can add your reflections and annotations to the conversation. The system is in use in Dorchester, MA to offer tours of the Field's Corner neighborhood. </p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "leob@media.mit.edu"], "title": "Timenesia", "modified": "2016-12-05T00:17:24.178Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "timenesia"}, {"website": "", "description": "<p>ToDoGo is a system for managing to-do lists on mobile devices. ToDoGo understands how to-do list entries relate to a user's everyday life activities, and can give them location-aware help in scheduling events and finding places to accomplish tasks.</p>", "people": ["lieber@media.mit.edu"], "title": "ToDoGo", "modified": "2016-12-05T00:17:24.283Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "todogo"}, {"website": "", "description": "<p>Computer games and learning environments increasingly involve humans relating to computational avatars and robots. Intelligence-modeling software requires real-time interaction between the parts, with channel capacities that must match the best of human-human performance. Many multi-modal activities challenge the real-time communication and comprehension speeds between participants. This project aims to enhance human-machine and machine-machine communication capacities between entities in order to encourage new models of interaction. </p>", "people": ["jmaloney@media.mit.edu", "bv@media.mit.edu"], "title": "Time-Critical Networks for Interaction Design", "modified": "2016-12-05T00:17:24.338Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-485A", "groups": ["music-mind-and-machine"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "time-critical-networks-for-interaction-design"}, {"website": "", "description": "<p>Our translation assistant applies common-sense logic to the problem of translating speech in real time from one language to another. Using speech recognition combined with a software translator to do word-by-word translation is not feasible because speech recognition is notorious for poor results. Word-by-word translation requires grammatically correct input to translate accurately. Therefore, translating speech that is potentially already fraught with errors is not expected to be good. Our translation assistant works around these problems by using the context of the conversation as a basis for translation; it takes the location and the speaker as input to establish the circumstances, and then uses a common-sense knowledge network to do topic-spotting using key words from the conversation. It only translates the most likely topics of conversation into the target language.  This system does not require perfect speech recognition, yet enables end-users to have a sense of the conversation.</p>", "people": ["lieber@media.mit.edu", "geek@media.mit.edu"], "title": "Topic-Spotting, Common-Sense Translation Assistant", "modified": "2016-12-05T00:17:24.439Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-384C", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "topic-spotting-common-sense-translation-assistant"}, {"website": "", "description": "<p>This project provides new ways to work with Topobo motion sequences. The Backpacks are a series of physical components with a knob that can be attached to a Topobo Active. The Backpacks will modify the way your recorded motion plays back. For instance, by turning the knob on the Bigger/Smaller (Amplitude) Backpack, you can make the motion larger or smaller. Other Backpacks include \"Time Delay,\" \"Faster-Slower,\" and \"Position Offset.\" The Position Offset Backpack has light sensors instead of a knob, allowing users to build creatures that can walk towards or away from light. The Backpacks introduce ideas about symbolic representation and feedback to Topobo.</p>", "people": ["ishii@media.mit.edu"], "title": "Topobo Backpacks", "modified": "2016-12-05T00:17:24.457Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-344", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2007-09-01", "slug": "topobo-backpacks"}, {"website": "", "description": "<p>TouchCounters are computational tags that track the use of physical objects. TouchCounters sense activity through magnetic, acceleration, and infrared sensors, and indicate their status on bright LED displays. Through magnetic snap connectors, TouchCounters can be networked to a Web server that generates use histograms for each object. Just as \"hit counters\" indicate frequency of use for Web pages, TouchCounters indicate usage frequency for physical objects.</p>", "people": ["ishii@media.mit.edu"], "title": "TouchCounters: Interactive Electronic Labels", "modified": "2016-12-05T00:17:24.490Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "E15-441", "groups": ["tangible-media"], "published": true, "active": false, "end_on": "2000-12-30", "slug": "touchcounters-interactive-electronic-labels"}, {"website": "", "description": "<p>This project attempts to address the practical problems involved with extracting behavioral information from large, multi-camera video corpora.  Ultra-dense video recordings offer new possibilities for in-depth, quantitative analysis of human behavior, with applications ranging from child development research to determining how people are affected by different retail environments.  Despite the growing sophistication of computer vision systems being developed for person tracking, gesture recognition, and object identification, these technologies remain error prone.  Accurate video annotation still requires substantial human input.  In order to analyze the hundreds of thousands of hours of video collected for the Human Speechome Project, we have developed a new software system for semi-automatically annotating longitudinal, multi-track video data.  This system combines computer vision algorithms with a novel interface design to enable human annotators to generate and edit video annotations with speed and accuracy.</p>", "people": ["dkroy@media.mit.edu", "decamp@media.mit.edu"], "title": "TrackMarks: Semi-Automatic Video Annotation", "modified": "2016-12-05T00:17:24.594Z", "visibility": "PUBLIC", "start_on": "2008-09-01", "location": "E15-441", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2010-09-01", "slug": "trackmarks-semi-automatic-video-annotation"}, {"website": "", "description": "<p>The Tribble project demonstrates the application of dense, distributed sensor networks to electronic skins. The Tribble is a one-foot diameter sphere tiled by a 32-node systolic sensor network, with each node capable of multi-modal sensing (pressure, light, sound, temperature) and actuation (glowing, purring). There is no central controller\ufffdall behavior evolves from communication between proximate patches of electronic skin. Current research on this platform is exploring algorithms for distributed sensor calibration.</p>", "people": ["joep@media.mit.edu"], "title": "Tribble: The Robotic Interactive Ball-Based Living Entity", "modified": "2016-12-05T00:17:24.658Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2005-01-01", "slug": "tribble-the-robotic-interactive-ball-based-living-entity"}, {"website": "", "description": "<p>Tricorder Net is a handheld sensor-network interface inspired by Star Trek's tricorder, a device that provided Science Officer Spock with information about the environment by simply pointing the device and twiddling its knobs. However, unlike Star Trek's tricorder, which had sensors embedded in the handheld device, Tricorder Net's intelligence comes from querying a sensor network distributed throughout the environment.</p>", "people": ["joep@media.mit.edu"], "title": "Tricorder Net: Mobile Browsing of Ubiquitous Sensor Network Information", "modified": "2016-12-05T00:17:24.741Z", "visibility": "PUBLIC", "start_on": "2006-09-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "tricorder-net-mobile-browsing-of-ubiquitous-sensor-network-information"}, {"website": "", "description": "<p>Trisk is a humanoid robot that integrates speech input, visual perception, and active touch in order to interact with humans and its environment. It can understand and obey natural language commands, and will soon be able to answer questions. The robot is a platform for designing new algorithms and multimodal knowledge representations for sensory-motor grounded language use. This research takes steps towards social robots that can coordinate activities with human partners using natural language and gesture.</p>", "people": ["tsourk@media.mit.edu", "dkroy@media.mit.edu", "soroush@media.mit.edu"], "title": "Trisk: A Conversational Robot", "modified": "2016-12-05T00:17:24.777Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-483", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2008-09-01", "slug": "trisk-a-conversational-robot"}, {"website": "", "description": "<p>Social Networking Services (SNS) such as MySpace and Facebook are increasing in popularity. They encourage and enable users to communicate with previously unknown people on an unprecedented scale. Now our increased social sphere is requiring us to potentially distinguish between legitimate and illegitimate strangers. Automatically rejecting 'unwanted' messages has long been the function of spam filtering; however, as we move from Viagra ads in email to Betty the saleswoman in SNS, identifying a nuisance requires placing value judgments. Nuisances take the shape of legitimate social humans and commercial entities, and sometimes both\ufffdwhat is Britney Spears?  We seek to redefine spam and how people are labeled more in the context of SNS by looking at users as prototypes.</p>", "people": ["judith@media.mit.edu"], "title": "Typecasting", "modified": "2016-12-05T00:17:24.818Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-390", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "typecasting"}, {"website": "", "description": "<p>Despite a vast body of knowledge about the importance of sleep, exercise, and healthy eating, our daily schedules are often planned around work and social events, not  healthy behaviors. We're prompted to plan&nbsp;throughout the day by devices and people, and we think about our schedules in terms of things to do; but sleep is rarely considered until we're tired and it's late. This project proposes a way that our everyday use of technology can help improve sleep habits and estimate mood.</p><p>A smartphone's unlock screen is an unobtrusive way of prompting user reflection throughout the day by posing \"microquestions\" as users unlock their phone. The questions are easily answered with a single swipe. Since we unlock our phones 50 to 200 times per day, microquestions can collect information with minimal intrusiveness to the user's daily life.&nbsp;</p><p>Can these swipe-questions help users mentally plan their day around wellbeing, and trigger healthier behaviors?</p>", "people": ["thariq@media.mit.edu", "picard@media.mit.edu", "sataylor@media.mit.edu"], "title": "Unlocking Wellbeing", "modified": "2018-05-06T22:38:11.891Z", "visibility": "LAB-INSIDERS", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["affective-computing", "advancing-wellbeing"], "published": true, "active": false, "end_on": "2018-01-01", "slug": "unlocking-sleep"}, {"website": "", "description": "<p>This project demonstrates how sensor networks can couple the real world with virtual worlds. A complete sensor network system (hardware and software) is being deployed for this purpose. Applications include interactively browsing large sensor data sets, expressing affect in a virtual social environment, and, in general, more closely coupling the real and virtual in order to enhance both.</p>", "people": ["joep@media.mit.edu"], "title": "Ubiquitous Sensor Networks and Virtual Worlds", "modified": "2016-12-05T00:17:24.864Z", "visibility": "PUBLIC", "start_on": "2006-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "ubiquitous-sensor-networks-and-virtual-worlds"}, {"website": "", "description": "<p>Although RFID technology is well established, there are some niches where RFID doesn't perform well (in the presence of metallic objects, or in applications that afford very limited surface area). We have developed an optical RFID tag, based on extrapolations of our \"FindIT Flashlight\" technology, that avoids these difficulties. Using our approach, a user searches for a particular tag by scanning the area with a flashlight, a common real-world activity extrapolated to the world of information. With minute power draw regulated via quasi-passive wakeup, the tags should last through the shelf-life of their small embedded battery. The tags will also have the ability to optically talk back to the reader, enabling two-way communication.</p>", "people": ["joep@media.mit.edu"], "title": "Ultra-Low Power Send/Receive Optical RFID Tag", "modified": "2016-12-05T00:17:24.933Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2006-01-01", "slug": "ultra-low-power-sendreceive-optical-rfid-tag"}, {"website": "", "description": "<p>We are exploring the potential of low-cost, networked sensing and actuation technologies to improve urban agriculture, promote public health, and foster deep connections between people and the natural environment.</p>", "people": [], "title": "Urban Spaces and Structures", "modified": "2016-12-05T00:17:25.086Z", "visibility": "PUBLIC", "start_on": "2007-09-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2008-01-01", "slug": "urban-spaces-and-structures"}, {"website": "", "description": "<p>Current voice-recognition software relies on statistical techniques to determine which words a user has said. This project attempts to leverage the semantic context of what the user has said previously to improve future predictions. We are using OMCSNet, a semantic network created from the Open Mind Common Sense knowledge base, to disambiguate phonetically similar words and improve overall recognition accuracy.</p>", "people": ["lieber@media.mit.edu"], "title": "Using Common-Sense Reasoning to Improve Voice Recognition", "modified": "2016-12-05T00:17:25.107Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-383", "groups": ["software-agents"], "published": true, "active": false, "end_on": "2006-09-01", "slug": "using-common-sense-reasoning-to-improve-voice-recognition"}, {"website": "", "description": "<p>Ventus is an attempt to address the problem of Medical Compliance.  Asthma medication can be dangerous if used improperly.  By tracking when it is  being used, Ventus aims to help asthma sufferers everywhere improve their health and self-awareness.</p>", "people": ["mike@media.mit.edu"], "title": "Ventus: The Intelligent Inhaler", "modified": "2016-12-05T00:17:25.216Z", "visibility": "PUBLIC", "start_on": "1998-12-31", "location": "", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "ventus-the-intelligent-inhaler"}, {"website": "", "description": "<p>Mobile devices with cameras have enough processing power to do simple machine-vision tasks, and we are exploring how this capability can enable new user interfaces to applications. Examples include dialing someone by pointing the camera at the person's photograph, or using the camera as an input to allow navigating virtual spaces larger than the device's screen.</p>", "people": ["vmb@media.mit.edu"], "title": "Vision-Based Interfaces for Mobile Devices", "modified": "2016-12-05T00:17:25.506Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "Garden Conference Room", "groups": ["object-based-media"], "published": true, "active": false, "end_on": "2012-01-01", "slug": "vision-based-interfaces-for-mobile-devices"}, {"website": "", "description": "<p>Visiphone is a communication sculpture that visualizes the sounds flowing between two distant locations. A continuous audio connection between the two placesallows the inhabitants to talk informally and easily while Visiphone's graphics express the dynamics of the conversation, providing visual feedback that one's voice has carried sufficiently and indicating the presence of those on the other end. The rendering is evocative rather than technical; our goal in building the Visiphone has been to create an aesthetic object that enhances sociable awareness.   \n</p>", "people": ["judith@media.mit.edu"], "title": "VisiPhone", "modified": "2016-12-05T00:17:25.550Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["sociable-media"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "visiphone"}, {"website": "", "description": "<p>Tired of waiting for public transportation to arrive?  Wait Loss will tell  you the expected arrival time of your favorite bus or train so you can be  more productive. \n</p>", "people": ["mike@media.mit.edu"], "title": "Wait Loss", "modified": "2016-12-05T00:17:25.958Z", "visibility": "PUBLIC", "start_on": "1997-12-31", "location": "", "groups": ["personal-information-architecture"], "published": true, "active": false, "end_on": "1998-12-30", "slug": "wait-loss"}, {"website": "", "description": "<p>In order to utilize digitized speech (a telephone message, recording of a meeting, lecture, etc.) more effectively as a computer-data-type, it is important to understand speech structure and allow rapid scanning of the audio data. Structure is derived from pauses, speaker changes and turn-taking, emphasis, and intonational cues. Presentation may be serial, including audio time-compression, or parallel, using both ears to listen to multiple audio streams simultaneously.</p>", "people": ["geek@media.mit.edu"], "title": "Voice as Data", "modified": "2016-12-05T00:17:25.686Z", "visibility": "PUBLIC", "start_on": "1999-12-31", "location": "", "groups": ["living-mobile"], "published": true, "active": false, "end_on": "1999-12-30", "slug": "voice-as-data"}, {"website": "", "description": "<p>VoIP Drupal is an innovative framework that brings the power of voice and Internet-telephony to Drupal sites. It can be used to build hybrid applications that combine regular touchtone phones, web, SMS, Twitter, IM, and other communication tools in a variety of ways, facilitating community outreach and providing an online presence to those who are illiterate or do not have regular access to computers. VoIP Drupal will change the way you interact with Drupal, your phone, and the web.</p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "leob@media.mit.edu"], "title": "VoIP Drupal", "modified": "2016-12-05T00:17:25.721Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "voip-drupal"}, {"website": "", "description": "<p>Vojo.co is a hosted mobile blogging platform that makes it easy for people to share content to the web from mobile phones via voice calls, SMS, or MMS. Our goal is to make it easier for people in low-income communities to participate in the digital public sphere. You don't need a smart phone or an app to post blog entries or digital stories to Vojo; any phone will do. You don't even need Internet access: Vojo lets you create an account via SMS and start posting right away. Vojo is powered by the VozMob Drupal Distribution, a customized version of the popular free and open-source content management system that is being developed through an ongoing codesign process by day laborers, household workers, and a diverse team from the Institute of Popular Education of Southern California (IDEPSCA).</p><p><a href=\"http://vojo.co\">vojo.co</a><br></p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu", "elplatt@media.mit.edu"], "title": "Vojo.co", "modified": "2016-12-05T00:17:25.771Z", "visibility": "PUBLIC", "start_on": "2012-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "vojoco"}, {"website": "", "description": "<p>The VozMob Drupal Distribution is Drupal customized as a mobile blogging platform. VozMob has been designed to make it easy to post content to the web from mobile phones via voice calls, SMS, or MMS. You don't need a smart phone or an app to post blog entries\ufffdany phone will do. VozMob allows civic journalists in low-income communities to participate in the digital public sphere. Features include groups, tags, geocoding and maps, MMS filters, and new user registration via SMS. Site editors can send multimedia content out to registered users' mobile phones. VozMob Drupal Distribution is developed through an ongoing codesign process by day laborers, household workers, and students from the Institute of Popular Education of Southern California (IDEPSCA.org). The project received early support from the Annenberg School for Communication and Journalism at the University of Southern California, Macarthur/HASTAC, Nokia, and others. </p>", "people": ["ethanz@media.mit.edu", "rahulb@media.mit.edu"], "title": "VozMob", "modified": "2016-12-05T00:17:25.844Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "vozmob"}, {"website": "", "description": "<p>The Vocal Augmentation and Manipulation Prosthesis (VAMP) is a gesture-based, wearable controller for live-time vocal performance. This controller allows a singer to capture and manipulate single notes that she sings, using a gestural vocabulary developed from that of choral conducting. By drawing from a familiar gestural vocabulary, this controller and the associated mappings can be more intuitive to both performer and audience. This instrument was inspired by the character of Nicholas in Death and the Powers.</p>", "people": ["ejessop@media.mit.edu", "tod@media.mit.edu"], "title": "Vocal Augmentation and Manipulation Prosthesis (VAMP)", "modified": "2016-12-05T00:17:25.874Z", "visibility": "PUBLIC", "start_on": "2009-01-01", "location": "--Choose Location", "groups": ["opera-of-the-future"], "published": true, "active": false, "end_on": "2010-01-01", "slug": "vocal-augmentation-and-manipulation-prosthesis-vamp"}, {"website": "", "description": "<p>YOUR URGE TO BREATHE IS A LIE (a direct quote from my free-diving instructor) is a progressive approach towards innovation, human response to ecological change, and the transhumanist project.&nbsp;&nbsp;</p><ol><li>Embrace change: don't build walls to hold the future back; adapt and dive right in;</li><li>Re-model the model species: to truly evolve, model the future of the human body-mind after the cephalopod;</li><li>Embrace the knowledge and abilities embedded in ancient rituals and global traditions of living in, on, under the water;</li><li>Train the human to biological limits of capacity and capability, and then add on technology; and</li><li>Embrace desire.</li></ol><p>YOUR URGE TO BREATHE IS A LIE is a manifesto, a call to action, an invitation to become the future of the human, streaming in new ways towards each other and the sea.<br></p>", "people": ["simun@media.mit.edu"], "title": "YOUR URGE TO BREATHE IS A LIE", "modified": "2018-10-23T15:18:15.909Z", "visibility": "PUBLIC", "start_on": "2017-10-11", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2018-08-31", "slug": "your-urge-to-breathe-is-a-lie"}, {"website": "", "description": "<p>The UbER-Badge is a small, wearable-computing platform for experimentation with distributed systems, and the analysis and enhancement of group social interaction. The UbER-Badge is equipped with multiple communication channels, including an RF channel for broadcast communication, and an IR channel for more intimate dialogues. It is also equipped with a large LED matrix display, vibratory feedback, audio input and output, and data memory. It is powered by three in-system reprogrammable microprocessors. It is designed to be easily augmented by other technologies to enable a wide span of research throughout the Media Lab and the sponsor community. It was used in October 2004 at the TTT sponsor event, providing a suite of different applications.</p>", "people": ["joep@media.mit.edu"], "title": "Wearable Badge for Distributed Systems with Multiple Channels of Communication", "modified": "2016-12-05T00:17:26.188Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2005-09-01", "slug": "wearable-badge-for-distributed-systems-with-multiple-channels-of-communication"}, {"website": "", "description": "<p>This project explores the idea that some \"intelligence\" is encoded in the habits that people assume in daily life. Adopting someone else's habits might allow you to break out of a personal rut, glean some success tactics from someone you admire, or to empathize with someone you care about. This is a wearable system with a Google Calendar backend that actively alerts users to perform a habit based on the events of their calendar.</p>", "people": ["pattie@media.mit.edu"], "title": "Wear Someone Else's Habits", "modified": "2016-12-05T00:17:26.036Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["fluid-interfaces"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "wear-someone-elses-habits"}, {"website": "", "description": "<p>The societal problem of physical abuse persists in part because of isolation and concealment. Emerging technologies have been adapted by abusers for methods of control and by victims for methods of resistance. This project examines the intimate position that wearable technologies have with our bodies and explores the design of a wearable computer system that could record and document physical forces to the body in an effort to quantify physical abuse. This apparel or smart clothing system utilizes large area, fabric-based pressure sensors to categorize and measure the intensity and patterns of forces to the wearer\ufffds body. As textile-based user interfaces find their way into clothing, the opportunity for computers to identify physical abuse will become apparent. This work will function both as a proof of concept and as a surveying stake to demonstrate a possible field of future investigation.</p>", "people": ["csik@media.mit.edu"], "title": "Wearable as Witness", "modified": "2016-12-05T00:17:26.064Z", "visibility": "PUBLIC", "start_on": "2007-01-01", "location": "E15-001", "groups": ["computing-culture"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "wearable-as-witness"}, {"website": "", "description": "<p>We have created a system for data collection, annotation, and feedback that is part of a longer-term research interest to gather data to understand more about stress and the physiological signals involved in its expression. First, we built a wearable apparatus for gathering data that allows the user to include as many accurate labels (annotations) as possible while going about natural daily activities. Gathering annotations is disruptive and likely to increase stress (thus interfering with the signals being measured). We hypothesized that empathetic ways of interrupting would be less stressful than non-empathetic and found significant effects on many of user's self-reported items such as preference for the more empathetic system, and also on behavioral items, such as the estimated number of times they were interrupted (significantly lower when system was more empathetic.)  </p>", "people": ["picard@media.mit.edu"], "title": "Wearable Relational Devices for Stress Monitoring", "modified": "2016-12-05T00:17:26.245Z", "visibility": "PUBLIC", "start_on": "2003-01-01", "location": "E15-001", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2004-09-01", "slug": "wearable-relational-devices-for-stress-monitoring"}, {"website": "", "description": "<p>This project is a system of compact, wearable, wireless sensor nodes, equipped with full six-degree-of-freedom inertial measurement units and node-to-node capacitive proximity sensing. A high-bandwidth, channel-shared RF protocol has been developed to acquire data from many (e.g., 25) of these sensors at 100 Hz full-state update rates, and software is being developed to fuse this data into a compact set of descriptive parameters in real time. A base station and central computer clock the network and process received data. We aim to capture and analyze the physical movements of multiple people in real time, using unobtrusive sensors worn on the body. Applications abound in biomotion analysis, sports medicine, health monitoring, interactive exercise, immersive gaming, and interactive dance ensemble performance.</p>", "people": ["mtl@media.mit.edu", "joep@media.mit.edu"], "title": "Wearable, Wireless Sensor System for Sports Medicine and Interactive Media", "modified": "2016-12-05T00:17:26.277Z", "visibility": "PUBLIC", "start_on": "2005-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "wearable-wireless-sensor-system-for-sports-medicine-and-interactive-media"}, {"website": "", "description": "<p>We are automating recognition of positive/negative experiences (valence) and affect from facial expressions. We present a toolkit, Acume, for interpreting and visualizing facial expressions whilst people interact with products and/or concepts.</p>", "people": ["picard@media.mit.edu"], "title": "What Do Facial Expressions Mean?", "modified": "2016-12-05T00:17:26.380Z", "visibility": "PUBLIC", "start_on": "2009-09-01", "location": "", "groups": ["affective-computing"], "published": true, "active": false, "end_on": "2011-01-01", "slug": "what-do-facial-expressions-mean"}, {"website": "", "description": "<p>Many Twitter users post questions with hashtags like #twoogle or #lazyweb. Is this an effective way to find expert answers beyond one's network, or are questions mostly answered by friends?</p><p>Rzeszotarski, J. M., Spiro, E. S., Matias, J. N., Monroy-Hern\u00e1ndez, A., &amp; Morris, M. R. (2014, April).<b><a href=\"https://arxiv.org/abs/1507.01311\"> Is anyone out there?: unpacking Q&amp;A hashtags on twitter.</a></b> In Proceedings of the 32nd annual ACM conference on Human factors in computing systems (pp. 2755-2758). ACM.<br></p>", "people": ["ethanz@media.mit.edu", "jnmatias@media.mit.edu"], "title": "Who Answers Questions on Twitter?", "modified": "2016-12-05T00:17:26.454Z", "visibility": "PUBLIC", "start_on": "2013-01-01", "location": "--Choose Location", "groups": ["civic-media"], "published": true, "active": false, "end_on": "2013-09-01", "slug": "who-answers-questions-on-twitter"}, {"website": "", "description": "<p>We are developing a system for wireless capture of gesture, useful, for example, when interacting with a wearable display or other mobile computing environment where a keyboard isn't appropriate.  We are putting small RFID-queried passive sensors on the fingertips of a user, interrogated by a reader mounted near a sleeve, for example.  This allows the fingers to be free while enabling simple gesture, such as finger curl and touch, to be reliably detected.  We are developing hardware to show a working demonstration of this concept, together with a gestural vocabulary appropriate to this interface.</p>", "people": ["joep@media.mit.edu"], "title": "Wireless Finger Gesture Capture with Passive Tag Sensing", "modified": "2016-12-05T00:17:26.539Z", "visibility": "PUBLIC", "start_on": "2010-01-01", "location": "", "groups": [], "published": true, "active": false, "end_on": "2011-01-01", "slug": "wireless-finger-gesture-capture-with-passive-tag-sensing"}, {"website": "", "description": "<p>Motion and gait analyses have a number of applications in the medical field. For example, change in gait over extended time is used in neurological exams to diagnose dementias. Changes in gait over short periods of time may be indicative of a number of conditions, such as the onset of a stroke and progression of congestive heart failure. Continuous monitoring of gait could be useful for physical therapy and rehabilitation. For instance, patients who have undergone knee replacement therapy are advised to place less weight on the corresponding foot; feedback when excessive weight is applied could help prevent re-injury. Gait feedback could also be used for athletic analysis, both to provide complex gait information to elite athletes and to provide real-time exercise feedback and incentives to everyday athletes. Precise gait analysis is currently done in a motion lab; however, these large labs are expensive to maintain and costly to use. We are developing a wireless wearable system that is capable of making many inexpensive measurements. These measurements may have less precision, but may provide more information by collecting data throughout the day in the patient's home environment. Our system will be optimized for gait analysis and for detecting medically relevant gait characteristics.</p>", "people": ["joep@media.mit.edu"], "title": "Wireless Wearable System for Gait Evaluation", "modified": "2016-12-05T00:17:26.572Z", "visibility": "PUBLIC", "start_on": "2000-01-01", "location": "E15-344", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2003-01-01", "slug": "wireless-wearable-system-for-gait-evaluation"}, {"website": "", "description": "<p>Muscles are biological actuators with unique properties compared to traditional mechanical actuators. The ability to simultaneously modify stiffness while providing power as well as the potential for self-repair make their use desirable when considering the design of robotic systems. Improving our understanding of how muscles work as actuators, struts, and springs is essential for both robotic design and for understanding animal locomotion. Unfortunately, our understanding of the energetics of muscles is incomplete, and traditional methods for studying muscles have inherent limitations. Our group has developed a novel apparatus that allows us to test muscles in unique ways by tethering the muscle to a movable platform coupled to a computer-simulated load. With this setup, we can vary the parameters of the \"virtual load\" to address more complex and relevant questions, and examine the dynamic interactions between muscles.</p>", "people": ["eswart@media.mit.edu", "hherr@media.mit.edu"], "title": "Workloop Energetics of Muscles and Interactive Loads", "modified": "2016-12-05T00:17:26.609Z", "visibility": "PUBLIC", "start_on": "2008-01-01", "location": "E15-054", "groups": ["biomechatronics"], "published": true, "active": false, "end_on": "2009-09-01", "slug": "workloop-energetics-of-muscles-and-interactive-loads"}, {"website": "", "description": "<p>While many wearable sensors have been developed, few are actually worn by people on a regular basis. WristQue is a wristband sensor that is comfortable and customizable to encourage widespread adoption. The hardware is 3D printable, giving users a choice of materials and colors. Internally, the wristband will include a main board with microprocessor, standard sensors, and localization/wireless communication, and an additional expansion board that can be replaced to customize functionality of the device for a wide variety of applications. Environmental sensors (temperature, humidity, light) combined with fine-grained indoor localization will enable smarter building infrastructure, allowing HVAC and lighting systems to optimize to the locations and ways that people are actually using the space. Users' preferences can be input through buttons on the wristband. Fine-grained localization also opens up possibilities for larger applications, such as visualizing building usage through DoppelLab and smart displays that react to users' presence.</p>", "people": ["bmayton@media.mit.edu", "joep@media.mit.edu"], "title": "WristQue: A Personal Wristband for Sensing and Smart Infrastructure", "modified": "2016-12-05T00:17:26.640Z", "visibility": "PUBLIC", "start_on": "2011-01-01", "location": "--Choose Location", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2013-01-01", "slug": "wristque-a-personal-wristband-for-sensing-and-smart-infrastructure"}, {"website": "", "description": "<p>Many people don't finish high school. We are studying the context, student population, and technical resources of an urban, community-based adult education program to understand the enormous challenges involved in delivering education there. We are introducing a computer-based curriculum to see how it might meet the social and contextual challenges to teaching and learning in this environment. We are particularly interested in seeing if both existing speech technology (such as speech dictation software) and novel technology we will design and introduce can improve students' writing performance. Through multiple design cycles, we aim to create a new piece of software that helps novice adult writers understand and use a writing process\ufffdmany for the first time. Part of this software will involve the capture and use of recorded talk, which the student can use as a direct resource in generating text. Our measurable goal is to improve the quality of writing in adult education.</p>", "people": [], "title": "Writing Technology in Adult Education", "modified": "2016-12-05T00:17:26.667Z", "visibility": "PUBLIC", "start_on": "2002-01-01", "location": "E15-312", "groups": [], "published": true, "active": false, "end_on": "2003-01-01", "slug": "writing-technology-in-adult-education"}, {"website": "", "description": "<p>The Boston Youth and Community Connections Program examined ways of using new technologies to help break down the social and cultural barriers that have historically existed between various communities in the Boston area. We engaged a diverse group of young people age 12 and up, from qualitatively different communities, in a series of face-to-face and online activities.  This project was a collaborative effort with the Center for Reflective Community Practice in the MIT Department of Urban Studies and Planning.</p>", "people": ["mres@media.mit.edu"], "title": "Youth and Community Connections", "modified": "2016-12-05T00:17:26.722Z", "visibility": "PUBLIC", "start_on": "1999-01-01", "location": "E15-120B", "groups": ["lifelong-kindergarten"], "published": true, "active": false, "end_on": "2000-09-01", "slug": "youth-and-community-connections"}, {"website": "", "description": "<p>The Zero Car is a testing platform and chassis for the Wheel Robots project. Zero Car is a customizable and modular platform that houses the energy source (batteries), control system, and mechanical connections to smart wheel assemblies. The Zero Car will also be the future testing platform for our virtual towing simulation. Zero Car comes in multiple sizes: bicycle, tricycle, 4-wheels, and mini. The mini-Zero tests four omni-directional wheels attached to a chassis and wireless RC controller.</p>", "people": ["ypod@media.mit.edu", "rchin@media.mit.edu"], "title": "Zero Car", "modified": "2016-12-05T00:17:26.770Z", "visibility": "PUBLIC", "start_on": "2004-01-01", "location": "E15-001", "groups": ["smart-cities"], "published": true, "active": false, "end_on": "2009-01-01", "slug": "zero-car"}, {"website": "", "description": "<p>\u00a1ONWARDS + UNDER! proposes that as sea levels rise, particularly in highly populated coastal cities, the current plan to keep nature at bay and build a wall against the sea is untenable (and that walls against perceived threats not only do not work, but are dangerous metaphors to deploy).&nbsp;&nbsp;\u00a1ONWARDS + UNDER! proposes that we embrace a changing climate and a rising sea level, and respond by adapting our bodies, abilities and lifestyles to live more intimately in, on and under the sea. To understand what this might look like, I dove into cultures and histories of humans living intimately with oceans, including the physical and ecological practices diving women of Japan (Ama) and Korea (Haenyo); the pearl divers of the Arabian/Iranian Gulf (and tin particular the music they use to organize their oceanic labors); the Moken people (sometimes called \u201csea-gypsies\u201d or \u201cwater-people\u201d) a buddhist Austronesian people that are trying to maintain a semi-nomadic hunter-gatherer lifestyle based almost exclusively on the sea amidst changing maritime and immigration regulations; the \u201cAquatic Ape\u201d hypothesis which first became popularized as a feminist critique of dominant evolutionary theory and proposes that a crucial step in the evolution of homo sapiens involved a period of semi-aquatic lifestyle in what is now South Africa (the ape first stood up in water); and the recently invented sport/meditation culture of free-diving (which borrows techniques developed by the US military). This research is a mix of first and secondary sources: historical, interviews, site visits, and a personal embodied physical practice (\u201cyour urge to breathe is a lie\u201d is a direct quotation from my free-diving instructor) - all research has been filmed and/or audio recorded. What can we learn from the knowledge and practices developed by indigenous groups, extreme sports sub-cultures, and alt-evolutionary theorists in order to imagine a human future amidst ecological crisis?<br></p>", "people": ["simun@media.mit.edu"], "title": "\u00a1Onwards+Under!", "modified": "2018-10-23T15:18:31.439Z", "visibility": "PUBLIC", "start_on": "2017-10-11", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2018-08-31", "slug": "onwards"}, {"website": "", "description": "<p>This project aims to create an agent that\ufffdusing common-sense reasoning\ufffdpredicts tasks users may be interested in adding to their to-do lists, based on the context of appointments that they are adding to their calendars. For instance, if the user makes the calendar appointment \"go fishing,\" the agent may recommend the task \"pack fishing equipment.\" By automatically generating potential tasks, the agent saves the users time, and assists them in remembering actions that they may commonly forget.</p>", "people": ["lieber@media.mit.edu"], "title": " Anticipating User Tasks Using Common-Sense Reasoning", "modified": "2016-10-17T18:56:37.860Z", "visibility": "PUBLIC", "start_on": "2004-09-01", "location": "E15-383", "groups": ["software-agents"], "published": false, "active": false, "end_on": "2006-09-01", "slug": "anticipating-user-tasks-using-common-sense-reasoning"}, {"website": "", "description": "\u200b<h2>A transhumanist project towards co-evolution, rooted in desire</h2><p>I want to become a cephalopod&nbsp;is a trans-humanist proposal for using mostly the octopus, but also cuttlefish and squid, as a model species for the future of the human. This research began as a response to the ideology dominating the promises of genetic modification: the scouring of earthly genotypes in order to augment and \"improve\" human capabilities, or solve human \"problems.\" This positions other species and their DNA as yet another resource to be mined, extracted and manipulated, a sort of \"shopping\" or \"collage\" project for biology. In our work, we ask \"what if?\": What if the future of biology was rather an encounter with a species, as-is, and perhaps on its own terms? What if the term \"model species\" meant not that the species was for humans of high utilitarian use, easy to produce, maintain, practice science upon\u2014what if \"model\" meant something more like the \"mode\" or \"role model\"? What if I wanted to abandon the oncoming future of becoming a computer-aided AI robot, and become a cephalopod instead? What if the role of desire and fantasy in the discourse and practice of innovation was made explicit, even foregrounded? And what if the epic project of transhumanism focused first on training as a technology, rather than immediately moving to material, product-based intervention? What if we were to start&nbsp;<i>Training Transhumanism</i>&nbsp;(informed by the practices of the communities studied in&nbsp;&nbsp;\u00a1ONWARDS + UNDER!)?</p><p>To limit our thinking of the future of the human to its current form is limiting, and rooted in traditionalism. The first stages of becoming a cephalopod&nbsp;focuses on three main attributes:&nbsp;</p><ol><li>Camouflage. To practice camouflage is to be a shape-shifter, constantly aware of one\u2019s hyper-local environment, and ready in response. It is a practice of fluidity, flexibility, impermanent identity. I will create a training program for myself in order to develop these sensitivities and capabilities for the human, pulling from somatic practices, attention training, flexibility training (for both body and mind), ego relinquishment (the consistent internal representation of who we are). This will involve as few props as possible. A demonstration, and research interviews towards developing the practice, will be filmed.</li><li>The Decentralized Brain. The octopus has 3/5 of its neurons in her arms, and her arms can make decisions independent of the main brain (sometimes she has to watch her different arms to know what they are up to), while sharing a single intention \u201cset\u201d by the main brain. But how can a human decentralize her brain? She is a very centralized organism\u2014head, on top of spinal cord, two appendages on either side, and so on\u2026 Perhaps it takes more than one person to become a cephalopod. Perhaps the question is not how does one human decentralize her brain, but how do two humans become a single organism (at least for some period of time)? How do we push beyond negotiation, beyond collaboration, to a place where two humans can share a single intention, and work as two arms in achieving it? And how might one train humans to be more sensitive and better adept in such relations? A training regimen is under development.</li><li>Embodied knowledge, and relation to the world. Cephalopods are basically giant tongues. They relate to the world through touch, chemically sensing what they encounter. Although octopuses in particular have excellent eyesight, what if the future of the human relied more on physical contact and chemical sensing (through our nose and mouth) in order to interact, understand, and act upon the world? How do we develop and extend our abilities to physically, tactically, and chemically sense the world?</li></ol>", "people": ["simun@media.mit.edu"], "title": "I want to become a cephalopod", "modified": "2018-10-23T15:18:45.248Z", "visibility": "PUBLIC", "start_on": "2016-10-04", "location": "", "groups": ["responsive-environments"], "published": true, "active": false, "end_on": "2018-08-31", "slug": "i-want-to-become-a-cephalopod"}, {"website": "", "description": "", "people": [], "title": "Worldbug: A Procedurally Generated Mobile Adventure", "modified": "2016-10-21T18:47:09.521Z", "visibility": "PUBLIC", "start_on": "2015-11-15", "location": "", "groups": [], "published": false, "active": false, "end_on": "2016-04-20", "slug": "Worldbug"}, {"website": "", "description": "", "people": [], "title": "Worldbug: A Procedurally Generated Mobile Adventure", "modified": "2016-10-21T20:06:57.400Z", "visibility": "PUBLIC", "start_on": "2015-10-01", "location": "", "groups": [], "published": false, "active": false, "end_on": "2016-04-01", "slug": "worldbug"}, {"website": "", "description": "<p>Invisible Ink is a certified mail application that demonstrates the utility of the blockchain for maintaining a public ledger of transactions while keeping the content of those transactions private. In this case, the idea is a method for guaranteeing the delivery, receipt and existence of email messages. It archives the transaction in the Bitcoin blockchain and uses secure off-chain storage for the other details. Invisible Ink demonstrates the extensibility of this distributed technology for contracts, audits, and recovery of sensitive information. It is an evolution of work begun as the Ethos project. Since Bitcoin has shown that a distributed system of trust can be workable for irreversibly storing time-stamped information, these extensions and applications are potentially important for a wide variety of cases from finance to personal information.</p>", "people": ["amirl@media.mit.edu", "lip@media.mit.edu"], "title": "Invisible Ink", "modified": "2017-06-25T16:35:11.855Z", "visibility": "PUBLIC", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "invisible-ink"}, {"website": "", "description": "<p>A physical interface designed for simultaneous social interaction with visual material. We built a hemispherical, multi-person, interactive touch display that allows a small group of people in the same place or in equivalently equipped ones to jointly interact on the same surface. We created an application that runs on this platform and presents a selection of visual media and offers recommendations for common viewing.</p>", "people": ["amirl@media.mit.edu", "lip@media.mit.edu", "novysan@media.mit.edu", "vmb@media.mit.edu"], "title": "Crystal Ball", "modified": "2017-06-25T00:47:05.431Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["viral-communications", "object-based-media", "ultimate-media"], "published": true, "active": false, "end_on": "2015-01-01", "slug": "crystal-ball"}, {"website": "", "description": "<p>Content and curriculum lie at the heart of every educational system, learning environment, and learning technology. Yet the infrastructure to support this main pillar is severely lacking, and largely consists of a scattered set of elements: research on learning constructs, various curricula and syllabi, loosely tied together by jurisdiction-based standards documents (i.e. the Common Core State Standards, Finland\u2019s education standards, etc.). These elements evolved before the digital era, based on different constituent's needs. However, the disjointedness of these structures leave the management of knowledge on learning constructs\u2014and the design/access of learning tools and technologies for these learning constructs\u2014in a messy, challenging state, unable to successfully carry us into the 21st century and the future of learning technologies and educational systems.</p><p>The LearningGraph&nbsp;is a research initiative that uses systems engineering practices to reengineer this infrastructure, using learning sciences frameworks and modern data modeling and ontological design. The goal of the project is to create a unified data model that supports the multiple stakeholders who work with learning constructs (teachers, learners, learning and assessment designers, learning scientists, educational technologist, informal learning facilitators, etc.), and provide an ongoing \"living sandbox\" where this knowledge is modeled and used to support teaching and learning across learning environments, technologies, and experiences.<br></p>", "people": ["jgroff@media.mit.edu"], "title": "The LearningGraph", "modified": "2018-06-20T20:00:43.152Z", "visibility": "PUBLIC", "start_on": "2016-10-20", "location": "", "groups": [], "published": true, "active": false, "end_on": "2018-05-01", "slug": "learning-genome"}, {"website": "", "description": "", "people": ["cynthiab@media.mit.edu", "ndepalma@media.mit.edu"], "title": "Human-Robot Teamwork in Urban Search and Rescue", "modified": "2017-10-16T02:24:39.676Z", "visibility": "PUBLIC", "start_on": "2011-09-01", "location": "", "groups": ["personal-robots"], "published": false, "active": false, "end_on": "2013-02-27", "slug": "human-robot-teamwork"}, {"website": "", "description": "<p>Ethos is a decentralized, Bitcoin-like network for storing and sharing valuable information. We provide transparency, control, and ownership over personal data and its distribution. Validation and maintenance is distributed throughout the data community and automatically maintained without needing a safe deposit box or a commercial site. What Bitcoin has done for currency and BitTorrent for media, Ethos does for personal data. Nodes in the network are incentivized by collecting transaction fees, coinbase transactions (\"finding blocks\"), and proof-of-storage fees to sustain the distribution of personal data. Fees are paid with the underlying cryptocurrency represented by the network, also known as \"PrivacyCoin.\" The role of nodes, besides the usual proof-of-work, which protects against \"double spending,\" is to maintain shredded pieces of information and present them to the network on-demand. </p>", "people": ["amirl@media.mit.edu", "lip@media.mit.edu", "guyzys@media.mit.edu"], "title": "Ethos", "modified": "2017-06-25T16:32:52.708Z", "visibility": "PUBLIC", "start_on": "2014-01-01", "location": "--Choose Location", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "ethos"}, {"website": "", "description": "<p><i>Conversational agents and intelligent toys are present in children's homes and influence their development. The introduction of these devices in the home raises questions about how children's development and their interactions with their peers and family may be impacted. Most of the current intelligent agents (connected toys, conversational devices, robots, computers) can only be placed along an animate-inanimate continuum (Keil 1991; Van Duuren &amp; Scaife, 1995). As these devices become more human-like in form or function, they are being attributed more social and moral characteristics (Kahn 2012).&nbsp;</i></p><p><i>This raises the question of family engagement and interventions in children's interaction with connected toys and intelligent agents (Druga 2017, Mcreynolds 2017). In order to further explore this I am designing a platform where children could program and customize embodied intelligent agents together with their family, and especially with their grand-parents. The agents will play an active role in prompting and guiding the users through the learning activities. This novel paradigm of interaction aims to explore new potential learning pathways and outcomes from having a reflective conversation with an artifact in the making.&nbsp;</i></p><br><p>&nbsp;</p>", "people": [], "title": "Cognimates: collaborative creative learning with embodied intelligent agents", "modified": "2018-10-24T01:05:35.065Z", "visibility": "PUBLIC", "start_on": "2017-08-01", "location": "", "groups": ["personal-robots"], "published": true, "active": false, "end_on": "2018-08-31", "slug": "cognimate"}, {"website": "", "description": "<p>An interface for smashing filter bubbles,&nbsp;<a href=\"http://panorama.um-dokku.media.mit.edu/\">Panorama</a> is built to allow open, transparent, and collaborative exploration of news from all across the political map. It presents different perspectives and encourages serendipity in news exploration, versus getting all of our news from one single source.&nbsp;Panorama is a human-in-the-loop interface.&nbsp;The computer processes more than 10,000 news stories each day, both broadcast and written, and it uses machine learning algorithms to decide  what topics each story is talking about and if the stories are positive, subjective, or trending.&nbsp;The machine learning process pours over massive datasets and learns to generalize in smart ways, but not in the same smart ways that humans generalize. As a result, it can be brilliant and also get very confused. With Panorama, some of the training data was a large open set of movie reviews, and while this is a great dataset to start with, it is not mapped so well to news stories.&nbsp;As humans interact with Panorama, they are encouraged to give better labels to stories; those labels are fed back into the algorithm to make it better.</p><p>Having a lot of information about each news story and all stories together allows us to create an open-box news aggregator. With most aggregators we use today (like the Facebook News feed), the user has no idea what are the algorithms and filters that decide what s/he will see. Panorama is open: the user can decide to view everything, or filter only to specific things that he s/he is interested in, by playing with the sliders and seeing in real time how the news feed changes accordingly.&nbsp;For example, you could easily get all stories about animals, from the right side of the political map, that are also positive and objective.&nbsp;Panorama also exposes interesting patterns, such as the topics that different news sources focus on every day, and what sources had many objective versus subjective stories.</p>", "people": ["lip@media.mit.edu", "jasrub@media.mit.edu"], "title": "Panorama", "modified": "2018-10-17T23:55:32.429Z", "visibility": "PUBLIC", "start_on": "2016-10-01", "location": "", "groups": ["viral-communications"], "published": true, "active": false, "end_on": "2018-06-06", "slug": "panorama"}, {"website": "", "description": "<p>One future of media experience lies within a socially connected virtual world. VR started almost 30 years ago and is now wearable, real-time, integrated with sensing, and becoming transparent. Sphera realizes a socially driven 360-degree media space that includes ambient scenery, visual exploration, and integration with friends. By combining an Oculus Rift (VR heads-on display), Microsoft Kinect (depth sensor), and a natural voice command interface, we created a socially connected, 360-degree immersive virtual world for media exploration and selection, as well as big-data manipulation and visualization.</p>", "people": ["amirl@media.mit.edu", "lip@media.mit.edu"], "title": "Sphera", "modified": "2017-06-25T00:48:42.463Z", "visibility": "PUBLIC", "start_on": "2013-09-01", "location": "--Choose Location", "groups": ["viral-communications", "ultimate-media"], "published": true, "active": false, "end_on": "2015-09-01", "slug": "sphera"}, {"website": "", "description": "<p>Segregation, or income inequality, is one of the major problems of our society. Residential segregation has long been of research interest in fields such as sociology, economics, and psychology. But our behaviors are also segregated. In this project we want to visualize how human behaviors\u2014like conversations in Twitter, mobility around the cities, or purchasing\u2014show patterns of segregation. Our objective is to allow people to understand the segregation of behaviors in their cities to increase the awareness of that problem, but also to show how we can address potential solutions by using different layers of big datasets.</p>", "people": ["eaman@media.mit.edu", "alotaibi@media.mit.edu", "alfredom@media.mit.edu", "sandy@media.mit.edu", "xdong@media.mit.edu"], "title": "Visualizing patterns of segregation", "modified": "2017-04-05T18:53:49.527Z", "visibility": "PUBLIC", "start_on": "2017-03-27", "location": "", "groups": ["human-dynamics"], "published": true, "active": false, "end_on": "2017-06-30", "slug": "visualizing-patterns-of-segregation"}, {"website": "http://web.media.mit.edu/~barmak/", "description": "<p>We use terahertz transmission through sedimentary rock samples to assess the macro and micro porosity. We exploit the notable water absorption in terahertz spectrum to interact with the pores that are two orders of magnitude smaller than the terahertz wavelength. Terahertz water sensitivity provides us with the dehydration profile of the rock samples. The results show that there is a correlation between such a profile and distribution of micro to macro porosity of the rock. The study further estimates the absolute value of total porosity based on diffusion theory. We compare our results with mercury injection capillary pressure as a benchmark to confirm our analytic framework. This porosimetry method can set a foundation for a more affordable, less invasive porosimetry that can be used in geological studies and in other industries without the need for hazardous mercury or ionizing radiation. \n                    \n                </p>", "people": ["barmak@media.mit.edu"], "title": "Rock porosity measurement using optical scattering", "modified": "2017-04-03T20:57:41.280Z", "visibility": "PUBLIC", "start_on": "2016-07-06", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2017-04-06", "slug": "rock-porosity-measurement-using-optical-scattering"}, {"website": "", "description": "<p>This is the 3rd concept exhibition of imaginarium of technology which explores the future with advancement of batteries and how battery technology may be improved. \n                    \n                </p>", "people": ["barmak@media.mit.edu"], "title": "Future of batteries and future with better batteries", "modified": "2017-04-03T20:58:19.940Z", "visibility": "PUBLIC", "start_on": "2016-09-09", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2017-04-28", "slug": "future-of-batteries-and-future-with-better-batteries"}, {"website": "http://imtspace.com/index.php", "description": "<p>This exciting concept exhibition which is the 4th concept exhibition of the imaginarium of technology series will explore the practical possibilities for the future of artificial intelligence. &nbsp;Check it out at E14-274</p>", "people": ["barmak@media.mit.edu"], "title": "A practical look at the future of Artificial Intelligence- iMT 4th Concept exhibition", "modified": "2017-04-03T20:58:05.877Z", "visibility": "PUBLIC", "start_on": "2017-04-05", "location": "", "groups": ["camera-culture"], "published": true, "active": false, "end_on": "2017-05-31", "slug": "a-practical-look-at-the-future-of-artificial-intelligence-concept-exhibition"}, {"website": "", "description": "<p><b>Smart Communal Spaces </b>is a project from MIT Media Lab Dubai Workshop 2016, which was deployed and filmed at the Dubai Museum of the Future. It explores how Mixed Reality could enhance team communication in shared office spaces by looking into the interplay of Slack and HoloLens. While existing co-working spaces provide the benefits of openness and flexibility, they sacrifice our privacy and personal experience. However, Mixed Reality enables us to project holograms into our physical reality, visually and auditorily reorganizing how bits and atoms exist around us. In the demo video, we try to imagine and tell a story of how human-computer interactions might look like in an office when team communication tools are operated in a spatial context without any streaming device. We broke down the most important elements in the office space by researching the core functions of a team communication software and several holographic applications.\n                    \n                </p><p><b>Design Lead:</b> Chrisoula Kapelonis,&nbsp;</p><p><b>Technology Lead:</b> Poseidon Hai-Chi Ho&nbsp;</p><p><b>Students:&nbsp;</b>Rajeev Mylapalli, Yazan Fanous, Lamees Alhashimi, Moza Al Naimi, Esra'a Alsanie, Asalah Aranki</p><p><b>Special Thanks: </b>Joichi Ito, Noah Raford, Nick DuPey, Ashley Shaffer<br>MIT Media Lab, IDEO, Wamda, Dubai Museum of the Future</p>", "people": ["kapeloni@media.mit.edu", "oi7@media.mit.edu"], "title": "Smart Communal Spaces", "modified": "2018-05-04T10:51:42.799Z", "visibility": "PUBLIC", "start_on": "2016-08-29", "location": "", "groups": [], "published": true, "active": false, "end_on": "2017-08-29", "slug": "smart-communal-spaces"}, {"website": "", "description": "<p>Can we enable social connectivity between astronauts and people on Earth through an embodied agent?</p><p>Astronauts actively communicate with their families on Earth through several forms of digital and voice communication, including phone calls, video conferencing, and email. However, as astronaut Scott Kelly describes in the <i>Time</i> documentary <i>A Year in Space</i>, the experience can be incredibly isolating despite these affordances. Shortcomings of these modes of communication lie in their inability to translate emotion effectively, failure to facilitate shared experiences, lack of physical feedback, and the resulting perceived lack of control. The psychological effects of these limitations can become heightened over time, and peak during moments when the family on Earth is in need of support. As space becomes more accessible, it is important to consider how we design for social connectivity between people on Earth and in space.</p><p>What if embodied social agents, besides being the astronaut's personal sidekick, could help to facilitate a more connected experience between space and Earth? From C3PO in <i>Star Wars</i> to Rosie the Maid in <i>The Jetsons</i>, the idea of robots in space has been well explored in fiction universe. On Earth, embodied social agents have been shown to be effective in providing companionship, relieving stress and anxiety, and fostering connection among people. In this project to send an&nbsp;embodied social agent into zero gravity, we explore several key themes relating to the potential for this technology to offer better connection and shared experience between astronauts and people on Earth.</p><p>While in zero gravity, the embodied social agent interacts with people on cognitive, creative, and social tasks with varying degrees of proactive behavior. We collect physiological, audio, and video data of the experience as individuals complete a series of tasks with the agent with the goal of designing agents that can enable us to be more socially connected.</p>", "people": ["igrover@media.mit.edu", "nikhita@media.mit.edu", "pcuellar@media.mit.edu", "haewon@media.mit.edu"], "title": "Social Robots in Space: Initial Explorations", "modified": "2017-12-05T19:04:57.085Z", "visibility": "PUBLIC", "start_on": "2017-10-01", "location": "", "groups": ["space-exploration", "personal-robots"], "published": true, "active": false, "end_on": "2018-12-31", "slug": "social-robots-in-zero-gravity-scenarios"}, {"website": "http://oi7.me", "description": "<p>Mission Wildlife is a research collaboration between San&nbsp;<span style=\"font-size: 18px; font-weight: 400;\">Diego Zoo Global and the MIT Center for Civic Media to&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">explore the potential for interactive technologies in&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">conservation education. In particular, we used&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">augmented reality (AR) to focus visitors\u2019 attention&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">towards survival threats to endangered species. The&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">project was deployed during the 100th anniversary&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">year (2016) of the San Diego Zoo. Visitors competed&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">against each other to trigger 3D animations from&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">animal signage in the zoo and shared results on social&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">media to spread awareness about conservation issues.&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">This case study demonstrates how AR can be tied to&nbsp;</span><span style=\"font-size: 18px; font-weight: 400;\">efforts to expand awareness of social issues.</span></p>", "people": ["ethanz@media.mit.edu", "oi7@media.mit.edu", "gabem@media.mit.edu"], "title": "Mission Wildlife", "modified": "2018-05-04T10:49:30.350Z", "visibility": "PUBLIC", "start_on": "2016-06-20", "location": "", "groups": [], "published": true, "active": false, "end_on": "2017-06-20", "slug": "mission-wildlife"}, {"website": "", "description": "<p>This project aims to map and analyze the publicly knowable social connections of various communities, allowing us to gain unprecedented insights about the social dynamics in such communities. Most analyses of this sort map online social networks, such as Twitter, Facebook, or LinkedIn. While these networks encode important aspects of our lives (e.g., our professional connections) they fail to capture many real-world relationships. Most of these relationships are, in fact, public and known to the community members. By mapping this publicly knowable graph, we get a unique view of the community that allows us to gain deeper understanding of its social dynamics. To this end, we built a web-based tool that is simple, easy to use, and allows the community to map itself. Our goal is to deploy this tool in communities of different sizes, including the Media Lab community and the Spanish town of Jun.</p>", "people": ["dkroy@media.mit.edu", "msaveski@media.mit.edu", "echu@media.mit.edu", "soroush@media.mit.edu"], "title": "Human Atlas", "modified": "2017-04-11T21:40:23.904Z", "visibility": "LAB-INSIDERS", "start_on": "2015-01-01", "location": "--Choose Location", "groups": ["social-machines"], "published": true, "active": false, "end_on": "2016-12-31", "slug": "human-atlas"}]